2025-09-05 20:09:09,638 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict BatchEmbedder, {'type': 'BatchEmbedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('embedder', {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': None, '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://api.openai.com/v1', 'chat_completion_parser': <function get_first_message_content at 0x000001AF3C58AAC0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-3-small', 'dimensions': 256, 'encoding_format': 'float', 'input': ['import { render } from \'@testing-library/react\';\nimport { IntlProvider } from \'react-intl\';\n\nimport { TagAssignmentRow } from \'./TagAssignmentRow\';\n\ndescribe(\'TagAssignmentRow\', () => {\n  it(\'should throw an error if more than 3 children are passed\', () => {\n    const children = Array(4)\n      .fill(null)\n      .map((_, i) => <div key={i} />);\n\n    const renderComponent = () =>\n      render(\n        <IntlProvider locale="en">\n          <TagAssignmentRow>{children}</TagAssignmentRow>\n        </IntlProvider>,\n      );\n\n    expect(renderComponent).toThrow(\'TagAssignmentRow must have 3 children or less\');\n  });\n\n  it(\'should render children\', () => {\n    const children = Array(3)\n      .fill(null)\n      .map((_, i) => <div key={i} />);\n\n    const { container } = render(\n      <IntlProvider locale="en">\n        <TagAssignmentRow>{children}</TagAssignmentRow>\n      </IntlProvider>,\n    );\n\n    expect(container).toMatchSnapshot();\n  });\n});\n', "import invariant from 'invariant';\nimport React from 'react';\n\nimport { useDesignSystemTheme } from '@databricks/design-system';\n\nexport function TagAssignmentRow({ children }: { children: React.ReactNode }) {\n  const { theme } = useDesignSystemTheme();\n\n  const stableChildren = React.Children.toArray(children);\n  invariant(stableChildren.length <= 3, 'TagAssignmentRow must have 3 children or less');\n\n  const parsedChildren = Array(3)\n    .fill(null)\n    .map((_, i) => stableChildren[i] ?? <span key={i} style={{ width: theme.general.heightSm }} />); // Sync width with only icon button width\n\n  return (\n    <div css={{ display: 'grid', gridTemplateColumns: '1fr 1fr min-content', gap: theme.spacing.sm }}>\n      {parsedChildren}\n    </div>\n  );\n}\n", '// Do not modify this file\n\nimport type { ControllerProps, FieldValues, Path } from \'react-hook-form\';\nimport { Controller } from \'react-hook-form\';\n\nimport { TagAssignmentInput } from \'./TagAssignmentField/TagAssignmentInput\';\nimport { useTagAssignmentContext } from \'../context/TagAssignmentContextProvider\';\n\ninterface TagAssignmentValueProps<T extends FieldValues> {\n  rules?: ControllerProps<T>[\'rules\'];\n  index: number;\n  render?: ControllerProps<T>[\'render\'];\n}\n\nexport function TagAssignmentValue<T extends FieldValues>({ rules, index, render }: TagAssignmentValueProps<T>) {\n  const { name, valueProperty } = useTagAssignmentContext<T>();\n\n  return (\n    <Controller\n      rules={rules}\n      name={`${name}.${index}.${valueProperty}` as Path<T>}\n      render={({ field, fieldState, formState }) => {\n        if (render) {\n          return render({ field, fieldState, formState });\n        }\n\n        return (\n          <TagAssignmentInput\n            componentId="TagAssignmentValue.Default.Input"\n            errorMessage={fieldState.error?.message}\n            {...field}\n          />\n        );\n      }}\n    />\n  );\n}\n', 'import { forwardRef } from \'react\';\n\nimport type { InputProps, InputRef } from \'@databricks/design-system\';\nimport { FormUI, Input } from \'@databricks/design-system\';\n\ninterface TagAssignmentInputProps extends InputProps {\n  errorMessage?: string;\n}\n\nexport const TagAssignmentInput: React.ForwardRefExoticComponent<\n  TagAssignmentInputProps & React.RefAttributes<InputRef>\n> = forwardRef<InputRef, TagAssignmentInputProps>(({ errorMessage, ...otherProps }: TagAssignmentInputProps, ref) => {\n  return (\n    <div css={{ flex: 1 }}>\n      <Input validationState={errorMessage ? \'error\' : \'info\'} {...otherProps} ref={ref} />\n      {errorMessage && <FormUI.Message message={errorMessage} type="error" />}\n    </div>\n  );\n});\n', 'import { render, screen } from \'@testing-library/react\';\nimport userEvent from \'@testing-library/user-event\';\n\nimport { TagAssignmentRemoveButtonUI } from \'./TagAssignmentRemoveButtonUI\';\n\ndescribe(\'TagAssignmentRemoveButtonUI\', () => {\n  it(\'should render a button\', async () => {\n    const handleClick = jest.fn();\n    render(<TagAssignmentRemoveButtonUI componentId="test" onClick={handleClick} />);\n\n    const button = screen.getByRole(\'button\');\n    await userEvent.click(button);\n\n    expect(handleClick).toHaveBeenCalledTimes(1);\n  });\n});\n', "import type { ButtonProps } from '@databricks/design-system';\nimport { Button, TrashIcon } from '@databricks/design-system';\n\nexport function TagAssignmentRemoveButtonUI(props: Omit<ButtonProps, 'icon'>) {\n  return <Button icon={<TrashIcon />} {...props} />;\n}\n", "import { render, screen } from '@testing-library/react';\n\nimport { TagAssignmentRowContainer } from './TagAssignmentRowContainer';\n\ndescribe('TagAssignmentRowContainer', () => {\n  it('should render children', () => {\n    render(\n      <TagAssignmentRowContainer>\n        <div>child</div>\n      </TagAssignmentRowContainer>,\n    );\n\n    expect(screen.getByText('child')).toBeInTheDocument();\n  });\n});\n", "import { useDesignSystemTheme } from '@databricks/design-system';\n\nexport function TagAssignmentRowContainer({ children }: { children: React.ReactNode }) {\n  const { theme } = useDesignSystemTheme();\n  return <div css={{ display: 'flex', flexDirection: 'column', gap: theme.spacing.sm }}>{children}</div>;\n}\n", "import invariant from 'invariant';\nimport { createContext, useContext } from 'react';\nimport type { FieldValues, ArrayPath, FieldArray } from 'react-hook-form';\n\nimport type { UseTagAssignmentFormReturn } from '../hooks/useTagAssignmentForm';\n\nexport const TagAssignmentContext = createContext<UseTagAssignmentFormReturn | null>(null);\n\nexport function TagAssignmentContextProvider<\n  T extends FieldValues = FieldValues,\n  K extends ArrayPath<T> = ArrayPath<T>,\n  V extends FieldArray<T, K> = FieldArray<T, K>,\n>({ children, ...props }: { children: React.ReactNode } & UseTagAssignmentFormReturn<T, K, V>) {\n  return <TagAssignmentContext.Provider value={props as any}>{children}</TagAssignmentContext.Provider>;\n}\n\nexport function useTagAssignmentContext<\n  T extends FieldValues = FieldValues,\n  K extends ArrayPath<T> = ArrayPath<T>,\n  V extends FieldArray<T, K> = FieldArray<T, K>,\n>() {\n  const context = useContext(TagAssignmentContext as React.Context<UseTagAssignmentFormReturn<T, K, V> | null>);\n  invariant(context, 'useTagAssignmentContext must be used within a TagAssignmentRoot');\n  return context;\n}\n", 'import { renderHook } from \'@testing-library/react\';\nimport { useForm, FormProvider } from \'react-hook-form\';\nimport { IntlProvider } from \'react-intl\';\n\nimport { useTagAssignmentFieldArray } from \'./useTagAssignmentFieldArray\';\n\nconst DefaultWrapper = ({ children }: { children: React.ReactNode }) => {\n  return <IntlProvider locale="en">{children}</IntlProvider>;\n};\n\ndescribe(\'useTagAssignmentFieldArray\', () => {\n  it(\'should use passed form as prop\', () => {\n    const { result: formResult } = renderHook(() =>\n      useForm<{ input: string; tags: { key: string; value: string }[] }>({\n        defaultValues: { input: \'test_input\', tags: [{ key: \'key1\', value: \'value1\' }] },\n      }),\n    );\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentFieldArray({\n          name: \'tags\',\n          emptyValue: { key: \'\', value: \'\' },\n          form: formResult.current,\n          keyProperty: \'key\',\n        }),\n      { wrapper: DefaultWrapper },\n    );\n    result.current.appendIfPossible({ key: \'foo\', value: \'bar\' }, {});\n\n    const values = result.current.form.getValues();\n    expect(values.tags).toStrictEqual([\n      { key: \'key1\', value: \'value1\' },\n      { key: \'foo\', value: \'bar\' },\n    ]);\n    expect(values.input).toBe(\'test_input\');\n  });\n\n  it(\'should use context form if no form prop is passed\', () => {\n    const Wrapper = ({ children }: { children: React.ReactNode }) => {\n      const methods = useForm<{ input: string; tags: { key: string; value: string }[] }>({\n        defaultValues: { input: \'test_input\', tags: [{ key: \'key1\', value: \'value1\' }] },\n      });\n      return (\n        <IntlProvider locale="en">\n         ', 'expect(values.input).toBe(\'test_input\');\n  });\n\n  it(\'should use context form if no form prop is passed\', () => {\n    const Wrapper = ({ children }: { children: React.ReactNode }) => {\n      const methods = useForm<{ input: string; tags: { key: string; value: string }[] }>({\n        defaultValues: { input: \'test_input\', tags: [{ key: \'key1\', value: \'value1\' }] },\n      });\n      return (\n        <IntlProvider locale="en">\n          c<FormProvider {...methods}>{children}</FormProvider>\n        </IntlProvider>\n      );\n    };\n\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentFieldArray({\n          name: \'tags\',\n          emptyValue: { key: \'\', value: \'\' },\n          keyProperty: \'key\',\n        }),\n      { wrapper: Wrapper },\n    );\n    result.current.appendIfPossible({ key: \'foo\', value: \'bar\' }, {});\n\n    const values = result.current.form.getValues();\n    expect(values[\'tags\']).toStrictEqual([\n      { key: \'key1\', value: \'value1\' },\n      { key: \'foo\', value: \'bar\' },\n    ]);\n    expect(values[\'input\']).toBe(\'test_input\');\n  });\n\n  it(\'should throw an error if no form is passed and not in a form context\', () => {\n    expect(() =>\n      renderHook(\n        () =>\n          useTagAssignmentFieldArray({\n            name: \'tags\',\n            emptyValue: { key: \'\', value: undefined },\n            keyProperty: \'key\',\n ', "]);\n    expect(values['input']).toBe('test_input');\n  });\n\n  it('should throw an error if no form is passed and not in a form context', () => {\n    expect(() =>\n      renderHook(\n        () =>\n          useTagAssignmentFieldArray({\n            name: 'tags',\n            emptyValue: { key: '', value: undefined },\n            keyProperty: 'key',\n          }),\n        { wrapper: DefaultWrapper },\n      ),\n    ).toThrow('Nest your component on a FormProvider or pass a form prop');\n  });\n\n  it('should not add the empty value to the form via appendIfPossible if maxLength is reached', () => {\n    const { result: formResult } = renderHook(() =>\n      useForm<{ tags: { key: string; value: string }[] }>({\n        defaultValues: {\n          tags: [\n            { key: 'key1', value: 'value1' },\n            { key: 'key2', value: 'value2' },\n          ],\n        },\n      }),\n    );\n\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentFieldArray<{ tags: { key: string; value: string }[] }>({\n          name: 'tags',\n          emptyValue: { key: '', value: '' },\n          maxLength: 2,\n          form: formResult.current,\n ", "],\n        },\n      }),\n    );\n\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentFieldArray<{ tags: { key: string; value: string }[] }>({\n          name: 'tags',\n          emptyValue: { key: '', value: '' },\n          maxLength: 2,\n          form: formResult.current,\n          keyProperty: 'key',\n        }),\n      { wrapper: DefaultWrapper },\n    );\n\n    result.current.appendIfPossible({ key: 'not-added', value: 'not-added' }, {});\n    expect(result.current.getTagsValues()).toStrictEqual([\n      { key: 'key1', value: 'value1' },\n      { key: 'key2', value: 'value2' },\n    ]);\n  });\n\n  it('should remove tag when removeOrUpdate is called for tag not at the end of the array', () => {\n    const { result: formResult } = renderHook(() =>\n      useForm<{ tags: { key: string; value: string }[] }>({\n        defaultValues: {\n          tags: [\n            { key: 'key1', value: 'value1' },\n            { key: 'key2', value: 'value2' },\n            { key: '', value: '' },\n          ],\n        },\n      }),\n    );\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentFieldArray({\n         ", "{ key: 'key1', value: 'value1' },\n            { key: 'key2', value: 'value2' },\n            { key: '', value: '' },\n          ],\n        },\n      }),\n    );\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentFieldArray({\n          name: 'tags',\n          emptyValue: { key: '', value: '' },\n          maxLength: 5,\n          form: formResult.current,\n          keyProperty: 'key',\n        }),\n      { wrapper: DefaultWrapper },\n    );\n\n    result.current.removeOrUpdate(0);\n\n    expect(result.current.getTagsValues()).toStrictEqual([\n      { key: 'key2', value: 'value2' },\n      { key: '', value: '' },\n    ]);\n  });\n\n  it('should set last tag to the empty value when removeOrUpdate is called for last tag', () => {\n    const { result: formResult } = renderHook(() =>\n      useForm<{ tags: { key: string; value: string }[] }>({\n        defaultValues: {\n          tags: [\n            { key: 'key1', value: 'value1' },\n            { key: 'key2', value: 'value2' },\n          ],\n        },\n      }),\n    );\n    const { result } = renderHook(\n      () ", "string }[] }>({\n        defaultValues: {\n          tags: [\n            { key: 'key1', value: 'value1' },\n            { key: 'key2', value: 'value2' },\n          ],\n        },\n      }),\n    );\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentFieldArray({\n          name: 'tags',\n          emptyValue: { key: '', value: '' },\n          maxLength: 2,\n          form: formResult.current,\n          keyProperty: 'key',\n        }),\n      { wrapper: DefaultWrapper },\n    );\n\n    result.current.removeOrUpdate(1);\n\n    expect(result.current.getTagsValues()).toStrictEqual([\n      { key: 'key1', value: 'value1' },\n      { key: '', value: '' },\n    ]);\n  });\n\n  it('should add an empty tag to the end of the array when removeOrUpdate is called when the max number of tags are present', () => {\n    const { result: formResult } = renderHook(() =>\n      useForm<{ tags: { key: string; value: string }[] }>({\n        defaultValues: {\n          tags: [\n            { key: 'key1', value: 'value1' },\n            { key: 'key2', value: 'value2' },\n          ],\n        },\n ", "() => {\n    const { result: formResult } = renderHook(() =>\n      useForm<{ tags: { key: string; value: string }[] }>({\n        defaultValues: {\n          tags: [\n            { key: 'key1', value: 'value1' },\n            { key: 'key2', value: 'value2' },\n          ],\n        },\n      }),\n    );\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentFieldArray({\n          name: 'tags',\n          emptyValue: { key: '', value: '' },\n          maxLength: 2,\n          form: formResult.current,\n          keyProperty: 'key',\n        }),\n      { wrapper: DefaultWrapper },\n    );\n\n    result.current.removeOrUpdate(0);\n\n    expect(result.current.getTagsValues()).toStrictEqual([\n      { key: 'key2', value: 'value2' },\n      { key: '', value: '' },\n    ]);\n  });\n});\n", 'import { renderHook } from \'@testing-library/react\';\nimport { useForm, FormProvider } from \'react-hook-form\';\nimport { IntlProvider } from \'react-intl\';\n\nimport { useTagAssignmentForm } from \'./useTagAssignmentForm\';\n\nconst DefaultWrapper = ({ children }: { children: React.ReactNode }) => {\n  return <IntlProvider locale="en">{children}</IntlProvider>;\n};\n\ndescribe(\'useTagAssignmentForm\', () => {\n  it(\'should use passed form as prop\', () => {\n    const { result: formResult } = renderHook(() =>\n      useForm<{ input: string; tags: { key: string; value: undefined }[] }>({ defaultValues: { input: \'test_input\' } }),\n    );\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm({\n          name: \'tags\',\n          emptyValue: { key: \'\', value: undefined },\n          form: formResult.current,\n          keyProperty: \'key\',\n          valueProperty: \'value\',\n        }),\n      { wrapper: DefaultWrapper },\n    );\n    const values = result.current.form.getValues();\n    expect(values.tags).toStrictEqual([{ key: \'\', value: undefined }]);\n    expect(values.input).toBe(\'test_input\');\n  });\n\n  it(\'should use context form if no form prop is passed\', () => {\n    const Wrapper = ({ children }: { children: React.ReactNode }) => {\n      const methods = useForm<{ input: string; tags: { key: string; value: undefined }[] }>({\n        defaultValues: { input: \'test_input\' },\n      });\n      return (\n        <FormProvider {...methods}>\n          <IntlProvider locale="en">{children}</IntlProvider>\n        </FormProvider>\n      );\n    };\n\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm({\n  ', '= useForm<{ input: string; tags: { key: string; value: undefined }[] }>({\n        defaultValues: { input: \'test_input\' },\n      });\n      return (\n        <FormProvider {...methods}>\n          <IntlProvider locale="en">{children}</IntlProvider>\n        </FormProvider>\n      );\n    };\n\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm({\n          name: \'tags\',\n          emptyValue: { key: \'\', value: undefined },\n          keyProperty: \'key\',\n          valueProperty: \'value\',\n        }),\n      { wrapper: Wrapper },\n    );\n\n    const values = result.current.form.getValues();\n    expect(values[\'tags\']).toStrictEqual([{ key: \'\', value: undefined }]);\n    expect(values[\'input\']).toBe(\'test_input\');\n  });\n\n  it(\'should add an empty value on default values provided by form context\', () => {\n    const Wrapper = ({ children }: { children: React.ReactNode }) => {\n      const methods = useForm<{ input: string; tags: { key: string; value: string | undefined }[] }>({\n        defaultValues: { input: \'test_input\', tags: [{ key: \'defaultKey\', value: \'defaultValue\' }] },\n      });\n      return (\n        <FormProvider {...methods}>\n          <IntlProvider locale="en">{children}</IntlProvider>\n        </FormProvider>\n      );\n    };\n\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm({\n          name: \'tags\',\n   ', 'defaultValues: { input: \'test_input\', tags: [{ key: \'defaultKey\', value: \'defaultValue\' }] },\n      });\n      return (\n        <FormProvider {...methods}>\n          <IntlProvider locale="en">{children}</IntlProvider>\n        </FormProvider>\n      );\n    };\n\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm({\n          name: \'tags\',\n          emptyValue: { key: \'\', value: undefined },\n          keyProperty: \'key\',\n          valueProperty: \'value\',\n        }),\n      { wrapper: Wrapper },\n    );\n\n    const values = result.current.form.getValues();\n    expect(values[\'tags\']).toStrictEqual([\n      { key: \'defaultKey\', value: \'defaultValue\' },\n      { key: \'\', value: undefined },\n    ]);\n    expect(values[\'input\']).toBe(\'test_input\');\n  });\n\n  it(\'should throw an error if no form is passed and not in a form context\', () => {\n    expect(() =>\n      renderHook(\n        () =>\n          useTagAssignmentForm({\n            name: \'tags\',\n            emptyValue: { key: \'\', value: undefined },\n            keyProperty: \'key\',\n            valueProperty: \'value\',\n          }),\n        { wrapper: DefaultWrapper },\n      ),\n    ).toThrow(\'Nest your component on a FormProvider or pass a form prop\');\n  });\n\n  ', '          name: \'tags\',\n            emptyValue: { key: \'\', value: undefined },\n            keyProperty: \'key\',\n            valueProperty: \'value\',\n          }),\n        { wrapper: DefaultWrapper },\n      ),\n    ).toThrow(\'Nest your component on a FormProvider or pass a form prop\');\n  });\n\n  it(\'should throw an error if default values are passed and in a form context\', () => {\n    const Wrapper = ({ children }: { children: React.ReactNode }) => {\n      const methods = useForm<{ input: string; tags: { key: string; value: string | undefined }[] }>({\n        defaultValues: { input: \'test_input\' },\n      });\n      return (\n        <FormProvider {...methods}>\n          <IntlProvider locale="en">{children}</IntlProvider>\n        </FormProvider>\n      );\n    };\n\n    expect(() =>\n      renderHook(\n        () =>\n          useTagAssignmentForm({\n            name: \'tags\',\n            emptyValue: { key: \'\', value: undefined },\n            keyProperty: \'key\',\n            valueProperty: \'value\',\n            defaultValues: [{ key: \'defaultKey\', value: \'defaultValue\' }],\n          }),\n        { wrapper: Wrapper },\n      ),\n   ', "       name: 'tags',\n            emptyValue: { key: '', value: undefined },\n            keyProperty: 'key',\n            valueProperty: 'value',\n            defaultValues: [{ key: 'defaultKey', value: 'defaultValue' }],\n          }),\n        { wrapper: Wrapper },\n      ),\n    ).toThrow('Define defaultValues at form context level');\n  });\n\n  it('should use empty value if no default values are passed', () => {\n    const { result: formResult } = renderHook(() => useForm());\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm({\n          name: 'tags',\n          emptyValue: { key: '', value: undefined },\n          keyProperty: 'key',\n          valueProperty: 'value',\n          form: formResult.current,\n        }),\n      { wrapper: DefaultWrapper },\n    );\n\n    const values = result.current.form.getValues();\n    expect(values['tags']).toStrictEqual([{ key: '', value: undefined }]);\n  });\n\n  it('should use default values + empty value if default values are passed', () => {\n    const { result: formResult } = renderHook(() => useForm<{ tags: { key: string; value: string | undefined }[] }>());\n    const defaultValues = [{ key: 'defaultKey', value: 'defaultValue' }];\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm<{ tags: { key: string; value: string | undefined }[] }>({\n         ", "expect(values['tags']).toStrictEqual([{ key: '', value: undefined }]);\n  });\n\n  it('should use default values + empty value if default values are passed', () => {\n    const { result: formResult } = renderHook(() => useForm<{ tags: { key: string; value: string | undefined }[] }>());\n    const defaultValues = [{ key: 'defaultKey', value: 'defaultValue' }];\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm<{ tags: { key: string; value: string | undefined }[] }>({\n          name: 'tags',\n          emptyValue: { key: '', value: undefined },\n          defaultValues,\n          form: formResult.current,\n          keyProperty: 'key',\n          valueProperty: 'value',\n        }),\n      { wrapper: DefaultWrapper },\n    );\n\n    const values = result.current.form.getValues();\n    expect(values.tags).toStrictEqual([\n      { key: 'defaultKey', value: 'defaultValue' },\n      { key: '', value: undefined },\n    ]);\n  });\n\n  it('should not add the empty value to the form if maxLength is reached', () => {\n    const { result: formResult } = renderHook(() => useForm<{ tags: { key: string; value: string | undefined }[] }>());\n    const defaultValues = [\n      { key: 'key1', value: 'value1' },\n      { key: 'key2', value: 'value2' },\n    ];\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm<{ tags: { key: string; value: string | undefined }[] }>({\n          name: 'tags',\n          emptyValue: { ", "useForm<{ tags: { key: string; value: string | undefined }[] }>());\n    const defaultValues = [\n      { key: 'key1', value: 'value1' },\n      { key: 'key2', value: 'value2' },\n    ];\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm<{ tags: { key: string; value: string | undefined }[] }>({\n          name: 'tags',\n          emptyValue: { key: '', value: undefined },\n          defaultValues,\n          maxLength: 2,\n          form: formResult.current,\n          keyProperty: 'key',\n          valueProperty: 'value',\n        }),\n      { wrapper: DefaultWrapper },\n    );\n\n    const values = result.current.form.getValues();\n    expect(values.tags).toStrictEqual([\n      { key: 'key1', value: 'value1' },\n      { key: 'key2', value: 'value2' },\n    ]);\n  });\n\n  it('should wait for loading before resetting', () => {\n    const { result: formResult } = renderHook(() => useForm());\n    const { result, rerender } = renderHook(\n      ({ loading }) =>\n        useTagAssignmentForm({\n          name: 'tags',\n          emptyValue: { key: '', value: undefined },\n          loading,\n          form: formResult.current,\n          keyProperty: 'key',\n          valueProperty: 'value',\n        }),\n     ", "renderHook(\n      ({ loading }) =>\n        useTagAssignmentForm({\n          name: 'tags',\n          emptyValue: { key: '', value: undefined },\n          loading,\n          form: formResult.current,\n          keyProperty: 'key',\n          valueProperty: 'value',\n        }),\n      { initialProps: { loading: true }, wrapper: DefaultWrapper },\n    );\n\n    const initialValues = result.current.form.getValues();\n    expect(initialValues['tags']).toStrictEqual([]);\n\n    rerender({ loading: false });\n\n    const values = result.current.form.getValues();\n    expect(values['tags']).toStrictEqual([{ key: '', value: undefined }]);\n  });\n\n  it('should not override the other filled values when setting default values', () => {\n    const { result: formResult } = renderHook(() =>\n      useForm<{ tags: { key: string; value: string | undefined }[]; input1: string; input2: string }>({\n        defaultValues: {\n          input1: 'test_input1',\n          input2: 'test_input2',\n        },\n      }),\n    );\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm({\n          name: 'tags',\n          emptyValue: { key: '', value: undefined },\n          form: formResult.current,\n          keyProperty: 'key',\n          valueProperty: 'value',\n        }),\n      { wrapper: DefaultWrapper },\n    );\n\n ", "result } = renderHook(\n      () =>\n        useTagAssignmentForm({\n          name: 'tags',\n          emptyValue: { key: '', value: undefined },\n          form: formResult.current,\n          keyProperty: 'key',\n          valueProperty: 'value',\n        }),\n      { wrapper: DefaultWrapper },\n    );\n\n    const values = result.current.form.getValues();\n    expect(values.tags).toStrictEqual([{ key: '', value: undefined }]);\n    expect(values.input1).toBe('test_input1');\n    expect(values.input2).toBe('test_input2');\n  });\n});\n", "import invariant from 'invariant';\nimport { useEffect, useState } from 'react';\nimport type { ArrayPath, FieldArray, FieldValues, Path, PathValue, UseFormReturn } from 'react-hook-form';\nimport { useFormContext } from 'react-hook-form';\n\nimport { useTagAssignmentFieldArray } from './useTagAssignmentFieldArray';\n\nexport interface UseTagAssignmentProps<\n  T extends FieldValues,\n  K extends ArrayPath<T> = ArrayPath<T>,\n  V extends FieldArray<T, K> = FieldArray<T, K>,\n> {\n  name: K;\n  maxLength?: number;\n  emptyValue: V;\n  loading?: boolean;\n  defaultValues?: V[];\n  form?: UseFormReturn<T>;\n  keyProperty: keyof V extends string ? keyof V : never;\n  valueProperty: keyof V extends string ? keyof V : never;\n}\n\nexport function useTagAssignmentForm<\n  T extends FieldValues,\n  K extends ArrayPath<T> = ArrayPath<T>,\n  V extends FieldArray<T, K> = FieldArray<T, K>,\n>({\n  name,\n  maxLength,\n  emptyValue,\n  defaultValues,\n  loading,\n  form,\n  keyProperty,\n  valueProperty,\n}: UseTagAssignmentProps<T, K, V>) {\n  const [_emptyValue] = useState(emptyValue);\n\n  const formCtx = useFormContext<T>();\n  const shouldUseFormContext = Boolean(formCtx) && !form;\n  const internalForm = shouldUseFormContext ? formCtx : form;\n\n  invariant(internalForm, 'Nest your component on a FormProvider or pass a form prop');\n  invariant(!(defaultValues && shouldUseFormContext), 'Define defaultValues at form context level');\n\n  const { setValue } = internalForm;\n\n  const fieldArrayMethods = useTagAssignmentFieldArray({\n    name,\n    maxLength,\n    emptyValue,\n    form: internalForm,\n    keyProperty,\n  });\n  const getTagsValues = fieldArrayMethods.getTagsValues;\n\n  useEffect(() => {\n    if (loading) return;\n    if (defaultValues) {\n      const newValues = [...defaultValues];\n      if (!maxLength || (maxLength && newValues.length < maxLength)) {\n        newValues.push(_emptyValue);\n      }\n      setValue(name as Path<T>, newValues as PathValue<T, Path<T>>);\n      return;\n    }\n\n    if (shouldUseFormContext) {\n      const existentValues = getTagsValues() ?? [];\n      if (!maxLength || (maxLength && existentValues.length < maxLength)) {\n        existentValues.push(_emptyValue);\n      }\n      setValue(name as Path<T>, existentValues ", ' if (!maxLength || (maxLength && newValues.length < maxLength)) {\n        newValues.push(_emptyValue);\n      }\n      setValue(name as Path<T>, newValues as PathValue<T, Path<T>>);\n      return;\n    }\n\n    if (shouldUseFormContext) {\n      const existentValues = getTagsValues() ?? [];\n      if (!maxLength || (maxLength && existentValues.length < maxLength)) {\n        existentValues.push(_emptyValue);\n      }\n      setValue(name as Path<T>, existentValues as PathValue<T, Path<T>>);\n      return;\n    }\n\n    setValue(name as Path<T>, [_emptyValue] as PathValue<T, Path<T>>);\n  }, [defaultValues, setValue, loading, maxLength, name, _emptyValue, shouldUseFormContext, getTagsValues]);\n\n  return {\n    ...fieldArrayMethods,\n    form: internalForm,\n    maxLength,\n    emptyValue,\n    name,\n    keyProperty,\n    valueProperty,\n  };\n}\n\nexport type UseTagAssignmentFormReturn<\n  T extends FieldValues = FieldValues,\n  K extends ArrayPath<T> = ArrayPath<T>,\n  V extends FieldArray<T, K> = FieldArray<T, K>,\n> = ReturnType<typeof useTagAssignmentForm<T, K, V>>;\n', "import { FormProvider, useForm } from 'react-hook-form';\n\nimport { TagAssignmentContext } from '../context/TagAssignmentContextProvider';\nimport { useTagAssignmentForm } from '../hooks/useTagAssignmentForm';\n\ninterface TestFormI {\n  tags: {\n    key: string;\n    value: string;\n  }[];\n}\n\ninterface TestTagAssignmentContextProviderProps extends Partial<ReturnType<typeof useTagAssignmentForm<TestFormI>>> {\n  children: React.ReactNode;\n}\n\nexport function TestTagAssignmentContextProvider({ children, ...props }: TestTagAssignmentContextProviderProps) {\n  const form = useForm<TestFormI>();\n  const tagForm = useTagAssignmentForm<TestFormI, 'tags', { key: string; value: string }>({\n    form,\n    name: 'tags',\n    emptyValue: { key: '', value: '' },\n    keyProperty: 'key',\n    valueProperty: 'value',\n  });\n  return (\n    <FormProvider {...form}>\n      <TagAssignmentContext.Provider\n        value={{\n          ...(tagForm as any),\n          ...props,\n        }}\n      >\n        {children}\n      </TagAssignmentContext.Provider>\n    </FormProvider>\n  );\n}\n", 'import { GenericSkeleton, ParagraphSkeleton, Typography, useDesignSystemTheme } from \'@databricks/design-system\';\nimport type { ReactNode } from \'react\';\nimport { useRef } from \'react\';\nimport { FormattedMessage } from \'react-intl\';\nimport useResponsiveContainer from \'./useResponsiveContainer\';\n\nexport interface AsideSectionProps {\n  id: string;\n  title?: ReactNode;\n  content: ReactNode;\n  isTitleLoading?: boolean;\n}\n\nexport type MaybeAsideSection = AsideSectionProps | null;\nexport type AsideSections = Array<MaybeAsideSection>;\n\nconst SIDEBAR_WIDTHS = {\n  sm: 316,\n  lg: 480,\n} as const;\nconst VERTICAL_MARGIN_PX = 16;\nconst DEFAULT_MAX_WIDTH = 450;\n\nexport const OverviewLayout = ({\n  isLoading,\n  asideSections,\n  children,\n  isTabLayout = true,\n  sidebarSize = \'sm\',\n  verticalStackOrder,\n}: {\n  isLoading?: boolean;\n  asideSections: AsideSections;\n  children: ReactNode;\n  isTabLayout?: boolean;\n  sidebarSize?: \'sm\' | \'lg\';\n  verticalStackOrder?: \'main-first\' | \'aside-first\';\n}) => {\n  const { theme } = useDesignSystemTheme();\n  const containerRef = useRef<HTMLDivElement>(null);\n\n  const stackVertically = useResponsiveContainer(containerRef, { small: theme.responsive.breakpoints.lg }) === \'small\';\n\n  // Determine vertical stack order, i.e. should the main content be on top or bottom\n  const verticalDisplayPrimaryContentOnTop = verticalStackOrder === \'main-first\';\n\n  const totalSidebarWidth = SIDEBAR_WIDTHS[sidebarSize];\n  const innerSidebarWidth = totalSidebarWidth - VERTICAL_MARGIN_PX;\n\n  const secondaryStackedStyles = stackVertically\n    ? verticalDisplayPrimaryContentOnTop\n      ? { width: \'100%\' }\n      : { borderBottom: `1px solid ${theme.colors.border}`, width: \'100%\' }\n    : verticalDisplayPrimaryContentOnTop\n    ? {\n        width: innerSidebarWidth,\n      }\n    : {\n        paddingBottom: theme.spacing.sm,\n        width: innerSidebarWidth,\n      };\n\n  return (\n    <div\n      data-testid="entity-overview-container"\n      ref={containerRef}\n      css={{\n        display: \'flex\',\n        flexDirection: stackVertically ? (verticalDisplayPrimaryContentOnTop ? \'column\' : \'column-reverse\') : \'row\',\n        gap: theme.spacing.lg,\n      }}\n    >\n      <div\n        css={{\n      ', '   width: innerSidebarWidth,\n      };\n\n  return (\n    <div\n      data-testid="entity-overview-container"\n      ref={containerRef}\n      css={{\n        display: \'flex\',\n        flexDirection: stackVertically ? (verticalDisplayPrimaryContentOnTop ? \'column\' : \'column-reverse\') : \'row\',\n        gap: theme.spacing.lg,\n      }}\n    >\n      <div\n        css={{\n          display: \'flex\',\n          flexGrow: 1,\n          flexDirection: \'column\',\n          gap: theme.spacing.md,\n          width: stackVertically ? \'100%\' : `calc(100% - ${totalSidebarWidth}px)`,\n        }}\n      >\n        {isLoading ? <GenericSkeleton /> : children}\n      </div>\n      <div\n        style={{\n          display: \'flex\',\n          ...(isTabLayout && { marginTop: -theme.spacing.md }), // remove the gap between tab list and sidebar content\n        }}\n      >\n        <div\n          css={{\n            display: \'flex\',\n            flexDirection: \'column\',\n            gap: theme.spacing.lg,\n            ...secondaryStackedStyles,\n          }}\n        >\n          {isLoading ', "  >\n        <div\n          css={{\n            display: 'flex',\n            flexDirection: 'column',\n            gap: theme.spacing.lg,\n            ...secondaryStackedStyles,\n          }}\n        >\n          {isLoading && <GenericSkeleton />}\n          {!isLoading && <SidebarWrapper secondarySections={asideSections} />}\n        </div>\n      </div>\n    </div>\n  );\n};\n\nconst SidebarWrapper = ({ secondarySections }: { secondarySections: AsideSections }) => {\n  return (\n    <div>\n      {secondarySections\n        .filter((section) => section !== null)\n        .filter((section) => section?.content !== null)\n        .map(({ title, isTitleLoading, content, id }, index) => (\n          <AsideSection title={title} isTitleLoading={isTitleLoading} content={content} key={id} index={index} />\n        ))}\n    </div>\n  );\n};\n\nexport const AsideSectionTitle = ({ children }: { children: ReactNode }) => {\n  const { theme } = useDesignSystemTheme();\n  return (\n    <Typography.Title\n      level={4}\n      style={{\n        whiteSpace: 'nowrap',\n        marginRight: theme.spacing.lg,\n        marginTop: 0,\n      }}\n    >\n      {children}\n    </Typography.Title>\n  );\n};\n\nconst AsideSection = ({\n  title,\n  content,\n  index,\n  isTitleLoading = false,\n}: Omit<AsideSectionProps, 'id'> & {\n  index: number;\n}) => {\n  const { theme } = useDesignSystemTheme();\n\n  const titleComponent ", ' return (\n    <Typography.Title\n      level={4}\n      style={{\n        whiteSpace: \'nowrap\',\n        marginRight: theme.spacing.lg,\n        marginTop: 0,\n      }}\n    >\n      {children}\n    </Typography.Title>\n  );\n};\n\nconst AsideSection = ({\n  title,\n  content,\n  index,\n  isTitleLoading = false,\n}: Omit<AsideSectionProps, \'id\'> & {\n  index: number;\n}) => {\n  const { theme } = useDesignSystemTheme();\n\n  const titleComponent = isTitleLoading ? (\n    <ParagraphSkeleton\n      label={\n        <FormattedMessage\n          defaultMessage="Section title loading"\n          description="Loading skeleton label for overview page section title in Catalog Explorer"\n        />\n      }\n    />\n  ) : title ? (\n    <AsideSectionTitle>{title}</AsideSectionTitle>\n  ) : null;\n\n  const compactStyles = { padding: `${theme.spacing.md}px 0 ${theme.spacing.md}px 0` };\n\n  return (\n    <div\n      css={{\n        ...compactStyles,\n        ...(index === 0 ? {} : { borderTop: `1px solid ${theme.colors.border}` }),\n      }}\n    >\n      {titleComponent}\n      {content}\n    </div>\n  );\n};\n\nexport const KeyValueProperty = ({\n  keyValue,\n  value,\n  maxWidth,\n}: {\n  keyValue: string;\n  value: React.ReactNode;\n  maxWidth?: number | string;\n}) => {\n  const { theme } = useDesignSystemTheme();\n  return (\n    <div\n      css={{\n        display: \'flex\',\n        alignItems: \'center\',\n        \'&:has(+ div)\': {\n          marginBottom: theme.spacing.xs,\n   ', '  {titleComponent}\n      {content}\n    </div>\n  );\n};\n\nexport const KeyValueProperty = ({\n  keyValue,\n  value,\n  maxWidth,\n}: {\n  keyValue: string;\n  value: React.ReactNode;\n  maxWidth?: number | string;\n}) => {\n  const { theme } = useDesignSystemTheme();\n  return (\n    <div\n      css={{\n        display: \'flex\',\n        alignItems: \'center\',\n        \'&:has(+ div)\': {\n          marginBottom: theme.spacing.xs,\n        },\n        maxWidth: maxWidth ?? DEFAULT_MAX_WIDTH,\n        wordBreak: \'break-word\',\n        lineHeight: theme.typography.lineHeightLg,\n      }}\n    >\n      <div\n        css={{\n          color: theme.colors.textSecondary,\n          flex: 0.5,\n          alignSelf: \'start\',\n        }}\n      >\n        {keyValue}\n      </div>\n      <div\n        css={{\n          flex: 1,\n          alignSelf: \'start\',\n          overflow: \'hidden\',\n        }}\n      >\n        {value}\n      </div>\n    </div>\n  );\n};\n\nexport const NoneCell = () => {\n  return (\n    <Typography.Text color="secondary">\n      <FormattedMessage defaultMessage="None" description="Cell value when there\'s no content" />\n    </Typography.Text>\n  );\n};\n', '<head>\n  <link\n    rel="stylesheet"\n    href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/xcode.min.css"\n  />\n  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>\n  <script>\n    hljs.highlightAll();\n  </script>\n  <style>\n    body {\n      margin: 0;\n      font-family: -apple-system, BlinkMacSystemFont, Segoe UI, Roboto,\n        Helvetica Neue, Arial, Noto Sans, sans-serif, Apple Color Emoji,\n        Segoe UI Emoji, Segoe UI Symbol, Noto Color Emoji;\n      -webkit-tap-highlight-color: rgba(0, 0, 0, 0);\n      margin: 0;\n      font-weight: 400;\n      font-size: 13px;\n      line-height: 18px;\n      color: rgb(17, 23, 28);\n    }\n    code {\n      line-height: 18px;\n      font-size: 11px;\n      background: rgb(250, 250, 250) !important;\n    }\n    pre {\n      background: rgb(250, 250, 250);\n      margin: 0;\n      display: none;\n    }\n    pre.active {\n      display: unset;\n    }\n    button {\n      white-space: nowrap;\n      text-align: center;\n      position: relative;\n      cursor: pointer;\n      background: rgba(34, 114, 180, 0) !important;\n      color: rgb(34, 114, 180) !important;\n      border-color: rgba(34, 114, 180, 0) !important;\n      padding: 4px 6px !important;\n      text-decoration: none !important;\n      line-height: 20px !important;\n      box-shadow: none !important;\n      height: 32px !important;\n      display: inline-flex !important;\n      -webkit-box-align: center !important;\n      align-items: center !important;\n      -webkit-box-pack: center ', ' background: rgba(34, 114, 180, 0) !important;\n      color: rgb(34, 114, 180) !important;\n      border-color: rgba(34, 114, 180, 0) !important;\n      padding: 4px 6px !important;\n      text-decoration: none !important;\n      line-height: 20px !important;\n      box-shadow: none !important;\n      height: 32px !important;\n      display: inline-flex !important;\n      -webkit-box-align: center !important;\n      align-items: center !important;\n      -webkit-box-pack: center !important;\n      justify-content: center !important;\n      vertical-align: middle !important;\n    }\n    p {\n      margin: 0;\n      padding: 0;\n    }\n    button:hover {\n      background: rgba(34, 114, 180, 0.08) !important;\n      color: rgb(14, 83, 139) !important;\n    }\n    button:active {\n      background: rgba(34, 114, 180, 0.16) !important;\n      color: rgb(4, 53, 93) !important;\n    }\n    h1 {\n      margin-top: 4px;\n      font-size: 22px;\n    }\n    .info {\n      font-size: 12px;\n      font-weight: 500;\n      line-height: 16px;\n      color: rgb(95, 114, 129);\n    }\n    .tabs {\n      margin-top: 10px;\n      border-bottom: 1px solid rgb(209, 217, 225) !important;\n      display: flex;\n      line-height: 24px;\n    }\n    .tab {\n      font-size: 13px;\n      font-weight: 600 !important;\n      cursor: pointer;\n      margin: 0 24px 0 2px;\n      ', ' line-height: 16px;\n      color: rgb(95, 114, 129);\n    }\n    .tabs {\n      margin-top: 10px;\n      border-bottom: 1px solid rgb(209, 217, 225) !important;\n      display: flex;\n      line-height: 24px;\n    }\n    .tab {\n      font-size: 13px;\n      font-weight: 600 !important;\n      cursor: pointer;\n      margin: 0 24px 0 2px;\n      padding-left: 2px;\n    }\n    .tab:hover {\n      color: rgb(14, 83, 139) !important;\n    }\n    .tab.active {\n      border-bottom: 3px solid rgb(34, 114, 180) !important;\n    }\n    .link {\n      margin-left: 12px;\n      display: inline-block;\n      text-decoration: none;\n      color: rgb(34, 114, 180) !important;\n      font-size: 13px;\n      font-weight: 400;\n    }\n    .link:hover {\n      color: rgb(14, 83, 139) !important;\n    }\n    .link-content {\n      display: flex;\n      gap: 6px;\n      align-items: center;\n    }\n    .caret-up {\n      transform: rotate(180deg);\n    }\n  </style>\n</head>\n<body>\n  <div style="display: flex; align-items: center">\n    The logged model is compatible with the Mosaic AI Agent Framework.\n    <button onclick="toggleCode()">\n      See how to evaluate the model&nbsp;\n      <span\n        role="img"\n        id="caret"\n        aria-hidden="true"\n        class="anticon css-6xix1i"\n        style="font-size: ', '  .caret-up {\n      transform: rotate(180deg);\n    }\n  </style>\n</head>\n<body>\n  <div style="display: flex; align-items: center">\n    The logged model is compatible with the Mosaic AI Agent Framework.\n    <button onclick="toggleCode()">\n      See how to evaluate the model&nbsp;\n      <span\n        role="img"\n        id="caret"\n        aria-hidden="true"\n        class="anticon css-6xix1i"\n        style="font-size: 14px"\n        ><svg\n          xmlns="http://www.w3.org/2000/svg"\n          width="1em"\n          height="1em"\n          fill="none"\n          viewBox="0 0 16 16"\n          aria-hidden="true"\n          focusable="false"\n          class=""\n        >\n          <path\n            fill="currentColor"\n            fill-rule="evenodd"\n            d="M8 8.917 10.947 6 12 7.042 8 11 4 7.042 5.053 6z"\n            clip-rule="evenodd"\n          ></path>\n        </svg>\n      </span>\n    </button>\n  </div>\n  <div id="code" style="display: none">\n    <h1>\n      Agent evaluation\n      <a\n        class="link"\n        href="https://docs.databricks.com/en/generative-ai/agent-evaluation/synthesize-evaluation-set.html?utm_source=mlflow.log_model&utm_medium=notebook"\n        target="_blank"\n      ', '7.042 8 11 4 7.042 5.053 6z"\n            clip-rule="evenodd"\n          ></path>\n        </svg>\n      </span>\n    </button>\n  </div>\n  <div id="code" style="display: none">\n    <h1>\n      Agent evaluation\n      <a\n        class="link"\n        href="https://docs.databricks.com/en/generative-ai/agent-evaluation/synthesize-evaluation-set.html?utm_source=mlflow.log_model&utm_medium=notebook"\n        target="_blank"\n      >\n        <span class="link-content">\n          Learn more\n          <span role="img" aria-hidden="true" class="anticon css-6xix1i"\n            ><svg\n              xmlns="http://www.w3.org/2000/svg"\n              width="1em"\n              height="1em"\n              fill="none"\n              viewBox="0 0 16 16"\n              aria-hidden="true"\n              focusable="false"\n              class=""\n            >\n              <path\n                fill="currentColor"\n                d="M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z"\n              ></path>\n             ', '           class=""\n            >\n              <path\n                fill="currentColor"\n                d="M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z"\n              ></path>\n              <path\n                fill="currentColor"\n                d="M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z"\n              ></path></svg></span></span\n      ></a>\n    </h1>\n    <p class="info">\n      Copy the following code snippet in a notebook cell (right click â†’ copy)\n    </p>\n    <div class="tabs">\n      <div class="tab active" onclick="tabClicked(0)">Using synthetic data</div>\n      <div class="tab" onclick="tabClicked(1)">Using your own dataset</div>\n    </div>\n    <div style="height: 472px">\n      <pre\n        class="active"\n      ><code class="language-python">{{eval_with_synthetic_code}}</code></pre>\n\n      <pre><code class="language-python">{{eval_with_dataset_code}}</code></pre>\n    </div>\n  </div>\n  <script>\n    var codeShown = false;\n    function clip(el) {\n      var range = document.createRange();\n      range.selectNodeContents(el);\n      var sel = window.getSelection();\n      sel.removeAllRanges();\n      sel.addRange(range);\n    }\n\n    function toggleCode() {\n      if (codeShown) {\n        document.getElementById("code").style.display = "none";\n    ', '   ><code class="language-python">{{eval_with_synthetic_code}}</code></pre>\n\n      <pre><code class="language-python">{{eval_with_dataset_code}}</code></pre>\n    </div>\n  </div>\n  <script>\n    var codeShown = false;\n    function clip(el) {\n      var range = document.createRange();\n      range.selectNodeContents(el);\n      var sel = window.getSelection();\n      sel.removeAllRanges();\n      sel.addRange(range);\n    }\n\n    function toggleCode() {\n      if (codeShown) {\n        document.getElementById("code").style.display = "none";\n        codeShown = false;\n      } else {\n        document.getElementById("code").style.display = "block";\n        clip(document.querySelector("pre.active"));\n        codeShown = true;\n      }\n      document.getElementById("caret").classList.toggle("caret-up");\n    }\n\n    function tabClicked(tabIndex) {\n      document.querySelectorAll(".tab").forEach((tab, index) => {\n        if (index === tabIndex) {\n          tab.classList.add("active");\n        } else {\n          tab.classList.remove("active");\n        }\n      });\n      document.querySelectorAll("pre").forEach((pre, index) => {\n        if (index === tabIndex) {\n          pre.classList.add("active");\n        } else {\n          pre.classList.remove("active");\n        }\n      });\n      clip(document.querySelector("pre.active"));\n    }\n  </script>\n</body>\n', '<!DOCTYPE html>\n<html lang="en">\n  <head>\n    <meta charset="utf-8" />\n    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />\n    <link rel="shortcut icon" href="./static-files/favicon.ico" />\n    <meta name="theme-color" content="#000000" />\n    <!--\n      manifest.json provides metadata used when your web app is added to the\n      homescreen on Android. See https://developers.google.com/web/fundamentals/engage-and-retain/web-app-manifest/\n    -->\n    <link rel="manifest" href="./static-files/manifest.json" crossorigin="use-credentials" />\n    <title>MLflow</title>\n  </head>\n\n  <body>\n    <noscript> You need to enable JavaScript to run this app. </noscript>\n    <div id="root" class="mlflow-ui-container"></div>\n    <div id="modal" class="mlflow-ui-container"></div>\n  </body>\n</html>\n', '<!DOCTYPE html>\n<html>\n  <head>\n    <title>Test HTML</title>\n  </head>\n  <body>\n    <h1>Test HTML</h1>\n    <p>This is a test HTML file.</p>\n  </body>\n</html>', '<html>\n  <head></head>\n  <body>\n    <div id="root"></div>\n  </body>\n</html>\n', '<head>\n  <link\n    rel="stylesheet"\n    href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/xcode.min.css"\n  />\n  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>\n  <script>\n    hljs.highlightAll();\n  </script>\n  <style>\n    body {\n      margin: 0;\n      font-family: -apple-system, BlinkMacSystemFont, Segoe UI, Roboto,\n        Helvetica Neue, Arial, Noto Sans, sans-serif, Apple Color Emoji,\n        Segoe UI Emoji, Segoe UI Symbol, Noto Color Emoji;\n      -webkit-tap-highlight-color: rgba(0, 0, 0, 0);\n      margin: 0;\n      font-weight: 400;\n      font-size: 13px;\n      line-height: 18px;\n      color: rgb(17, 23, 28);\n    }\n    code {\n      line-height: 18px;\n      font-size: 11px;\n      background: rgb(250, 250, 250) !important;\n    }\n    pre {\n      background: rgb(250, 250, 250);\n      margin: 0;\n      display: none;\n    }\n    pre.active {\n      display: unset;\n    }\n    button {\n      white-space: nowrap;\n      text-align: center;\n      position: relative;\n      cursor: pointer;\n      background: rgba(34, 114, 180, 0) !important;\n      color: rgb(34, 114, 180) !important;\n      border-color: rgba(34, 114, 180, 0) !important;\n      padding: 4px 6px !important;\n      text-decoration: none !important;\n      line-height: 20px !important;\n      box-shadow: none !important;\n      height: 32px !important;\n      display: inline-flex !important;\n      -webkit-box-align: center !important;\n      align-items: center !important;\n      -webkit-box-pack: center ', ' background: rgba(34, 114, 180, 0) !important;\n      color: rgb(34, 114, 180) !important;\n      border-color: rgba(34, 114, 180, 0) !important;\n      padding: 4px 6px !important;\n      text-decoration: none !important;\n      line-height: 20px !important;\n      box-shadow: none !important;\n      height: 32px !important;\n      display: inline-flex !important;\n      -webkit-box-align: center !important;\n      align-items: center !important;\n      -webkit-box-pack: center !important;\n      justify-content: center !important;\n      vertical-align: middle !important;\n    }\n    p {\n      margin: 0;\n      padding: 0;\n    }\n    button:hover {\n      background: rgba(34, 114, 180, 0.08) !important;\n      color: rgb(14, 83, 139) !important;\n    }\n    button:active {\n      background: rgba(34, 114, 180, 0.16) !important;\n      color: rgb(4, 53, 93) !important;\n    }\n    h1 {\n      margin-top: 4px;\n      font-size: 22px;\n    }\n    .info {\n      font-size: 12px;\n      font-weight: 500;\n      line-height: 16px;\n      color: rgb(95, 114, 129);\n    }\n    .tabs {\n      margin-top: 10px;\n      border-bottom: 1px solid rgb(209, 217, 225) !important;\n      display: flex;\n      line-height: 24px;\n    }\n    .tab {\n      font-size: 13px;\n      font-weight: 600 !important;\n      cursor: pointer;\n      margin: 0 24px 0 2px;\n      ', ' line-height: 16px;\n      color: rgb(95, 114, 129);\n    }\n    .tabs {\n      margin-top: 10px;\n      border-bottom: 1px solid rgb(209, 217, 225) !important;\n      display: flex;\n      line-height: 24px;\n    }\n    .tab {\n      font-size: 13px;\n      font-weight: 600 !important;\n      cursor: pointer;\n      margin: 0 24px 0 2px;\n      padding-left: 2px;\n    }\n    .tab:hover {\n      color: rgb(14, 83, 139) !important;\n    }\n    .tab.active {\n      border-bottom: 3px solid rgb(34, 114, 180) !important;\n    }\n    .link {\n      margin-left: 12px;\n      display: inline-block;\n      text-decoration: none;\n      color: rgb(34, 114, 180) !important;\n      font-size: 13px;\n      font-weight: 400;\n    }\n    .link:hover {\n      color: rgb(14, 83, 139) !important;\n    }\n    .link-content {\n      display: flex;\n      gap: 6px;\n      align-items: center;\n    }\n    .caret-up {\n      transform: rotate(180deg);\n    }\n  </style>\n</head>\n<body>\n  <div style="display: flex; align-items: center">\n    The logged model is compatible with the Mosaic AI Agent Framework.\n    <button onclick="toggleCode()">\n      See how to evaluate the model&nbsp;\n      <span\n        role="img"\n        id="caret"\n        aria-hidden="true"\n        class="anticon css-6xix1i"\n        style="font-size: ', '  .caret-up {\n      transform: rotate(180deg);\n    }\n  </style>\n</head>\n<body>\n  <div style="display: flex; align-items: center">\n    The logged model is compatible with the Mosaic AI Agent Framework.\n    <button onclick="toggleCode()">\n      See how to evaluate the model&nbsp;\n      <span\n        role="img"\n        id="caret"\n        aria-hidden="true"\n        class="anticon css-6xix1i"\n        style="font-size: 14px"\n        ><svg\n          xmlns="http://www.w3.org/2000/svg"\n          width="1em"\n          height="1em"\n          fill="none"\n          viewBox="0 0 16 16"\n          aria-hidden="true"\n          focusable="false"\n          class=""\n        >\n          <path\n            fill="currentColor"\n            fill-rule="evenodd"\n            d="M8 8.917 10.947 6 12 7.042 8 11 4 7.042 5.053 6z"\n            clip-rule="evenodd"\n          ></path>\n        </svg>\n      </span>\n    </button>\n  </div>\n  <div id="code" style="display: none">\n    <h1>\n      Agent evaluation\n      <a\n        class="link"\n        href="https://docs.databricks.com/en/generative-ai/agent-evaluation/synthesize-evaluation-set.html?utm_source=mlflow.log_model&utm_medium=notebook"\n        target="_blank"\n      ', '7.042 8 11 4 7.042 5.053 6z"\n            clip-rule="evenodd"\n          ></path>\n        </svg>\n      </span>\n    </button>\n  </div>\n  <div id="code" style="display: none">\n    <h1>\n      Agent evaluation\n      <a\n        class="link"\n        href="https://docs.databricks.com/en/generative-ai/agent-evaluation/synthesize-evaluation-set.html?utm_source=mlflow.log_model&utm_medium=notebook"\n        target="_blank"\n      >\n        <span class="link-content">\n          Learn more\n          <span role="img" aria-hidden="true" class="anticon css-6xix1i"\n            ><svg\n              xmlns="http://www.w3.org/2000/svg"\n              width="1em"\n              height="1em"\n              fill="none"\n              viewBox="0 0 16 16"\n              aria-hidden="true"\n              focusable="false"\n              class=""\n            >\n              <path\n                fill="currentColor"\n                d="M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z"\n              ></path>\n             ', '           class=""\n            >\n              <path\n                fill="currentColor"\n                d="M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z"\n              ></path>\n              <path\n                fill="currentColor"\n                d="M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z"\n              ></path></svg></span></span\n      ></a>\n    </h1>\n    <p class="info">\n      Copy the following code snippet in a notebook cell (right click â†’ copy)\n    </p>\n    <div class="tabs">\n      <div class="tab active" onclick="tabClicked(0)">Using synthetic data</div>\n      <div class="tab" onclick="tabClicked(1)">Using your own dataset</div>\n    </div>\n    <div style="height: 472px">\n      <pre\n        class="active"\n      ><code class="language-python">%pip install -U databricks-agents\ndbutils.library.restartPython()\n## Run the above in a separate cell ##\n\nfrom databricks.agents.evals import generate_evals_df\nimport mlflow\n\nagent_description = &quot;A chatbot that answers questions about Databricks.&quot;\nquestion_guidelines = &quot;&quot;&quot;\n# User personas\n- A developer new to the Databricks platform\n# Example questions\n- What API lets me parallelize operations over rows of a delta table?\n&quot;&quot;&quot;\n# TODO: Spark/Pandas DataFrame with &quot;content&quot; and &quot;doc_uri&quot; columns.\ndocs = spark.table(&quot;catalog.schema.my_table_of_docs&quot;)\nevals = generate_evals_df(\n    docs=docs,\n    num_evals=25,\n    agent_description=agent_description,\n    question_guidelines=question_guidelines,\n)\neval_result = mlflow.evaluate(data=evals, model=&quot;runs:/1/model&quot;, model_type=&quot;databricks-agent&quot;)\n</code></pre>\n\n      <pre><code class="language-python">%pip install -U databricks-agents\ndbutils.library.restartPython()\n## Run the above in a ', '   ><code class="language-python">%pip install -U databricks-agents\ndbutils.library.restartPython()\n## Run the above in a separate cell ##\n\nfrom databricks.agents.evals import generate_evals_df\nimport mlflow\n\nagent_description = &quot;A chatbot that answers questions about Databricks.&quot;\nquestion_guidelines = &quot;&quot;&quot;\n# User personas\n- A developer new to the Databricks platform\n# Example questions\n- What API lets me parallelize operations over rows of a delta table?\n&quot;&quot;&quot;\n# TODO: Spark/Pandas DataFrame with &quot;content&quot; and &quot;doc_uri&quot; columns.\ndocs = spark.table(&quot;catalog.schema.my_table_of_docs&quot;)\nevals = generate_evals_df(\n    docs=docs,\n    num_evals=25,\n    agent_description=agent_description,\n    question_guidelines=question_guidelines,\n)\neval_result = mlflow.evaluate(data=evals, model=&quot;runs:/1/model&quot;, model_type=&quot;databricks-agent&quot;)\n</code></pre>\n\n      <pre><code class="language-python">%pip install -U databricks-agents\ndbutils.library.restartPython()\n## Run the above in a separate cell ##\n\nimport pandas as pd\nimport mlflow\n\nevals = [\n    {\n        &quot;request&quot;: {\n            &quot;messages&quot;: [\n                {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;How do I convert a Spark DataFrame to Pandas?&quot;}\n            ],\n        },\n        # Optional, needed for judging correctness.\n        &quot;expected_facts&quot;: [\n            &quot;To convert a Spark DataFrame to Pandas, you can use the toPandas() method.&quot;\n        ],\n    }\n]\neval_result = mlflow.evaluate(\n    data=pd.DataFrame.from_records(evals), model=&quot;runs:/1/model&quot;, model_type=&quot;databricks-agent&quot;\n)\n</code></pre>\n    </div>\n  </div>\n  <script>\n    var codeShown = false;\n    function clip(el) {\n      var range = document.createRange();\n      range.selectNodeContents(el);\n      var sel = window.getSelection();\n      sel.removeAllRanges();\n      sel.addRange(range);\n    }\n\n    function toggleCode() {\n      if (codeShown) {\n        document.getElementById("code").style.display = "none";\n        codeShown = false;\n      } else ', ' </div>\n  </div>\n  <script>\n    var codeShown = false;\n    function clip(el) {\n      var range = document.createRange();\n      range.selectNodeContents(el);\n      var sel = window.getSelection();\n      sel.removeAllRanges();\n      sel.addRange(range);\n    }\n\n    function toggleCode() {\n      if (codeShown) {\n        document.getElementById("code").style.display = "none";\n        codeShown = false;\n      } else {\n        document.getElementById("code").style.display = "block";\n        clip(document.querySelector("pre.active"));\n        codeShown = true;\n      }\n      document.getElementById("caret").classList.toggle("caret-up");\n    }\n\n    function tabClicked(tabIndex) {\n      document.querySelectorAll(".tab").forEach((tab, index) => {\n        if (index === tabIndex) {\n          tab.classList.add("active");\n        } else {\n          tab.classList.remove("active");\n        }\n      });\n      document.querySelectorAll("pre").forEach((pre, index) => {\n        if (index === tabIndex) {\n          pre.classList.add("active");\n        } else {\n          pre.classList.remove("active");\n        }\n      });\n      clip(document.querySelector("pre.active"));\n    }\n  </script>\n</body>\n', "@import 'reset.css';\n@import 'common/components/EditableNote.css';\n@import 'model-registry/index.css';\n\na {\n  color: #2374bb;\n}\na:hover,\na:focus {\n  color: #005580;\n}\n\nbody {\n  margin: 0;\n  padding: 0;\n}\n\n#root {\n  height: 100%;\n  display: flex;\n  flex-direction: column;\n}\n", "[class^=ant-]::-ms-clear,\n[class*= ant-]::-ms-clear,\n[class^=ant-] input::-ms-clear,\n[class*= ant-] input::-ms-clear,\n[class^=ant-] input::-ms-reveal,\n[class*= ant-] input::-ms-reveal {\n  display: none;\n}\nhtml,\nbody {\n  width: 100%;\n  height: 100%;\n}\ninput::-ms-clear,\ninput::-ms-reveal {\n  display: none;\n}\n*,\n*::before,\n*::after {\n  box-sizing: border-box;\n}\nhtml {\n  font-family: sans-serif;\n  line-height: 1.15;\n  -webkit-text-size-adjust: 100%;\n  -ms-text-size-adjust: 100%;\n  -ms-overflow-style: scrollbar;\n  -webkit-tap-highlight-color: rgba(0, 0, 0, 0);\n}\n@-ms-viewport {\n  width: device-width;\n}\nbody {\n  margin: 0;\n  color: rgba(0, 0, 0, 0.85);\n  font-size: 14px;\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, 'Noto Sans', sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n  font-variant: tabular-nums;\n  line-height: 1.5715;\n  background-color: #fff;\n  font-feature-settings: 'tnum';\n}\n[tabindex='-1']:focus {\n  outline: none !important;\n}\nhr {\n  box-sizing: content-box;\n  height: 0;\n  overflow: visible;\n}\nh1,\nh2,\nh3,\nh4,\nh5,\nh6 {\n  margin-top: 0;\n  margin-bottom: 0.5em;\n  font-weight: 500;\n}\np {\n  margin-top: 0;\n  margin-bottom: 1em;\n}\nabbr[title],\nabbr[data-original-title] {\n  text-decoration: underline;\n  -webkit-text-decoration: underline dotted;\n          text-decoration: underline dotted;\n  border-bottom: 0;\n  cursor: help;\n}\naddress {\n  margin-bottom: 1em;\n  font-style: normal;\n  line-height: inherit;\n}\ninput[type='text'],\ninput[type='password'],\ninput[type='number'],\ntextarea {\n  -webkit-appearance: none;\n}\nol,\nul,\ndl {\n  margin-top: 0;\n  margin-bottom: 1em;\n}\nol ol,\nul ul,\nol ul,\nul ol {\n  margin-bottom: 0;\n}\ndt {\n  font-weight: 500;\n}\ndd {\n  margin-bottom: 0.5em;\n  margin-left: 0;\n}\nblockquote {\n  margin: 0 0 1em;\n}\ndfn {\n  font-style: italic;\n}\nb,\nstrong {\n  font-weight: bolder;\n}\nsmall {\n  font-size: 80%;\n}\nsub,\nsup {\n  position: relative;\n  font-size: 75%;\n  line-height: 0;\n  vertical-align: baseline;\n}\nsub {\n  bottom: -0.25em;\n}\nsup {\n  top: -0.5em;\n}\na {\n  color: #1890ff;\n  text-decoration: none;\n  background-color: transparent;\n  outline: none;\n  cursor: pointer;\n  transition: color 0.3s;\n  -webkit-text-decoration-skip: objects;\n}\na:hover {\n  color: #40a9ff;\n}\na:active {\n  color: #096dd9;\n}\na:active,\na:hover {\n  text-decoration: none;\n  outline: 0;\n}\na:focus {\n  text-decoration: none;\n  outline: 0;\n}\na[disabled] {\n  color: rgba(0, 0, 0, 0.25);\n  cursor: not-allowed;\n}\npre,\ncode,\nkbd,\nsamp {\n  font-size: 1em;\n  font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;\n}\npre {\n  margin-top: 0;\n  margin-bottom: 1em;\n  overflow: auto;\n}\nfigure {\n  margin: 0 0 1em;\n}\nimg {\n  vertical-align: middle;\n  border-style: none;\n}\nsvg:not(:root) {\n  overflow: hidden;\n}\na,\narea,\nbutton,\n[role='button'],\ninput:not([type='range']),\nlabel,\nselect,\nsummary,\ntextarea {\n  touch-action: manipulation;\n}\ntable {\n  border-collapse: collapse;\n}\ncaption {\n  padding-top: 0.75em;\n  padding-bottom: 0.3em;\n ", ' outline: none;\n  cursor: pointer;\n  transition: color 0.3s;\n  -webkit-text-decoration-skip: objects;\n}\na:hover {\n  color: #40a9ff;\n}\na:active {\n  color: #096dd9;\n}\na:active,\na:hover {\n  text-decoration: none;\n  outline: 0;\n}\na:focus {\n  text-decoration: none;\n  outline: 0;\n}\na[disabled] {\n  color: rgba(0, 0, 0, 0.25);\n  cursor: not-allowed;\n}\npre,\ncode,\nkbd,\nsamp {\n  font-size: 1em;\n  font-family: \'SFMono-Regular\', Consolas, \'Liberation Mono\', Menlo, Courier, monospace;\n}\npre {\n  margin-top: 0;\n  margin-bottom: 1em;\n  overflow: auto;\n}\nfigure {\n  margin: 0 0 1em;\n}\nimg {\n  vertical-align: middle;\n  border-style: none;\n}\nsvg:not(:root) {\n  overflow: hidden;\n}\na,\narea,\nbutton,\n[role=\'button\'],\ninput:not([type=\'range\']),\nlabel,\nselect,\nsummary,\ntextarea {\n  touch-action: manipulation;\n}\ntable {\n  border-collapse: collapse;\n}\ncaption {\n  padding-top: 0.75em;\n  padding-bottom: 0.3em;\n  color: rgba(0, 0, 0, 0.45);\n  text-align: left;\n  caption-side: bottom;\n}\ninput,\nbutton,\nselect,\noptgroup,\ntextarea {\n  margin: 0;\n  color: inherit;\n  font-size: inherit;\n  font-family: inherit;\n  line-height: inherit;\n}\nbutton,\ninput {\n  overflow: visible;\n}\nbutton,\nselect {\n  text-transform: none;\n}\nbutton,\nhtml [type="button"],\n[type="reset"],\n[type="submit"] {\n  -webkit-appearance: button;\n}\nbutton::-moz-focus-inner,\n[type=\'button\']::-moz-focus-inner,\n[type=\'reset\']::-moz-focus-inner,\n[type=\'submit\']::-moz-focus-inner {\n  padding: 0;\n  border-style: none;\n}\ninput[type=\'radio\'],\ninput[type=\'checkbox\'] {\n  box-sizing: border-box;\n  padding: 0;\n}\ninput[type=\'date\'],\ninput[type=\'time\'],\ninput[type=\'datetime-local\'],\ninput[type=\'month\'] {\n  -webkit-appearance: listbox;\n}\ntextarea {\n  overflow: auto;\n  resize: vertical;\n}\nfieldset {\n  min-width: 0;\n  margin: 0;\n  padding: 0;\n  border: 0;\n}\nlegend {\n  display: block;\n  width: 100%;\n  max-width: 100%;\n  margin-bottom: 0.5em;\n  padding: 0;\n  color: inherit;\n  font-size: 1.5em;\n  line-height: inherit;\n  white-space: normal;\n}\nprogress {\n  vertical-align: baseline;\n}\n[type=\'number\']::-webkit-inner-spin-button,\n[type=\'number\']::-webkit-outer-spin-button {\n  height: auto;\n}\n[type=\'search\'] {\n  outline-offset: -2px;\n  -webkit-appearance: none;\n}\n[type=\'search\']::-webkit-search-cancel-button,\n[type=\'search\']::-webkit-search-decoration {\n  -webkit-appearance: none;\n}\n::-webkit-file-upload-button {\n  font: inherit;\n  -webkit-appearance: button;\n}\noutput {\n  display: inline-block;\n}\nsummary {\n  display: list-item;\n}\ntemplate {\n  display: none;\n}\n[hidden] {\n  display: none !important;\n}\nmark {\n  padding: 0.2em;\n  background-color: #feffe6;\n}\n::-moz-selection {\n  color: #fff;\n  background: #1890ff;\n}\n::selection {\n  color: #fff;\n  background: #1890ff;\n}\n.clearfix::before {\n  display: table;\n  content: \'\';\n}\n.clearfix::after {\n  display: table;\n  clear: both;\n  content: \'\';\n}\n', '.mlflow-editable-note-actions {\n  margin-top: 16px;\n}\n\n.mlflow-editable-note-actions button + button {\n  margin-left: 16px;\n}\n\n.mde-header {\n  background: none;\n}\n', '.mlflow-center {\n  text-align: center;\n}\n\n.mlflow-error-image {\n  margin: 12% auto 60px;\n  display: block;\n}\n', '/* Styles for antd `copyable` code snippets */\n\n.mlflow-ui-container .code-keyword {\n  color: rgb(204, 120, 50);\n}\n.mlflow-ui-container .code {\n  color: rgb(100, 110, 120);\n}\n.mlflow-ui-container .code-comment {\n  color: rgb(140, 140, 140);\n}\n.mlflow-ui-container .code-string {\n  color: rgb(106, 165, 89);\n}\n.mlflow-ui-container .code-number {\n  color: rgb(104, 151, 187);\n}\n', 'div.mlflow-artifact-view {\n  display: flex;\n  overflow: hidden;\n}\n\n.mlflow-artifact-left {\n  min-width: 200px;\n  max-width: 400px;\n  flex: 1;\n}\n\n.mlflow-artifact-left li {\n  white-space: nowrap;\n}\n\n.mlflow-artifact-right {\n  flex: 3;\n  min-width: 400px;\n  max-width: calc(100% - 200px); /* 200px is the min-width of .mlflow-artifact-left */\n\n  overflow: hidden;\n  display: flex;\n  flex-direction: column;\n  height: 100%;\n}\n\n.mlflow-artifact-info-left {\n  flex: 1;\n  max-width: 75%;\n}\n.mlflow-artifact-info-right {\n  margin-left: auto;\n}\n\n.mlflow-artifact-info-path {\n  display: flex;\n  align-items: center;\n}\n\n.mlflow-artifact-info-text {\n  min-width: 0;\n}\n\n.mlflow-artifact-info-size {\n  overflow: hidden;\n  text-overflow: ellipsis;\n}\n\n.mlflow-loading-spinner {\n  height: 20px;\n  opacity: 0;\n  -webkit-animation: spin 3s linear infinite;\n  -moz-animation: spin 3s linear infinite;\n  animation: spin 3s linear infinite;\n}\n\n.mlflow-artifact-info-right .model-version-link {\n  display: flex;\n  align-items: baseline;\n  max-width: 140px;\n  padding-top: 1px;\n  padding-left: 4px;\n}\n\n.mlflow-artifact-info-right .model-version-link .model-name {\n  overflow: hidden;\n  text-overflow: ellipsis;\n}\n\n.mlflow-artifact-info-right .model-version-info {\n  font-size: 12px;\n}\n\n.mlflow-artifact-info-right .model-version-info .model-version-link-section {\n  display: flex;\n  align-items: center;\n}\n\n.mlflow-artifact-info-right .model-version-info .model-version-status-text {\n  overflow: hidden;\n  max-width: 160px;\n  text-overflow: ellipsis;\n}\n', '.mlflow-sticky-header {\n  position: sticky;\n  position: -webkit-sticky;\n  left: 0;\n}\n\n.mlflow-compare-run-table {\n  display: block;\n  overflow: auto;\n  width: 100%;\n}\n\n.mlflow-compare-table th.inter-title {\n  padding: 20px 0 0;\n  background: transparent;\n}\n\n.mlflow-compare-table .head-value {\n  overflow: hidden;\n  overflow-wrap: break-word;\n}\n\n.mlflow-compare-table td.data-value,\n.mlflow-compare-table th.data-value {\n  overflow: hidden;\n  max-width: 120px;\n  text-overflow: ellipsis;\n}\n\n.mlflow-responsive-table-container {\n  width: 100%;\n  overflow-x: auto;\n}\n\n.mlflow-compare-table .diff-row .data-value {\n  background-color: rgba(249, 237, 190, 0.5);\n  color: #555;\n}\n\n.mlflow-compare-table .diff-row .head-value {\n  background-color: rgba(249, 237, 190, 1);\n  color: #555;\n}\n\n.mlflow-compare-table .diff-row:hover {\n  background-color: rgba(249, 237, 190, 1);\n  color: #555;\n}\n\n/* Overrides to make it look more like antd */\n.mlflow-compare-table {\n  width: 100%;\n  max-width: 100%;\n  margin-bottom: 20px;\n}\n.mlflow-compare-table th,\n.mlflow-compare-table td {\n  padding: 12px 8px;\n  border-bottom: 1px solid #e8e8e8;\n}\n.mlflow-compare-table th {\n  color: rgba(0, 0, 0, 0.85);\n  font-weight: 500;\n  background-color: rgb(250, 250, 250);\n  text-align: left;\n}\n.mlflow-compare-table > tbody > tr:hover:not(.diff-row) > td:not(.highlight-data) {\n  background-color: rgb(250, 250, 250);\n}\n', '/* Overriding the table styles since antd tables take the full screen by default.\nWe would like to change it to auto to automatically grow based on the columns */\n\n.mlflow-html-table-view table {\n  width: auto;\n  min-width: 400px;\n}\n\n.mlflow-html-table-view th {\n  width: auto;\n  min-width: 200px;\n  margin-right: 80px;\n  font-size: 13px;\n  color: #888;\n}\n', '.mlflow-metrics-plot-container {\n  display: flex;\n  width: 100%;\n  align-items: flex-start;\n}\n\n.mlflow-metrics-plot-container .plot-controls {\n  display: flex;\n  flex-direction: column;\n  min-height: 500px;\n}\n\n.mlflow-metrics-plot-container .plot-controls .inline-control {\n  margin-top: 25px;\n  display: flex;\n  align-items: center;\n}\n\n.mlflow-metrics-plot-container .plot-controls .inline-control .control-label {\n  margin-right: 10px;\n}\n\n.mlflow-metrics-plot-container .plot-controls .block-control {\n  margin-top: 25px;\n}\n\n.mlflow-metrics-plot-container .metrics-plot-data {\n  flex: 1;\n  display: flex;\n  flex-direction: column;\n}\n\n.mlflow-metrics-plot-container .metrics-plot-view-container {\n  min-height: 500px;\n  flex: 1;\n}\n\n.mlflow-metrics-plot-container .metrics-summary {\n  margin: 20px 20px 20px 60px;\n}\n\n.mlflow-metrics-plot-container .metrics-summary .mlflow-html-table-view {\n  margin-bottom: 25px;\n  /* Shrink to fit, so that scroll bars are aligned with the edge of the table */\n  display: inline-block;\n}\n\n/* Reset min-width which is overridden in HtmlTableView as this breaks the\n   table layout when scrolling is enabled and widths are specified */\n.mlflow-metrics-plot-container .metrics-summary .mlflow-html-table-view th {\n  min-width: auto;\n}\n', '.mlflow-html-iframe {\n  border: none;\n}\n\n.mlflow-artifact-html-view {\n  width: 100%;\n  height: 100%;\n  overflow: auto;\n}\n', '.mlflow-show-artifact-logged-model-view {\n  width: 100%;\n  height: 100%;\n  overflow: auto;\n}\n', '.mlflow-ui-container .map-container {\n  height: 100%;\n  width: 100%;\n}\n\n.mlflow-ui-container .leaflet-container {\n  height: 100%;\n  width: 100%;\n}\n', '.mlflow-pdf-outer-container {\n  display: flex;\n  justify-content: center;\n  align-items: center;\n  height: 100%;\n  width: 100%;\n  padding-left: 16px;\n  overflow: hidden;\n}\n.mlflow-pdf-viewer {\n  display: flex;\n  flex-direction: column;\n  overflow-y: scroll;\n  height: 100%;\n}\n\n.mlflow-paginator {\n  display: flex;\n  justify-content: center;\n  align-items: center;\n  position: sticky;\n  z-index: 1001;\n  top: 0;\n  padding-bottom: 15px;\n  background-color: rgba(250, 250, 250, 0.6);\n  padding-top: 10px;\n}\n\n.mlflow-document {\n  display: flex;\n  justify-content: center;\n  align-items: center;\n}\n', '.mlflow-ShowArtifactPage .text-area {\n  box-sizing: border-box;\n  width: 100%;\n  height: 100%;\n  font-family: Menlo, Consolas, monospace;\n}\n\n.mlflow-ShowArtifactPage,\n.mlflow-ShowArtifactPage .text-area-border-box {\n  width: 100%;\n  height: 100%;\n  overflow: hidden;\n}\n', '.mlflow-ui-container .parcoords > svg,\n.mlflow-ui-container .parcoords > canvas {\n  overflow: visible;\n}\n\n.mlflow-ui-container .parcoords svg text.label {\n  cursor: pointer;\n}\n\n.mlflow-ui-container .parcoords svg g.axis-label-tooltip rect {\n  outline: 1px solid black;\n}\n\n.mlflow-ui-container .parcoords svg g.axis-label-tooltip {\n  visibility: hidden;\n  pointer-events: none;\n}\n.mlflow-ui-container .parcoords svg text.label:hover:not(:active) + g.axis-label-tooltip {\n  visibility: visible;\n}\n\n.mlflow-ui-container .parcoords svg g.tick-label-tooltip rect {\n  outline: 1px solid black;\n}\n\n.mlflow-ui-container .parcoords svg g.tick-label-tooltip {\n  visibility: hidden;\n  pointer-events: none;\n}\n.mlflow-ui-container .parcoords svg text:hover:not(:active) + g.tick-label-tooltip {\n  visibility: visible;\n}\n', "@import 'components/ModelVersionTable.css';\n@import 'components/ModelVersionView.css';\n@import 'components/ModelStageTransitionDropdown.css';\n\n/** TODO(Zangr) migrate globally common components and styles into src/common folder */\n.mlflow-metadata-container {\n  display: flex;\n  flex-wrap: wrap;\n  align-items: center;\n}\n\n.mlflow-metadata-entry {\n  margin-bottom: 16px;\n}\n\n.mlflow-icon-fail {\n  color: red;\n}\n", '.sticky-header {\n  position: sticky;\n  left: 0;\n}\n\n.compare-model-table {\n  display: block;\n  overflow: auto;\n  width: 100%;\n}\n\n.compare-table-row {\n  display: inline-flex;\n}\n\n.compare-table .head-value {\n  overflow: hidden;\n  overflow-wrap: break-word;\n  z-index: 10;\n}\n\n.compare-table .diff-row .data-value {\n  background-color: rgba(249, 237, 190, 0.5) ;\n  color: #555;\n}\n\n.compare-table .diff-row:hover,\n.compare-table .diff-row .head-value,\n.compare-table .diff-row .head-value > span {\n  background-color: rgba(249, 237, 190, 1.0) ;\n  color: #555;\n}\n', '.mlflow-pagination-section {\n  padding-bottom: 30px;\n}\n\n.mlflow-ui-container .ant-alert-info .ant-alert-icon {\n  color: #00b379;\n}\n\n.mlflow-search-input-tooltip .du-bois-light-popover-inner .du-bois-light-popover-inner-content {\n  background-color: rgba(0, 0, 0, 0.75);\n  color: white;\n  border-radius: 4px;\n}\n\n.mlflow-search-input-tooltip .du-bois-light-popover-arrow-content {\n  background-color: rgba(0, 0, 0, 0.75);\n}\n', '.mlflow-stage-transition-dropdown .ant-tag {\n  cursor: pointer;\n  border-radius: 4px;\n}\n', '.mlflow-table-endpoint-text {\n  white-space: nowrap;\n  text-overflow: ellipsis;\n  display: block;\n  overflow: hidden;\n}\n', '/* >>> Extract to our own Alert wrapper component */\n.mlflow-status-alert {\n  margin-bottom: 16px;\n  border-radius: 2px;\n}\n\n.mlflow-status-alert .model-version-status-icon {\n  margin-left: -3px;\n}\n\n.mlflow-status-alert.mlflow-status-alert-info {\n  border-left: 2px solid #3895d3;\n}\n\n.mlflow-status-alert.mlflow-status-alert-error {\n  border-left: 2px solid red;\n}\n\n.mlflow-version-follow-icon {\n  margin-left: auto;\n}\n\n.ant-popover-content {\n  max-width: 500px;\n}\n/* <<< Extract to our own Alert wrapper component */\n', '.mlflow-model-select-dropdown .ant-select-dropdown-menu-item-group-title {\n  color: #666;\n  font-weight: bold;\n}\n\n.mlflow-model-select-dropdown .mlflow-create-new-model-option {\n  border-top: 1px solid #ccc;\n}\n\n.mlflow-register-model-form .modal-explanatory-text {\n  color: rgba(0, 0, 0, 0.52);\n  font-size: 13px;\n}\n', "/* Replaceing AntD Image */\n.mlflow-ui-container .rc-image-preview {\n  height: 100%;\n  pointer-events: none;\n  text-align: center;\n}\n\n.mlflow-ui-container .rc-image-preview-mask {\n  position: fixed;\n  top: 0;\n  left: 0;\n  right: 0;\n  bottom: 0;\n  height: 100%;\n  background-color: rgba(0, 0, 0, 0.45);\n  z-index: 1000;\n}\n\n.mlflow-ui-container .rc-image-preview-mask img {\n  max-width: 100%;\n  max-height: 100%;\n  position: absolute;\n  top: 50%;\n  left: 50%;\n  transform: translate(-50%, -50%);\n}\n\n.mlflow-ui-container .rc-image-preview-mask {\n  background-color: rgba(0, 0, 0, 0.45);\n  bottom: 0;\n  height: 100%;\n  left: 0;\n  position: fixed;\n  right: 0;\n  top: 0;\n  z-index: 1000;\n}\n\n.mlflow-ui-container .rc-image-preview-mask-hidden {\n  display: none;\n}\n\n.mlflow-ui-container .rc-image-preview-wrap {\n  -webkit-overflow-scrolling: touch;\n  bottom: 0;\n  left: 0;\n  outline: 0;\n  overflow: auto;\n  position: fixed;\n  right: 0;\n  top: 0px;\n}\n\n.mlflow-ui-container .rc-image-preview-body {\n  bottom: 0;\n  left: 0;\n  overflow: hidden;\n  position: absolute;\n  right: 0;\n  top: 0;\n}\n\n.mlflow-ui-container .rc-image-preview-img {\n  cursor: grab;\n  max-height: 100%;\n  max-width: 100%;\n  pointer-events: auto;\n  transform: scaleX(1);\n  -webkit-user-select: none;\n  -moz-user-select: none;\n  -ms-user-select: none;\n  user-select: none;\n  vertical-align: middle;\n}\n\n.mlflow-ui-container .rc-image-preview-img,\n.mlflow-ui-container .rc-image-preview-img-wrapper {\n  transition: transform 0.3s cubic-bezier(0.215, 0.61, 0.355, 1) 0s;\n}\n\n.mlflow-ui-container .rc-image-preview-img-wrapper {\n  bottom: 0;\n  left: 0;\n  position: absolute;\n  right: 0;\n  top: 0;\n}\n\n.mlflow-ui-container .rc-image-preview-img-wrapper:before {\n  content: '';\n  display: inline-block;\n  height: 50%;\n  margin-right: -1px;\n  width: 1px;\n}\n\n.mlflow-ui-container .rc-image-preview-moving .mlflow-ui-container .rc-image-preview-img {\n  cursor: grabbing;\n}\n\n.mlflow-ui-container .rc-image-preview-moving .mlflow-ui-container .rc-image-preview-img-wrapper {\n  transition-duration: 0s;\n}\n\n.mlflow-ui-container .rc-image-preview-wrap {\n  z-index: 1080;\n}\n\n.mlflow-ui-container .rc-image-preview-operations {\n  font-feature-settings: 'tnum', 'tnum';\n  align-items: center;\n  background: rgba(0, 0, 0, 0.1);\n  box-sizing: border-box;\n  color: rgba(0, 0, 0, 0.85);\n  color: hsla(0, 0%, 100%, 0.85);\n  display: flex;\n  flex-direction: row-reverse;\n  font-size: 14px;\n  font-variant: tabular-nums;\n  line-height: 1.5715;\n  list-style: none;\n  margin: 0;\n  padding: 0;\n  pointer-events: auto;\n  position: absolute;\n  right: 0;\n  top: 0;\n  width: 100%;\n  z-index: 1;\n}\n\n.mlflow-ui-container .rc-image-preview-operations-operation {\n  cursor: pointer;\n  margin-left: 12px;\n  padding: 12px;\n}\n\n.mlflow-ui-container .rc-image-preview-operations-operation-disabled {\n  color: hsla(0, 0%, 100%, 0.25);\n  pointer-events: none;\n}\n\n.mlflow-ui-container .rc-image-preview-operations-operation:last-of-type {\n  margin-left: 0;\n}\n\n.mlflow-ui-container .rc-image-preview-operations-icon {\n  font-size: 18px;\n}\n\n.mlflow-ui-container .rc-image-preview-switch-left,\n.mlflow-ui-container .rc-image-preview-switch-right ", ' align-items: center;\n  background: rgba(0, 0, 0, 0.1);\n  box-sizing: border-box;\n  color: rgba(0, 0, 0, 0.85);\n  color: hsla(0, 0%, 100%, 0.85);\n  display: flex;\n  flex-direction: row-reverse;\n  font-size: 14px;\n  font-variant: tabular-nums;\n  line-height: 1.5715;\n  list-style: none;\n  margin: 0;\n  padding: 0;\n  pointer-events: auto;\n  position: absolute;\n  right: 0;\n  top: 0;\n  width: 100%;\n  z-index: 1;\n}\n\n.mlflow-ui-container .rc-image-preview-operations-operation {\n  cursor: pointer;\n  margin-left: 12px;\n  padding: 12px;\n}\n\n.mlflow-ui-container .rc-image-preview-operations-operation-disabled {\n  color: hsla(0, 0%, 100%, 0.25);\n  pointer-events: none;\n}\n\n.mlflow-ui-container .rc-image-preview-operations-operation:last-of-type {\n  margin-left: 0;\n}\n\n.mlflow-ui-container .rc-image-preview-operations-icon {\n  font-size: 18px;\n}\n\n.mlflow-ui-container .rc-image-preview-switch-left,\n.mlflow-ui-container .rc-image-preview-switch-right {\n  align-items: center;\n  background: rgba(0, 0, 0, 0.1);\n  border-radius: 50%;\n  color: hsla(0, 0%, 100%, 0.85);\n  cursor: pointer;\n  display: flex;\n  height: 44px;\n  justify-content: center;\n  margin-top: -22px;\n  pointer-events: auto;\n  position: absolute;\n  right: 10px;\n  top: 50%;\n  width: 44px;\n  z-index: 1;\n}\n\n.mlflow-ui-container .rc-image-preview-switch-left-disabled,\n.mlflow-ui-container .rc-image-preview-switch-right-disabled {\n  color: hsla(0, 0%, 100%, 0.25);\n  cursor: not-allowed;\n}\n\n.mlflow-ui-container .rc-image-preview-switch-left-disabled > .anticon,\n.mlflow-ui-container .rc-image-preview-switch-right-disabled > .anticon {\n  cursor: not-allowed;\n}\n\n.mlflow-ui-container .rc-image-preview-switch-left > .anticon,\n.mlflow-ui-container .rc-image-preview-switch-right > .anticon {\n  font-size: 18px;\n}\n\n.mlflow-ui-container .rc-image-preview-switch-left {\n  left: 10px;\n}\n\n.mlflow-ui-container .rc-image-preview-switch-right {\n  right: 10px;\n}\n\n.mlflow-ui-container .fade-enter,\n.mlflow-ui-container .fade-appear {\n  animation-duration: 0.3s;\n  animation-fill-mode: both;\n  animation-play-state: paused;\n}\n.mlflow-ui-container .fade-leave {\n  animation-duration: 0.3s;\n  animation-fill-mode: both;\n  animation-play-state: paused;\n}\n.mlflow-ui-container .fade-enter.fade-enter-active,\n.mlflow-ui-container .fade-appear.fade-appear-active {\n  animation-name: mlflow-rcImageFadeIn;\n  animation-play-state: running;\n}\n.mlflow-ui-container .fade-leave.fade-leave-active {\n  animation-name: mlflow-rcImageFadeOut;\n  animation-play-state: running;\n  pointer-events: none;\n}\n.mlflow-ui-container .fade-enter,\n.mlflow-ui-container .fade-appear {\n  opacity: 0;\n  animation-timing-function: linear;\n}\n.mlflow-ui-container .fade-leave {\n  animation-timing-function: linear;\n}\n\n@keyframes mlflow-rcImageFadeIn {\n  0% {\n    opacity: 0;\n  }\n  100% {\n    opacity: 1;\n  }\n}\n\n@keyframes mlflow-rcImageFadeOut {\n  0% {\n    opacity: 1;\n  }\n  100% {\n    opacity: 0;\n  }\n}\n\n.mlflow-ui-container .zoom-enter,\n.mlflow-ui-container .zoom-appear {\n  animation-duration: 0.3s;\n  animation-fill-mode: both;\n  animation-play-state: paused;\n}\n.mlflow-ui-container .zoom-leave {\n  animation-duration: 0.3s;\n  animation-fill-mode: both;\n  animation-play-state: paused;\n}\n.mlflow-ui-container .zoom-enter.zoom-enter-active,\n.mlflow-ui-container .zoom-appear.zoom-appear-active {\n  animation-name: mlflow-rcImageZoomIn;\n  animation-play-state: running;\n}\n.mlflow-ui-container .zoom-leave.zoom-leave-active {\n  animation-name: mlflow-rcImageZoomOut;\n  animation-play-state: running;\n  pointer-events: none;\n}\n.mlflow-ui-container .zoom-enter,\n.mlflow-ui-container .zoom-appear {\n  transform: scale(0);\n  opacity: 0;\n  animation-timing-function: cubic-bezier(0.08, ', 'linear;\n}\n\n@keyframes mlflow-rcImageFadeIn {\n  0% {\n    opacity: 0;\n  }\n  100% {\n    opacity: 1;\n  }\n}\n\n@keyframes mlflow-rcImageFadeOut {\n  0% {\n    opacity: 1;\n  }\n  100% {\n    opacity: 0;\n  }\n}\n\n.mlflow-ui-container .zoom-enter,\n.mlflow-ui-container .zoom-appear {\n  animation-duration: 0.3s;\n  animation-fill-mode: both;\n  animation-play-state: paused;\n}\n.mlflow-ui-container .zoom-leave {\n  animation-duration: 0.3s;\n  animation-fill-mode: both;\n  animation-play-state: paused;\n}\n.mlflow-ui-container .zoom-enter.zoom-enter-active,\n.mlflow-ui-container .zoom-appear.zoom-appear-active {\n  animation-name: mlflow-rcImageZoomIn;\n  animation-play-state: running;\n}\n.mlflow-ui-container .zoom-leave.zoom-leave-active {\n  animation-name: mlflow-rcImageZoomOut;\n  animation-play-state: running;\n  pointer-events: none;\n}\n.mlflow-ui-container .zoom-enter,\n.mlflow-ui-container .zoom-appear {\n  transform: scale(0);\n  opacity: 0;\n  animation-timing-function: cubic-bezier(0.08, 0.82, 0.17, 1);\n}\n.mlflow-ui-container .zoom-leave {\n  animation-timing-function: cubic-bezier(0.78, 0.14, 0.15, 0.86);\n}\n\n@keyframes mlflow-rcImageZoomIn {\n  0% {\n    transform: scale(0.2);\n    opacity: 0;\n  }\n  100% {\n    transform: scale(1);\n    opacity: 1;\n  }\n}\n\n@keyframes mlflow-rcImageZoomOut {\n  0% {\n    transform: scale(1);\n  }\n  100% {\n    transform: scale(0.2);\n    opacity: 0;\n  }\n}\n', "input,\nselect,\noption,\nbutton,\ntextarea,\nbody,\nthead {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, 'Noto Sans', sans-serif,\n    'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n}\n\nbody,\nthead {\n  font-size: 13px;\n  line-height: 18px;\n  font-weight: 400;\n  box-shadow: none;\n}\n\nhtml,\nbody,\npre,\ncode {\n  margin: 0;\n  padding: 0;\n}\n\nbody {\n  min-height: 100vh;\n}\n\nhtml {\n  overflow-y: hidden;\n}\n", '# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n**For contribution guidelines, code standards, and additional development information not covered here, please refer to [CONTRIBUTING.md](./CONTRIBUTING.md).**\n\n## Repository Overview\n\nMLflow is an open-source platform for managing the end-to-end machine learning lifecycle. It provides tools for:\n\n- Experiment tracking\n- Model versioning and deployment\n- LLM observability and tracing\n- Model evaluation\n- Prompt management\n\n## Quick Start: Development Server\n\n### Start the Full Development Environment (Recommended)\n\n```bash\n# Kill any existing servers\npkill -f "mlflow server" || true; pkill -f "yarn start" || true\n\n# Start both MLflow backend and React frontend dev servers\nnohup uv run bash dev/run-dev-server.sh > /tmp/mlflow-dev-server.log 2>&1 &\n\n# Monitor the logs\ntail -f /tmp/mlflow-dev-server.log\n\n# Servers will be available at:\n# - MLflow backend: http://localhost:5000\n# - React frontend: http://localhost:3000\n```\n\nThis uses `uv` (fast Python package manager) to automatically manage dependencies and run the development environment.\n\n### Start Development Server with Databricks Backend\n\nTo run the MLflow dev server that proxies requests to a Databricks workspace:\n\n```bash\n# IMPORTANT: All four environment variables below are REQUIRED for proper Databricks backend operation\n# Set them in this exact order:\nexport DATABRICKS_HOST="https://your-workspace.databricks.com"  # Your Databricks workspace URL\nexport DATABRICKS_TOKEN="your-databricks-token"                # Your Databricks personal access token\nexport MLFLOW_TRACKING_URI="databricks"                        # Must be set to "databricks"\nexport MLFLOW_REGISTRY_URI="databricks-uc"                     # Use "databricks-uc" for Unity Catalog, or "databricks" for workspace model registry\n\n# Start the dev server with these environment variables\nnohup uv run bash dev/run-dev-server.sh > /tmp/mlflow-dev-server.log 2>&1 &\n\n# Monitor the logs\ntail -f /tmp/mlflow-dev-server.log\n\n# The MLflow server will now proxy tracking and model registry requests to Databricks\n# Access the UI at http://localhost:3000 to see your Databricks experiments and models\n```\n\n**Note**: The MLflow server acts as a proxy, forwarding API requests to your Databricks workspace while serving the local React frontend. This allows you to develop and test UI changes against real Databricks data.\n\n## Development Commands\n\n### Testing\n\n```bash\n# First-time setup: ', '    # Use "databricks-uc" for Unity Catalog, or "databricks" for workspace model registry\n\n# Start the dev server with these environment variables\nnohup uv run bash dev/run-dev-server.sh > /tmp/mlflow-dev-server.log 2>&1 &\n\n# Monitor the logs\ntail -f /tmp/mlflow-dev-server.log\n\n# The MLflow server will now proxy tracking and model registry requests to Databricks\n# Access the UI at http://localhost:3000 to see your Databricks experiments and models\n```\n\n**Note**: The MLflow server acts as a proxy, forwarding API requests to your Databricks workspace while serving the local React frontend. This allows you to develop and test UI changes against real Databricks data.\n\n## Development Commands\n\n### Testing\n\n```bash\n# First-time setup: Install test dependencies\nuv sync\nuv pip install -r requirements/test-requirements.txt\n\n# Run Python tests\nuv run pytest tests/\n\n# Run specific test file\nuv run pytest tests/test_version.py\n\n# Run JavaScript tests\nyarn --cwd mlflow/server/js test\n```\n\n### Code Quality\n\n```bash\n# Python linting and formatting with Ruff\nuv run ruff check . --fix         # Lint with auto-fix\nuv run ruff format .              # Format code\n\n# Check for MLflow spelling typos\nuv run bash dev/mlflow-typo.sh .\n\n# JavaScript linting and formatting\nyarn --cwd mlflow/server/js lint\nyarn --cwd mlflow/server/js prettier:check\nyarn --cwd mlflow/server/js prettier:fix\n\n# Type checking\nyarn --cwd mlflow/server/js type-check\n\n# Run all checks\nyarn --cwd mlflow/server/js check-all\n```\n\n### Special Testing\n\n```bash\n# Run tests with minimal dependencies (skinny client)\nuv run bash dev/run-python-skinny-tests.sh\n\n# Test in Docker container\nuv run bash dev/run-test-container.sh\n```\n\n### Documentation\n\n```bash\n# Build documentation site (needs gateway extras for API doc generation)\nuv run --all-extras bash dev/build-docs.sh --build-api-docs\n\n# Build with R docs included\nuv run --all-extras bash dev/build-docs.sh --build-api-docs --with-r-docs\n\n# Serve documentation locally (after building)\ncd docs && yarn serve --port 8080\n```\n\n## Important Files\n\n- `pyproject.toml`: Package configuration and tool settings\n- `.python-version`: Minimum Python version (3.10)\n- `requirements/`: Dependency specifications\n- `mlflow/ml-package-versions.yml`: Supported ML framework versions\n\n## Common Development Tasks\n\n### Modifying the UI\n\nSee `mlflow/server/js/` for frontend development.\n\n## Language-Specific Style Guides\n\n- [Python](/dev/guides/python.md)\n\n## Git Workflow\n\n### Committing Changes\n\n**IMPORTANT**: After making your commits, run pre-commit hooks on your PR changes to ensure code quality:\n\n```bash\n# Make your commit first (with DCO sign-off)\ngit commit -s -m "Your commit message"\n\n# Then check all files changed in your PR\nuv run pre-commit run --from-ref origin/master --to-ref HEAD\n\n# Fix any issues and amend your ', '--with-r-docs\n\n# Serve documentation locally (after building)\ncd docs && yarn serve --port 8080\n```\n\n## Important Files\n\n- `pyproject.toml`: Package configuration and tool settings\n- `.python-version`: Minimum Python version (3.10)\n- `requirements/`: Dependency specifications\n- `mlflow/ml-package-versions.yml`: Supported ML framework versions\n\n## Common Development Tasks\n\n### Modifying the UI\n\nSee `mlflow/server/js/` for frontend development.\n\n## Language-Specific Style Guides\n\n- [Python](/dev/guides/python.md)\n\n## Git Workflow\n\n### Committing Changes\n\n**IMPORTANT**: After making your commits, run pre-commit hooks on your PR changes to ensure code quality:\n\n```bash\n# Make your commit first (with DCO sign-off)\ngit commit -s -m "Your commit message"\n\n# Then check all files changed in your PR\nuv run pre-commit run --from-ref origin/master --to-ref HEAD\n\n# Fix any issues and amend your commit if needed\ngit add <fixed files>\ngit commit --amend -s\n\n# Re-run pre-commit to verify fixes\nuv run pre-commit run --from-ref origin/master --to-ref HEAD\n\n# Only push once all checks pass\ngit push origin <your-branch>\n```\n\nThis workflow ensures you only check files you\'ve actually modified in your PR, avoiding false positives from unrelated files.\n\n**IMPORTANT**: You MUST sign all commits with DCO (Developer Certificate of Origin). Always use the `-s` flag:\n\n```bash\n# REQUIRED: Always use -s flag when committing\ngit commit -s -m "Your commit message"\n\n# This will NOT work - missing -s flag\n# git commit -m "Your commit message"  âŒ\n```\n\nCommits without DCO sign-off will be rejected by CI.\n\n**Frontend Changes**: If your PR touches any code in `mlflow/server/js/`, you MUST run `yarn check-all` before committing:\n\n```bash\nyarn --cwd mlflow/server/js check-all\n```\n\n### Creating Pull Requests\n\nFollow [the PR template](./.github/pull_request_template.md) when creating pull requests. The template will automatically appear when you create a PR on GitHub.\n\n### Checking CI Status\n\nUse GitHub CLI to check for failing CI:\n\n```bash\n# Check workflow runs for current branch\ngh run list --branch $(git branch --show-current)\n\n# View details of a specific run\ngh run view <run-id>\n\n# Watch a run in progress\ngh run watch\n```\n\n## Pre-commit Hooks\n\nThe repository uses pre-commit for code quality. Install hooks with:\n\n```bash\nuv run pre-commit install --install-hooks\n```\n\nRun pre-commit manually:\n\n```bash\n# Run on all files\nuv run pre-commit run --all-files\n\n# Run on all files, skipping hooks that require external tools\nSKIP=taplo,typos,conftest uv run pre-commit run --all-files\n\n# Run on specific files\nuv run pre-commit run --files path/to/file.py\n\n# Run a specific hook\nuv run pre-commit run ruff --all-files\n```\n\nThis runs Ruff, typos checker, and other tools automatically before commits.\n\n**Note about external ', "failing CI:\n\n```bash\n# Check workflow runs for current branch\ngh run list --branch $(git branch --show-current)\n\n# View details of a specific run\ngh run view <run-id>\n\n# Watch a run in progress\ngh run watch\n```\n\n## Pre-commit Hooks\n\nThe repository uses pre-commit for code quality. Install hooks with:\n\n```bash\nuv run pre-commit install --install-hooks\n```\n\nRun pre-commit manually:\n\n```bash\n# Run on all files\nuv run pre-commit run --all-files\n\n# Run on all files, skipping hooks that require external tools\nSKIP=taplo,typos,conftest uv run pre-commit run --all-files\n\n# Run on specific files\nuv run pre-commit run --files path/to/file.py\n\n# Run a specific hook\nuv run pre-commit run ruff --all-files\n```\n\nThis runs Ruff, typos checker, and other tools automatically before commits.\n\n**Note about external tools**: Some pre-commit hooks require external tools that aren't Python packages:\n\n- `taplo` - TOML formatter\n- `typos` - Spell checker\n- `conftest` - Policy testing tool\n\nTo install these tools:\n\n```bash\n# Install all tools at once (recommended)\nuv run bin/install.py\n```\n\nThis automatically downloads and installs the correct versions of all external tools to the `bin/` directory. The tools work on both Linux and ARM Macs.\n\nThese tools are optional. Use `SKIP=taplo,typos,conftest` if they're not installed.\n\n**Note**: If the typos hook fails, you only need to fix typos in code that was changed by your PR, not pre-existing typos in the codebase.\n", '### Evaluation Criteria\n\nWhen evaluating potential new MLflow committers, the following criteria will be considered:\n\n- **Code Contributions**: Should have multiple non-trivial code contributions accepted and committed to the MLflow codebase. This demonstrates the ability to produce quality code aligned with the project\'s standards.\n- **Technical Expertise**: Should demonstrate a deep understanding of MLflow\'s architecture and design principles, evidenced by making appropriate design choices and technical recommendations. History of caring about code quality, testing, maintainability, and ability to critically evaluate technical artifacts (PRs, designs, etc.) and provide constructive suggestions for improvement.\n- **Subject Matter Breadth**: Contributions and learnings span multiple areas of the codebase, APIs, and integration points rather than a narrow niche.\n- **Community Participation**: Active participation for at least 3 months prior to nomination by authoring code contributions and engaging in the code review process. Involvement in mailing lists, Slack channels, Stack Overflow, and GitHub issues is valued but not strictly required.\n- **Communication**: Should maintain a constructive tone in communications, be receptive to feedback, and collaborate well with existing committers and other community members.\n- **Project Commitment**: Demonstrate commitment to MLflow\'s long-term success, uphold project principles and values, and willingness to pitch in for "unglamorous" work.\n\n### Committership Nomination\n\n- Any current MLflow committer can nominate a contributor for committership by emailing MLflow\'s TSC members with a nomination packet.\n- The nomination packet should provide details on the nominee\'s salient contributions, as well as justification on how they meet the evaluation criteria. Links to GitHub activity, mailing list threads, and other artifacts should be included.\n- In addition to the nominator, every nomination must have a seconder -- a separate committer who advocates for the nominee. The seconder should be a more senior committer (active committer for >1 year) familiar with the nominee\'s work.\n- It is the nominator\'s responsibility to identify a willing seconder and include their recommendation in the nomination packet.\n- If no eligible seconder is available or interested, it may indicate insufficient support to proceed with the nomination at that time. This ensures there are two supporting committers invested in each nomination - the nominator and the seconder. The seconder\'s seniority and familiarity with the situation ', "to the nominator, every nomination must have a seconder -- a separate committer who advocates for the nominee. The seconder should be a more senior committer (active committer for >1 year) familiar with the nominee's work.\n- It is the nominator's responsibility to identify a willing seconder and include their recommendation in the nomination packet.\n- If no eligible seconder is available or interested, it may indicate insufficient support to proceed with the nomination at that time. This ensures there are two supporting committers invested in each nomination - the nominator and the seconder. The seconder's seniority and familiarity with the situation also help build more consensus among the TSC members during evaluation.\n\n### Evaluation Process\n\n- When a committer nomination is made, the TSC members closely review the proposal and evaluate the nominee's qualifications.\n- Throughout the review, the nominator is responsible for addressing any questions from the TSC, and providing clarification or additional evidence as requested by TSC members.\n- After adequate discussion (~1 week), the nominator calls for a formal consensus check among the TSC.\n- A positive consensus requires at least 2 TSC +1 binding votes and no vetoes.\n- Any vetoes must be accompanied by a clear rationale that can be debated.\n- If consensus is not achieved, the nomination is rejected at that time.\n- If consensus fails, the nominator summarizes substantive feedback and remaining gaps to the nominee for their growth and potential re-nomination later. Nomination can be tried again in 3 months after addressing any gaps identified.\n\n### Onboarding a new committer\n\n- Upon a positive consensus being reached, one of the TSC members will extend the formal invitation to the nominee to become a committer. They also field the private initial response from the nominee on willingness to accept.\n- If the proposal is accepted, the nominator grants them the commit access and the new committer will be:\n  - Added to the committer list in the README.md\n  - Announced on the MLflow mailing lists, Slack channels, and the MLflow website\n  - Spotlighted through a post on the MLflow LinkedIn and X handles\n- The nominator will work with the new committer to ", 'a positive consensus being reached, one of the TSC members will extend the formal invitation to the nominee to become a committer. They also field the private initial response from the nominee on willingness to accept.\n- If the proposal is accepted, the nominator grants them the commit access and the new committer will be:\n  - Added to the committer list in the README.md\n  - Announced on the MLflow mailing lists, Slack channels, and the MLflow website\n  - Spotlighted through a post on the MLflow LinkedIn and X handles\n- The nominator will work with the new committer to identify well-scoped initial areas for the new committer to focus on, such as improvements to a specific component.\n- The nominator will also set up periodic 1:1 mentorship check-ins with the new committer over their first month to provide guidance where needed.\n', '# Issue Policy\n\nThe MLflow Issue Policy outlines the categories of MLflow GitHub issues and discusses the guidelines & processes\nassociated with each type of issue.\n\nBefore filing an issue, make sure to [search for related issues](https://github.com/mlflow/mlflow/issues) and check if\nthey address yours.\n\nFor support (ex. "How do I do X?"), please ask on [Stack Overflow](https://stackoverflow.com/questions/tagged/mlflow).\n\n## Issue Categories\n\nOur policy is that GitHub issues fall into one of the following categories:\n\n1. Feature Requests\n2. Bug reports\n3. Documentation fixes\n4. Installation issues\n\nEach category has its own GitHub issue template. Please do not delete the issue template unless you are certain your\nissue is outside its scope.\n\n### Feature Requests\n\n#### Guidelines\n\nFeature requests that are likely to be accepted:\n\n- Are minimal in scope (note that it\'s always easier to add additional functionality later than remove functionality)\n- Are extensible (e.g. if adding an integration with an ML framework, is it possible to add similar integrations with other frameworks?)\n- Have user impact & value that justifies the maintenance burden of supporting the feature moving forwards. The\n  [JQuery contributor guide](https://contribute.jquery.org/open-source/#contributing-something-new) has an excellent discussion on this.\n\n#### Lifecycle\n\nFeature requests typically go through the following lifecycle:\n\n1. A feature request GitHub Issue is submitted, which contains a high-level description of the proposal and its motivation.\n   We encourage requesters to provide an overview of the feature\'s implementation as well, if possible.\n2. The [issue is triaged](ISSUE_TRIAGE.rst) to identify whether more information is needed from the author, give an indication of priority, and route feature requests to appropriate committers.\n3. The feature request is discussed with a committer. The committer will provide input on the implementation overview or\n   ask for a more detailed design, if applicable.\n4. After discussion & agreement on the feature request and its implementation, an implementation owner is identified.\n5. The implementation owner begins developing the feature and ultimately files associated pull requests against the\n   MLflow Repository or packages the feature as an MLflow Plugin.\n\n### Bug reports\n\n#### Guidelines\n\nIn order to ensure that maintainers are able to assist in any reported bug:\n\n- Ensure that the bug report template is filled out in its entirety with appropriate levels of detail, particularly in the `Code to reproduce ', "will provide input on the implementation overview or\n   ask for a more detailed design, if applicable.\n4. After discussion & agreement on the feature request and its implementation, an implementation owner is identified.\n5. The implementation owner begins developing the feature and ultimately files associated pull requests against the\n   MLflow Repository or packages the feature as an MLflow Plugin.\n\n### Bug reports\n\n#### Guidelines\n\nIn order to ensure that maintainers are able to assist in any reported bug:\n\n- Ensure that the bug report template is filled out in its entirety with appropriate levels of detail, particularly in the `Code to reproduce issue` section.\n- Verify that the bug you are reporting meets one of the following criteria:\n  - A recent release of MLflow does not support the operation you are doing that an earlier release did (a regression).\n  - A [documented feature](https://mlflow.org/docs/latest/index.html) or functionality does not work properly by executing a provided example from the docs.\n  - Any exception raised is directly from MLflow and is not the result of an underlying package's exception (e.g., don't file an issue that MLflow can't log a model that can't be trained due to a tensorflow Exception)\n- Make a best effort to diagnose and troubleshoot the issue prior to filing.\n- Verify that the environment that you're experiencing the bug in is supported as defined in the docs.\n- Validate that MLflow supports the functionality that you're having an issue with. _A lack of a feature does not constitute a bug_.\n- Read the docs on the feature for the issue that you're reporting. If you're certain that you're following documented guidelines, please file a bug report.\n\nBug reports typically go through the following lifecycle:\n\n1. A bug report GitHub Issue is submitted, which contains a high-level description of the bug and information required to reproduce it.\n2. The [bug report is triaged](ISSUE_TRIAGE.rst) to identify whether more information is needed from the author, give an indication of priority, and route to request appropriate committers.\n3. An MLflow committer reproduces the bug and provides feedback about how to implement a fix.\n4. After an approach has been agreed upon, an owner ", "on the feature for the issue that you're reporting. If you're certain that you're following documented guidelines, please file a bug report.\n\nBug reports typically go through the following lifecycle:\n\n1. A bug report GitHub Issue is submitted, which contains a high-level description of the bug and information required to reproduce it.\n2. The [bug report is triaged](ISSUE_TRIAGE.rst) to identify whether more information is needed from the author, give an indication of priority, and route to request appropriate committers.\n3. An MLflow committer reproduces the bug and provides feedback about how to implement a fix.\n4. After an approach has been agreed upon, an owner for the fix is identified. MLflow committers may choose to adopt\n   ownership of severe bugs to ensure a timely fix.\n5. The fix owner begins implementing the fix and ultimately files associated pull requests.\n\n### Documentation fixes\n\nDocumentation issues typically go through the following lifecycle:\n\n1. A documentation GitHub Issue is submitted, which contains a description of the issue and its location(s) in the MLflow documentation.\n2. The [issue is triaged](ISSUE_TRIAGE.rst) to identify whether more information is needed from the author, give an indication of priority, and route the request to appropriate committers.\n3. An MLflow committer confirms the documentation issue and provides feedback about how to implement a fix.\n4. After an approach has been agreed upon, an owner for the fix is identified. MLflow committers may choose to adopt\n   ownership of severe documentation issues to ensure a timely fix.\n5. The fix owner begins implementing the fix and ultimately files associated pull requests.\n\n### Installation issues\n\nInstallation issues typically go through the following lifecycle:\n\n1. An installation GitHub Issue is submitted, which contains a description of the issue and the platforms its affects.\n2. The [issue is triaged](ISSUE_TRIAGE.rst) to identify whether more information is needed from the author, give an indication of priority, and route the issue to appropriate committers.\n3. An MLflow committer confirms the installation issue and provides feedback about how to implement a fix.\n4. After an approach has been agreed upon, an owner for the fix is identified. MLflow committers may choose to adopt\n   ownership of severe installation issues to ensure a ", 'requests.\n\n### Installation issues\n\nInstallation issues typically go through the following lifecycle:\n\n1. An installation GitHub Issue is submitted, which contains a description of the issue and the platforms its affects.\n2. The [issue is triaged](ISSUE_TRIAGE.rst) to identify whether more information is needed from the author, give an indication of priority, and route the issue to appropriate committers.\n3. An MLflow committer confirms the installation issue and provides feedback about how to implement a fix.\n4. After an approach has been agreed upon, an owner for the fix is identified. MLflow committers may choose to adopt\n   ownership of severe installation issues to ensure a timely fix.\n5. The fix owner begins implementing the fix and ultimately files associated pull requests.\n', '<h1 align="center" style="border-bottom: none">\n    <a href="https://mlflow.org/">\n        <img alt="MLflow logo" src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/logo.svg" width="200" />\n    </a>\n</h1>\n<h2 align="center" style="border-bottom: none">Open-Source Platform for Productionizing AI</h2>\n\nMLflow is an open-source developer platform to build AI/LLM applications and models with confidence. Enhance your AI applications with end-to-end **experiment tracking**, **observability**, and **evaluations**, all in one integrated platform.\n\n<div align="center">\n\n[![Python SDK](https://img.shields.io/pypi/v/mlflow)](https://pypi.org/project/mlflow/)\n[![PyPI Downloads](https://img.shields.io/pypi/dm/mlflow)](https://pepy.tech/projects/mlflow)\n[![License](https://img.shields.io/github/license/mlflow/mlflow)](https://github.com/mlflow/mlflow/blob/main/LICENSE)\n<a href="https://twitter.com/intent/follow?screen_name=mlflow" target="_blank">\n<img src="https://img.shields.io/twitter/follow/mlflow?logo=X&color=%20%23f5f5f5"\n      alt="follow on X(Twitter)"></a>\n<a href="https://www.linkedin.com/company/mlflow-org/" target="_blank">\n<img src="https://custom-icon-badges.demolab.com/badge/LinkedIn-0A66C2?logo=linkedin-white&logoColor=fff"\n      alt="follow on LinkedIn"></a>\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/mlflow/mlflow)\n\n</div>\n\n<div align="center">\n   <div>\n      <a href="https://mlflow.org/"><strong>Website</strong></a> Â·\n      <a href="https://mlflow.org/docs/latest/index.html"><strong>Docs</strong></a> Â·\n      <a href="https://github.com/mlflow/mlflow/issues/new/choose"><strong>Feature Request</strong></a> Â·\n      <a href="https://mlflow.org/blog"><strong>News</strong></a> Â·\n      <a href="https://www.youtube.com/@mlflowoss"><strong>YouTube</strong></a> Â·\n      <a href="https://lu.ma/mlflow?k=c"><strong>Events</strong></a>\n   </div>\n</div>\n\n<br>\n\n## ðŸš€ Installation\n\nTo install the MLflow Python package, run the following command:\n\n```\npip install mlflow\n```\n\n## ðŸ“¦ Core Components\n\nMLflow is **the only platform that provides a unified solution for all your AI/ML needs**, including LLMs, Agents, Deep Learning, and traditional machine learning.\n\n### ðŸ’¡ For LLM / GenAI Developers\n\n<table>\n  <tr>\n    <td>\n    <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-tracing.png" alt="Tracing" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/llms/tracing/index.html"><strong>ðŸ” Tracing / Observability</strong></a>\n        <br><br>\n        <div>Trace the internal states of your LLM/agentic applications for debugging quality issues and monitoring performance with ease.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/tracing/quickstart/python-openai/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n    <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-llm-eval.png" alt="LLM Evaluation" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/genai/eval-monitor/"><strong>ðŸ“Š LLM Evaluation</strong></a>\n        <br><br>\n        <div>A suite of automated model evaluation tools, seamlessly integrated with experiment tracking to compare ', 'debugging quality issues and monitoring performance with ease.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/tracing/quickstart/python-openai/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n    <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-llm-eval.png" alt="LLM Evaluation" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/genai/eval-monitor/"><strong>ðŸ“Š LLM Evaluation</strong></a>\n        <br><br>\n        <div>A suite of automated model evaluation tools, seamlessly integrated with experiment tracking to compare across multiple versions.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/eval-monitor/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-prompt.png" alt="Prompt Management">\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/prompt-registry/"><strong>ðŸ¤– Prompt Management</strong></a>\n        <br><br>\n        <div>Version, track, and reuse prompts across your organization, helping maintain consistency and improve collaboration in prompt development.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-registry/create-and-edit-prompts/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-logged-model.png" alt="MLflow Hero">\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/version-tracking/"><strong>ðŸ“¦ App Version Tracking</strong></a>\n        <br><br>\n        <div>MLflow keeps track of many moving parts in your AI applications, such as models, prompts, tools, and code, with end-to-end lineage.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/version-tracking/quickstart/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n</table>\n\n### ðŸŽ“ For Data Scientists\n\n<table>\n  <tr>\n    <td colspan="2" align="center" >\n     ', '     <br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/version-tracking/"><strong>ðŸ“¦ App Version Tracking</strong></a>\n        <br><br>\n        <div>MLflow keeps track of many moving parts in your AI applications, such as models, prompts, tools, and code, with end-to-end lineage.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/version-tracking/quickstart/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n</table>\n\n### ðŸŽ“ For Data Scientists\n\n<table>\n  <tr>\n    <td colspan="2" align="center" >\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-experiment.png" alt="Tracking" width=50%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/ml/tracking/"><strong>ðŸ“ Experiment Tracking</strong></a>\n        <br><br>\n        <div>Track your models, parameters, metrics, and evaluation results in ML experiments and compare them using an interactive UI.</div><br>\n        <a href="https://mlflow.org/docs/latest/ml/tracking/quickstart/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-model-registry.png" alt="Model Registry" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/ml/model-registry/"><strong>ðŸ’¾ Model Registry</strong></a>\n        <br><br>\n        <div> A centralized model store designed to collaboratively manage the full lifecycle and deployment of machine learning models.</div><br>\n        <a href="https://mlflow.org/docs/latest/ml/model-registry/tutorial/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-deployment.png" alt="Deployment" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/ml/deployment/"><strong>ðŸš€ Deployment</strong></a>\n        <br><br>\n        <div> Tools for seamless model deployment to batch ', 'store designed to collaboratively manage the full lifecycle and deployment of machine learning models.</div><br>\n        <a href="https://mlflow.org/docs/latest/ml/model-registry/tutorial/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-deployment.png" alt="Deployment" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/ml/deployment/"><strong>ðŸš€ Deployment</strong></a>\n        <br><br>\n        <div> Tools for seamless model deployment to batch and real-time scoring on platforms like Docker, Kubernetes, Azure ML, and AWS SageMaker.</div><br>\n        <a href="https://mlflow.org/docs/latest/ml/deployment/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n</table>\n\n## ðŸŒ Hosting MLflow Anywhere\n\n<div align="center" >\n  <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-providers.png" alt="Providers" width=100%>\n</div>\n\nYou can run MLflow in many different environments, including local machines, on-premise servers, and cloud infrastructure.\n\nTrusted by thousands of organizations, MLflow is now offered as a managed service by most major cloud providers:\n\n- [Amazon SageMaker](https://aws.amazon.com/sagemaker-ai/experiments/)\n- [Azure ML](https://learn.microsoft.com/en-us/azure/machine-learning/concept-mlflow?view=azureml-api-2)\n- [Databricks](https://www.databricks.com/product/managed-mlflow)\n- [Nebius](https://nebius.com/services/managed-mlflow)\n\nFor hosting MLflow on your own infrastructure, please refer to [this guidance](https://mlflow.org/docs/latest/ml/tracking/#tracking-setup).\n\n## ðŸ—£ï¸ Supported Programming Languages\n\n- [Python](https://pypi.org/project/mlflow/)\n- [TypeScript / JavaScript](https://www.npmjs.com/package/mlflow-tracing)\n- [Java](https://mvnrepository.com/artifact/org.mlflow/mlflow-client)\n- [R](https://cran.r-project.org/web/packages/mlflow/readme/README.html)\n\n## ðŸ”— Integrations\n\nMLflow is natively integrated with many popular machine learning frameworks and GenAI libraries.\n\n![Integrations](https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-integrations.png)\n\n## Usage Examples\n\n### Experiment Tracking ([Doc](https://mlflow.org/docs/latest/ml/tracking/))\n\nThe following examples trains a simple regression model with scikit-learn, while enabling MLflow\'s [autologging](https://mlflow.org/docs/latest/tracking/autolog.html) feature for experiment tracking.\n\n```python\nimport mlflow\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Enable MLflow\'s automatic experiment tracking for scikit-learn\nmlflow.sklearn.autolog()\n\n# Load the training dataset\ndb = load_diabetes()\nX_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n\nrf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\n# MLflow triggers logging automatically upon model fitting\nrf.fit(X_train, y_train)\n```\n\nOnce the above code finishes, run the following command in a separate terminal and access the MLflow UI via the printed URL. An MLflow **Run** should be automatically created, which tracks the training dataset, hyper parameters, performance metrics, the trained model, dependencies, and even more.\n\n```\nmlflow ui\n```\n\n### Evaluating Models ([Doc](https://mlflow.org/docs/latest/model-evaluation/index.html))\n\nThe following example runs automatic evaluation for question-answering tasks with several built-in metrics.\n\n```python\nimport mlflow\nimport ', 'import train_test_split\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Enable MLflow\'s automatic experiment tracking for scikit-learn\nmlflow.sklearn.autolog()\n\n# Load the training dataset\ndb = load_diabetes()\nX_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n\nrf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\n# MLflow triggers logging automatically upon model fitting\nrf.fit(X_train, y_train)\n```\n\nOnce the above code finishes, run the following command in a separate terminal and access the MLflow UI via the printed URL. An MLflow **Run** should be automatically created, which tracks the training dataset, hyper parameters, performance metrics, the trained model, dependencies, and even more.\n\n```\nmlflow ui\n```\n\n### Evaluating Models ([Doc](https://mlflow.org/docs/latest/model-evaluation/index.html))\n\nThe following example runs automatic evaluation for question-answering tasks with several built-in metrics.\n\n```python\nimport mlflow\nimport pandas as pd\n\n# Evaluation set contains (1) input question (2) model outputs (3) ground truth\ndf = pd.DataFrame(\n    {\n        "inputs": ["What is MLflow?", "What is Spark?"],\n        "outputs": [\n            "MLflow is an innovative fully self-driving airship powered by AI.",\n            "Sparks is an American pop and rock duo formed in Los Angeles.",\n        ],\n        "ground_truth": [\n            "MLflow is an open-source platform for productionizing AI.",\n            "Apache Spark is an open-source, distributed computing system.",\n        ],\n    }\n)\neval_dataset = mlflow.data.from_pandas(\n    df, predictions="outputs", targets="ground_truth"\n)\n\n# Start an MLflow Run to record the evaluation results to\nwith mlflow.start_run(run_name="evaluate_qa"):\n    # Run automatic evaluation with a set of built-in metrics for question-answering models\n    results = mlflow.evaluate(\n        data=eval_dataset,\n        model_type="question-answering",\n    )\n\nprint(results.tables["eval_results_table"])\n```\n\n### Observability ([Doc](https://mlflow.org/docs/latest/llms/tracing/index.html))\n\nMLflow Tracing provides LLM observability for various GenAI libraries such as OpenAI, LangChain, LlamaIndex, DSPy, AutoGen, and more. To enable auto-tracing, call `mlflow.xyz.autolog()` before running your models. Refer to the documentation for customization ', '],\n    }\n)\neval_dataset = mlflow.data.from_pandas(\n    df, predictions="outputs", targets="ground_truth"\n)\n\n# Start an MLflow Run to record the evaluation results to\nwith mlflow.start_run(run_name="evaluate_qa"):\n    # Run automatic evaluation with a set of built-in metrics for question-answering models\n    results = mlflow.evaluate(\n        data=eval_dataset,\n        model_type="question-answering",\n    )\n\nprint(results.tables["eval_results_table"])\n```\n\n### Observability ([Doc](https://mlflow.org/docs/latest/llms/tracing/index.html))\n\nMLflow Tracing provides LLM observability for various GenAI libraries such as OpenAI, LangChain, LlamaIndex, DSPy, AutoGen, and more. To enable auto-tracing, call `mlflow.xyz.autolog()` before running your models. Refer to the documentation for customization and manual instrumentation.\n\n```python\nimport mlflow\nfrom openai import OpenAI\n\n# Enable tracing for OpenAI\nmlflow.openai.autolog()\n\n# Query OpenAI LLM normally\nresponse = OpenAI().chat.completions.create(\n    model="gpt-4o-mini",\n    messages=[{"role": "user", "content": "Hi!"}],\n    temperature=0.1,\n)\n```\n\nThen navigate to the "Traces" tab in the MLflow UI to find the trace records OpenAI query.\n\n## ðŸ’­ Support\n\n- For help or questions about MLflow usage (e.g. "how do I do X?") visit the [documentation](https://mlflow.org/docs/latest/index.html).\n- In the documentation, you can ask the question to our AI-powered chat bot. Click on the **"Ask AI"** button at the right bottom.\n- Join the [virtual events](https://lu.ma/mlflow?k=c) like office hours and meetups.\n- To report a bug, file a documentation issue, or submit a feature request, please [open a GitHub issue](https://github.com/mlflow/mlflow/issues/new/choose).\n- For release announcements and other discussions, please subscribe to our mailing list (mlflow-users@googlegroups.com)\n  or join us on [Slack](https://mlflow.org/slack).\n\n## ðŸ¤ Contributing\n\nWe happily welcome contributions to MLflow!\n\n- Submit [bug reports](https://github.com/mlflow/mlflow/issues/new?template=bug_report_template.yaml) and [feature requests](https://github.com/mlflow/mlflow/issues/new?template=feature_request_template.yaml)\n- Contribute for [good-first-issues](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) and [help-wanted](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22)\n- Writing about MLflow and sharing your experience\n\nPlease see our [contribution guide](CONTRIBUTING.md) to learn more about contributing to MLflow.\n\n## â­ï¸ Star History\n\n<a href="https://star-history.com/#mlflow/mlflow&Date">\n <picture>\n   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date&theme=dark" />\n   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date" />\n   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date" />\n </picture>\n</a>\n\n## âœï¸ Citation\n\nIf you use MLflow in your research, please cite it using the "Cite this repository" button at the top of the [GitHub repository page](https://github.com/mlflow/mlflow), which will provide you with citation formats including APA and BibTeX.\n\n## ðŸ‘¥ Core Members\n\nMLflow is currently maintained by the following core members with significant contributions from ', '[good-first-issues](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) and [help-wanted](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22)\n- Writing about MLflow and sharing your experience\n\nPlease see our [contribution guide](CONTRIBUTING.md) to learn more about contributing to MLflow.\n\n## â­ï¸ Star History\n\n<a href="https://star-history.com/#mlflow/mlflow&Date">\n <picture>\n   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date&theme=dark" />\n   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date" />\n   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date" />\n </picture>\n</a>\n\n## âœï¸ Citation\n\nIf you use MLflow in your research, please cite it using the "Cite this repository" button at the top of the [GitHub repository page](https://github.com/mlflow/mlflow), which will provide you with citation formats including APA and BibTeX.\n\n## ðŸ‘¥ Core Members\n\nMLflow is currently maintained by the following core members with significant contributions from hundreds of exceptionally talented community members.\n\n- [Ben Wilson](https://github.com/BenWilson2)\n- [Corey Zumar](https://github.com/dbczumar)\n- [Daniel Lok](https://github.com/daniellok-db)\n- [Gabriel Fu](https://github.com/gabrielfu)\n- [Harutaka Kawamura](https://github.com/harupy)\n- [Serena Ruan](https://github.com/serena-ruan)\n- [Tomu Hirata](https://github.com/TomeHirata)\n- [Weichen Xu](https://github.com/WeichenXu123)\n- [Yuki Watanabe](https://github.com/B-Step62)\n', '# Security Policy\n\nMLflow and its community take security bugs seriously. We appreciate efforts to improve the security of MLflow\nand follow the [GitHub coordinated disclosure of security vulnerabilities](https://docs.github.com/en/code-security/security-advisories/about-coordinated-disclosure-of-security-vulnerabilities#about-reporting-and-disclosing-vulnerabilities-in-projects-on-github)\nfor responsible disclosure and prompt mitigation. We are committed to working with security researchers to\nresolve the vulnerabilities they discover.\n\n## Supported Versions\n\nThe latest version of MLflow has continued support. If a critical vulnerability is found in the current version\nof MLflow, we may opt to backport patches to previous versions.\n\n## Reporting a Vulnerability\n\nWhen finding a security vulnerability in MLflow, please perform the following actions:\n\n- [Open an issue](https://github.com/mlflow/mlflow/issues/new?assignees=&labels=bug&template=bug_report_template.md&title=%5BBUG%5D%20Security%20Vulnerability) on the MLflow repository. Ensure that you use `[BUG] Security Vulnerability` as the title and _do not_ mention any vulnerability details in the issue post.\n- Send a notification [email](mailto:mlflow-oss-maintainers@googlegroups.com) to `mlflow-oss-maintainers@googlegroups.com` that contains, at a minimum:\n  - The link to the filed issue stub.\n  - Your GitHub handle.\n  - Detailed information about the security vulnerability, evidence that supports the relevance of the finding and any reproducibility instructions for independent confirmation.\n\nThis first stage of reporting is to ensure that a rapid validation can occur without wasting the time and effort of a reporter. Future communication and vulnerability resolution will be conducted after validating\nthe veracity of the reported issue.\n\nAn MLflow maintainer will, after validating the report:\n\n- Acknowledge the [bug](ISSUE_POLICY.md#bug-reports) during [triage](ISSUE_TRIAGE.rst)\n- Mark the issue as `priority/critical-urgent`\n- Open a draft [GitHub Security Advisory](https://docs.github.com/en/code-security/security-advisories/creating-a-security-advisory)\n  to discuss the vulnerability details in private.\n\nThe private Security Advisory will be used to confirm the issue, prepare a fix, and publicly disclose it after the fix has been released.\n', '## MLflow Dev Scripts\n\nThis directory contains automation scripts for MLflow developers and the build infrastructure.\n\n## Job Statuses\n\n[![Examples Action Status](https://img.shields.io/github/actions/workflow/status/mlflow/dev/examples.yml.svg?branch=master&event=schedule&label=Examples&style=for-the-badge&logo=github)](https://github.com/mlflow/dev/actions/workflows/examples.yml?query=workflow%3AExamples+event%3Aschedule)\n[![Cross Version Tests Status](https://img.shields.io/github/actions/workflow/status/mlflow/dev/cross-version-tests.yml.svg?branch=master&event=schedule&label=Cross%20version%20tests&style=for-the-badge&logo=github)](https://github.com/mlflow/dev/actions/workflows/cross-version-tests.yml?query=workflow%3A%22Cross+version+tests%22+event%3Aschedule)\n[![R-devel Action Status](https://img.shields.io/github/actions/workflow/status/mlflow/dev/r.yml.svg?branch=master&event=schedule&label=r-devel&style=for-the-badge&logo=github)](https://github.com/mlflow/dev/actions/workflows/r.yml?query=workflow%3AR+event%3Aschedule)\n[![Test Requirements Status](https://img.shields.io/github/actions/workflow/status/mlflow/dev/requirements.yml.svg?branch=master&event=schedule&label=test%20requirements&logo=github&style=for-the-badge)](https://github.com/mlflow/dev/actions/workflows/requirements.yml?query=workflow%3A%22Test+requirements%22+event%3Aschedule)\n[![Push Images Status](https://img.shields.io/github/actions/workflow/status/mlflow/mlflow/push-images.yml.svg?event=release&label=push-images&logo=github&style=for-the-badge)](https://github.com/mlflow/mlflow/actions/workflows/push-images.yml?query=event%3Arelease)\n[![Slow Tests Status](https://img.shields.io/github/actions/workflow/status/mlflow/dev/slow-tests.yml.svg?branch=master&event=schedule&label=slow-tests&logo=github&style=for-the-badge)](https://github.com/mlflow/dev/actions/workflows/slow-tests.yml?query=event%3Aschedule)\n[![Website E2E Tests Status](https://img.shields.io/github/actions/workflow/status/mlflow/mlflow-website/e2e.yml.svg?branch=main&event=schedule&label=website-e2e&logo=github&style=for-the-badge)](https://github.com/mlflow/mlflow-website/actions/workflows/e2e.yml?query=event%3Aschedule)\n', '# Typos\n\nA quick guide on how to use [`typos`](https://github.com/crate-ci/typos) to find, fix, and ignore typos.\n\n## Installation\n\n```sh\n# Replace `<version>` with the version installed in `dev/install-typos.sh`.\nbrew install typos-cli@<version>\n\n```\n\nSee https://github.com/crate-ci/typos?tab=readme-ov-file#install for other installation methods.\n\n## Finding typos\n\n```sh\npre-commit run --all-files typos\n```\n\n## Fixing typos\n\nYou can fix typos either manually or by running the following command:\n\n```sh\ntypos --write-changes [PATH]\n```\n\n## Ignoring false positives\n\nThere are two ways to ignore false positives:\n\n### Option 1: Ignore a line/block containing false positives\n\nThis option is preferred if the false positive is a one-off.\n\n```python\n# Ignore a line containing a typo:\n\n"<false_positive>"  # spellchecker: disable-line\n\n# Ignore a block containing typos:\n\n# spellchecker: off\n"<false_positive>"\n"<another_false_positive>"\n# spellchecker: on\n```\n\n### Option 2: Extend the ignore list in [`pyproject.toml`](../pyproject.toml)\n\nThis option is preferred if the false positive is common across multiple files/lines.\n\n```toml\n# pyproject.toml\n\n[tool.typos.default]\nextend-ignore-re = [\n  ...,\n  "false_positive",\n]\n```\n\n## Found a typo, but `typos` doesn\'t recognize it?\n\n`typos` only recognizes typos that are in its dictionary.\nIf you find a typo that `typos` doesn\'t recognize,\nyou can extend the `extend-words` list in [`pyproject.toml`](../pyproject.toml).\n\n```toml\n# pyproject.toml\n\n[tool.typos.default.extend-words]\n...\nmflow = "mlflow"\n```\n', '# Clint\n\nA custom linter for mlflow to enforce rules that ruff doesn\'t cover.\n\n## Installation\n\n```\npip install -e dev/clint\n```\n\n## Usage\n\n```bash\nclint file.py ...\n```\n\n## Integrating with Visual Studio Code\n\n1. Install [the Pylint extension](https://marketplace.visualstudio.com/items?itemName=ms-python.pylint)\n2. Add the following setting in your `settings.json` file:\n\n```json\n{\n  "pylint.path": ["${interpreter}", "-m", "clint"]\n}\n```\n\n## Ignoring Rules for Specific Files or Lines\n\n**To ignore a rule on a specific line (recommended):**\n\n```python\nfoo()  # clint: disable=<rule_name>\n```\n\nReplace `<rule_name>` with the actual rule you want to disable.\n\n**To ignore a rule for an entire file:**\n\nAdd the file path to the `exclude` list in your `pyproject.toml`:\n\n```toml\n[tool.clint]\nexclude = [\n  # ...existing entries...\n  "path/to/file.py",\n]\n```\n\n## Testing\n\n```bash\npytest --confcutdir dev/clint dev/clint\n```\n', '# Python Style Guide\n\nThis guide documents Python coding conventions that go beyond what [ruff](https://docs.astral.sh/ruff/) and [clint](../../dev/clint/) can enforce. The practices below require human judgment to implement correctly and improve code readability, maintainability, and testability across the MLflow codebase.\n\n## Avoid Redundant Test Docstrings\n\nOmit docstrings that merely echo the function name without adding value. Test names should be self-documenting.\n\n```python\n# Bad\ndef test_foo():\n    """Test foo"""\n    ...\n\n\n# Good\ndef test_foo():\n    ...\n```\n\n## Use Type Hints for All Functions\n\nAdd type hints to all function parameters and return values. This enables better IDE support, catches bugs early, and serves as inline documentation.\n\n```python\n# Bad\ndef foo(s):\n    return len(s)\n\n\n# Good\ndef foo(s: str) -> int:\n    return len(s)\n```\n\n### Exceptions\n\n**Test functions:** The `-> None` return type can be omitted for test functions since they implicitly return `None` and the return value is not used.\n\n```python\n# Acceptable\ndef test_foo(s: str):\n    ...\n\n\n# Also acceptable (but not required)\ndef test_foo(s: str) -> None:\n    ...\n```\n\n**`__init__` methods:** The `-> None` return type can be omitted for `__init__` methods since they always return `None` by definition.\n\n```python\n# Acceptable\nclass Foo:\n    def __init__(self, s: str):\n        ...\n\n\n# Also acceptable (but not required)\nclass Foo:\n    def __init__(self, s: str) -> None:\n        ...\n```\n\n## Minimize Try-Catch Block Scope\n\nWrap only the specific operations that can raise exceptions. Keep safe operations outside the try block to improve debugging and avoid masking unexpected errors.\n\n```python\n# Bad\ntry:\n    never_fails()\n    can_fail()\nexcept ...:\n    handle_error()\n\n# Good\nnever_fails()\ntry:\n    can_fail()\nexcept ...:\n    handle_error()\n```\n\n## Use Dataclasses Instead of Complex Tuples\n\nReplace tuples with 3+ elements with named dataclasses. This improves code clarity, prevents positional argument errors, and enables type checking on individual fields.\n\n```python\n# Bad\ndef get_user() -> tuple[str, int, str]:\n    return "Alice", 30, "Engineer"\n\n\n# Good\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass User:\n    name: str\n    age: int\n    occupation: str\n\n\ndef get_user() -> User:\n    return User(name="Alice", age=30, occupation="Engineer")\n```\n\n## Use next() to Find First ', 'Bad\ntry:\n    never_fails()\n    can_fail()\nexcept ...:\n    handle_error()\n\n# Good\nnever_fails()\ntry:\n    can_fail()\nexcept ...:\n    handle_error()\n```\n\n## Use Dataclasses Instead of Complex Tuples\n\nReplace tuples with 3+ elements with named dataclasses. This improves code clarity, prevents positional argument errors, and enables type checking on individual fields.\n\n```python\n# Bad\ndef get_user() -> tuple[str, int, str]:\n    return "Alice", 30, "Engineer"\n\n\n# Good\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass User:\n    name: str\n    age: int\n    occupation: str\n\n\ndef get_user() -> User:\n    return User(name="Alice", age=30, occupation="Engineer")\n```\n\n## Use next() to Find First Match Instead of Loop-and-Break\n\nUse the `next()` builtin function with a generator expression to find the first item that matches a condition. This is more concise and functional than manually looping with break statements.\n\n```python\n# Bad\nresult = None\nfor item in items:\n    if item.name == "target":\n        result = item\n        break\n\n# Good\nresult = next((item for item in items if item.name == "target"), None)\n```\n\n## Always Verify Mock Calls with Assertions\n\nEvery mocked function must have an assertion (`assert_called`, `assert_called_once`, etc.) to verify it was invoked correctly. Without assertions, tests may pass even when the mocked code isn\'t executed.\n\n```python\nfrom unittest import mock\n\n\n# Bad\ndef test_foo():\n    with mock.patch("foo.bar"):\n        calls_bar()\n\n\n# Good\ndef test_bar():\n    with mock.patch("foo.bar") as mock_bar:\n        calls_bar()\n        mock_bar.assert_called_once()\n```\n\n## Set Mock Behaviors in Patch Declaration\n\nDefine `return_value` and `side_effect` directly in the `patch()` call rather than assigning them afterward. This keeps mock configuration explicit and reduces setup code.\n\n```python\nfrom unittest import mock\n\n\n# Bad\ndef test_foo():\n    with mock.patch("foo.bar") as mock_bar:\n        mock_bar.return_value = 42\n        calls_bar()\n\n    with mock.patch("foo.bar") as mock_bar:\n        mock_bar.side_effect = Exception("Error")\n        calls_bar()\n\n\n# Good\ndef test_foo():\n    with mock.patch("foo.bar", return_value=42) as mock_bar:\n      ', ' mock_bar.assert_called_once()\n```\n\n## Set Mock Behaviors in Patch Declaration\n\nDefine `return_value` and `side_effect` directly in the `patch()` call rather than assigning them afterward. This keeps mock configuration explicit and reduces setup code.\n\n```python\nfrom unittest import mock\n\n\n# Bad\ndef test_foo():\n    with mock.patch("foo.bar") as mock_bar:\n        mock_bar.return_value = 42\n        calls_bar()\n\n    with mock.patch("foo.bar") as mock_bar:\n        mock_bar.side_effect = Exception("Error")\n        calls_bar()\n\n\n# Good\ndef test_foo():\n    with mock.patch("foo.bar", return_value=42) as mock_bar:\n        calls_bar()\n\n    with mock.patch("foo.bar", side_effect=Exception("Error")) as mock_bar:\n        calls_bar()\n```\n\n## Use Pytest\'s Monkeypatch for Directory Changes\n\nUse `monkeypatch.chdir()` instead of manual `os.chdir()` with try/finally blocks. Pytest automatically restores the original directory after the test, preventing side effects.\n\n```python\nimport os\nimport pytest\n\n\n# Bad\ndef test_foo():\n    cwd = os.getcwd()\n    try:\n        os.chdir("some/directory")\n    finally:\n        os.chdir(cwd)\n\n\n# Good\ndef test_foo(monkeypatch: pytest.MonkeyPatch):\n    monkeypatch.chdir("some/directory")\n```\n\n## Parametrize Tests with Multiple Input Cases\n\nUse `@pytest.mark.parametrize` to test multiple inputs instead of repeating assertions. This creates separate test cases for each input, making failures easier to diagnose and tests more maintainable.\n\n```python\n# Bad\ndef test_foo():\n    assert foo("a") == 0\n    assert foo("b") == 1\n    assert foo("c") == 2\n\n\n# Good\n@pytest.mark.parametrize(\n    ("input", "expected"),\n    [\n        ("a", 0),\n        ("b", 1),\n        ("c", 2),\n    ],\n)\ndef test_foo(input: str, expected: int):\n    assert foo(input) == expected\n```\n\n## Use Pytest\'s Monkeypatch for Mocking Environment Variables\n\nUse `monkeypatch.setenv()` and `monkeypatch.delenv()` instead of `mock.patch.dict()` for environment variables. Pytest\'s monkeypatch fixture automatically restores the original environment after the test, providing cleaner and more reliable test isolation.\n\n```python\n# Bad - Setting environment variables\ndef test_foo():\n    with mock.patch.dict("os.environ", {"FOO": "True"}):\n        ...\n\n\n# Bad - ', '[\n        ("a", 0),\n        ("b", 1),\n        ("c", 2),\n    ],\n)\ndef test_foo(input: str, expected: int):\n    assert foo(input) == expected\n```\n\n## Use Pytest\'s Monkeypatch for Mocking Environment Variables\n\nUse `monkeypatch.setenv()` and `monkeypatch.delenv()` instead of `mock.patch.dict()` for environment variables. Pytest\'s monkeypatch fixture automatically restores the original environment after the test, providing cleaner and more reliable test isolation.\n\n```python\n# Bad - Setting environment variables\ndef test_foo():\n    with mock.patch.dict("os.environ", {"FOO": "True"}):\n        ...\n\n\n# Bad - Removing environment variables\ndef test_bar():\n    with mock.patch.dict("os.environ", {}, clear=True):\n        ...\n\n\n# Good - Setting environment variables\ndef test_foo(monkeypatch: pytest.MonkeyPatch):\n    monkeypatch.setenv("FOO", "True")\n    ...\n\n\n# Good - Removing environment variables\ndef test_bar(monkeypatch: pytest.MonkeyPatch):\n    # raising=False prevents KeyError if FOO doesn\'t exist\n    monkeypatch.delenv("FOO", raising=False)\n    ...\n```\n\n## Use Pytest\'s tmp_path Fixture for Temporary Files\n\nUse `tmp_path` fixture instead of manual `tempfile.TemporaryDirectory()` for handling temporary files and directories in tests. Pytest automatically cleans up the temporary directory after the test, provides better test isolation, and integrates seamlessly with pytest\'s fixture system.\n\n```python\n# Bad\nimport tempfile\n\n\ndef test_foo():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        ...\n\n\n# Good\nfrom pathlib import Path\n\n\ndef test_foo(tmp_path: Path):\n    ...\n```\n', '# MLflow Proto To GraphQL Autogeneration\n\n## What is this\n\nThe system in `dev/proto_to_graphql` parses proto rpc definitions and generates graphql schema based on the proto rpc definition. The goal of this system is to quickly generate base GraphQL schema and resolver code so that we can easily take advantage of the data joining functionalities of GraphQL.\n\nThe autogenerated schema and resolver are in the following file: `mlflow/server/graphql/autogenerated_graphql_schema.py`\n\nThe autogenerated schema and resolvers are referenced and can be extended in this file `mlflow/server/graphql/graphql_schema_extensions.py`\n\nYou can run `python ./dev/proto_to_graphql/code_generator.py` or `./dev/generate-protos.sh` to trigger the codegen process.\n\n## FAQs\n\n### How to onboard a new rpc to GraphQL\n\n- In your proto rpc definition, add `option (graphql) = {};` and re-run `./dev/generate-protos.sh`. You should see the changes in the generated schema. [Example](https://github.com/mlflow/mlflow/pull/11215/files#diff-8ab2ad3109b67a713e147edf557d4da88853563398ce354cc895bb5930950dc5R175).\n- In `mlflow/server/handlers.py`, identify the handler function for your rpc, for example `_get_run`, make sure there exists a corresponding `get_run_impl` function that takes in a `request_message` and returns a response messages that is of the generated service_pb proto type. If no such function exists, you can easily extract it out like in this [example](https://github.com/mlflow/mlflow/pull/11215/files#diff-5c10a4e2ca47745f06fa9e7201087acfc102849756cb8d85e774a5ac468cb037R1779-R1795).\n- Test manually with a localhost server, as well as adding a unit test in `tests/tracking/test_rest_tracking.py`. [Example](https://github.com/mlflow/mlflow/pull/11215/files#diff-2ec8756f67a20ecbaeec2d2c5e7bf33310a50c015fc3aa487e27100fc4c2f9a7R1771-R1802).\n\n### How to customize a generated query/mutation to join multiple rpc endpoints\n\nThe proto to graphql autogeneration only supports 1 to 1 mapping from proto rpc to graphql operation. However, the power of GraphQL is to join multiple rpc endpoints together as one query. So we often would like to customize or extend the autogenerated operations to join these multiple endpoints.\n\nFor example, we would like to query data about `Experiment`, `ModelVersions` and `Run` in one query by extending the `MlflowRun` object.\n\n```\nquery testQuery {\n    mlflowGetRun(input: {runId: "my-id"}) {\n        run {\n            experiment {\n                name\n            }\n            modelVersions {\n         ', 'example, we would like to query data about `Experiment`, `ModelVersions` and `Run` in one query by extending the `MlflowRun` object.\n\n```\nquery testQuery {\n    mlflowGetRun(input: {runId: "my-id"}) {\n        run {\n            experiment {\n                name\n            }\n            modelVersions {\n                name\n            }\n        }\n    }\n}\n```\n\nTo achieve joins, follow the steps below:\n\n- Make sure the rpcs you would like to join are already onboarded to GraphQL by following the `How to onboard a new rpc to GraphQL` section\n- Identify the class you would like to extend in `autogenerated_graphql_schema.py` and create a new class that inherits the target class, put it in `graphql_schema_extensions.py`. Add the new fields and the resolver function as you intended. [Example](https://github.com/mlflow/mlflow/pull/11173/files#diff-9e4f7bdf4d7f9d362338bed9ce6607a51b8f520ee605e2fd4c9bda5e43cb617cR21-R31)\n- Run `python ./dev/proto_to_graphql/code_generator.py` or `./dev/generate-protos.sh`, you should see the autogenerated schema being updated to reference the extension class you just created.\n- Add a test case in `tests/tracking/test_rest_tracking.py` [Example](https://github.com/mlflow/mlflow/pull/11173/files#diff-2ec8756f67a20ecbaeec2d2c5e7bf33310a50c015fc3aa487e27100fc4c2f9a7R1771-R1795)\n\n### How to generate typescript types for a GraphQL operation\n\nTo generate typescript types, first make sure the generated schema is up-to-date by running `python ./dev/proto_to_graphql/code_generator.py`\n\nThen write your new query or mutation in the mlflow/server/js/src folder, after that run the following commands:\n\n- cd mlflow/server/js\n- yarn graphql-codegen\n\nYou should be able to see the generated types in `mlflow/server/js/src/graphql/__generated__/`\n', "# MLflow with Docker Compose (PostgreSQL + MinIO)\n\nThis directory provides a **Docker Compose** setup for running **MLflow** locally with a **PostgreSQL** backend store and **MinIO** (S3-compatible) artifact storage. It's intended for quick evaluation and local development.\n\n---\n\n## Overview\n\n- **MLflow Tracking Server** â€” exposed on your host (default `http://localhost:5000`).\n- **PostgreSQL** â€” persists MLflow's metadata (experiments, runs, params, metrics).\n- **MinIO** â€” stores run artifacts via an S3-compatible API.\n\nCompose automatically reads configuration from a local `.env` file in this directory.\n\n---\n\n## Prerequisites\n\n- **Git**\n- **Docker** and **Docker Compose**\n  - Windows/macOS: [Docker Desktop](https://www.docker.com/products/docker-desktop/)\n  - Linux: Docker Engine + the `docker compose` plugin\n\nVerify your setup:\n\n```bash\ndocker --version\ndocker compose version\n```\n\n---\n\n## 1. Clone the Repository\n\n```bash\ngit clone https://github.com/mlflow/mlflow.git\ncd docker-compose\n```\n\n---\n\n## 2. Configure Environment\n\nCopy the example environment file and modify as needed:\n\n```bash\ncp .env.dev.example .env\n```\n\nThe `.env` file defines container image tags, ports, credentials, and storage configuration. Open it and review values before starting the stack.\n\n**Common variables** :\n\n- **MLflow**\n  - `MLFLOW_PORT=5000` â€” host port for the MLflow UI/API\n  - `MLFLOW_DEFAULT_ARTIFACT_ROOT=s3://mlflow/` â€” artifact store URI\n  - `MLFLOW_S3_ENDPOINT_URL=http://minio:9000` â€” S3 endpoint (inside the Compose network)\n- **PostgreSQL**\n  - `POSTGRES_USER=mlflow`\n  - `POSTGRES_PASSWORD=mlflow`\n  - `POSTGRES_DB=mlflow`\n- **MinIO (S3-compatible)**\n  - `MINIO_ROOT_USER=minio`\n  - `MINIO_ROOT_PASSWORD=minio123`\n  - `MINIO_HOST=minio`\n  - `MINIO_PORT=9000`\n  - `MINIO_BUCKET=mlflow`\n\n---\n\n## 3. Launch the Stack\n\n```bash\ndocker compose up -d\n```\n\nThis:\n\n- Builds/pulls images as needed\n- Creates a user-defined network\n- Starts **postgres**, **minio**, and **mlflow** containers\n\nCheck status:\n\n```bash\ndocker compose ps\n```\n\nView logs (useful on first run):\n\n```bash\ndocker compose logs -f\n```\n\n---\n\n## 4. Access MLflow\n\nOpen the MLflow UI:\n\n- **URL**: `http://localhost:5000` (or the port set in `.env`)\n\nYou can now create experiments, run training scripts, and log metrics, parameters, and artifacts to this local MLflow instance.\n\n---\n\n## 5. Shutdown\n\nTo stop and remove the containers and network:\n\n```bash\ndocker compose down\n```\n\n> Data is preserved in Docker **volumes**. To remove volumes as well (irreversible), run:\n>\n> ```bash\n> docker compose down -v\n> ```\n\n---\n\n## Tips & Troubleshooting\n\n- **Verify connectivity**  \n  If MLflow can't write artifacts, confirm your S3 settings:\n\n  - `MLFLOW_DEFAULT_ARTIFACT_ROOT` points to your MinIO bucket (e.g., `s3://mlflow/`)\n  - `MLFLOW_S3_ENDPOINT_URL` is reachable from the MLflow container (often `http://minio:9000`)\n\n- **Resetting the environment**  \n  If you want a clean slate, stop the stack and remove volumes:\n\n  ", "training scripts, and log metrics, parameters, and artifacts to this local MLflow instance.\n\n---\n\n## 5. Shutdown\n\nTo stop and remove the containers and network:\n\n```bash\ndocker compose down\n```\n\n> Data is preserved in Docker **volumes**. To remove volumes as well (irreversible), run:\n>\n> ```bash\n> docker compose down -v\n> ```\n\n---\n\n## Tips & Troubleshooting\n\n- **Verify connectivity**  \n  If MLflow can't write artifacts, confirm your S3 settings:\n\n  - `MLFLOW_DEFAULT_ARTIFACT_ROOT` points to your MinIO bucket (e.g., `s3://mlflow/`)\n  - `MLFLOW_S3_ENDPOINT_URL` is reachable from the MLflow container (often `http://minio:9000`)\n\n- **Resetting the environment**  \n  If you want a clean slate, stop the stack and remove volumes:\n\n  ```bash\n  docker compose down -v\n  docker compose up -d\n  ```\n\n- **Logs**\n\n  - MLflow server: `docker compose logs -f mlflow`\n  - PostgreSQL: `docker compose logs -f postgres`\n  - MinIO: `docker compose logs -f minio`\n\n- **Port conflicts**  \n  If `5000` (or any other port) is in use, change it in `.env` and restart:\n  ```bash\n  docker compose down\n  docker compose up -d\n  ```\n\n---\n\n## How It Works (at a Glance)\n\n- MLflow uses **PostgreSQL** as the _backend store_ for experiment/run metadata.\n- MLflow uses **MinIO** as the _artifact store_ via S3 APIs.\n- Docker Compose wires services on a shared network; MLflow talks to PostgreSQL and MinIO by container name (e.g., `postgres`, `minio`).\n\n---\n\n## Next Steps\n\n- Point your training scripts to this server:\n  ```bash\n  export MLFLOW_TRACKING_URI=http://localhost:5000\n  ```\n- Start logging runs with `mlflow.start_run()` (Python) or the MLflow CLI.\n- Customize the `.env` and `docker-compose.yml` to fit your local workflow (e.g., change image tags, add volumes, etc.).\n\n---\n\n**You now have a fully local MLflow stack with persistent metadata and artifact storageâ€”ideal for development and experimentation.**\n", "## MLflow examples\n\n### Quick Start example\n\n- `quickstart/mlflow_tracking.py` is a basic example to introduce MLflow concepts.\n\n## Tutorials\n\nVarious examples that depict MLflow tracking, project, and serving use cases.\n\n- `h2o` depicts how MLflow can be use to track various random forest architectures to train models\n  for predicting wine quality.\n- `hyperparam` shows how to do hyperparameter tuning with MLflow and some popular optimization libraries.\n- `keras` modifies\n  [a Keras classification example](https://github.com/keras-team/keras/blob/ed07472bc5fc985982db355135d37059a1f887a9/examples/reuters_mlp.py)\n  and uses MLflow's `mlflow.tensorflow.autolog()` API to automatically log metrics and parameters\n  to MLflow during training.\n- `multistep_workflow` is an end-to-end of a data ETL and ML training pipeline built as an MLflow\n  project. The example shows how parts of the workflow can leverage from previously run steps.\n- `pytorch` uses CNN on MNIST dataset for character recognition. The example logs TensorBoard events\n  and stores (logs) them as MLflow artifacts.\n- `remote_store` has a usage example of REST based backed store for tracking.\n- `r_wine` demonstrates how to log parameters, metrics, and models from R.\n- `sklearn_elasticnet_diabetes` uses the sklearn diabetes dataset to predict diabetes progression\n  using ElasticNet.\n- `sklearn_elasticnet_wine_quality` is an example for MLflow projects. This uses the Wine\n  Quality dataset and Elastic Net to predict quality. The example uses `MLproject` to set up a\n  Conda environment, define parameter types and defaults, entry point for training, etc.\n- `sklearn_logistic_regression` is a simple MLflow example with hooks to log training data to MLflow\n  tracking server.\n- `supply_chain_security` shows how to strengthen the security of ML projects against supply-chain attacks by enforcing hash checks on Python packages.\n- `tensorflow` contains end-to-end one run examples from train to predict for TensorFlow 2.8+ It includes usage of MLflow's\n  `mlflow.tensorflow.autolog()` API, which captures TensorBoard data and logs to MLflow with no code change.\n- `docker` demonstrates how to create and run an MLflow project using docker (rather than conda)\n  to manage project dependencies\n- `johnsnowlabs` gives you access to [20.000+ state-of-the-art enterprise NLP models in 200+ languages](https://nlp.johnsnowlabs.com/models) for medical, finance, legal and many more domains.\n\n## Demos\n\n- `demos` folder contains notebooks used during MLflow presentations.\n", "# Basic authentication example\n\nThis example demonstrates the authentication and authorization feature of MLflow.\n\nTo run this example,\n\n1. Start the tracking server\n   ```shell\n   mlflow ui --app-name=basic-auth\n   ```\n2. Go to `http://localhost:5000/signup` and register two users:\n   - `(user_a, password_a)`\n   - `(user_b, password_b)`\n3. Run the script\n   ```shell\n   python auth.py\n   ```\n   Expected output:\n   ```\n   2023/05/02 14:03:58 INFO mlflow.tracking.fluent: Experiment with name 'experiment_a' does not exist. Creating a new experiment.\n   {}\n   API request to endpoint /api/2.0/mlflow/runs/create failed with error code 403 != 200. Response body: 'Permission denied'\n   ```\n", '# MLflow 3 Examples\n\n## Pre-requisites\n\nBefore running the examples, run the following command to install mlflow 3.0:\n\n```sh\npip install git+https://github.com/mlflow/mlflow.git@mlflow-3\n```\n', '# MLflow Deployments\n\nThe examples provided within this directory show how to get started with MLflow Deployments using:\n\n- Databricks (see the `databricks` subdirectory)\n', '### MLflow evaluation Examples\n\nThe examples in this directory demonstrate how to use the `mlflow.evaluate()` API. Specifically,\nthey show how to evaluate a PyFunc model on a specified dataset using the builtin default evaluator\nand specified extra metrics, where the resulting metrics & artifacts are logged to MLflow Tracking.\nThey also show how to specify validation thresholds for the resulting metrics to validate the quality\nof your model. See full list of examples below:\n\n- Example `evaluate_on_binary_classifier.py` evaluates an xgboost `XGBClassifier` model on dataset loaded by\n  `shap.datasets.adult`.\n- Example `evaluate_on_multiclass_classifier.py` evaluates a scikit-learn `LogisticRegression` model on dataset\n  generated by `sklearn.datasets.make_classification`.\n- Example `evaluate_on_regressor.py` evaluate as scikit-learn `LinearRegression` model on dataset loaded by\n  `sklearn.datasets.fetch_california_housing`\n- Example `evaluate_with_custom_metrics.py` evaluates a scikit-learn `LinearRegression`\n  model with a custom metric function on dataset loaded by `sklearn.datasets.fetch_california_housing`\n- Example `evaluate_with_custom_metrics_comprehensive.py` evaluates a scikit-learn `LinearRegression` model\n  with a comprehensive list of custom metric functions on dataset loaded by `sklearn.datasets.fetch_california_housing`\n- Example `evaluate_with_model_validation.py` trains both a candidate xgboost `XGBClassifier` model\n  and a baseline `DummyClassifier` model on dataset loaded by `shap.datasets.adult`. Then, it validates\n  the candidate model against specified thresholds on both builtin and extra metrics and the dummy model.\n\n#### Prerequisites\n\n```\npip install scikit-learn xgboost shap>=0.40 matplotlib\n```\n\n#### How to run the examples\n\nRun in this directory with Python.\n\n```sh\npython evaluate_on_binary_classifier.py\npython evaluate_on_multiclass_classifier.py\npython evaluate_on_regressor.py\npython evaluate_with_custom_metrics.py\npython evaluate_with_custom_metrics_comprehensive.py\npython evaluate_with_model_vaidation.py\n```\n', '# MLflow AI Gateway\n\nThe examples provided within this directory show how to get started with individual providers and at least\none of the supported endpoint types. When configuring an instance of the MLflow AI Gateway, multiple providers,\ninstances of endpoint types, and model versions can be specified for each query endpoint on the server.\n\n## Example configuration files\n\nWithin this directory are example config files for each of the supported providers. If using these as a guide\nfor configuring a large number of endpoints, ensure that the placeholder names (i.e., "completions", "chat", "embeddings")\nare modified to prevent collisions. These names are provided for clarity only for the examples and real-world\nuse cases should define a relevant and meaningful endpoint name to eliminate ambiguity and minimize the chances of name collisions.\n\n# Getting Started with MLflow AI Gateway for OpenAI\n\nThis guide will walk you through the installation and basic setup of the MLflow AI Gateway.\nWithin sub directories of this examples section, you can find specific executable examples\nthat can be used to validate a given provider\'s configuration through the MLflow AI Gateway.\nLet\'s get started.\n\n## Step 1: Installing the MLflow AI Gateway\n\nThe MLflow AI Gateway is best installed from PyPI. Open your terminal and use the following pip command:\n\n```sh\n# Installation from PyPI\npip install \'mlflow[genai]\'\n```\n\nFor those interested in development or in using the most recent build of the MLflow AI Gateway, you may choose to install from the fork of the repository:\n\n```sh\n# Installation from the repository\npip install -e \'.[genai]\'\n```\n\n## Step 2: Configuring Endpoints\n\nEach provider has a distinct set of allowable endpoint types (i.e., chat, completions, etc) and\nspecific requirements for the initialization of the endpoints to interface with their services.\nFor full examples of configurations and supported endpoint types, see:\n\n- [OpenAI](openai/config.yaml)\n- [MosaicML](mosaicml/config.yaml)\n- [Anthropic](anthropic/config.yaml)\n- [Cohere](cohere/config.yaml)\n- [AI21 Labs](ai21labs/config.yaml)\n- [PaLM](palm/config.yaml)\n- [AzureOpenAI](azure_openai/config.yaml)\n- [Mistral](mistral/config.yaml)\n- [TogetherAI](togetherai/config.yaml)\n\n## Step 3: Setting Access Keys\n\nSee information on specific methods of obtaining and setting the access keys within the provider-specific documentation within this directory.\n\n## Step 4: Starting the MLflow AI Gateway\n\nWith the MLflow configuration file in place and access key(s) set, you can now start the MLflow AI Gateway.\nReplace `<provider>` with the actual path to the MLflow configuration file for the provider of your choice:\n\n```sh\nmlflow ', "(i.e., chat, completions, etc) and\nspecific requirements for the initialization of the endpoints to interface with their services.\nFor full examples of configurations and supported endpoint types, see:\n\n- [OpenAI](openai/config.yaml)\n- [MosaicML](mosaicml/config.yaml)\n- [Anthropic](anthropic/config.yaml)\n- [Cohere](cohere/config.yaml)\n- [AI21 Labs](ai21labs/config.yaml)\n- [PaLM](palm/config.yaml)\n- [AzureOpenAI](azure_openai/config.yaml)\n- [Mistral](mistral/config.yaml)\n- [TogetherAI](togetherai/config.yaml)\n\n## Step 3: Setting Access Keys\n\nSee information on specific methods of obtaining and setting the access keys within the provider-specific documentation within this directory.\n\n## Step 4: Starting the MLflow AI Gateway\n\nWith the MLflow configuration file in place and access key(s) set, you can now start the MLflow AI Gateway.\nReplace `<provider>` with the actual path to the MLflow configuration file for the provider of your choice:\n\n```sh\nmlflow gateway start --config-path examples/gateway/<provider>/config.yaml --port 7000\n\n# For example:\nmlflow gateway start --config-path examples/gateway/openai/config.yaml --port 7000\n```\n\n## Step 5: Accessing the Interactive API Documentation\n\nWith the MLflow AI Gateway up and running, access its interactive API documentation by navigating to the following URL:\n\nhttp://127.0.0.1:7000/docs\n\n## Step 6: Sending Test Requests\n\nAfter successfully setting up the MLflow AI Gateway, you can send a test request using the provided Python script.\nReplace <provider> with the name of the provider example test script that you'd like to use:\n\n```sh\npython examples/gateway/<provider>/example.py\n```\n", "## Example endpoint configuration for AI21 Labs\n\nTo set up your MLflow configuration file, include a single endpoint for the completions endpoint as shown in the [AI21 labs configuration](config.yaml) YAML file.\n\n## Obtaining and Setting the AI21 Labs API Key\n\nTo obtain an AI21 Labs API key, you need to create an account and subscribe to the service at [AI21 Labs](https://studio.ai21.com/account/api-key?source=docs).\n\nAfter obtaining the key, you can export it to your environment variables. Make sure to replace the '...' with your actual API key:\n\n```sh\nexport AI21LABS_API_KEY=...\n```\n", "## Example endpoint configuration for Anthropic\n\nTo set up your MLflow configuration file, include a single endpoint for the completions endpoint as shown in the [anthropic configuration](config.yaml) YAML file.\n\n## Obtaining and Setting the Anthropic API Key\n\nTo obtain an Anthropic API key, you need to create an account and subscribe to the service at [Anthropic](https://docs.anthropic.com/claude/docs/getting-access-to-claude).\n\nAfter obtaining the key, you can export it to your environment variables. Make sure to replace the '...' with your actual API key:\n\n```sh\nexport ANTHROPIC_API_KEY=...\n```\n", "## Example endpoint configuration for Azure OpenAI\n\nThe following example configuration shows the 3 supported endpoints for Azure OpenAI: chat, completions, and embeddings.\nAdditionally, it illustrates the two separate api types that are supported for this service.\n\n- `azure` api type: uses a generated token that is applied by setting the API token key directly to an environment variable\n- `azuread` api type: uses Azure Active Directory for supplying the active directory key to be used to an environment variable\n\nDepending on how your users will be interacting with the MLflow AI Gateway, a single access paradigm (either `azure` **or** `azuread` is recommended, not a mix of both).\n\nSee the [Azure OpenAI configuration](config.yaml) YAML file for example configurations showing all supported endpoint types and the different token access types.\n\n## Setting the Azure OpenAI API Key\n\nIn order to get access to the Azure OpenAI service, [see the documentation](https://azure.microsoft.com/en-us/products/cognitive-services/openai-service) guidance in the cognitive services portal.\nWith the key, export it to your environment variables.\n\nReplace the '...' with your actual API key:\n\n```sh\nexport OPENAI_API_KEY=...\n```\n\n## Validating the Azure OpenAI endpoint\n\nSee the [OpenAI Example](../openai/example.py) for testing the Azure OpenAI endpoints. The usage is identical to the standard OpenAI integration from an API perspective.\n", '## Example endpoint configuration for Amazon Bedrock\n\nTo view an example of a Bedrock endpoint configuration, see [the configuration example](config.yaml) YAML file.\n\n## Credentials\n\nValid AWS credentials are required for this example. Set `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` to valid credentials, or run in an environment with those variables set.\n', "## Example endpoint configuration for Cohere\n\nTo see an example of specifying both the completions and the embeddings endpoints for Cohere, see [the configuration](config.yaml) YAML file.\n\nThis configuration file specifies two endpoints: 'completions' and 'embeddings', both using Cohere's models 'command' and 'embed-english-light-v2.0', respectively.\n\n## Setting a Cohere API Key\n\nThis example requires a [Cohere API key](https://docs.cohere.com/docs/going-live):\n\n```sh\nexport COHERE_API_KEY=...\n```\n", "## Example endpoint configuration for GEMINI\n\nTo see an example of specifying both the completions and embeddings endpoints for Gemini, see [the configuration](config.yaml) YAML file.\n\nThis configuration file specifies three endpoints: 'completions', 'embeddings', and 'chat', using Gemini's model gemini-2.0-flash for completions and chat and gemini-embedding-exp-03-07 for embeddings.\n\n## Setting a GEMINI API Key\n\nThis example requires a [GEMINI API key](https://ai.google.dev/gemini-api/docs/api-key):\n\n```sh\nexport GEMINI_API_KEY=...\n```\n", '## Example endpoint configuration for Huggingface Text Generation Inference\n\n[Huggingface Text Generation Inference (TGI)](https://huggingface.co/docs/text-generation-inference/index) is a comprehensive toolkit designed for deploying and serving Large Language Models (LLMs) efficiently. It offers optimized support for various popular open-source LLMs such as Llama, Falcon, StarCoder, BLOOM, and GPT-Neo. TGI comes with various built-in optimizations and features, such as:\n\n- Simple launcher to serve most popular LLMs\n- Tensor Parallelism for faster inference on multiple GPUs\n- Safetensors weight loading\n- Optimized transformers code for inference using Flash Attention and Paged Attention on the most popular architectures\n\nIt should be noted that only a [selection of models](https://huggingface.co/docs/text-generation-inference/supported_models) are optimized for TGI, which uses custom CUDA kernels for faster inference. You can add the flag `--disable-custom-kernels`` at the end of the docker run command if you wish to disable them. If the above list lacks the model you would like to serve, or in the case you created a custom created model, you can try to initialize and serve the model anyways. However, since the model is not optimized for TGI, performance is not guaranteed.\n\nFor a more detailed description of all features, please go to the [documentation](https://huggingface.co/docs/text-generation-inference/index).\n\n## Getting Started\n\n> **NOTE** This example is tested on a Linux Machine (Debian 11) with a NVIDIA A100 GPU.\n\nTo configure the MLflow AI Gateway with Huggingface Text Generation Inference, a few additional steps need to be followed. The initial step involves deploying a Huggingface model on the TGI server, which is illustrated in the next section.\n\nThe recommended approach for deploying the TGI server is by utilizing the [official Docker container](ghcr.io/huggingface/text-generation-inference:1.1.1). Docker is an open-source platform that provides a streamlined solution for automating the deployment, scaling, and management of applications through containers. These containers encompass all the essential dependencies required for seamless execution, including libraries, binaries, and configuration files. To install Docker, please refer to the [installation guide](https://docs.docker.com/get-docker/).\n\nBefore proceeding, it is important to verify that your machine has the appropriate hardware to initiate the server. TGI optimized models are compatible with NVIDIA A100, A10G, and T4 GPUs. While other GPU hardware may still provide performance advantages, certain operations such as flash attention and paged attention will ', 'utilizing the [official Docker container](ghcr.io/huggingface/text-generation-inference:1.1.1). Docker is an open-source platform that provides a streamlined solution for automating the deployment, scaling, and management of applications through containers. These containers encompass all the essential dependencies required for seamless execution, including libraries, binaries, and configuration files. To install Docker, please refer to the [installation guide](https://docs.docker.com/get-docker/).\n\nBefore proceeding, it is important to verify that your machine has the appropriate hardware to initiate the server. TGI optimized models are compatible with NVIDIA A100, A10G, and T4 GPUs. While other GPU hardware may still provide performance advantages, certain operations such as flash attention and paged attention will not be executed. If you intend to run the container on a machine lacking GPUs or CUDA support, you can eliminate the `--gpus all` flag and include `--disable-custom-kernels`. However, please note that the CPU is not the intended platform for the server, and this choice significantly impacts performance.\n\n#### Installing the NVIDIA Container Toolkit\n\nTo begin, the installation of the NVIDIA container toolkit is necessary. This toolkit is essential for running GPU-accelerated containers. Execute the following command to acquire all the requisite packages [ref the code]:\n\n```sh\ncurl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\\n  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\\n    sed \'s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g\' | \\\n    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list \\\n  && \\\n    sudo apt-get update\n```\n\nInstall the NVIDIA Container toolkit by running the following command.\n\n```\nsudo apt-get install -y nvidia-container-toolkit\n```\n\n#### Running the TGI server.\n\nAfter you installed the NVIDIA Container toolkit, you can run the following Docker command to to start a TGI server on your local machine on port `8000`. This will load a [falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) model on the TGI server.\n\n```\nmodel=tiiuae/falcon-7b-instruct\nvolume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run\ndocker run --gpus all --shm-size 1g -p 8000:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:1.1.1 --model-id $model\n```\n\nAfter the TGI server is deployed, run the following script to verify that it is working correctly:\n\n```\nimport requests\nheaders = {\n    "Content-Type": "application/json",\n}\ndata = {\n    \'inputs\': \'What is Deep Learning?\',\n    \'parameters\': {\n      ', 'you can run the following Docker command to to start a TGI server on your local machine on port `8000`. This will load a [falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) model on the TGI server.\n\n```\nmodel=tiiuae/falcon-7b-instruct\nvolume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run\ndocker run --gpus all --shm-size 1g -p 8000:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:1.1.1 --model-id $model\n```\n\nAfter the TGI server is deployed, run the following script to verify that it is working correctly:\n\n```\nimport requests\nheaders = {\n    "Content-Type": "application/json",\n}\ndata = {\n    \'inputs\': \'What is Deep Learning?\',\n    \'parameters\': {\n        \'max_new_tokens\': 20,\n    },\n}\nresponse = requests.post(\'http://127.0.0.1:8000/generate\', headers=headers, json=data)\nprint(response.json())\n# {\'generated_text\': \'\\nDeep learning is a branch of machine learning that uses artificial neural networks to learn and make decisions.\'}\n```\n\n## Update the config.yaml to add a new embeddings endpoint\n\nAfter you started the server, update the MLflow AI Gateway configuration file [config.yaml](config.yaml) and add the server as a new endpoint:\n\n```\nendpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: "huggingface-text-generation-inference"\n      name: llm\n      config:\n        hf_server_url: http://127.0.0.1:8000/generate\n```\n\n## Starting the MLflow AI Gateway\n\nAfter the configuration file is created, you can start the MLflow AI Gateway by running the following command:\n\n```\nmlflow gateway start --config-path examples/gateway/huggingface/config.yaml --port 7000\n```\n\n## Querying the endpoint\n\nSee the [example script](example.py) within this directory to see how to query the `falcon-7b-instruct` model that is served.\n\n## Setting the parameters of TGI\n\nWhen you make a request to the MLflow Depoyments server, the information you provide in the request body will be sent to TGI. This gives you more control over the output you receive from TGI. However, it\'s important to note that you cannot turn off `details` and `decoder_input_details`, as they are necessary for TGI endpoints to work correctly.\n', "## Example endpoint configuration for Mistral\n\nTo see an example of specifying both the completions and the embeddings endpoints for Mistral, see [the configuration](config.yaml) YAML file.\n\nThis configuration file specifies two endpoints: 'completions' and 'embeddings', both using Mistral's models 'mistral-tiny' and 'mistral-embed', respectively.\n\n## Setting a Mistral API Key\n\nThis example requires a [Mistral API key](https://docs.mistral.ai/):\n\n```sh\nexport MISTRAL_API_KEY=...\n```\n", '# Guide to using an MLflow served model with MLflow Deployments\n\nIn order to utilize MLflow Deployments with MLflow model serving, a few steps must be taken\nin addition to those for configuring access to SaaS models (such as Anthropic and OpenAI). The first and most obvious\nstep that must be taken prior to interfacing with an MLflow served model is that a model needs to be logged to the\nMLflow tracking server.\n\nAn important consideration for deciding whether to interface MLflow Deployments with a specific model is to evaluate the PyFunc interface that the model will\nreturn after being called for inference. Due to the fact that the MLflow AI Gateway defines a specific response signature, expectations for each endpoint type\'s payload contents\nmust be met in order for a endpoint to be valid.\n\nFor example, an embeddings endpoint (llm/v1/embeddings endpoint type) is designed to return embeddings data as a collection (a list) of floats that correspond to each of the\ninput strings that are sent for embeddings inference to a service. The expectation that the embeddings endpoint definition has is that the data is in a particular format. Specifically one that\nis capable of having the embeddings data extractable from a service response. Therefore, an MLflow model that returns data in the format below is perfectly valid.\n\n```json\n{\n  "predictions": [\n    [0.0, 0.1],\n    [1.0, 0.0]\n  ]\n}\n```\n\nHowever, a return value from a serving endpoint via a custom PyFunc of the form below will not work.\n\n```json\n{\n  "predictions": [\n    {\n      "embedding": [0.0, 0.1]\n    },\n    {\n      "embedding": [1.0, 0.0]\n    }\n  ]\n}\n```\n\nIt is important to note that the MLflow AI Gateway does not perform validation on a configured endpoint until the point of querying. Creating a endpoint that interfaces with the\nMLflow model server that is returning a payload that is incompatible with the configured endpoint type definition will raise 502 exceptions only when queried.\n\n> **NOTE:** It is important to validate the output response of a model served by MLflow to ensure compatibility with ', '    "embedding": [0.0, 0.1]\n    },\n    {\n      "embedding": [1.0, 0.0]\n    }\n  ]\n}\n```\n\nIt is important to note that the MLflow AI Gateway does not perform validation on a configured endpoint until the point of querying. Creating a endpoint that interfaces with the\nMLflow model server that is returning a payload that is incompatible with the configured endpoint type definition will raise 502 exceptions only when queried.\n\n> **NOTE:** It is important to validate the output response of a model served by MLflow to ensure compatibility with the MLflow Deployments endpoint definitions. Not all model outputs are compatible with given endpoint types.\n\n## Creating and logging an embeddings model\n\nTo start, we need a model that is capable of generating embeddings. For this example, we\'ll use\nthe `sentence_transformers` library and the corresponding MLflow flavor.\n\n```python\nfrom sentence_transformers import SentenceTransformer\nimport mlflow\n\n\nmodel = SentenceTransformer(model_name_or_path="all-MiniLM-L6-v2")\nartifact_path = "embeddings_model"\n\nwith mlflow.start_run():\n    model_info = mlflow.sentence_transformers.log_model(\n        model,\n        name=artifact_path,\n    )\n```\n\n## Generate the cli command for starting a local MLflow Model Serving endpoint for this embeddings model\n\n```python\nprint(f"mlflow models serve -m {model_info.model_uri} -h 127.0.0.1 -p 9020 --no-conda")\n```\n\nCopy the output from the print statement to the clipboard.\n\n## Starting the model server for the embeddings model\n\nWith the printed string from running the above command copied to the clipboard, open a new terminal\nand paste the string. Leave the terminal window open and running.\n\n```commandline\nmlflow models serve -m file:///Users/me/demos/mlruns/0/2bfcdcb66eaf4c88abe8e0c7bcab639e/artifacts/embeddings_model -h 127.0.0.1 -p 9020 --no-conda\n```\n\n## Update the config.yaml to add a new embeddings endpoint\n\nAfter assigning a valid port and ensuring that the model server starts correctly:\n\n```commandline\n2023/08/08 17:36:44 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor \'python_function\'\n2023/08/08 17:36:44 INFO mlflow.pyfunc.backend: === Running command \'exec uvicorn --host 127.0.0.1 --port 9020 --workers 1 mlflow.pyfunc.scoring_server.app:app\'\nINFO:     Started server process [6992]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:9020\n```\n\nThe scoring server is ready to receive traffic.\n\nUpdate the MLflow AI Gateway configuration file (config.yaml) with the new endpoint:\n\n```yaml\nendpoints:\n  ', 'file:///Users/me/demos/mlruns/0/2bfcdcb66eaf4c88abe8e0c7bcab639e/artifacts/embeddings_model -h 127.0.0.1 -p 9020 --no-conda\n```\n\n## Update the config.yaml to add a new embeddings endpoint\n\nAfter assigning a valid port and ensuring that the model server starts correctly:\n\n```commandline\n2023/08/08 17:36:44 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor \'python_function\'\n2023/08/08 17:36:44 INFO mlflow.pyfunc.backend: === Running command \'exec uvicorn --host 127.0.0.1 --port 9020 --workers 1 mlflow.pyfunc.scoring_server.app:app\'\nINFO:     Started server process [6992]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:9020\n```\n\nThe scoring server is ready to receive traffic.\n\nUpdate the MLflow AI Gateway configuration file (config.yaml) with the new endpoint:\n\n```yaml\nendpoints:\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: mlflow-model-serving\n      name: sentence-transformer\n      config:\n        model_server_url: http://127.0.0.1:9020\n```\n\nThe key component here is the `model_server_url`. For serving an MLflow LLM, this url must match to the service that you are specifying for the\nModel Serving server.\n\n> **NOTE:** The MLflow Model Server does not have to be running in order to update the configuration file or to start the MLflow AI Gateway. In order to respond to submitted queries, it is required to be running.\n\n## Creating and logging a fill mask model\n\nTo support an additional endpoint for generating a mask fill response from masked input text, we need to log an appropriate model.\nFor this tutorial example, we\'ll use a `transformers` `Pipeline` wrapping a `BertForMaskedLM` torch model and will log this pipeline using the MLflow `transformers` flavor.\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nimport mlflow\n\n\nlm_architecture = "bert-base-cased"\nartifact_path = "mask_fill_model"\n\ntokenizer = AutoTokenizer.from_pretrained(lm_architecture)\nmodel = AutoModelForMaskedLM.from_pretrained(lm_architecture)\n\ncomponents = {"model": model, "tokenizer": tokenizer}\n\nwith mlflow.start_run():\n    model_info = mlflow.transformers.log_model(\n        transformers_model=components,\n        name=artifact_path,\n    )\n```\n\n## Generate the cli command for starting a local MLflow Model Serving endpoint for this fill mask model\n\n```python\nprint(f"mlflow models serve -m {model_info.model_uri} -h 127.0.0.1 -p 9010 --no-conda")\n```\n\n## Starting the model server for the fill mask model\n\nUsing the command printed to stdout from above, open a new terminal (do not close ', 'pipeline using the MLflow `transformers` flavor.\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nimport mlflow\n\n\nlm_architecture = "bert-base-cased"\nartifact_path = "mask_fill_model"\n\ntokenizer = AutoTokenizer.from_pretrained(lm_architecture)\nmodel = AutoModelForMaskedLM.from_pretrained(lm_architecture)\n\ncomponents = {"model": model, "tokenizer": tokenizer}\n\nwith mlflow.start_run():\n    model_info = mlflow.transformers.log_model(\n        transformers_model=components,\n        name=artifact_path,\n    )\n```\n\n## Generate the cli command for starting a local MLflow Model Serving endpoint for this fill mask model\n\n```python\nprint(f"mlflow models serve -m {model_info.model_uri} -h 127.0.0.1 -p 9010 --no-conda")\n```\n\n## Starting the model server for the fill mask model\n\nUsing the command printed to stdout from above, open a new terminal (do not close the terminal that is currently running the embeddings model being served!)\nand paste the command.\n\n```commandline\nmlflow models serve -m file:///Users/me/demos/mlruns/0/bc8bdb7fb90c406eb95603a97742cef8/artifacts/mask_fill_model -h 127.0.0.1 -p 9010 --no-conda\n```\n\n## Update the config.yaml to add a new completions endpoint\n\nEnsure that the MLflow serving endpoint starts and is ready for traffic.\n\n```commandline\n2023/08/08 17:39:14 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor \'python_function\'\n2023/08/08 17:39:14 INFO mlflow.pyfunc.backend: === Running command \'exec uvicorn --host 127.0.0.1 --port 9010 --workers 1 mlflow.pyfunc.scoring_server.app:app\'\nINFO:     Started server process [6992]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:9010\n```\n\nAdd the entry to the MLflow AI Gateway configuration file. The final file should match [the config file](config.yaml)\n\n## Create a completions model using MPT-7B-instruct (optional, see notes below)\n\n> **NOTE:** If your system does not have a CUDA-compatible GPU and you have not installed torch with the appropriate CUDA libraries, it is not recommended to attempt to run this portion of the example.\n> The inference performance of the MPT-7B-instruct model running on CPU is very slow.\n> It is also not recommended to add this model to an MLflow model serving environment that does not have a sufficiently powerful GPU available.\n\n### Download the MPT-7B instruct model and tokenizer to a local directory cache\n\n```python\nfrom huggingface_hub import snapshot_download\n\nsnapshot_location = snapshot_download(\n    repo_id="mosaicml/mpt-7b-instruct", local_dir="mpt-7b"\n)\n```\n\n### Define the PyFunc model that will be used for the completions endpoint\n\n```python\nimport transformers\nimport mlflow\nimport torch\n\n\nclass MPT(mlflow.pyfunc.PythonModel):\n    def load_context(self, context):\n        """\n    ', 'attempt to run this portion of the example.\n> The inference performance of the MPT-7B-instruct model running on CPU is very slow.\n> It is also not recommended to add this model to an MLflow model serving environment that does not have a sufficiently powerful GPU available.\n\n### Download the MPT-7B instruct model and tokenizer to a local directory cache\n\n```python\nfrom huggingface_hub import snapshot_download\n\nsnapshot_location = snapshot_download(\n    repo_id="mosaicml/mpt-7b-instruct", local_dir="mpt-7b"\n)\n```\n\n### Define the PyFunc model that will be used for the completions endpoint\n\n```python\nimport transformers\nimport mlflow\nimport torch\n\n\nclass MPT(mlflow.pyfunc.PythonModel):\n    def load_context(self, context):\n        """\n        This method initializes the tokenizer and language model\n        using the specified model snapshot directory.\n        """\n        # Initialize tokenizer and language model\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n            context.artifacts["snapshot"], padding_side="left"\n        )\n\n        config = transformers.AutoConfig.from_pretrained(\n            context.artifacts["snapshot"], trust_remote_code=True\n        )\n        # Comment out this configuration setting if not running on a GPU or if triton is not installed.\n        # Note that triton dramatically improves the inference speed performance\n        config.attn_config["attn_impl"] = "triton"\n\n        self.model = transformers.AutoModelForCausalLM.from_pretrained(\n            context.artifacts["snapshot"],\n            config=config,\n            torch_dtype=torch.bfloat16,\n            trust_remote_code=True,\n        )\n\n        # NB: If you do not have a CUDA-capable device or have torch installed with CUDA support\n ', '       config.attn_config["attn_impl"] = "triton"\n\n        self.model = transformers.AutoModelForCausalLM.from_pretrained(\n            context.artifacts["snapshot"],\n            config=config,\n            torch_dtype=torch.bfloat16,\n            trust_remote_code=True,\n        )\n\n        # NB: If you do not have a CUDA-capable device or have torch installed with CUDA support\n        # this setting will not function correctly. Setting device to \'cpu\' is valid, but\n        # the performance will be very slow.\n        self.model.to(device="cuda")\n\n        self.model.eval()\n\n    def _build_prompt(self, instruction):\n        """\n        This method generates the prompt for the model.\n        """\n        INSTRUCTION_KEY = "### Instruction:"\n        RESPONSE_KEY = "### Response:"\n        INTRO_BLURB = (\n            "Below is an instruction that describes a task. "\n            "Write a response that appropriately completes the request."\n        )\n\n        return f"""{INTRO_BLURB}\n        {INSTRUCTION_KEY}\n        {instruction}\n        {RESPONSE_KEY}\n        """\n\n    def predict(self, context, model_input, params=None):\n        """\n        This method generates prediction for the given input.\n        """\n    ', ' "Write a response that appropriately completes the request."\n        )\n\n        return f"""{INTRO_BLURB}\n        {INSTRUCTION_KEY}\n        {instruction}\n        {RESPONSE_KEY}\n        """\n\n    def predict(self, context, model_input, params=None):\n        """\n        This method generates prediction for the given input.\n        """\n        prompt = model_input["prompt"][0]\n        temperature = model_input.get("temperature", [1.0])[0]\n        max_tokens = model_input.get("max_tokens", [100])[0]\n\n        # Build the prompt\n        prompt = self._build_prompt(prompt)\n\n        # Encode the input and generate prediction\n        # NB: Sending the tokenized inputs to the GPU here explicitly will not work if your system does not have CUDA support.\n        # If attempting to run this with only CPU support, change \'cuda\' to \'cpu\'\n        encoded_input = self.tokenizer.encode(prompt, return_tensors="pt").to("cuda")\n        output = self.model.generate(\n            encoded_input,\n            do_sample=True,\n            temperature=temperature,\n            max_new_tokens=max_tokens,\n        )\n\n        # Decode the prediction to text\n        generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n\n        # Removing the prompt from the generated text\n        prompt_length = len(self.tokenizer.encode(prompt, return_tensors="pt")[0])\n        generated_response = ', '        do_sample=True,\n            temperature=temperature,\n            max_new_tokens=max_tokens,\n        )\n\n        # Decode the prediction to text\n        generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n\n        # Removing the prompt from the generated text\n        prompt_length = len(self.tokenizer.encode(prompt, return_tensors="pt")[0])\n        generated_response = self.tokenizer.decode(\n            output[0][prompt_length:], skip_special_tokens=True\n        )\n\n        return {"candidates": [generated_response]}\n```\n\n### Specify the model signature, input example, and log the custom model\n\n```python\nimport pandas as pd\nimport mlflow\nfrom mlflow.models.signature import ModelSignature\nfrom mlflow.types import DataType, Schema, ColSpec\n\n# Define input and output schema\ninput_schema = Schema(\n    [\n        ColSpec(DataType.string, "prompt"),\n        ColSpec(DataType.double, "temperature"),\n        ColSpec(DataType.long, "max_tokens"),\n    ]\n)\noutput_schema = Schema([ColSpec(DataType.string, "candidates")])\nsignature = ModelSignature(inputs=input_schema, outputs=output_schema)\n\n\n# Define input example\ninput_example = pd.DataFrame(\n    {"prompt": ["What is machine learning?"], "temperature": [0.5], "max_tokens": [100]}\n)\n\nwith mlflow.start_run():\n    mlflow.pyfunc.log_model(\n        name="mpt-7b-instruct",\n        python_model=MPT(),\n        artifacts={"snapshot": snapshot_location},\n        pip_requirements=[\n            "torch",\n            "transformers",\n            "accelerate",\n            "einops",\n            "sentencepiece",\n        ],\n        input_example=input_example,\n        signature=signature,\n    )\n```\n\n## Starting the model server ', '       pip_requirements=[\n            "torch",\n            "transformers",\n            "accelerate",\n            "einops",\n            "sentencepiece",\n        ],\n        input_example=input_example,\n        signature=signature,\n    )\n```\n\n## Starting the model server for mpt-7B-instruct (Optional)\n\nDue to the size and complexity of the MPT-7B-instruct model, it is highly advised to only attempt to serve this model in an environment that has:\n\n- A powerful GPU that is capable of holding the model weights in GPU memory\n- triton installed\n\nIn order to initialize the MLflow Model Server for a large model such as MPT-7B, a slightly modified cli command must be used. Most notably, the timeout duration must be increased from the\ndefault of 60 seconds and it is highly recommended to utilize only a single Gunicorn worker (since each worker will load its own copy of the model, there is a distinct possibility of crashing the server environment with an out of memory fault).\n\n```commandline\nmlflow models serve -m file:///Users/me/demos/mlruns/0/92d017e23ca04ffa919a935ed54e9334/artifacts/mpt-7b-instruct -h 127.0.0.1 -p 9030 -t 1200 -w 1 --no-conda\n```\n\n## Update the config.yaml to add the MPT-7B-instruct endpoint (Optional)\n\n> **NOTE** If you are adding this endpoint for the example, you will have to manually edit the config.yaml. If the server that is running the MPT-7B-instruct custom PyFunc model\'s inference does not have GPU support,\n> the performance for inference will take a very long time (CPU inference with this model can take tens of minutes for a single query).\n\n```yaml\nendpoints:\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: mlflow-model-serving\n      name: sentence-transformer\n      config:\n        model_server_url: http://127.0.0.1:9020\n  - name: fillmask\n    endpoint_type: llm/v1/completions\n ', 'you will have to manually edit the config.yaml. If the server that is running the MPT-7B-instruct custom PyFunc model\'s inference does not have GPU support,\n> the performance for inference will take a very long time (CPU inference with this model can take tens of minutes for a single query).\n\n```yaml\nendpoints:\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: mlflow-model-serving\n      name: sentence-transformer\n      config:\n        model_server_url: http://127.0.0.1:9020\n  - name: fillmask\n    endpoint_type: llm/v1/completions\n    model:\n      provider: mlflow-model-serving\n      name: fill-mask\n      config:\n        model_server_url: http://127.0.0.1:9010\n  - name: mpt-instruct\n    endpoint_type: llm/v1/completions\n    model:\n      provider: mlflow-model-serving\n      name: mpt-7b-instruct\n      config:\n        model_server_url: http://127.0.0.1:9030\n```\n\n## Start the MLflow AI Gateway\n\nNow that both endpoints (or all 3, if adding in the optional MPT-7B-instruct model endpoint) are defined within the configuration YAML file and the Model Serving servers are ready to receive queries, we can start the MLflow AI Gateway.\n\n```sh\nmlflow gateway start --config-path examples/gateway/mlflow_serving/config.yaml --port 7000\n```\n\nIf adding the mpt-7b-instruct model, start the MLflow AI Gateway by directing the `--config-path` argument to the location of the `config.yaml` file that you\'ve created with the endpoint\'s addition.\n\n## Query the MLflow AI Gateway\n\nSee the [example script](example.py) within this directory to see how to query these two models that are being served.\n\n### Query the mpt-7B-instruct endpoint (Optional)\n\nIn order to query the mpt-7b-instruct model, the example shown in the script can be modified by adding an additional query call, as shown below:\n\n```python\n# Querying the optional mpt-7b-instruct endpoint\nresponse_mpt = query(\n    endpoint="mpt-instruct",\n    data={\n        "prompt": "What is the purpose of an attention mask in a transformers model?",\n        "temperature": 0.1,\n   ', 'endpoint\'s addition.\n\n## Query the MLflow AI Gateway\n\nSee the [example script](example.py) within this directory to see how to query these two models that are being served.\n\n### Query the mpt-7B-instruct endpoint (Optional)\n\nIn order to query the mpt-7b-instruct model, the example shown in the script can be modified by adding an additional query call, as shown below:\n\n```python\n# Querying the optional mpt-7b-instruct endpoint\nresponse_mpt = query(\n    endpoint="mpt-instruct",\n    data={\n        "prompt": "What is the purpose of an attention mask in a transformers model?",\n        "temperature": 0.1,\n        "max_tokens": 200,\n    },\n)\nprint(f"Fluent API response for mpt-instruct: {response_mpt}")\n```\n', "## Example endpoint configuration for MosaicML\n\nTo see an example of specifying both the completions and the embeddings endpoints for MosaicML, see [the configuration](config.yaml) YAML file.\n\nThis configuration file specifies three endpoints: 'completions', 'embeddings', and 'chat', using MosaicML's models 'mpt-7b-instruct', 'instructor-xl', and 'llama2-70b-chat', respectively.\n\n## Setting a MosaicML API Key\n\nThis example requires a [MosaicML API key](https://docs.mosaicml.com/en/latest/getting_started.html):\n\n```sh\nexport MOSAICML_API_KEY=...\n```\n", "## Example endpoint configuration for OpenAI\n\nTo view an example of OpenAI endpoint configurations, see [the configuration example](config.yaml) YAML file for OpenAI.\n\nThis configuration shows all 3 supported endpoint types: chat, completions, and embeddings.\n\n## Setting the OpenAI API Key\n\nAn OpenAI API key is required for the configuration. If you haven't already, obtain an [OpenAI API key](https://platform.openai.com/account/api-keys).\n\nWith the key, export it to your environment variables. Replace the '...' with your actual API key:\n\n```sh\nexport OPENAI_API_KEY=...\n```\n", "## Example endpoint configuration for PaLM\n\nTo see an example of specifying both the completions and the embeddings endpoints for PaLM, see [the configuration](config.yaml) YAML file.\n\nThis configuration file specifies three endpoints: 'completions', 'embeddings', and 'chat', using PaLM's models 'text-bison-001', 'embedding-gecko-001', and 'chat-bison-001', respectively.\n\n## Setting a PaLM API Key\n\nThis example requires a [PaLM API key](https://developers.generativeai.google/tutorials/setup):\n\n```sh\nexport PALM_API_KEY=...\n```\n", "## Example endpoint configuration for plugin provider\n\nTo see an example of specifying the chat endpoint for a plugin provider,\nsee [the configuration](config.yaml) YAML file.\n\nWe implement our plugin provider package `my_llm` under `./my-llm` folder. It implements the chat method.\n\nThis configuration file specifies one endpoint: 'chat', using the model 'my-model-0.1.2'.\n\n## Setting up the server\n\nFirst, install the provider package `my_llm`:\n\n```sh\npip install -e ./my-llm\n```\n\nThen, start the server:\n\n```sh\nMY_LLM_API_KEY=some-api-key mlflow gateway start --config-path config.yaml --port 7000\n```\n\nTo clean up the installed package after the example, run\n\n```sh\npip uninstall my_llm\n```\n", "## Example endpoint configuration for TogetherAI\n\nTo see an example of specifying both the completions and the embeddings endpoints for TogetherAI, see [the configuration](config.yaml) YAML file.\n\nThis configuration file specifies two endpoints: 'completions' and 'embeddings', both using TogetherAI's provided models 'mistralai/Mixtral-8x7B-v0.1' and 'togethercomputer/m2-bert-80M-8k-retrieval', respectively.\n\n## Setting a Mistral API Key\n\nThis example requires a [TogetherAI API key](https://docs.together.ai/docs/):\n\n```sh\nexport TOGETHERAI_API_KEY=...\n```\n", '# Unity Catalog Integration\n\nThis example demonstrates how to use the Unity Catalog (UC) integration with MLflow AI Gateway.\n\n## Pre-requisites\n\n1. Install the required packages:\n\n```bash\npip install mlflow openai databricks-sdk\n```\n\n2. Create the UC function used in `run.py` by running the following command on Databricks notebook:\n\n```\n%sql\n\nCREATE OR REPLACE FUNCTION\nmy.uc_func.add (\n  x INTEGER COMMENT \'The first number to add.\',\n  y INTEGER COMMENT \'The second number to add.\'\n)\nRETURNS INTEGER\nLANGUAGE SQL\nRETURN x + y\n```\n\nTo define your own function, see https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.html#create-function-sql-and-python.\n\n3. Create a SQL warehouse in Databricks by following the instructions at https://docs.databricks.com/en/compute/sql-warehouse/create.html.\n\n## Running the example script\n\nFirst, run the deployments server:\n\n```bash\n# Required to authenticate with Databricks. See https://docs.databricks.com/en/dev-tools/auth/index.html#supported-authentication-types-by-databricks-tool-or-sdk for other authentication methods.\nexport DATABRICKS_HOST="..."   # e.g. https://my.databricks.com\nexport DATABRICKS_TOKEN="..."\n\n# Required to execute UC functions. See https://docs.databricks.com/en/integrations/compute-details.html#get-connection-details-for-a-databricks-compute-resource for how to get the http path of your warehouse.\n# The last part of the http path is the warehouse ID.\n#\n# /sql/1.0/warehouses/1234567890123456\n#                     ^^^^^^^^^^^^^^^^\nexport DATABRICKS_WAREHOUSE_ID="..."\n\n# Enable Unity Catalog integration\nexport MLFLOW_ENABLE_UC_FUNCTIONS=true\n\nmlflow gateway start --config-path examples/gateway/openai/config.yaml --port 7000\n```\n\nOnce the server starts running, run the example script:\n\n```bash\n# Replace `my.uc_func.add` if your UC function has a different name\npython examples/gateway/uc_functions/run.py  --uc-function-name my.uc_func.add\n```\n', '# Examples for LightGBM Autologging\n\nLightGBM autologging functionalities are demonstrated through two examples. The first example in the `lightgbm_native` folder logs a Booster model trained by `lightgbm.train()`. The second example in the `lightgbm_sklearn` folder shows how autologging works for LightGBM scikit-learn models. The autologging for all LightGBM models is enabled via `mlflow.lightgbm.autolog()`.\n', '# LightGBM Example\n\nThis example trains a LightGBM classifier with the iris dataset and logs hyperparameters, metrics, and trained model.\n\n## Running the code\n\n```\npython train.py --colsample-bytree 0.8 --subsample 0.9\n```\n\nYou can try experimenting with different parameter values like:\n\n```\npython train.py --learning-rate 0.4 --colsample-bytree 0.7 --subsample 0.8\n```\n\nThen you can open the MLflow UI to track the experiments and compare your runs via:\n\n```\nmlflow ui\n```\n\n## Running the code as a project\n\n```\nmlflow run . -P learning_rate=0.2 -P colsample_bytree=0.8 -P subsample=0.9\n```\n', '# XGBoost Scikit-learn Model Example\n\nThis example trains an [`LightGBM.LGBMClassifier`](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html) with the diabetes dataset and logs hyperparameters, metrics, and trained model.\n\nLike the other LightGBM example, we enable autologging for LightGBM scikit-learn models via `mlflow.lightgbm.autolog()`. Saving / loading models also supports LightGBM scikit-learn models.\n\nYou can run this example using the following command:\n\n```shell\npython train.py\n```\n', '# MLflow LlamaIndex Workflow Example\n\nThis example demonstrates how to build and optimize a Retrieval-Augmented Generation (RAG) workflow using [LlamaIndex](https://www.llamaindex.ai/) integrated with [MLflow](https://mlflow.org/docs/latest/llms/llama-index/index.html). The example covers various retrieval strategies such as vector search, BM25, and web search, along with logging, model tracking, and performance evaluation in MLflow.\n\n![Hybrid RAG Concept](static/images/llama_index_workflow_hybrid_rag_concept.png)\n\n![Evaluation Result](static/images/llama_index_workflow_result_chart.png)\n\n## Set Up\n\nThis repository contains a complete workflow definition, a hands-on notebook, and a sample dataset for running experiments. To clone it to your working environment, use the following command:\n\n```shell\ngit clone https://github.com/mlflow/mlflow.git\n```\n\nAfter cloning the repository, set up the virtual environment by running:\n\n```\ncd mlflow/examples/llama_index/workflow\nchmod +x install.sh\n./install.sh\n```\n\nOnce the installation is complete, start Jupyter Notebook within the Poetry environment using:\n\n```\npoetry run jupyter notebook\n```\n', "# MLflow examples for LLM use cases\n\nThis directory includes several examples for tracking, evaluating, and scoring models with LLMs.\n\n## Summarization\n\nThe `summarization/summarization.py` script uses prompt engineering to build two summarization models for news articles with LangChain. It leverages the `mlflow.langchain` flavor to package and log the models to MLflow, `mlflow.evaluate()` to evaluate each model's performance on a small example dataset, and `mlflow.pyfunc.load_model()` to load and score the best packaged model on a new example article.\n\nTo run the example as an MLflow Project, simply execute the following command from this directory:\n\n```\n$ cd summarization && mlflow run .\n```\n\nTo run the example as a Python script, simply execute the following command from this directory:\n\n```\n$ cd summarization && python summarization.py\n```\n\nNote that this example requires MLflow 2.4.0 or greater to run. Additionally, you must have [LangChain](https://python.langchain.com/en/latest/index.html) and the [OpenAI Python client](https://pypi.org/project/openai/) installed in order to run the example. We also recommend installing the [Hugging Face Evaluate library](https://huggingface.co/docs/evaluate/index) to compute [ROUGE metrics](<https://en.wikipedia.org/wiki/ROUGE_(metric)>) for summary quality. Finally, you must specify a valid OpenAI API key in the `OPENAI_API_KEY` environment variable.\n\n## Question answering\n\nThe `question_answering/question_answering.py` script uses prompt engineering to build two models that answer questions about MLflow.\n\nIt leverages the `mlflow.openai` flavor to package and log the models to MLflow, `mlflow.evaluate()` to evaluate each model's performance on some example questions, and `mlflow.pyfunc.load_model()` to load and score the best packaged model on a new example question.\n\nTo run the example as an MLflow Project, simply execute the following command from this directory:\n\n```\n$ cd question_answering && mlflow run .\n```\n\nTo run the example as a Python script, simply execute the following command from this directory:\n\n```\n$ cd question_answering && python question_answering.py\n```\n\nNote that this example requires MLflow 2.4.0 or greater to run. Additionally, you must have the [OpenAI Python client](https://pypi.org/project/openai/), [tiktoken](https://pypi.org/project/tiktoken/), and [tenacity](https://pypi.org/project/tenacity/) installed in order to run the example. Finally, you must specify a valid OpenAI API key in the `OPENAI_API_KEY` environment variable.\n", '# MLflow Artifacts Example\n\nThis directory contains a set of files for demonstrating the MLflow Artifacts Service.\n\n## What does the MLflow Artifacts Service do?\n\nThe MLflow Artifacts Service serves as a proxy between the client and artifact storage (e.g. S3)\nand allows the client to upload, download, and list artifacts via REST API without configuring\na set of credentials required to access resources in the artifact storage (e.g. `AWS_ACCESS_KEY_ID`\nand `AWS_SECRET_ACCESS_KEY` for S3).\n\n## Quick start\n\nFirst, launch the tracking server with the artifacts service via `mlflow server`:\n\n```sh\n# Launch a tracking server with the artifacts service\n$ mlflow server \\\n    --backend-store-uri=mlruns \\\n    --artifacts-destination ./mlartifacts \\\n    --default-artifact-root http://localhost:5000/api/2.0/mlflow-artifacts/artifacts/experiments \\\n    --gunicorn-opts "--log-level debug"\n```\n\nNotes:\n\n- `--artifacts-destination` specifies the base artifact location from which to resolve artifact upload/download/list requests. In this examples, we\'re using a local directory `./mlartifacts`, but it can be changed to a s3 bucket or\n- `--default-artifact-root` points to the `experiments` directory of the artifacts service. Therefore, the default artifact location of a newly-created experiment is set to `./mlartifacts/experiments/<experiment_id>`.\n- `--gunicorn-opts "--log-level debug"` is specified to print out request logs but can be omitted if unnecessary.\n- `--artifacts-only` disables all other endpoints for the tracking server apart from those involved in listing, uploading, and downloading artifacts. This makes the MLflow server a single-purpose proxy for artifact handling only.\n\nThen, run `example.py` that performs upload, download, and list operations for artifacts:\n\n```\n$ MLFLOW_TRACKING_URI=http://localhost:5000 python example.py\n```\n\nAfter running the command above, the server should print out request logs for artifact operations:\n\n```diff\n...\n[2021-11-05 19:13:34 +0900] [92800] [DEBUG] POST /api/2.0/mlflow/runs/create\n[2021-11-05 19:13:34 +0900] [92800] [DEBUG] GET /api/2.0/mlflow/runs/get\n[2021-11-05 19:13:34 +0900] [92802] [DEBUG] PUT /api/2.0/mlflow-artifacts/artifacts/0/a1b2c3d4/artifacts/a.txt\n[2021-11-05 19:13:34 +0900] [92802] [DEBUG] PUT /api/2.0/mlflow-artifacts/artifacts/0/a1b2c3d4/artifacts/dir/b.txt\n[2021-11-05 19:13:34 +0900] [92802] [DEBUG] POST /api/2.0/mlflow/runs/update\n[2021-11-05 19:13:34 +0900] [92802] [DEBUG] GET /api/2.0/mlflow-artifacts/artifacts\n...\n```\n\nThe contents of the `mlartifacts` directory should look like this:\n\n```sh\n$ tree mlartifacts\nmlartifacts\nâ””â”€â”€ experiments\n    â””â”€â”€ 0  # experiment ID\n        â””â”€â”€ a1b2c3d4  # run ID\n            â””â”€â”€ artifacts\n                â”œâ”€â”€ a.txt\n      ', '/api/2.0/mlflow/runs/create\n[2021-11-05 19:13:34 +0900] [92800] [DEBUG] GET /api/2.0/mlflow/runs/get\n[2021-11-05 19:13:34 +0900] [92802] [DEBUG] PUT /api/2.0/mlflow-artifacts/artifacts/0/a1b2c3d4/artifacts/a.txt\n[2021-11-05 19:13:34 +0900] [92802] [DEBUG] PUT /api/2.0/mlflow-artifacts/artifacts/0/a1b2c3d4/artifacts/dir/b.txt\n[2021-11-05 19:13:34 +0900] [92802] [DEBUG] POST /api/2.0/mlflow/runs/update\n[2021-11-05 19:13:34 +0900] [92802] [DEBUG] GET /api/2.0/mlflow-artifacts/artifacts\n...\n```\n\nThe contents of the `mlartifacts` directory should look like this:\n\n```sh\n$ tree mlartifacts\nmlartifacts\nâ””â”€â”€ experiments\n    â””â”€â”€ 0  # experiment ID\n        â””â”€â”€ a1b2c3d4  # run ID\n            â””â”€â”€ artifacts\n                â”œâ”€â”€ a.txt\n                â””â”€â”€ dir\n                    â””â”€â”€ b.txt\n\n5 directories, 2 files\n```\n\nTo delete the logged artifacts, run the following command:\n\n```bash\nmlflow gc --backend-store-uri=mlruns --run-ids <run_id>\n```\n\n### Clean up\n\n```sh\n# Remove experiment and run data\n$ rm -rf mlruns\n\n# Remove artifacts\n$ rm -rf mlartifacts\n```\n\n## Advanced example using `docker-compose`\n\n[`docker-compose.yml`](./docker-compose.yml) provides a more advanced setup than the quick-start example above:\n\n- Tracking service uses PostgreSQL as a backend store.\n- Artifact service uses MinIO as a artifact store.\n- Tracking and artifacts services are running on different servers.\n\n```sh\n# Build services\n$ docker-compose build\n\n# Launch tracking and artifacts servers in the background\n$ docker-compose up -d\n\n# Run `example.py` in the client container\n$ docker-compose run -v ${PWD}/example.py:/app/example.py client python example.py\n```\n\nYou can view the logged artifacts on MinIO Console served at http://localhost:9001. The login username and password are `user` and `password`.\n\n### Clean up\n\n```sh\n# Remove containers, networks, volumes, and images\n$ docker-compose down --rmi all --volumes --remove-orphans\n```\n\n### Development\n\n```sh\n# Build services using the dev version of mlflow\n$ ./build.sh\n$ docker-compose run -v ${PWD}/example.py:/app/example.py client python example.py\n```\n', '# OpenAI Autologging Examples\n\n## Using OpenAI client\n\nThe recommended way of using `openai` is to instantiate a client\nusing `openai.OpenAI()`. You can run the following example to use\nautologging using such client.\n\nBefore running these examples, ensure that you have the following additional libraries installed:\n\n```shell\npip install tenacity tiktoken \'openai>=1.17\'\n```\n\nYou can run the example via your command prompt as follows:\n\n```shell\npython examples/openai/autologging/instantiated_client.py --api-key="your-api-key"\n```\n\n## Using module-level client\n\n`openai` exposes a module client instance that can be used to make requests.\nYou can run the following example to use autologging with the module client.\n\n```shell\nexport OPENAI_API_KEY="your-api-key"\npython examples/openai/autologging/module_client.py\n```\n', '# Pyfunc model example\n\nThis example demonstrates the use of a pyfunc model with custom inference logic.\nMore specifically:\n\n- train a simple classification model\n- create a _pyfunc_ model that encapsulates the classification model with an attached module for custom inference logic\n\n## Structure of this example\n\nThis examples contains a `train.py` file that trains a scikit-learn model with iris dataset and uses MLflow Tracking APIs to log the model. The nested **mlflow run** delivers the packaging of `pyfunc` model and `custom_code` module is attached\nto act as a custom inference logic layer in inference time.\n\n```\nâ”œâ”€â”€ train.py\nâ”œâ”€â”€ infer_model_code_path.py\nâ””â”€â”€ custom_code.py\n```\n\n## Running this example\n\n1. Train and log the model\n\n```\n$ python train.py\n```\n\nor train and log the model using inferred code paths\n\n```\n$ python infer_model_code_paths.py\n```\n\n2. Serve the pyfunc model\n\n```bash\n# Replace <pyfunc_run_id> with the run ID obtained in the previous step\n$ mlflow models serve -m "runs:/<pyfunc_run_id>/model" -p 5001\n```\n\n3. Send a request\n\n```\n$ curl http://127.0.0.1:5001/invocations -H \'Content-Type: application/json\' -d \'{\n  "dataframe_records": [[1, 1, 1, 1]]\n}\'\n```\n\nThe response should look like this:\n\n```\n[0]\n```\n', '# PySpark ML Autologging Examples\n\nThis directory contains examples for demonstrating how PySpark ML autologging works.\n\n| File                     | Description                        |\n| :----------------------- | :--------------------------------- |\n| `logistic_regression.py` | Train a `LogisticRegression` model |\n| `one_vs_rest.py`         | Train a `OneVsRest` model          |\n', '# PySpark ML connect Examples\n\nThis directory contains examples for demonstrating how to log PySpark ML connect model.\n\n| File          | Description                                           |\n| :------------ | :---------------------------------------------------- |\n| `pipeline.py` | Use mlflow to Log a PySpark ML connect pipeline model |\n', "## Ax Hyperparameter Optimization Example\n\nIn this example, we train a Pytorch Lightning model to classify Iris flower classification dataset.\nA parent run will be created during the training process,which would dump the baseline model and relevant parameters,metrics and model along with its summary,subsequently followed by a set of nested child runs, which will dump the trial results.\nThe best parameters would be dumped into the parent run once the experiments are completed.\n\n### Running the code\n\nTo run the example via MLflow, navigate to the `mlflow/examples/pytorch/AxHyperOptimizationPTL` directory and run the command\n\n```\nmlflow run .\n```\n\nThis will run `AxHyperOptimizationPTL.py` with the default set of parameters such as `max_epochs=3` and `total_trials=3`. You can see the default value in the `MLproject` file.\n\nIn order to run the file with custom parameters, run the command\n\n```\nmlflow run . -P max_epochs=X -P total_trials=Y\n```\n\nwhere `X` is your desired value for `max_epochs` and `Y` is your desired value for `total_trials`.\n\nIf you have the required modules for the file and would like to skip the creation of a conda environment, add the argument `--env-manager=local`.\n\n```\nmlflow run . --env-manager=local\n```\n\n### Viewing results in the MLflow UI\n\nOnce the code is finished executing, you can view the run's metrics, parameters, and details by running the command\n\n```\nmlflow ui\n```\n\nand navigating to [http://localhost:5000](http://localhost:5000).\n\nFor more details on MLflow tracking, see [the docs](https://www.mlflow.org/docs/latest/tracking.html#mlflow-tracking).\n\n### Passing custom training parameters\n\nThe parameters can be overridden via the command line:\n\n1. max_epochs - Number of epochs to train model. Training can be interrupted early via Ctrl+C\n2. total_trials - Number of experimental trials\n\nFor example:\n\n```\nmlflow run . -P max_epochs=3 -P total_trials=3\n```\n\nOr to run the training script directly with custom parameters:\n\n```\npython ax_hpo_iris.py --max_epochs 3 --total_trials 3\n```\n\nBy running the above mentioned script, the hyperparameters are logged into MLFLow as nested runs.\n\n![Ax HPO Runs](screenshots/ax_hpo.png)\n\nThe child run contains the details of the hyperparameters used during that particular trial.\n\n![Trial Run](screenshots/trial_run.png)\n\nAnd the parent run contains the details of the optimum parameters derived by running n trials.\n\n![Parent Run](screenshots/parent_run.png)\n\n## Logging to a custom tracking server\n\nTo configure MLflow to log to a custom (non-default) tracking location, set the `MLFLOW_TRACKING_URI` environment variable, e.g. via `export MLFLOW_TRACKING_URI=http://localhost:5000`. For more details, see [the docs](https://mlflow.org/docs/latest/tracking.html#where-runs-are-recorded).\n", '## BERT news classification example\n\nIn this example, we train a Pytorch Lightning model to classify news articles into "World", "Sports", "Business" and "Sci/Tech" categories. The code, adapted from this [repository](https://github.com/ricardorei/lightning-text-classification/blob/master/classifier.py), is almost entirely dedicated to model training, with the addition of a single `mlflow.pytorch.autolog()` call to enable automatic logging of params, metrics, and models.\n\n### Running the code\n\nTo run the example via MLflow, navigate to the `mlflow/examples/pytorch/BertNewsClassification` directory and run the command\n\n```bash\nmlflow run .\n```\n\nThis will run `bert_classification.py` with the default set of parameters such as `--max_epochs=5`. You can see the default value in the `MLproject` file.\n\nIn order to run the file with custom parameters, run the command\n\n```bash\nmlflow run . -P max_epochs=X\n```\n\nwhere `X` is your desired value for `max_epochs`.\n\nIf you have the required modules for the file and would like to skip the creation of a conda environment, add the argument `--env-manager=local`.\n\n```bash\nmlflow run . --env-manager=local\n```\n\n### Viewing results in the MLflow UI\n\nOnce the code is finished executing, you can view the run\'s metrics, parameters, and details by running the command\n\n```bash\nmlflow ui\n```\n\nand navigating to [http://localhost:5000](http://localhost:5000).\n\nFor more details on MLflow tracking, see [the docs](https://www.mlflow.org/docs/latest/tracking.html#mlflow-tracking).\n\n### Passing custom training parameters\n\nThe parameters can be overridden via the command line:\n\n1. max_epochs - Number of epochs to train model. Training can be interrupted early via Ctrl+C\n2. devices - Number of GPUs.\n3. strategy - [strategy](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#trainer-class-api) (e.g. "ddp" for the Distributed Data Parallel backend) to use for training. By default, no strategy is used.\n4. accelerator - [accelerator](https://lightning.ai/docs/pytorch/stable/extensions/accelerator.html) (e.g. "gpu" - for running in GPU environment. Set to "cpu" by default)\n5. batch_size - Input batch size for training\n6. num_workers - Number of worker threads to load training data\n7. lr - Learning rate\n\nFor example:\n\n```bash\nmlflow run . -P max_epochs=5 -P devices=1 -P batch_size=32 -P num_workers=2 -P learning_rate=0.01 -P strategy="ddp" -P accelerator=gpu\n```\n\nOr to run the training script directly with custom parameters:\n\n```bash\npython bert_classification.py \\\n    --trainer.max_epochs 5 \\\n    --trainer.devices 1 \\\n    --trainer.strategy "ddp" \\\n    --trainer.accelerator "gpu" \\\n    --data.batch_size 64 \\\n    --data.num_workers 3 \\\n    --data.num_samples 2000 \\\n    --model.lr 0.001 \\\n    --data.dataset "20newsgroups"\n```\n\n## ', 'Input batch size for training\n6. num_workers - Number of worker threads to load training data\n7. lr - Learning rate\n\nFor example:\n\n```bash\nmlflow run . -P max_epochs=5 -P devices=1 -P batch_size=32 -P num_workers=2 -P learning_rate=0.01 -P strategy="ddp" -P accelerator=gpu\n```\n\nOr to run the training script directly with custom parameters:\n\n```bash\npython bert_classification.py \\\n    --trainer.max_epochs 5 \\\n    --trainer.devices 1 \\\n    --trainer.strategy "ddp" \\\n    --trainer.accelerator "gpu" \\\n    --data.batch_size 64 \\\n    --data.num_workers 3 \\\n    --data.num_samples 2000 \\\n    --model.lr 0.001 \\\n    --data.dataset "20newsgroups"\n```\n\n## Logging to a custom tracking server\n\nTo configure MLflow to log to a custom (non-default) tracking location, set the MLFLOW_TRACKING_URI environment variable, e.g. via export MLFLOW_TRACKING_URI=http://localhost:5000/. For more details, see [the docs](https://mlflow.org/docs/latest/tracking.html#where-runs-are-recorded).\n', "## Using Captum and MLflow to interpret Pytorch models\n\nIn this example, we will demonstrate the basic features of the [Captum](https://captum.ai/) interpretability,and logging those features using mlflow library through an example model trained on the Titanic survival data.\nWe will first train a deep neural network on the data using PyTorch and use Captum to understand which of the features were most important and how the network reached its prediction.\n\nyou can get more details about used attributions methods used in this example\n\n1. [Titanic_Basic_Interpret](https://captum.ai/tutorials/Titanic_Basic_Interpret)\n2. [integrated-gradients](https://captum.ai/docs/algorithms#primary-attribution)\n3. [layer-attributions](https://captum.ai/docs/algorithms#layer-attribution)\n\n### Running the code\n\nTo run the example via MLflow, navigate to the `mlflow/examples/pytorch/CaptumExample` directory and run the command\n\n```\nmlflow run .\n```\n\nThis will run `Titanic_Captum_Interpret.py` with default parameter values, e.g. `--max_epochs=100` and `--use_pretrained_model False`. You can see the full set of parameters in the `MLproject` file within this directory.\n\nIn order to run the file with custom parameters, run the command\n\n```\nmlflow run . -P max_epochs=X\n```\n\nwhere `X` is your desired value for `max_epochs`.\n\nIf you have the required modules for the file and would like to skip the creation of a conda environment, add the argument `--env-manager=local`.\n\n```\nmlflow run . --env-manager=local\n```\n\n### Viewing results in the MLflow UI\n\nOnce the code is finished executing, you can view the run's metrics, parameters, and details by running the command\n\n```\nmlflow ui\n```\n\nand navigating to [http://localhost:5000](http://localhost:5000).\n\nFor more details on MLflow tracking, see [the docs](https://www.mlflow.org/docs/latest/tracking.html#mlflow-tracking).\n\n### Passing custom training parameters\n\nThe parameters can be overridden via the command line:\n\n1. max_epochs - Number of epochs to train model. Training can be interrupted early via Ctrl+C\n2. lr - Learning rate\n3. use_pretrained_model - If want to use pretrained model\n\nFor example:\n\n```\nmlflow run . -P max_epochs=5 -P learning_rate=0.01 -P use_pretrained_model=True\n```\n\nOr to run the training script directly with custom parameters:\n\n```sh\npython Titanic_Captum_Interpret.py \\\n    --max_epochs 50 \\\n    --lr 0.1\n```\n\n## Logging to a custom tracking server\n\nTo configure MLflow to log to a custom (non-default) tracking location, set the MLFLOW_TRACKING_URI environment variable, e.g. via export MLFLOW_TRACKING_URI=http://localhost:5000/. For more details, see [the docs](https://mlflow.org/docs/latest/tracking.html#where-runs-are-recorded).\n", '## Iterative Pruning\n\nPruning is the process of compressing a neural network that involves removing weights from a trained model.\nPruning techniques include removing the neurons within a specific layer, or setting the weights of connections that are already near zero to zero. This script applies the latter technique.\nPruning a model reduces its size, at the cost of worsened model accuracy.\n\nFor more information check - [Pytorch Pruning Tutorial](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html)\n\nIn this example, we train a model to classify MNIST handwritten digit recognition dataset, and then apply iterative pruning to compress the model. The initial model ("base model") along with its parameters, metrics and summary are stored in mlflow.\nSubsequently, the base model is pruned iteratively by using the custom\ninputs provided from the cli. Ax is a platform for optimizing any kind of experiment, including machine learning experiments,\nA/B tests, and simulations. [Ax](https://ax.dev/docs/why-ax.html) can optimize discrete configurations using multi-armed bandit optimization,\nand continuous (e.g., integer or floating point)-valued configurations using Bayesian optimization.\n\nThe objective function of the experiment trials is "test_accuracy" based on which the model is evaluated at each trial and the best set of parameters are derived.\nAXClient is used to provide the initial pruning percentage as well as decides the number\nof trails to be run. The summary of the pruned model is captured in a separate file and stored as an artifact in MLflow.\n\n### Running the code to Iteratively Prune the Trained Model\n\nRun the command\n\n`python iterative_prune_mnist.py --max_epochs 10 --total_trials 3`\n\nOnce the code is finished executing, you can view the run\'s metrics, parameters, and details by running the command\n\n```\nmlflow ui\n```\n\nand navigating to [http://localhost:5000](http://localhost:5000).\n\nIn the MLflow UI, the Base Model is stored as the Parent Run and the runs for each iterations of the pruning is logged as nested child runs, as shown in the\nsnippets below:\n\n![prune_ankan](https://user-images.githubusercontent.com/51693147/100785435-a66d6e80-3436-11eb-967a-c96b23625d1c.JPG)\n\nWe can compare the child runs in the UI, as given below:\n\n![prune_capture](https://user-images.githubusercontent.com/51693147/100785071-2515dc00-3436-11eb-8e3a-de2d569287e6.JPG)\n\nFor more information on MLflow tracking, click [here](https://www.mlflow.org/docs/latest/tracking.html#mlflow-tracking) to view documentation.\n', '## MNIST example with MLflow\n\nIn this example, we train a Pytorch Lightning model to predict handwritten digits, leveraging early stopping.\nThe code is almost entirely dedicated to model training, with the addition of a single `mlflow.pytorch.autolog()` call to enable automatic logging of params, metrics, and models,\nincluding the best model from early stopping.\n\n### Running the code\n\nTo run the example via MLflow, navigate to the `mlflow/examples/pytorch/MNIST` directory and run the command\n\n```\nmlflow run .\n```\n\nThis will run `mnist_autolog_example.py` with the default set of parameters such as `max_epochs=5`. You can see the default value in the `MLproject` file.\n\nIn order to run the file with custom parameters, run the command\n\n```\nmlflow run . -P max_epochs=X\n```\n\nwhere `X` is your desired value for `max_epochs`.\n\nIf you have the required modules for the file and would like to skip the creation of a conda environment, add the argument `--env-manager=local`.\n\n```\nmlflow run . --env-manager=local\n```\n\n### Viewing results in the MLflow UI\n\nOnce the code is finished executing, you can view the run\'s metrics, parameters, and details by running the command\n\n```\nmlflow ui\n```\n\nand navigating to [http://localhost:5000](http://localhost:5000).\n\nFor more details on MLflow tracking, see [the docs](https://www.mlflow.org/docs/latest/tracking.html#mlflow-tracking).\n\n### Passing custom training parameters\n\nThe parameters can be overridden via the command line:\n\n1. max_epochs - Number of epochs to train model. Training can be interrupted early via Ctrl+C\n2. devices - Number of GPUs.\n3. strategy - [strategy](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#trainer-class-api) (e.g. "ddp" for the Distributed Data Parallel backend) to use for training. By default, no strategy is used.\n4. accelerator - [accelerator](https://lightning.ai/docs/pytorch/stable/extensions/accelerator.html) (e.g. "gpu" - for running in GPU environment. Set to "cpu" by default)\n5. batch_size - Input batch size for training\n6. num_workers - Number of worker threads to load training data\n7. learning_rate - Learning rate\n\nFor example:\n\n```\nmlflow run . -P max_epochs=5 -P devices=1 -P batch_size=32 -P num_workers=2 -P learning_rate=0.01 -P strategy="ddp"\n```\n\nOr to run the training script directly with custom parameters:\n\n```sh\npython mnist_autolog_example.py \\\n    --trainer.max_epochs 5 \\\n    --trainer.devices 1 \\\n    --trainer.strategy "ddp" \\\n    --trainer.accelerator "gpu" \\\n    --data.batch_size 64 \\\n    --data.num_workers 3 \\\n    --model.learning_rate 0.001\n```\n\n## Logging to a custom tracking server\n\nTo configure MLflow to log to a custom (non-default) tracking location, set ', 'for training\n6. num_workers - Number of worker threads to load training data\n7. learning_rate - Learning rate\n\nFor example:\n\n```\nmlflow run . -P max_epochs=5 -P devices=1 -P batch_size=32 -P num_workers=2 -P learning_rate=0.01 -P strategy="ddp"\n```\n\nOr to run the training script directly with custom parameters:\n\n```sh\npython mnist_autolog_example.py \\\n    --trainer.max_epochs 5 \\\n    --trainer.devices 1 \\\n    --trainer.strategy "ddp" \\\n    --trainer.accelerator "gpu" \\\n    --data.batch_size 64 \\\n    --data.num_workers 3 \\\n    --model.learning_rate 0.001\n```\n\n## Logging to a custom tracking server\n\nTo configure MLflow to log to a custom (non-default) tracking location, set the MLFLOW_TRACKING_URI environment variable, e.g. via export MLFLOW_TRACKING_URI=http://localhost:5000/. For more details, see [the docs](https://mlflow.org/docs/latest/tracking.html#where-runs-are-recorded).\n', "## Iris classification example with MLflow\n\nThis example demonstrates training a classification model on the Iris dataset, scripting the model with TorchScript, logging the\nscripted model to MLflow using\n[`mlflow.pytorch.log_model`](https://mlflow.org/docs/latest/python_api/mlflow.pytorch.html#mlflow.pytorch.log_model), and\nloading it back for inference using\n[`mlflow.pytorch.load_model`](https://mlflow.org/docs/latest/python_api/mlflow.pytorch.html#mlflow.pytorch.load_model)\n\n### Running the code\n\nTo run the example via MLflow, navigate to the `mlflow/examples/pytorch/torchscript/IrisClassification` directory and run the command\n\n```\nmlflow run .\n```\n\nThis will run `iris_classification.py` with the default set of parameters such as `--max_epochs=5`. You can see the default value in the `MLproject` file.\n\nIn order to run the file with custom parameters, run the command\n\n```\nmlflow run . -P epochs=X\n```\n\nwhere `X` is your desired value for `epochs`.\n\nIf you have the required modules for the file and would like to skip the creation of a conda environment, add the argument `--env-manager=local`.\n\n```\nmlflow run . --env-manager=local\n```\n\nOnce the code is finished executing, you can view the run's metrics, parameters, and details by running the command\n\n```\nmlflow ui\n```\n\nand navigating to [http://localhost:5000](http://localhost:5000).\n\n## Running against a custom tracking server\n\nTo configure MLflow to log to a custom (non-default) tracking location, set the `MLFLOW_TRACKING_URI` environment variable, e.g. via `export MLFLOW_TRACKING_URI=http://localhost:5000/`. For more details, see [the docs](https://mlflow.org/docs/latest/tracking.html#where-runs-are-recorded)\n", "## MNIST example with MLflow\n\nThis example demonstrates training of MNIST handwritten recognition model and logging it as torch scripted model.\n`mlflow.pytorch.log_model()` is used to log the scripted model to MLflow and `mlflow.pytorch.load_model()` to load it from MLflow\n\n### Code related to MLflow:\n\nThis will log the TorchScripted model into MLflow and load the logged model.\n\n## Setting Tracking URI\n\nMLflow tracking URI can be set using the environment variable `MLFLOW_TRACKING_URI`\n\nExample: `export MLFLOW_TRACKING_URI=http://localhost:5000/`\n\nFor more details - https://mlflow.org/docs/latest/tracking.html#where-runs-are-recorded\n\n### Running the code\n\nTo run the example via MLflow, navigate to the `mlflow/examples/pytorch/torchscript/MNIST` directory and run the command\n\n```\nmlflow run .\n```\n\nThis will run `mnist_torchscript.py` with the default set of parameters such as `--max_epochs=5`. You can see the default value in the `MLproject` file.\n\nIn order to run the file with custom parameters, run the command\n\n```\nmlflow run . -P epochs=X\n```\n\nwhere `X` is your desired value for `epochs`.\n\nIf you have the required modules for the file and would like to skip the creation of a conda environment, add the argument `--env-manager=local`.\n\n```\nmlflow run . --env-manager=local\n```\n\nOnce the code is finished executing, you can view the run's metrics, parameters, and details by running the command\n\n```\nmlflow ui\n```\n\nand navigating to [http://localhost:5000](http://localhost:5000).\n\nFor more information on MLflow tracking, click [here](https://www.mlflow.org/docs/latest/tracking.html#mlflow-tracking) to view documentation.\n", '### Train and Publish Locally With RAPIDS and MLflow\n\n**[RAPIDS](https://rapids.ai/)** is a suite of open source libraries for GPU-accelerated analytics.\n\n**[RAPIDS cuML](https://github.com/rapidsai/cuml)** matches the scikit-learn API, so it can build on MLflow\'s existing support for scikit-learn-like models to support\npersistence and deployment."\n\nThe example workflows below train RAPIDs regression models to predict airline flight delays, using\nMLflow to log models and deploy them as local REST API endpoints for real-time inference. You can run them:\n\n- On a GPU-enabled instance for free in Colab. If following this approach, we recommend using the "Jupyter notebook workflow" below\n  and following the setup steps in [this Colab notebook](https://colab.research.google.com/drive/1rY7Ln6rEE1pOlfSHCYOVaqt8OvDO35J0#forceEdit=true&offline=true&sandboxMode=true) to configure your\n  environment.\n\n- On your own machine with an NVIDIA GPU and CUDA installed. See the [RAPIDS getting-started guide](https://rapids.ai/start.html)\n  for more details on necessary prerequisites for running the examples on your own machine.\n\n#### Jupyter Notebook Workflow\n\n[Jupyter Notebook](notebooks/rapids_mlflow.ipynb)\n\n#### CLI Based Workflow\n\n1. Create data\n   1. `cd examples/rapids/mlflow_project`\n      ```shell script\n      # Create iris.csv\n      python -c "from sklearn.datasets import load_iris; d = load_iris(as_frame=True); d.frame.to_csv(\'iris.csv\', index=False)"\n      ```\n1. Set MLflow tracking uri\n   1. ```shell script\n       export MLFLOW_TRACKING_URI=sqlite:////tmp/mlflow-db.sqlite\n      ```\n1. Train the model using a single run.\n   1. ```shell script\n      # Launch the job\n      mlflow run . -e simple\\\n               --experiment-name RAPIDS-CLI \\\n               -P max_depth=10 -P max_features=0.75 -P n_estimators=500 \\\n               -P conda-env=$PWD/envs/conda.yaml \\\n               -P fpath=iris.csv\n      ```\n1. Train the model with Hyperopt\n\n   1. ```shell script\n      # Launch the job\n      mlflow run . -e hyperopt \\\n   ', '     --experiment-name RAPIDS-CLI \\\n               -P max_depth=10 -P max_features=0.75 -P n_estimators=500 \\\n               -P conda-env=$PWD/envs/conda.yaml \\\n               -P fpath=iris.csv\n      ```\n1. Train the model with Hyperopt\n\n   1. ```shell script\n      # Launch the job\n      mlflow run . -e hyperopt \\\n               --experiment-name RAPIDS-CLI \\\n               -P conda-env=$PWD/envs/conda.yaml \\\n               -P fpath=iris.csv\n      ```\n   1. In the output, note: "Created version \'[VERSION]\' of model \'rapids_mlflow\'"\n\n1. Deploy your model\n\n   1. Deploy your model\n      1. `$ mlflow models serve --env-manager=local -m models:/rapids_mlflow_cli/[VERSION] -p 55755`\n\n1. Query the deployed model with test data `src/sample_server_query.sh` example script.\n   1. `bash src/sample_server_query.sh`\n', "# MLflow-Ray-Serve deployment plugin\n\nIn this example, we will first train a model to classify the Iris dataset using `sklearn`. Next, we will deploy our model on Ray Serve and then scale it up, all using the MLflow Ray Serve plugin.\n\nThe plugin supports both a command line interface and a Python API. Below we will use the command line interface. For the full API documentation, see https://www.mlflow.org/docs/latest/cli.html#mlflow-deployments and https://www.mlflow.org/docs/latest/python_api/mlflow.deployments.html.\n\n## Plugin Installation\n\nPlease follow the installation instructions for the Ray Serve deployment plugin: https://github.com/ray-project/mlflow-ray-serve\n\n## Instructions\n\nFirst, navigate to the directory for this example, `mlflow/examples/ray_serve/`.\n\nSecond, run `python train_model.py`. This trains and saves our classifier to the MLflow Model Registry and sets up automatic logging to MLflow. It also prints the mean squared error and the target names, which are species of iris:\n\n```\nMSE: 1.04\nTarget names:  ['setosa' 'versicolor' 'virginica']\n```\n\nNext, set the MLflow Tracking URI environment variable to the location where the Model Registry resides:\n\n`export MLFLOW_TRACKING_URI=sqlite:///mlruns.db`\n\nNow start a Ray cluster with the following command:\n\n`ray start --head`\n\nNext, start a long-running Ray Serve instance on your Ray cluster:\n\n`serve start`\n\nRay Serve is now running and ready to deploy MLflow models. The MLflow Ray Serve plugin features both a Python API as well as a command-line interface. For this example, we'll use the command line interface.\n\nFinally, we can deploy our model by creating an instance using the following command:\n\n`mlflow deployments create -t ray-serve -m models:/RayMLflowIntegration/1 --name iris:v1`\n\nThe `-t` parameter here is the deployment target, which in our case is Ray Serve. The `-m` parameter is the Model URI, which consists of the registered model name and version in the Model Registry.\n\nWe can now run a prediction on our deployed model as follows. The file `input.json` contains a sample input containing the sepal length, sepal width, petal length, petal width of a sample flower. Now we can get the prediction using the following command:\n\n`mlflow deployments predict -t ray-serve --name iris:v1 --input-path input.json`\n\nThis will output `[0]`, `[1]`, or `[2]`, corresponding to the species listed above in the target names.\n\nWe can scale our deployed model up to use several replicas, improving throughput:\n\n`mlflow deployments update -t ray-serve --name iris:v1 --config num_replicas=2`\n\nHere we only used 2 ", 'the registered model name and version in the Model Registry.\n\nWe can now run a prediction on our deployed model as follows. The file `input.json` contains a sample input containing the sepal length, sepal width, petal length, petal width of a sample flower. Now we can get the prediction using the following command:\n\n`mlflow deployments predict -t ray-serve --name iris:v1 --input-path input.json`\n\nThis will output `[0]`, `[1]`, or `[2]`, corresponding to the species listed above in the target names.\n\nWe can scale our deployed model up to use several replicas, improving throughput:\n\n`mlflow deployments update -t ray-serve --name iris:v1 --config num_replicas=2`\n\nHere we only used 2 replicas, but you can use as many as you like, depending on how many CPU cores are available in your Ray cluster.\n\nThe deployed model instance can be deleted as follows:\n\n`mlflow deployments delete -t ray-serve --name iris:v1`\n\nTo tear down the Ray cluster, run the following command:\n\n`ray stop`\n', '### MLflow restore model dependencies examples\n\nThe example "restore_model_dependencies_example.ipynb" in this directory illustrates\nhow you can use the `mlflow.pyfunc.get_model_dependencies` API to get the dependencies from a model URI\nand install them, restoring the exact python environment that was used to build the model.\n\n#### Prerequisites\n\n```\npip install scikit-learn\n```\n\n#### How to run the example\n\nUse jupyter to load the notebook "restore_model_dependencies_example.ipynb" and run the notebook.\n', '# SHAP Examples\n\nExamples demonstrating use of the `mlflow.shap` APIs for model explainability.\n\n| File                                                         | Task                      | Description                                                    |\n| :----------------------------------------------------------- | :------------------------ | :------------------------------------------------------------- |\n| [regression.py](regression.py)                               | Regression                | Log explanations for a LinearRegression model                  |\n| [binary_classification.py](binary_classification.py)         | Binary classification     | Log explanations for a binary RandomForestClassifier model     |\n| [multiclass_classification.py](multiclass_classification.py) | Multiclass classification | Log explanations for a multiclass RandomForestClassifier model |\n\n## Prerequisites\n\nRun the following command to install required packages:\n\n```\npip install mlflow scikit-learn shap matplotlib\n```\n\n## How to run the scripts\n\n```bash\npython <script_name>\n```\n\n## How to view the logged explanations:\n\n- Run `mlflow ui` to launch the MLflow UI.\n- Open http://127.0.0.1:5000 on your browser.\n- Click the latest run in the runs table.\n- Scroll down to the artifact viewer.\n- Open a folder named `model_explanations_shap`.\n', '# Examples for scikit-learn Autologging\n\n| File                                           | Description                                         |\n| :--------------------------------------------- | :-------------------------------------------------- |\n| [linear_regression.py](./linear_regression.py) | Train a [LinearRegression][lr] model                |\n| [pipeline.py](./pipeline.py)                   | Train a [Pipeline][pipe] model                      |\n| [grid_search_cv.py](./grid_search_cv.py)       | Perform a parameter search using [GridSearchCV][gs] |\n\n[lr]: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n[pipe]: https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n[gs]: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n', '# Scikit-learn ElasticNet Diabetes Example\n\nThis example trains an ElasticNet regression model for predicting diabetes progression. The example uses [matplotlib](https://matplotlib.org/), which requires different Python dependencies for Linux and OSX. The [linux](linux) and [osx](osx) subdirectories include appropriate MLflow projects for each respective platform.\n', '# Sktime Example\n\nThis example trains a `Sktime` NaiveForecaster model using the Longley dataset for\nforecasting with exogenous variables. It shows a custom model type implementation\nthat logs the training hyper-parameters, evaluation metrics and the trained model\nas an artifact.\n\n## Running the code\n\nRun the `train.py` module to create a new MLflow experiment and to\ncompute interval forecasts loading the trained model in native `sktime`\nflavor and `pyfunc` flavor:\n\n```\npython train.py\n```\n\nTo view the newly created experiment and logged artifacts open the MLflow UI:\n\n```\nmlflow ui\n```\n\n## Model serving\n\nThis section illustrates an example of serving the `pyfunc` flavor to a local REST\nAPI endpoint and subsequently requesting a prediction from the served model. To serve the model run the command below where you substitute the run id printed during execution of the `train.py` module:\n\n```\nmlflow models serve -m runs:/<run_id>/model --env-manager local --host 127.0.0.1\n\n```\n\nOpen a new terminal and run the `score_model.py` module to request a prediction from the served model (for more details read the [MLflow deployment API reference](https://mlflow.org/docs/latest/models.html#deploy-mlflow-models)):\n\n```\npython score_model.py\n```\n\n## Running the code as a project\n\nYou can also run the code as a project as follows:\n\n```\nmlflow run .\n\n```\n\n## Running unit tests\n\nThe `test_sktime_model_export.py` module includes a number of tests that can be\nexecuted as follows:\n\n```\npytest test_sktime_model_export.py\n\n```\n\nWhile these tests will depend on the specifics of each individual flavor and in particular the design of the model wrapper interface (e.g. `_SktimeModelWrapper`), the above module can provide some orientation\nfor the type of tests that can be useful when creating a new custom model flavor.\n', '### MLflow Spark UDF Examples\n\nThe examples in this directory illustrate how you can use the `mlflow.pyfunc.spark_udf` API for batch inference,\nincluding environment reproducibility capabilities with argument `env_manager="conda"`,\nwhich creates a spark UDF for model inference that executes in an environment containing the exact dependency\nversions used during training.\n\n- Example `spark_udf.py` runs a sklearn model inference via spark UDF\n  using a python environment containing the precise versions of dependencies used during model training.\n\n#### Prerequisites\n\n```\npip install scikit-learn\n```\n\n#### How to run the examples\n\nSimple example:\n\n```\npython spark_udf.py\n```\n\nSpark UDF example with input data of datetime type:\n\n```\npython spark_udf_datetime.py\n```\n\nSpark UDF example with input data of struct and array type:\n\n```\npython structs_and_arrays.py\n```\n\nSpark UDF example using prebuilt model environment:\n\n```\npython spark_udf_with_prebuilt_env.py\n```\n', "# Statsmodels Example\n\nThis example trains a Statsmodels OLS (Ordinary Least Squares) model with synthetically generated data\nand logs hyperparameters, metric (MSE), and trained model.\n\n## Running the code\n\n```\npython train.py --inverse-method qr\n```\n\nThe inverse method is the method used to compute the inverse matrix, and can be either qr or pinv (default).\n'pinv' uses the Moore-Penrose pseudoinverse to solve the least squares problem. 'qr' uses the QR factorization.\nYou can try experimenting with both, as well as omitting the --inverse-method argument.\n\nThen you can open the MLflow UI to track the experiments and compare your runs via:\n\n```\nmlflow ui\n```\n\n## Running the code as a project\n\n```\nmlflow run . -P inverse_method=qr\n\n```\n", '## MLflow automatic Logging with SynapseML\n\n[MLflow automatic logging](https://www.mlflow.org/docs/latest/tracking.html#automatic-logging) allows you to log metrics, parameters, and models without the need for explicit log statements.\nSynapseML supports autologging for every model in the library.\n\nInstall SynapseML library following this [guidance](https://microsoft.github.io/SynapseML/docs/getting_started/installation/)\n\nDefault mlflow [log_model_allowlist file](https://github.com/mlflow/mlflow/blob/master/mlflow/pyspark/ml/log_model_allowlist.txt) already includes some SynapseML models. To enable more models, you could use `mlflow.pyspark.ml.autolog(log_model_allowlist=YOUR_SET_OF_MODELS)` function, or follow the below guidance by specifying a link to the file and update spark configuration.\n\nTo enable autologging with your custom log_model_allowlist file:\n\n1. Put your customized log_model_allowlist file at a place that your code has access to. ([SynapseML official log_model_allowlist file](https://mmlspark.blob.core.windows.net/publicwasb/log_model_allowlist.txt))\n   For example:\n\n- In Synapse `wasb://<containername>@<accountname>.blob.core.windows.net/PATH_TO_YOUR/log_model_allowlist.txt`\n- In Databricks `/dbfs/FileStore/PATH_TO_YOUR/log_model_allowlist.txt`.\n\n2. Set spark configuration `spark.mlflow.pysparkml.autolog.logModelAllowlistFile` to the path of your `log_model_allowlist.txt` file.\n3. Call `mlflow.pyspark.ml.autolog()` before your training code to enable autologging for all supported models.\n\nNote:\n\nIf you want to support autologging of PySpark models not present in the log_model_allowlist file, you can add such models to the file.\n\n## Configuration process in Databricks as an example\n\n1. Install latest MLflow via `%pip install mlflow -u`\n2. Upload your customized `log_model_allowlist.txt` file to dbfs by clicking File/Upload Data button on Databricks UI.\n3. Set Cluster Spark configuration following [this documentation](https://docs.microsoft.com/en-us/azure/databricks/clusters/configure#spark-configuration)\n\n```\nspark.mlflow.pysparkml.autolog.logModelAllowlistFile /dbfs/FileStore/PATH_TO_YOUR/log_model_allowlist.txt\n```\n\n4. Run the following line before your training code executes.\n\n```python\nimport mlflow\n\nmlflow.pyspark.ml.autolog()\n```\n\nYou can customize how autologging works by supplying appropriate [parameters](https://www.mlflow.org/docs/latest/python_api/mlflow.pyspark.ml.html#mlflow.pyspark.ml.autolog).\n\n5. To find your experiment\'s results via the `Experiments` tab of the MLflow UI.\n   <img src="https://mmlspark.blob.core.windows.net/graphics/adb_experiments.png" width="1200" />\n\n## Example for ConditionalKNNModel\n\n```python\nfrom pyspark.ml.linalg import Vectors\nfrom synapse.ml.nn import ConditionalKNN\n\ndf = spark.createDataFrame(\n    [\n        (Vectors.dense(2.0, 2.0, 2.0), "foo", 1),\n        (Vectors.dense(2.0, 2.0, 4.0), "foo", 3),\n        (Vectors.dense(2.0, 2.0, 6.0), "foo", 4),\n        (Vectors.dense(2.0, 2.0, 8.0), "foo", 3),\n        (Vectors.dense(2.0, 2.0, 10.0), "foo", 1),\n        (Vectors.dense(2.0, 2.0, 12.0), "foo", 2),\n        (Vectors.dense(2.0, 2.0, 14.0), "foo", 0),\n        (Vectors.dense(2.0, 2.0, 16.0), "foo", 1),\n        (Vectors.dense(2.0, 2.0, 18.0), "foo", 3),\n  ', '2.0), "foo", 1),\n        (Vectors.dense(2.0, 2.0, 4.0), "foo", 3),\n        (Vectors.dense(2.0, 2.0, 6.0), "foo", 4),\n        (Vectors.dense(2.0, 2.0, 8.0), "foo", 3),\n        (Vectors.dense(2.0, 2.0, 10.0), "foo", 1),\n        (Vectors.dense(2.0, 2.0, 12.0), "foo", 2),\n        (Vectors.dense(2.0, 2.0, 14.0), "foo", 0),\n        (Vectors.dense(2.0, 2.0, 16.0), "foo", 1),\n        (Vectors.dense(2.0, 2.0, 18.0), "foo", 3),\n        (Vectors.dense(2.0, 2.0, 20.0), "foo", 0),\n        (Vectors.dense(2.0, 4.0, 2.0), "foo", 2),\n        (Vectors.dense(2.0, 4.0, 4.0), "foo", 4),\n        (Vectors.dense(2.0, 4.0, 6.0), "foo", 2),\n        (Vectors.dense(2.0, 4.0, 8.0), "foo", 2),\n        (Vectors.dense(2.0, 4.0, 10.0), "foo", 4),\n        (Vectors.dense(2.0, 4.0, 12.0), "foo", 3),\n        (Vectors.dense(2.0, 4.0, 14.0), "foo", 2),\n        (Vectors.dense(2.0, 4.0, 16.0), "foo", 1),\n        (Vectors.dense(2.0, 4.0, 18.0), "foo", 4),\n        (Vectors.dense(2.0, 4.0, 20.0), "foo", 4),\n    ],\n    ["features", "values", "labels"],\n)\n\ncnn = ConditionalKNN().setOutputCol("prediction")\ncnnm = cnn.fit(df)\n\ntest_df = spark.createDataFrame(\n    [\n        (Vectors.dense(2.0, 2.0, 2.0), "foo", 1, [0, 1]),\n        (Vectors.dense(2.0, 2.0, 4.0), "foo", 4, [0, 1]),\n        (Vectors.dense(2.0, 2.0, 6.0), "foo", 2, [0, 1]),\n        (Vectors.dense(2.0, 2.0, 8.0), "foo", 4, [0, 1]),\n        (Vectors.dense(2.0, 2.0, 10.0), "foo", 4, [0, 1]),\n    ],\n    ["features", "values", "labels", "conditioner"],\n)\n\ndisplay(cnnm.transform(test_df))\n```\n\nThis code should log one run with a ConditionalKNNModel artifact and its parameters.\n<img src="https://mmlspark.blob.core.windows.net/graphics/autologgingRunSample.png" width="1200" />\n', '# Examples for XGBoost Autologging\n\nTwo examples are provided to demonstrate XGBoost autologging functionalities. The `xgboost_native` folder contains an example that logs a Booster model trained by `xgboost.train()`. The `xgboost_sklearn` includes another example showing how autologging works for XGBoost scikit-learn models. In fact, there is no difference in turning on autologging for all XGBoost models. That is, `mlflow.xgboost.autolog()` works for all XGBoost models.\n', '# XGBoost Example\n\nThis example trains an XGBoost classifier with the iris dataset and logs hyperparameters, metrics, and trained model.\n\n## Running the code\n\n```\npython train.py --learning-rate 0.2 --colsample-bytree 0.8 --subsample 0.9\n```\n\nYou can try experimenting with different parameter values like:\n\n```\npython train.py --learning-rate 0.4 --colsample-bytree 0.7 --subsample 0.8\n```\n\nThen you can open the MLflow UI to track the experiments and compare your runs via:\n\n```\nmlflow ui\n```\n\n## Running the code as a project\n\n```\nmlflow run . -P learning_rate=0.2 -P colsample_bytree=0.8 -P subsample=0.9\n```\n', '# XGBoost Scikit-learn Model Example\n\nThis example trains an [`XGBoost.XGBRegressor`](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor) with the diabetes dataset and logs hyperparameters, metrics, and trained model.\n\nLike the other XGBoost example, we enable autologging for XGBoost scikit-learn models via `mlflow.xgboost.autolog()`. Saving / loading models also supports XGBoost scikit-learn models.\n\nYou can run this example using the following command:\n\n```\npython train.py\n```\n', '# MLflow Skinny\n\n`mlflow-skinny` a lightweight version of MLflow that is designed to be used in environments where you want to minimize the size of the package.\n\n## Core Files\n\n| File               | Description                                                                     |\n| ------------------ | ------------------------------------------------------------------------------- |\n| `mlflow`           | A symlink that points to the `mlflow` directory in the root of the repository.  |\n| `pyproject.toml`   | The package metadata. Autogenerate by [`dev/pyproject.py`](../dev/pyproject.py) |\n| `README_SKINNY.md` | The package description. Autogenerate by [`dev/skinny.py`](../dev/pyproject.py) |\n\n## Installation\n\n```sh\n# If you have a local clone of the repository\npip install ./libs/skinny\n\n# If you want to install the latest version from GitHub\npip install git+https://github.com/mlflow/mlflow.git#subdirectory=libs/skinny\n```\n', '<!--  Autogenerated by dev/pyproject.py. Do not edit manually.  -->\n\nðŸ“£ This is the `mlflow-skinny` package, a lightweight MLflow package without SQL storage, server, UI, or data science dependencies.\nAdditional dependencies can be installed to leverage the full feature set of MLflow. For example:\n\n- To use the `mlflow.sklearn` component of MLflow Models, install `scikit-learn`, `numpy` and `pandas`.\n- To use SQL-based metadata storage, install `sqlalchemy`, `alembic`, and `sqlparse`.\n- To use serving-based features, install `flask` and `pandas`.\n\n---\n\n<br>\n<br>\n\n<h1 align="center" style="border-bottom: none">\n    <a href="https://mlflow.org/">\n        <img alt="MLflow logo" src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/logo.svg" width="200" />\n    </a>\n</h1>\n<h2 align="center" style="border-bottom: none">Open-Source Platform for Productionizing AI</h2>\n\nMLflow is an open-source developer platform to build AI/LLM applications and models with confidence. Enhance your AI applications with end-to-end **experiment tracking**, **observability**, and **evaluations**, all in one integrated platform.\n\n<div align="center">\n\n[![Python SDK](https://img.shields.io/pypi/v/mlflow)](https://pypi.org/project/mlflow/)\n[![PyPI Downloads](https://img.shields.io/pypi/dm/mlflow)](https://pepy.tech/projects/mlflow)\n[![License](https://img.shields.io/github/license/mlflow/mlflow)](https://github.com/mlflow/mlflow/blob/main/LICENSE)\n<a href="https://twitter.com/intent/follow?screen_name=mlflow" target="_blank">\n<img src="https://img.shields.io/twitter/follow/mlflow?logo=X&color=%20%23f5f5f5"\n      alt="follow on X(Twitter)"></a>\n<a href="https://www.linkedin.com/company/mlflow-org/" target="_blank">\n<img src="https://custom-icon-badges.demolab.com/badge/LinkedIn-0A66C2?logo=linkedin-white&logoColor=fff"\n      alt="follow on LinkedIn"></a>\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/mlflow/mlflow)\n\n</div>\n\n<div align="center">\n   <div>\n      <a href="https://mlflow.org/"><strong>Website</strong></a> Â·\n      <a href="https://mlflow.org/docs/latest/index.html"><strong>Docs</strong></a> Â·\n      <a href="https://github.com/mlflow/mlflow/issues/new/choose"><strong>Feature Request</strong></a> Â·\n      <a href="https://mlflow.org/blog"><strong>News</strong></a> Â·\n      <a href="https://www.youtube.com/@mlflowoss"><strong>YouTube</strong></a> Â·\n      <a href="https://lu.ma/mlflow?k=c"><strong>Events</strong></a>\n   </div>\n</div>\n\n<br>\n\n## ðŸš€ Installation\n\nTo install the MLflow Python package, run the following command:\n\n```\npip install mlflow\n```\n\n## ðŸ“¦ Core Components\n\nMLflow is **the only platform that provides a unified solution for all your AI/ML needs**, including LLMs, Agents, Deep Learning, and traditional machine learning.\n\n### ðŸ’¡ For LLM / GenAI Developers\n\n<table>\n  <tr>\n    <td>\n    <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-tracing.png" alt="Tracing" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/llms/tracing/index.html"><strong>ðŸ” Tracing / Observability</strong></a>\n        <br><br>\n        <div>Trace the internal states of your LLM/agentic applications for debugging quality issues and monitoring performance with ease.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/tracing/quickstart/python-openai/">Getting Started â†’</a>\n        <br><br>\n ', 'Learning, and traditional machine learning.\n\n### ðŸ’¡ For LLM / GenAI Developers\n\n<table>\n  <tr>\n    <td>\n    <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-tracing.png" alt="Tracing" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/llms/tracing/index.html"><strong>ðŸ” Tracing / Observability</strong></a>\n        <br><br>\n        <div>Trace the internal states of your LLM/agentic applications for debugging quality issues and monitoring performance with ease.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/tracing/quickstart/python-openai/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n    <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-llm-eval.png" alt="LLM Evaluation" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/genai/eval-monitor/"><strong>ðŸ“Š LLM Evaluation</strong></a>\n        <br><br>\n        <div>A suite of automated model evaluation tools, seamlessly integrated with experiment tracking to compare across multiple versions.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/eval-monitor/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-prompt.png" alt="Prompt Management">\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/prompt-registry/"><strong>ðŸ¤– Prompt Management</strong></a>\n        <br><br>\n        <div>Version, track, and reuse prompts across your organization, helping maintain consistency and improve collaboration in prompt development.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-registry/create-and-edit-prompts/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-logged-model.png" alt="MLflow Hero">\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/version-tracking/"><strong>ðŸ“¦ App Version Tracking</strong></a>\n        <br><br>\n  ', ' <br><br>\n        <div>Version, track, and reuse prompts across your organization, helping maintain consistency and improve collaboration in prompt development.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-registry/create-and-edit-prompts/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-logged-model.png" alt="MLflow Hero">\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/version-tracking/"><strong>ðŸ“¦ App Version Tracking</strong></a>\n        <br><br>\n        <div>MLflow keeps track of many moving parts in your AI applications, such as models, prompts, tools, and code, with end-to-end lineage.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/version-tracking/quickstart/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n</table>\n\n### ðŸŽ“ For Data Scientists\n\n<table>\n  <tr>\n    <td colspan="2" align="center" >\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-experiment.png" alt="Tracking" width=50%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/ml/tracking/"><strong>ðŸ“ Experiment Tracking</strong></a>\n        <br><br>\n        <div>Track your models, parameters, metrics, and evaluation results in ML experiments and compare them using an interactive UI.</div><br>\n        <a href="https://mlflow.org/docs/latest/ml/tracking/quickstart/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-model-registry.png" alt="Model Registry" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/ml/model-registry/"><strong>ðŸ’¾ Model Registry</strong></a>\n        <br><br>\n        <div> A centralized model store designed to collaboratively manage the full lifecycle and deployment of machine learning models.</div><br>\n        <a href="https://mlflow.org/docs/latest/ml/model-registry/tutorial/">Getting Started â†’</a>\n   ', '   <br><br>\n    </div>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-model-registry.png" alt="Model Registry" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/ml/model-registry/"><strong>ðŸ’¾ Model Registry</strong></a>\n        <br><br>\n        <div> A centralized model store designed to collaboratively manage the full lifecycle and deployment of machine learning models.</div><br>\n        <a href="https://mlflow.org/docs/latest/ml/model-registry/tutorial/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-deployment.png" alt="Deployment" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/ml/deployment/"><strong>ðŸš€ Deployment</strong></a>\n        <br><br>\n        <div> Tools for seamless model deployment to batch and real-time scoring on platforms like Docker, Kubernetes, Azure ML, and AWS SageMaker.</div><br>\n        <a href="https://mlflow.org/docs/latest/ml/deployment/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n</table>\n\n## ðŸŒ Hosting MLflow Anywhere\n\n<div align="center" >\n  <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-providers.png" alt="Providers" width=100%>\n</div>\n\nYou can run MLflow in many different environments, including local machines, on-premise servers, and cloud infrastructure.\n\nTrusted by thousands of organizations, MLflow is now offered as a managed service by most major cloud providers:\n\n- [Amazon SageMaker](https://aws.amazon.com/sagemaker-ai/experiments/)\n- [Azure ML](https://learn.microsoft.com/en-us/azure/machine-learning/concept-mlflow?view=azureml-api-2)\n- [Databricks](https://www.databricks.com/product/managed-mlflow)\n- [Nebius](https://nebius.com/services/managed-mlflow)\n\nFor hosting MLflow on your own infrastructure, please refer to [this guidance](https://mlflow.org/docs/latest/ml/tracking/#tracking-setup).\n\n## ðŸ—£ï¸ Supported Programming Languages\n\n- [Python](https://pypi.org/project/mlflow/)\n- [TypeScript / JavaScript](https://www.npmjs.com/package/mlflow-tracing)\n- [Java](https://mvnrepository.com/artifact/org.mlflow/mlflow-client)\n- [R](https://cran.r-project.org/web/packages/mlflow/readme/README.html)\n\n## ðŸ”— Integrations\n\nMLflow is natively integrated with many popular machine learning frameworks and GenAI libraries.\n\n![Integrations](https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-integrations.png)\n\n## Usage Examples\n\n### Experiment Tracking ([Doc](https://mlflow.org/docs/latest/ml/tracking/))\n\nThe following examples trains a simple regression model with scikit-learn, while enabling MLflow\'s [autologging](https://mlflow.org/docs/latest/tracking/autolog.html) feature for experiment tracking.\n\n```python\nimport mlflow\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Enable MLflow\'s automatic experiment tracking for scikit-learn\nmlflow.sklearn.autolog()\n\n# Load the training dataset\ndb = load_diabetes()\nX_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n\nrf ', 'a managed service by most major cloud providers:\n\n- [Amazon SageMaker](https://aws.amazon.com/sagemaker-ai/experiments/)\n- [Azure ML](https://learn.microsoft.com/en-us/azure/machine-learning/concept-mlflow?view=azureml-api-2)\n- [Databricks](https://www.databricks.com/product/managed-mlflow)\n- [Nebius](https://nebius.com/services/managed-mlflow)\n\nFor hosting MLflow on your own infrastructure, please refer to [this guidance](https://mlflow.org/docs/latest/ml/tracking/#tracking-setup).\n\n## ðŸ—£ï¸ Supported Programming Languages\n\n- [Python](https://pypi.org/project/mlflow/)\n- [TypeScript / JavaScript](https://www.npmjs.com/package/mlflow-tracing)\n- [Java](https://mvnrepository.com/artifact/org.mlflow/mlflow-client)\n- [R](https://cran.r-project.org/web/packages/mlflow/readme/README.html)\n\n## ðŸ”— Integrations\n\nMLflow is natively integrated with many popular machine learning frameworks and GenAI libraries.\n\n![Integrations](https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-integrations.png)\n\n## Usage Examples\n\n### Experiment Tracking ([Doc](https://mlflow.org/docs/latest/ml/tracking/))\n\nThe following examples trains a simple regression model with scikit-learn, while enabling MLflow\'s [autologging](https://mlflow.org/docs/latest/tracking/autolog.html) feature for experiment tracking.\n\n```python\nimport mlflow\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Enable MLflow\'s automatic experiment tracking for scikit-learn\nmlflow.sklearn.autolog()\n\n# Load the training dataset\ndb = load_diabetes()\nX_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n\nrf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\n# MLflow triggers logging automatically upon model fitting\nrf.fit(X_train, y_train)\n```\n\nOnce the above code finishes, run the following command in a separate terminal and access the MLflow UI via the printed URL. An MLflow **Run** should be automatically created, which tracks the training dataset, hyper parameters, performance metrics, the trained model, dependencies, and even more.\n\n```\nmlflow ui\n```\n\n### Evaluating Models ([Doc](https://mlflow.org/docs/latest/model-evaluation/index.html))\n\nThe following example runs automatic evaluation for question-answering tasks with several built-in metrics.\n\n```python\nimport mlflow\nimport pandas as pd\n\n# Evaluation set contains (1) input question (2) model outputs (3) ground truth\ndf = pd.DataFrame(\n    {\n        "inputs": ["What is MLflow?", "What is Spark?"],\n        "outputs": [\n            "MLflow is an innovative fully self-driving airship powered by AI.",\n            "Sparks is an American pop and rock duo formed in Los Angeles.",\n        ],\n        "ground_truth": [\n            "MLflow is an open-source platform for productionizing AI.",\n            "Apache Spark is an open-source, distributed computing system.",\n        ],\n    }\n)\neval_dataset = mlflow.data.from_pandas(\n    df, predictions="outputs", targets="ground_truth"\n)\n\n# Start an MLflow Run to record the evaluation results to\nwith mlflow.start_run(run_name="evaluate_qa"):\n    ', 'is an American pop and rock duo formed in Los Angeles.",\n        ],\n        "ground_truth": [\n            "MLflow is an open-source platform for productionizing AI.",\n            "Apache Spark is an open-source, distributed computing system.",\n        ],\n    }\n)\neval_dataset = mlflow.data.from_pandas(\n    df, predictions="outputs", targets="ground_truth"\n)\n\n# Start an MLflow Run to record the evaluation results to\nwith mlflow.start_run(run_name="evaluate_qa"):\n    # Run automatic evaluation with a set of built-in metrics for question-answering models\n    results = mlflow.evaluate(\n        data=eval_dataset,\n        model_type="question-answering",\n    )\n\nprint(results.tables["eval_results_table"])\n```\n\n### Observability ([Doc](https://mlflow.org/docs/latest/llms/tracing/index.html))\n\nMLflow Tracing provides LLM observability for various GenAI libraries such as OpenAI, LangChain, LlamaIndex, DSPy, AutoGen, and more. To enable auto-tracing, call `mlflow.xyz.autolog()` before running your models. Refer to the documentation for customization and manual instrumentation.\n\n```python\nimport mlflow\nfrom openai import OpenAI\n\n# Enable tracing for OpenAI\nmlflow.openai.autolog()\n\n# Query OpenAI LLM normally\nresponse = OpenAI().chat.completions.create(\n    model="gpt-4o-mini",\n    messages=[{"role": "user", "content": "Hi!"}],\n    temperature=0.1,\n)\n```\n\nThen navigate to the "Traces" tab in the MLflow UI to find the trace records OpenAI query.\n\n## ðŸ’­ Support\n\n- For help or questions about MLflow usage (e.g. "how do I do X?") visit the [documentation](https://mlflow.org/docs/latest/index.html).\n- In the documentation, you can ask the question to our AI-powered chat bot. Click on the **"Ask AI"** button at the right bottom.\n- Join the [virtual events](https://lu.ma/mlflow?k=c) like office hours and meetups.\n- To report a bug, file a documentation issue, or submit a feature request, please [open a GitHub issue](https://github.com/mlflow/mlflow/issues/new/choose).\n- For release announcements and other discussions, please subscribe to our mailing list (mlflow-users@googlegroups.com)\n  or join us on [Slack](https://mlflow.org/slack).\n\n## ðŸ¤ Contributing\n\nWe happily welcome contributions to MLflow!\n\n- Submit [bug reports](https://github.com/mlflow/mlflow/issues/new?template=bug_report_template.yaml) and [feature requests](https://github.com/mlflow/mlflow/issues/new?template=feature_request_template.yaml)\n- Contribute for [good-first-issues](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) and [help-wanted](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22)\n- Writing about MLflow and sharing your experience\n\nPlease see our [contribution guide](CONTRIBUTING.md) to learn more about contributing to MLflow.\n\n## â­ï¸ Star History\n\n<a href="https://star-history.com/#mlflow/mlflow&Date">\n <picture>\n  ', 'chat bot. Click on the **"Ask AI"** button at the right bottom.\n- Join the [virtual events](https://lu.ma/mlflow?k=c) like office hours and meetups.\n- To report a bug, file a documentation issue, or submit a feature request, please [open a GitHub issue](https://github.com/mlflow/mlflow/issues/new/choose).\n- For release announcements and other discussions, please subscribe to our mailing list (mlflow-users@googlegroups.com)\n  or join us on [Slack](https://mlflow.org/slack).\n\n## ðŸ¤ Contributing\n\nWe happily welcome contributions to MLflow!\n\n- Submit [bug reports](https://github.com/mlflow/mlflow/issues/new?template=bug_report_template.yaml) and [feature requests](https://github.com/mlflow/mlflow/issues/new?template=feature_request_template.yaml)\n- Contribute for [good-first-issues](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) and [help-wanted](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22)\n- Writing about MLflow and sharing your experience\n\nPlease see our [contribution guide](CONTRIBUTING.md) to learn more about contributing to MLflow.\n\n## â­ï¸ Star History\n\n<a href="https://star-history.com/#mlflow/mlflow&Date">\n <picture>\n   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date&theme=dark" />\n   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date" />\n   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date" />\n </picture>\n</a>\n\n## âœï¸ Citation\n\nIf you use MLflow in your research, please cite it using the "Cite this repository" button at the top of the [GitHub repository page](https://github.com/mlflow/mlflow), which will provide you with citation formats including APA and BibTeX.\n\n## ðŸ‘¥ Core Members\n\nMLflow is currently maintained by the following core members with significant contributions from hundreds of exceptionally talented community members.\n\n- [Ben Wilson](https://github.com/BenWilson2)\n- [Corey Zumar](https://github.com/dbczumar)\n- [Daniel Lok](https://github.com/daniellok-db)\n- [Gabriel Fu](https://github.com/gabrielfu)\n- [Harutaka Kawamura](https://github.com/harupy)\n- [Serena Ruan](https://github.com/serena-ruan)\n- [Tomu Hirata](https://github.com/TomeHirata)\n- [Weichen Xu](https://github.com/WeichenXu123)\n- [Yuki Watanabe](https://github.com/B-Step62)\n', '# MLflow Tracing: An Open-Source SDK for Observability and Monitoring GenAI ApplicationsðŸ”\n\n[![Latest Docs](https://img.shields.io/badge/docs-latest-success.svg?style=for-the-badge)](https://mlflow.org/docs/latest/index.html)\n[![Apache 2 License](https://img.shields.io/badge/license-Apache%202-brightgreen.svg?style=for-the-badge&logo=apache)](https://github.com/mlflow/mlflow/blob/master/LICENSE.txt)\n[![Slack](https://img.shields.io/badge/slack-@mlflow--users-CF0E5B.svg?logo=slack&logoColor=white&labelColor=3F0E40&style=for-the-badge)](https://mlflow.org/community/#slack)\n[![Twitter](https://img.shields.io/twitter/follow/MLflow?style=for-the-badge&labelColor=00ACEE&logo=twitter&logoColor=white)](https://twitter.com/MLflow)\n\nMLflow Tracing (`mlflow-tracing`) is an open-source, lightweight Python package that only includes the minimum set of dependencies and functionality\nto instrument your code/models/agents with [MLflow Tracing Feature](https://mlflow.org/docs/latest/tracing). It is designed to be a perfect fit for production environments where you want:\n\n- **âš¡ï¸ Faster Deployment**: The package size and dependencies are significantly smaller than the full MLflow package, allowing for faster deployment times in dynamic environments such as Docker containers, serverless functions, and cloud-based applications.\n- **ðŸ”§ Simplified Dependency Management**: A smaller set of dependencies means less work keeping up with dependency updates, security patches, and breaking changes from upstream libraries.\n- **ðŸ“¦ Portability**: With the less number of dependencies, MLflow Tracing can be easily deployed across different environments and platforms, without worrying about compatibility issues.\n- **ðŸ”’ Fewer Security Risks**: Each dependency potentially introduces security vulnerabilities. By reducing the number of dependencies, MLflow Tracing minimizes the attack surface and reduces the risk of security breaches.\n\n## âœ¨ Features\n\n- [Automatic Tracing](https://mlflow.org/docs/latest/tracing/integrations/) for AI libraries (OpenAI, LangChain, DSPy, Anthropic, etc...). Follow the link for the full list of supported libraries.\n- [Manual instrumentation APIs](https://mlflow.org/docs/latest/tracing/api/manual-instrumentation) such as `@trace` decorator.\n- [Production Monitoring](https://mlflow.org/docs/latest/tracing/production)\n- Other tracing APIs such as `mlflow.set_trace_tag`, `mlflow.search_traces`, etc.\n\n## ðŸŒ Choose Backend\n\nThe MLflow Trace package is designed to work with a remote hosted MLflow server as a backend. This allows you to log your traces to a central location, making it easier to manage and analyze your traces. There are several different options for hosting your MLflow server, including:\n\n- [Databricks](https://docs.databricks.com/machine-learning/mlflow/managed-mlflow.html) - Databricks offers a FREE, fully managed MLflow server as a part of their platform. This is the easiest way to get started with MLflow tracing, without having to set up any infrastructure.\n- [Amazon SageMaker](https://aws.amazon.com/sagemaker-ai/experiments/) - MLflow on Amazon SageMaker is a fully managed service offered as part of the SageMaker platform by AWS, including tracing and other MLflow features such as model registry.\n- [Nebius](https://nebius.com/) - Nebius, a cutting-edge cloud platform for GenAI explorers, offers a fully managed MLflow server.\n- [Self-hosting](https://mlflow.org/docs/latest/tracking/#tracking_setup) - MLflow is a fully open-source project, allowing you to self-host your own MLflow ', 'your MLflow server, including:\n\n- [Databricks](https://docs.databricks.com/machine-learning/mlflow/managed-mlflow.html) - Databricks offers a FREE, fully managed MLflow server as a part of their platform. This is the easiest way to get started with MLflow tracing, without having to set up any infrastructure.\n- [Amazon SageMaker](https://aws.amazon.com/sagemaker-ai/experiments/) - MLflow on Amazon SageMaker is a fully managed service offered as part of the SageMaker platform by AWS, including tracing and other MLflow features such as model registry.\n- [Nebius](https://nebius.com/) - Nebius, a cutting-edge cloud platform for GenAI explorers, offers a fully managed MLflow server.\n- [Self-hosting](https://mlflow.org/docs/latest/tracking/#tracking_setup) - MLflow is a fully open-source project, allowing you to self-host your own MLflow server and keep your data private. This is a great option if you want to have full control over your data and infrastructure.\n\n## ðŸš€ Getting Started\n\n### Installation\n\nTo install the MLflow Python package, run the following command:\n\n```bash\npip install mlflow-tracing\n```\n\nTo install from the source code, run the following command:\n\n```bash\npip install git+https://github.com/mlflow/mlflow.git#subdirectory=libs/tracing\n```\n\n> **NOTE:** It is **not** recommended to co-install this package with the full MLflow package together, as it may cause version mismatches issues.\n\n### Connect to the MLflow Server\n\nTo connect to your MLflow server to log your traces, set the `MLFLOW_TRACKING_URI` environment variable or use the `mlflow.set_tracking_uri` function:\n\n```python\nimport mlflow\n\nmlflow.set_tracking_uri("databricks")\n# Specify the experiment to log the traces to\nmlflow.set_experiment("/Path/To/Experiment")\n```\n\n### Start Logging Traces\n\n```python\nimport openai\n\nclient = openai.OpenAI(api_key="<your-api-key>")\n\n# Enable auto-tracing for OpenAI\nmlflow.openai.autolog()\n\n# Call the OpenAI API as usual\nresponse = client.chat.completions.create(\n    model="gpt-4.1-mini",\n    messages=[{"role": "user", "content": "Hello, how are you?"}],\n)\n```\n\n## ðŸ“˜ Documentation\n\nOfficial documentation for MLflow Tracing can be found at [here](https://mlflow.org/docs/latest/tracing).\n\n## ðŸ›‘ Features _Not_ Included\n\nThe following MLflow features are not included in this package.\n\n- MLflow tracking server and UI.\n- MLflow\'s other tracking capabilities such as Runs, Model Registry, Projects, etc.\n- Evaluate models/agents and log evaluation results.\n\nTo leverage the full feature set of MLflow, install the full package by running `pip install mlflow`.\n', '<h1 align="center" style="border-bottom: none">\n    <div>\n        <a href="https://mlflow.org/"><picture>\n            <img alt="MLflow Logo" src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/logo.svg" width="200" />\n        </picture></a>\n        <br>\n        MLflow TypeScript SDK\n    </div>\n</h1>\n<h2 align="center" style="border-bottom: none"></h2>\n\n<p align="center">\n  <a href="https://github.com/mlflow/mlflow"><img src="https://img.shields.io/github/stars/mlflow/mlflow?style=social" alt="stars"></a>\n  <a href="https://www.npmjs.com/package/mlflow-tracing"><img src="https://img.shields.io/npm/v/mlflow-tracing.svg" alt="version"></a>\n  <a href="https://www.npmjs.com/package/mlflow-tracing"><img src="https://img.shields.io/npm/dt/mlflow-tracing.svg" alt="downloads"></a>\n  <a href="https://github.com/mlflow/mlflow/blob/main/LICENSE"><img src="https://img.shields.io/github/license/mlflow/mlflow" alt="license"></a>\n</p>\n\nMLflow Typescript SDK is a variant of the [MLflow Python SDK](https://github.com/mlflow/mlflow) that provides a TypeScript API for MLflow.\n\n> [!IMPORTANT]\n> MLflow Typescript SDK is catching up with the Python SDK. Currently only support [Tracing]() and [Feedback Collection]() features. Please raise an issue in Github if you need a feature that is not supported.\n\n## Packages\n\n| Package                                | NPM                                                                                                                                         | Description                                       ', "                                                            | Description                                                |\n| -------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------- |\n| [mlflow-tracing](./core)               | [![npm package](https://img.shields.io/npm/v/mlflow-tracing?style=flat-square)](https://www.npmjs.com/package/mlflow-tracing)               | The core tracing functionality and manual instrumentation. |\n| [mlflow-openai](./integrations/openai) | [![npm package](https://img.shields.io/npm/v/mlflow-tracing-openai?style=flat-square)](https://www.npmjs.com/package/mlflow-tracing-openai) | Auto-instrumentation integration for OpenAI.               |\n\n## Installation\n\n```bash\nnpm install mlflow-tracing\n```\n\n> [!NOTE]\n> MLflow Typescript SDK requires Node.js 20 or higher.\n\n## Quickstart\n\nStart MLflow Tracking Server if you don't have one already:\n\n```bash\npip install mlflow\nmlflow server --backend-store-uri sqlite:///mlruns.db --port 5000\n```\n\nSelf-hosting MLflow server requires Python 3.10 or higher. If you don't have one, you can also use [managed MLflow service](https://mlflow.org/#get-started) for free to get started quickly.\n\nInstantiate MLflow SDK in your application:\n\n```typescript\nimport * as mlflow from 'mlflow-tracing';\n\nmlflow.init({\n  trackingUri: 'http://localhost:5000',\n  experimentId: '<experiment-id>'\n});\n```\n\nCreate a trace:\n\n```typescript\n// Wrap a function with mlflow.trace to generate a span when the function is called.\n// MLflow will automatically record the function name, arguments, return value,\n// latency, and exception information to the span.\nconst getWeather = mlflow.trace(\n  (city: string) => {\n    return `The weather in ${city} is sunny`;\n  },\n  // Pass options to set span name. See https://mlflow.org/docs/latest/genai/tracing/app-instrumentation/typescript-sdk\n  // for the full list of options.\n  { name: 'get-weather' }\n);\ngetWeather('San Francisco');\n\n// Alternatively, start and end span manually\nconst span = mlflow.startSpan({ name: 'my-span' });\nspan.end();\n```\n\nView traces in MLflow UI:\n\n![MLflow Tracing UI](https://github.com/mlflow/mlflow/blob/891fed9a746477f808dd2b82d3abb2382293c564/docs/static/images/llms/tracing/quickstart/openai-tool-calling-trace-detail.png?raw=true)\n\n## Trace Usage\n\nMLflow Tracing empowers ", '\'<experiment-id>\'\n});\n```\n\nCreate a trace:\n\n```typescript\n// Wrap a function with mlflow.trace to generate a span when the function is called.\n// MLflow will automatically record the function name, arguments, return value,\n// latency, and exception information to the span.\nconst getWeather = mlflow.trace(\n  (city: string) => {\n    return `The weather in ${city} is sunny`;\n  },\n  // Pass options to set span name. See https://mlflow.org/docs/latest/genai/tracing/app-instrumentation/typescript-sdk\n  // for the full list of options.\n  { name: \'get-weather\' }\n);\ngetWeather(\'San Francisco\');\n\n// Alternatively, start and end span manually\nconst span = mlflow.startSpan({ name: \'my-span\' });\nspan.end();\n```\n\nView traces in MLflow UI:\n\n![MLflow Tracing UI](https://github.com/mlflow/mlflow/blob/891fed9a746477f808dd2b82d3abb2382293c564/docs/static/images/llms/tracing/quickstart/openai-tool-calling-trace-detail.png?raw=true)\n\n## Trace Usage\n\nMLflow Tracing empowers you throughout the end-to-end lifecycle of your application. Here\'s how it helps you at each step of the workflow, click on each section to learn more:\n\n<details>\n<summary><strong>ðŸ” Build & Debug</strong></summary>\n\n<table>\n<tr>\n<td width="60%">\n\n#### Smooth Debugging Experience\n\nMLflow\'s tracing capabilities provide deep insights into what happens beneath the abstractions of your application, helping you precisely identify where issues occur.\n\n[Learn more â†’](https://mlflow.org/docs/latest/genai/tracing/observe-with-traces)\n\n</td>\n<td width="40%">\n\n![Trace Debug](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-trace-debug.png)\n\n</td>\n</tr>\n</table>\n\n</details>\n\n<details>\n<summary><strong>ðŸ’¬ Human Feedback</strong></summary>\n\n<table>\n<tr>\n<td width="60%">\n\n#### Track Annotation and User Feedback Attached to Traces\n\nCollecting and managing feedback is essential for improving your application. MLflow Tracing allows you to attach user feedback and annotations directly to traces, creating a rich dataset for analysis.\n\nThis feedback data helps you understand user satisfaction, identify areas for improvement, and build better evaluation datasets based on real user interactions.\n\n[Learn more â†’](https://mlflow.org/docs/latest/genai/tracing/collect-user-feedback)\n\n</td>\n<td width="40%">\n\n![Human Feedback](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-human-feedback.png)\n\n</td>\n</tr>\n</table>\n\n</details>\n\n<details>\n<summary><strong>ðŸ“Š Evaluation</strong></summary>\n\n<table>\n<tr>\n<td width="60%">\n\n#### Systematic Quality Assessment Throughout Your Application\n\nEvaluating the performance of your application is crucial, but creating a reliable evaluation process can be challenging. Traces serve as a rich data source, helping you assess quality with precise metrics for all components.\n\nWhen combined with MLflow\'s evaluation capabilities, you get a seamless experience for assessing and improving your application\'s performance.\n\n[Learn more â†’](https://mlflow.org/docs/latest/genai/eval-monitor)\n\n</td>\n<td width="40%">\n\n![Evaluation](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-trace-evaluation.png)\n\n</td>\n</tr>\n</table>\n\n</details>\n\n<details>\n<summary><strong>ðŸš€ Production Monitoring</strong></summary>\n\n<table>\n<tr>\n<td width="60%">\n\n#### Monitor Applications with Your Favorite Observability Stack\n\nMachine learning projects don\'t end with the first launch. Continuous monitoring and incremental improvement are critical to long-term success.\n\nIntegrated with various observability platforms such as Databricks, Datadog, Grafana, and Prometheus, MLflow Tracing provides a comprehensive solution for monitoring your applications in production.\n\n[Learn more â†’](https://mlflow.org/docs/latest/genai/tracing/prod-tracing)\n\n</td>\n<td width="40%">\n\n![Monitoring](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-monitoring.png)\n\n</td>\n</tr>\n</table>\n\n</details>\n\n<details>\n<summary><strong>ðŸ“¦ Dataset Collection</strong></summary>\n\n<table>\n<tr>\n<td width="60%">\n\n#### Create High-Quality Evaluation Datasets from Production Traces\n\nTraces from production are ', 'rich data source, helping you assess quality with precise metrics for all components.\n\nWhen combined with MLflow\'s evaluation capabilities, you get a seamless experience for assessing and improving your application\'s performance.\n\n[Learn more â†’](https://mlflow.org/docs/latest/genai/eval-monitor)\n\n</td>\n<td width="40%">\n\n![Evaluation](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-trace-evaluation.png)\n\n</td>\n</tr>\n</table>\n\n</details>\n\n<details>\n<summary><strong>ðŸš€ Production Monitoring</strong></summary>\n\n<table>\n<tr>\n<td width="60%">\n\n#### Monitor Applications with Your Favorite Observability Stack\n\nMachine learning projects don\'t end with the first launch. Continuous monitoring and incremental improvement are critical to long-term success.\n\nIntegrated with various observability platforms such as Databricks, Datadog, Grafana, and Prometheus, MLflow Tracing provides a comprehensive solution for monitoring your applications in production.\n\n[Learn more â†’](https://mlflow.org/docs/latest/genai/tracing/prod-tracing)\n\n</td>\n<td width="40%">\n\n![Monitoring](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-monitoring.png)\n\n</td>\n</tr>\n</table>\n\n</details>\n\n<details>\n<summary><strong>ðŸ“¦ Dataset Collection</strong></summary>\n\n<table>\n<tr>\n<td width="60%">\n\n#### Create High-Quality Evaluation Datasets from Production Traces\n\nTraces from production are invaluable for building comprehensive evaluation datasets. By capturing real user interactions and their outcomes, you can create test cases that truly represent your application\'s usage patterns.\n\nThis comprehensive data capture enables you to create realistic test scenarios, validate model performance on actual usage patterns, and continuously improve your evaluation datasets.\n\n[Learn more â†’](https://mlflow.org/docs/latest/genai/tracing/search-traces#creating-evaluation-datasets)\n\n</td>\n<td width="40%">\n\n![Dataset Collection](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-trace-dataset.png)\n\n</td>\n</tr>\n</table>\n\n</details>\n\n## Documentation ðŸ“˜\n\nOfficial documentation for MLflow Typescript SDK can be found [here](https://mlflow.org/docs/latest/genai/tracing/app-instrumentation/typescript-sdk).\n\n## License\n\nThis project is licensed under the [Apache License 2.0](https://github.com/mlflow/mlflow/blob/master/LICENSE.txt).\n', "# MLflow Typescript SDK - Core\n\nThis is the core package of the [MLflow Typescript SDK](https://github.com/mlflow/mlflow/tree/main/libs/typescript). It is a skinny package that includes the core tracing functionality and manual instrumentation.\n\n| Package              | NPM                                                                                                                           | Description                                                |\n| -------------------- | ----------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------- |\n| [mlflow-tracing](./) | [![npm package](https://img.shields.io/npm/v/mlflow-tracing?style=flat-square)](https://www.npmjs.com/package/mlflow-tracing) | The core tracing functionality and manual instrumentation. |\n\n## Installation\n\n```bash\nnpm install mlflow-tracing\n```\n\n## Quickstart\n\nStart MLflow Tracking Server if you don't have one already:\n\n```bash\npip install mlflow\nmlflow server --backend-store-uri sqlite:///mlruns.db --port 5000\n```\n\nSelf-hosting MLflow server requires Python 3.10 or higher. If you don't have one, you can also use [managed MLflow service](https://mlflow.org/#get-started) for free to get started quickly.\n\nInstantiate MLflow SDK in your application:\n\n```typescript\nimport * as mlflow from 'mlflow-tracing';\n\nmlflow.init({\n  trackingUri: 'http://localhost:5000',\n  experimentId: '<experiment-id>'\n});\n```\n\nCreate a trace:\n\n```typescript\n// Wrap a function with mlflow.trace to generate a span when the function is called.\n// MLflow will automatically record the function name, arguments, return value,\n// latency, and exception information to the span.\nconst getWeather = mlflow.trace(\n  (city: string) => {\n    return `The weather in ${city} is sunny`;\n  },\n ", "mlflow\nmlflow server --backend-store-uri sqlite:///mlruns.db --port 5000\n```\n\nSelf-hosting MLflow server requires Python 3.10 or higher. If you don't have one, you can also use [managed MLflow service](https://mlflow.org/#get-started) for free to get started quickly.\n\nInstantiate MLflow SDK in your application:\n\n```typescript\nimport * as mlflow from 'mlflow-tracing';\n\nmlflow.init({\n  trackingUri: 'http://localhost:5000',\n  experimentId: '<experiment-id>'\n});\n```\n\nCreate a trace:\n\n```typescript\n// Wrap a function with mlflow.trace to generate a span when the function is called.\n// MLflow will automatically record the function name, arguments, return value,\n// latency, and exception information to the span.\nconst getWeather = mlflow.trace(\n  (city: string) => {\n    return `The weather in ${city} is sunny`;\n  },\n  // Pass options to set span name. See https://mlflow.org/docs/latest/genai/tracing/app-instrumentation/typescript-sdk\n  // for the full list of options.\n  { name: 'get-weather' }\n);\ngetWeather('San Francisco');\n\n// Alternatively, start and end span manually\nconst span = mlflow.startSpan({ name: 'my-span' });\nspan.end();\n```\n\n## Documentation ðŸ“˜\n\nOfficial documentation for MLflow Typescript SDK can be found [here](https://mlflow.org/docs/latest/genai/tracing/app-instrumentation/typescript-sdk).\n\n## License\n\nThis project is licensed under the [Apache License 2.0](https://github.com/mlflow/mlflow/blob/master/LICENSE.txt).\n", "# MLflow Typescript SDK - OpenAI\n\nSeamlessly integrate [MLflow Tracing](https://github.com/mlflow/mlflow/tree/main/libs/typescript) with OpenAI to automatically trace your OpenAI API calls.\n\n| Package             | NPM                                                                                                                                         | Description                                  |\n| ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |\n| [mlflow-openai](./) | [![npm package](https://img.shields.io/npm/v/mlflow-tracing-openai?style=flat-square)](https://www.npmjs.com/package/mlflow-tracing-openai) | Auto-instrumentation integration for OpenAI. |\n\n## Installation\n\n```bash\nnpm install mlflow-openai\n```\n\nThe package includes the [`mlflow-tracing`](https://github.com/mlflow/mlflow/tree/main/libs/typescript) package and `openai` package as peer dependencies. Depending on your package manager, you may need to install these two packages separately.\n\n## Quickstart\n\nStart MLflow Tracking Server if you don't have one already:\n\n```bash\npip install mlflow\nmlflow server --backend-store-uri sqlite:///mlruns.db --port 5000\n```\n\nSelf-hosting MLflow server requires Python 3.10 or higher. If you don't have one, you can also use [managed MLflow service](https://mlflow.org/#get-started) for free to get started quickly.\n\nInstantiate MLflow SDK in your application:\n\n```typescript\nimport * as mlflow from 'mlflow-tracing';\n\nmlflow.init({\n  trackingUri: 'http://localhost:5000',\n  experimentId: '<experiment-id>'\n});\n```\n\nCreate a trace:\n\n```typescript\nimport { OpenAI } from 'openai';\nimport { tracedOpenAI } from 'mlflow-openai';\n\n// Wrap the OpenAI client with the tracedOpenAI function\nconst client = tracedOpenAI(new OpenAI());\n\n// Invoke the client as usual\nconst response = await client.chat.completions.create({\n  model: 'o4-mini',\n  messages: [\n    { ", 'MLflow Tracking Server if you don\'t have one already:\n\n```bash\npip install mlflow\nmlflow server --backend-store-uri sqlite:///mlruns.db --port 5000\n```\n\nSelf-hosting MLflow server requires Python 3.10 or higher. If you don\'t have one, you can also use [managed MLflow service](https://mlflow.org/#get-started) for free to get started quickly.\n\nInstantiate MLflow SDK in your application:\n\n```typescript\nimport * as mlflow from \'mlflow-tracing\';\n\nmlflow.init({\n  trackingUri: \'http://localhost:5000\',\n  experimentId: \'<experiment-id>\'\n});\n```\n\nCreate a trace:\n\n```typescript\nimport { OpenAI } from \'openai\';\nimport { tracedOpenAI } from \'mlflow-openai\';\n\n// Wrap the OpenAI client with the tracedOpenAI function\nconst client = tracedOpenAI(new OpenAI());\n\n// Invoke the client as usual\nconst response = await client.chat.completions.create({\n  model: \'o4-mini\',\n  messages: [\n    { role: \'system\', content: \'You are a helpful weather assistant.\' },\n    { role: \'user\', content: "What\'s the weather like in Seattle?" }\n  ]\n});\n```\n\nView traces in MLflow UI:\n\n![MLflow Tracing UI](https://github.com/mlflow/mlflow/blob/891fed9a746477f808dd2b82d3abb2382293c564/docs/static/images/llms/tracing/quickstart/single-openai-trace-detail.png?raw=true)\n\n## Documentation ðŸ“˜\n\nOfficial documentation for MLflow Typescript SDK can be found [here](https://mlflow.org/docs/latest/genai/tracing/app-instrumentation/typescript-sdk).\n\n## License\n\nThis project is licensed under the [Apache License 2.0](https://github.com/mlflow/mlflow/blob/master/LICENSE.txt).\n', '---\nnamespace: genai\ndescription: Analyzes the traces logged in an MLflow experiment to find operational and quality issues automatically, generating a markdown report.\n---\n\n# Analyze Experiment\n\nAnalyzes traces in an MLflow experiment for quality issues, performance problems, and patterns.\n\n## Step 1: Setup and Configuration\n\n### 1.1 Collect Experiment Information\n\n- **REQUIRED FIRST**: Ask user "How do you want to authenticate to MLflow?"\n\n  **Option 1: Local/Self-hosted MLflow**\n\n  - Ask for tracking URI (one of):\n    - SQLite: `sqlite:////path/to/mlflow.db`\n    - PostgreSQL: `postgresql://user:password@host:port/database`\n    - MySQL: `mysql://user:password@host:port/database`\n    - File Store: `file:///path/to/mlruns` or just `/path/to/mlruns`\n  - Ask user to create an environment file (e.g., `mlflow.env`) containing:\n    ```\n    MLFLOW_TRACKING_URI=<provided_uri>\n    ```\n\n  **Option 2: Databricks**\n\n  - Ask which authentication method:\n    - **PAT Auth**: Request `DATABRICKS_HOST` and `DATABRICKS_TOKEN`\n    - **Profile Auth**: Request `DATABRICKS_CONFIG_PROFILE` name\n  - Ask user to create an environment file (e.g., `mlflow.env`) containing:\n\n    ```\n    # For PAT Auth:\n    MLFLOW_TRACKING_URI=databricks\n    DATABRICKS_HOST=<provided_host>\n    DATABRICKS_TOKEN=<provided_token>\n\n    # OR for Profile Auth:\n    MLFLOW_TRACKING_URI=databricks\n    DATABRICKS_CONFIG_PROFILE=<provided_profile>\n    ```\n\n  **Option 3: Environment Variables Already Set**\n\n  - Ask user "Do you already have MLflow environment variables set in your shell (bashrc/zshrc)?"\n  - If yes, test connection directly: `uv run python -m mlflow experiments search --max-results 10`\n  - If this works, skip env file creation and use commands without `--env-file` flag\n  - If not, fall back to Options 1 or 2\n\n- Ask user for the path to their environment file (if using Options 1-2)\n- Verify connection by listing experiments: `uv run --env-file <env_file_path> python -m mlflow experiments search --max-results 10`\n- **Option to search by name**: If user knows the experiment name, use `--filter-string` parameter:\n  - `uv run --env-file <env_file_path> python -m mlflow experiments search --filter-string "name LIKE \'%experiment_name%\'" --max-results 10`\n- Ask user for experiment ID or let them choose from the list\n- **WAIT for user response** - do not continue ', 'env file creation and use commands without `--env-file` flag\n  - If not, fall back to Options 1 or 2\n\n- Ask user for the path to their environment file (if using Options 1-2)\n- Verify connection by listing experiments: `uv run --env-file <env_file_path> python -m mlflow experiments search --max-results 10`\n- **Option to search by name**: If user knows the experiment name, use `--filter-string` parameter:\n  - `uv run --env-file <env_file_path> python -m mlflow experiments search --filter-string "name LIKE \'%experiment_name%\'" --max-results 10`\n- Ask user for experiment ID or let them choose from the list\n- **WAIT for user response** - do not continue until they provide the experiment ID\n- Add `MLFLOW_EXPERIMENT_ID=<experiment_id>` to their environment file\n- Run `uv run --env-file <env_file_path> python -m mlflow traces --help` to understand the CLI commands and options\n\n### 1.2 Test Trace Retrieval\n\n- Call `uv run --env-file <env_file_path> python -m mlflow traces search --max-results 5` to verify:\n  - Traces exist in the experiment\n  - CLI is working properly (using local MLflow installation)\n  - Database connection is valid\n- Extract sample trace IDs for testing\n- Get one full trace with `uv run --env-file <env_file_path> python -m mlflow traces get --trace-id <id>` to understand the data structure\n\n## Step 2: Analysis Phase\n\n### 2.1 Bulk Trace Collection\n\n- Search for a larger sample using `--max-results` parameter (start with 20-50 traces for initial analysis)\n- **IMPORTANT**: Use `--max-results` to limit results for users with hundreds of thousands of experiments/traces\n- Extract key fields: trace_id, state, execution_duration_ms, request_preview, response_preview\n\n### 2.1.5 Understand Agent Purpose and Capabilities\n\n- Analyze trace inputs/outputs to understand the agent\'s task:\n  - Extract trace inputs/outputs: `--extract-fields info.trace_metadata.\\`mlflow.traceInputs\\`,info.trace_metadata.\\`mlflow.traceOutputs\\``\n  - Examine these fields to understand:\n    - Types of questions users ask\n    - Types of responses the agent provides\n    - Common patterns in user interactions\n  - Identify available tools by examining spans with type "TOOL":\n    - What tools are available to the agent?\n    - What data sources can the agent access?\n    - What capabilities do these tools provide?\n- Generate a 1-paragraph agent description covering:\n  - **What ', 'inputs/outputs to understand the agent\'s task:\n  - Extract trace inputs/outputs: `--extract-fields info.trace_metadata.\\`mlflow.traceInputs\\`,info.trace_metadata.\\`mlflow.traceOutputs\\``\n  - Examine these fields to understand:\n    - Types of questions users ask\n    - Types of responses the agent provides\n    - Common patterns in user interactions\n  - Identify available tools by examining spans with type "TOOL":\n    - What tools are available to the agent?\n    - What data sources can the agent access?\n    - What capabilities do these tools provide?\n- Generate a 1-paragraph agent description covering:\n  - **What the agent\'s job is** (e.g., "a boating agent that answers questions about weather and helps users plan trips")\n  - **What data sources it has access to** (APIs, databases, etc.)\n- **Present this description to the user** and ask for confirmation/corrections\n- **WAIT for user response** - do not proceed until they confirm or provide corrections\n- **Ask if they want to focus the analysis on anything specific** (or do a general report)\n  - If they provide specific focus areas, use these as additional context for hypothesis formation\n  - Don\'t overfit to their focus - still do comprehensive analysis, but prioritize their areas of interest\n  - Their specific concerns should become hypotheses to validate/invalidate during analysis\n- **WAIT for user response** before proceeding to section 2.2\n- Use agent context + any specific focus areas for all subsequent hypothesis testing in sections 2.2+\n\n### 2.2 Operational Issues Analysis (Hypothesis-Driven Approach)\n\n**NOTE: Use MLflow CLI commands for trace exploration - DO NOT use inline Python scripts during this phase**\n\n**Show your thinking as you go**: Always explain your hypothesis development process including:\n\n- Current hypothesis being tested\n- Evidence found: ALWAYS show BOTH trace input (user request) AND trace output (agent response), plus tools called\n- Reasoning for supporting/refuting the hypothesis\n\nProcess traces in batches of 10, building and refining hypotheses with each batch:\n\n1. Form initial hypotheses from first batch\n2. With each new batch: validate, refute, or expand hypotheses\n3. Continue until patterns stabilize\n\n**After confirming ANY hypothesis (operational or quality)**: Track assessments for inclusion in final report:\n\n- **1:1 Correspondence**: Each assessment ', 'commands for trace exploration - DO NOT use inline Python scripts during this phase**\n\n**Show your thinking as you go**: Always explain your hypothesis development process including:\n\n- Current hypothesis being tested\n- Evidence found: ALWAYS show BOTH trace input (user request) AND trace output (agent response), plus tools called\n- Reasoning for supporting/refuting the hypothesis\n\nProcess traces in batches of 10, building and refining hypotheses with each batch:\n\n1. Form initial hypotheses from first batch\n2. With each new batch: validate, refute, or expand hypotheses\n3. Continue until patterns stabilize\n\n**After confirming ANY hypothesis (operational or quality)**: Track assessments for inclusion in final report:\n\n- **1:1 Correspondence**: Each assessment must correspond to ONE specific issue/hypothesis\n- Use snake_case names as assessment keys (e.g., `overly_verbose`, `tool_failure`, `rate_limited`, `slow_response`)\n- Track which traces exhibit each issue with detailed rationales\n- Document specifics like:\n\n  - For quality issues: exact character counts, repetition counts, unnecessary sections\n  - For operational issues: exact durations, error messages, timeout values\n\n- **Error Analysis**\n\n  - Filter for ERROR traces: `uv run --env-file <env_file_path> python -m mlflow traces search --filter "info.state = \'ERROR\'" --max-results 10`\n  - **Adjust --max-results as needed**: Start with 10-20, increase if you need more examples to identify patterns\n  - **Pattern Analysis Focus**: Identify WHY errors occur by examining:\n    - Tool/API failures in spans (look for spans with type "TOOL" that failed)\n    - Rate limiting responses from external APIs\n    - Authentication/permission errors\n    - Timeout patterns (compare execution_duration_ms)\n    - Input validation failures\n    - Resource unavailability (databases, services down)\n  - Example hypotheses to test:\n    - Certain types of queries consistently trigger tool failures\n    - Errors cluster around specific time ranges (service outages)\n    - Fast failures (~2s) indicate input validation vs slower failures (~30s) indicate timeouts\n    - Specific tools/APIs are unreliable and cause cascading failures\n    - Rate limiting from external services causes batch failures\n  - **Note**: You may discover other operational error patterns as you analyze the traces\n\n- **Performance Problems (High Latency Analysis)**\n ', 'failures\n    - Resource unavailability (databases, services down)\n  - Example hypotheses to test:\n    - Certain types of queries consistently trigger tool failures\n    - Errors cluster around specific time ranges (service outages)\n    - Fast failures (~2s) indicate input validation vs slower failures (~30s) indicate timeouts\n    - Specific tools/APIs are unreliable and cause cascading failures\n    - Rate limiting from external services causes batch failures\n  - **Note**: You may discover other operational error patterns as you analyze the traces\n\n- **Performance Problems (High Latency Analysis)**\n  - Filter for OK traces with high latency: `uv run --env-file <env_file_path> python -m mlflow traces search --filter "info.state = \'OK\'" --max-results 10`\n  - **Adjust --max-results as needed**: Start with 10-20, increase if you need more examples to identify patterns\n  - **Pattern Analysis Focus**: Identify WHY traces are slow by examining:\n    - Tool call duration patterns in spans\n    - Number of sequential vs parallel tool calls\n    - Specific slow APIs/tools (database queries, web requests, etc.)\n    - Cold start vs warm execution patterns\n    - Resource contention indicators\n  - Example hypotheses to test:\n    - Complex queries with multiple sequential tool calls have multiplicative latency\n    - Certain tools/APIs are consistent performance bottlenecks (>5s per call)\n    - First queries in sessions are slower due to cold start overhead\n    - Database queries without proper indexing cause delays\n    - Network timeouts or retries inflate execution time\n    - Parallel tool execution is not properly implemented\n  - **Note**: You may discover other performance patterns as you analyze the traces\n\n### 2.3 Quality Issues Analysis (Hypothesis-Driven Approach)\n\n**NOTE: Use MLflow CLI commands for trace exploration - DO NOT use inline Python scripts during this phase**\n\nFocus on response quality, not operational performance:\n\n- **Content Quality Issues**\n  - Sample both OK and ERROR traces\n  - Example hypotheses to test:\n    - Agent ', "start overhead\n    - Database queries without proper indexing cause delays\n    - Network timeouts or retries inflate execution time\n    - Parallel tool execution is not properly implemented\n  - **Note**: You may discover other performance patterns as you analyze the traces\n\n### 2.3 Quality Issues Analysis (Hypothesis-Driven Approach)\n\n**NOTE: Use MLflow CLI commands for trace exploration - DO NOT use inline Python scripts during this phase**\n\nFocus on response quality, not operational performance:\n\n- **Content Quality Issues**\n  - Sample both OK and ERROR traces\n  - Example hypotheses to test:\n    - Agent provides overly verbose responses for simple questions\n    - Some text/information is repeated unnecessarily across responses\n    - Conversation context carries over inappropriately\n    - Agent asks follow-up questions instead of attempting tasks\n    - Responses are inconsistent for similar queries\n    - Agent provides incorrect or outdated information\n    - Response format is inappropriate for the query type\n  - **Note**: You may discover other quality issues as you analyze the traces\n\n### 2.4 Strengths and Successes Analysis (Hypothesis-Driven Approach)\n\n**NOTE: Use MLflow CLI commands for trace exploration - DO NOT use inline Python scripts during this phase**\n\nProcess successful traces to identify what's working well:\n\n- **Successful Interactions**\n\n  - Filter for OK traces with good outcomes\n  - Example hypotheses to test:\n    - Agent provides comprehensive, helpful responses for complex queries\n    - Certain types of questions consistently get high-quality answers\n    - Tool usage is appropriate and effective for specific scenarios\n    - Response format is well-structured for particular use cases\n\n- **Effective Tool Usage**\n\n  - Examine traces where tools are used successfully\n  - Example hypotheses to test:\n    - Agent selects appropriate tools for different query types\n    - Multi-step tool usage produces better outcomes\n    - Certain tool combinations work particularly well together\n\n- **Quality Responses**\n  - Identify traces with good response quality\n  - Example hypotheses to test:\n  ", 'types of questions consistently get high-quality answers\n    - Tool usage is appropriate and effective for specific scenarios\n    - Response format is well-structured for particular use cases\n\n- **Effective Tool Usage**\n\n  - Examine traces where tools are used successfully\n  - Example hypotheses to test:\n    - Agent selects appropriate tools for different query types\n    - Multi-step tool usage produces better outcomes\n    - Certain tool combinations work particularly well together\n\n- **Quality Responses**\n  - Identify traces with good response quality\n  - Example hypotheses to test:\n    - Agent provides right level of detail for complex questions\n    - Safety/important information is appropriately included\n    - Agent successfully handles follow-up questions in context\n\n### 2.5 Generate Final Report\n\n- Ask user where to save the report (markdown file path, e.g., `experiment_analysis.md`)\n- **ONLY NOW use uv inline Python scripts for statistical calculations** - never compute stats manually\n- Inline Python scripts are ONLY for final math/statistics, NOT for trace exploration\n- Use `uv run --env-file <env_file_path> python -c "..."` for any Python calculations that need MLflow access\n- Generate a single comprehensive markdown report with:\n  - **Summary statistics** (computed via `uv run --env-file <env_file_path> python -c "..."` with collected trace data):\n    - Total traces analyzed\n    - Success rate (OK vs ERROR percentage)\n    - Average, median, p95 latency for successful traces\n    - Error rate distribution by duration (fast fails vs timeouts)\n  - **Operational Issues** (errors, latency, performance):\n    - For each confirmed operational hypothesis:\n      - Clear statement of the hypothesis\n      - Example trace IDs that support the hypothesis\n      - BOTH trace input (user request) AND trace output (agent response) excerpts from those traces\n      - Tools called (spans of type "TOOL") and their durations/failures\n      - Root cause analysis: WHY the issue occurs (rate limiting, API failures, timeouts, etc.)\n ', 'by duration (fast fails vs timeouts)\n  - **Operational Issues** (errors, latency, performance):\n    - For each confirmed operational hypothesis:\n      - Clear statement of the hypothesis\n      - Example trace IDs that support the hypothesis\n      - BOTH trace input (user request) AND trace output (agent response) excerpts from those traces\n      - Tools called (spans of type "TOOL") and their durations/failures\n      - Root cause analysis: WHY the issue occurs (rate limiting, API failures, timeouts, etc.)\n      - **Trace assessments**: List specific trace IDs that exhibit this issue with detailed rationales explaining why each trace demonstrates the pattern\n      - Quantitative evidence (frequency, timing patterns, etc.) - computed via Python\n  - **Quality Issues** (content problems, user experience):\n    - For each confirmed quality hypothesis:\n      - Clear statement of the hypothesis\n      - Example trace IDs that support the hypothesis\n      - BOTH trace input (user request) AND trace output (agent response) excerpts from those traces\n      - **Trace assessments**: List specific trace IDs that exhibit this issue with detailed rationales explaining why each trace demonstrates the pattern\n      - Quantitative evidence (frequency, assessment patterns, etc.) - computed via Python\n  - **Refuted Hypotheses** (briefly noted)\n  - Recommendations for improvement based on confirmed issues\n', '# MLflow Claude Code Integration\n\nThis module provides automatic tracing integration between Claude Code and MLflow.\n\n## Module Structure\n\n- **`config.py`** - Configuration management (settings files, environment variables)\n- **`hooks.py`** - Claude Code hook setup and management\n- **`cli.py`** - MLflow CLI commands (`mlflow autolog claude`)\n- **`tracing.py`** - Core tracing logic and processors\n- **`hooks/`** - Hook implementation handlers\n\n## Installation\n\n```bash\npip install mlflow\n```\n\n## Usage\n\nSet up Claude Code tracing in any project directory:\n\n```bash\n# Set up tracing in current directory\nmlflow autolog claude\n\n# Set up tracing in specific directory\nmlflow autolog claude ~/my-project\n\n# Set up with custom tracking URI\nmlflow autolog claude -u file://./custom-mlruns\nmlflow autolog claude -u sqlite:///mlflow.db\n\n# Set up with Databricks\nmlflow autolog claude -u databricks -e 123456789\n\n# Check status\nmlflow autolog claude --status\n\n# Disable tracing\nmlflow autolog claude --disable\n```\n\n## How it Works\n\n1. **Setup**: The `mlflow autolog claude` command configures Claude Code hooks in a `.claude/settings.json` file\n2. **Automatic Tracing**: When you use the `claude` command in the configured directory, your conversations are automatically traced to MLflow\n3. **View Traces**: Use `mlflow ui` to view your conversation traces\n\n## Configuration\n\nThe setup creates two types of configuration:\n\n### Claude Code Hooks\n\n- **PostToolUse**: Captures tool usage during conversations\n- **Stop**: Processes complete conversations into MLflow traces\n\n### Environment Variables\n\n- `MLFLOW_CLAUDE_TRACING_ENABLED=true`: Enables tracing\n- `MLFLOW_TRACKING_URI`: Where to store traces (defaults to local `.claude/mlflow/runs`)\n- `MLFLOW_EXPERIMENT_ID` or `MLFLOW_EXPERIMENT_NAME`: Which experiment to use\n\n## Examples\n\n### Basic Local Setup\n\n```bash\nmlflow autolog claude\ncd .\nclaude "help me write a function"\nmlflow ui --backend-store-uri sqlite:///mlflow.db\n```\n\n### Databricks Integration\n\n```bash\nmlflow autolog claude -u databricks -e 123456789\nclaude "analyze this data"\n# View traces in Databricks\n```\n\n### Custom Project Setup\n\n```bash\nmlflow autolog claude ~/my-ai-project -u sqlite:///mlflow.db -n "My AI Project"\ncd ~/my-ai-project\nclaude "refactor this code"\nmlflow ui --backend-store-uri sqlite:///mlflow.db\n```\n\n## Troubleshooting\n\n### Check Status\n\n```bash\nmlflow autolog claude --status\n```\n\n### Disable Tracing\n\n```bash\nmlflow autolog claude --disable\n```\n\n### View Raw Configuration\n\nThe configuration is stored in `.claude/settings.json`:\n\n```bash\ncat .claude/settings.json\n```\n\n## Requirements\n\n- Python 3.10+ (required by MLflow)\n- MLflow installed (`pip install mlflow`)\n- Claude Code CLI installed\n', '# MLflow Java Client\n\nJava client for [MLflow](https://mlflow.org) REST API.\nSee also the MLflow [Python API](https://mlflow.org/docs/latest/python_api/index.html)\nand [REST API](https://mlflow.org/docs/latest/rest-api.html).\n\n## Requirements\n\n- Java 1.8\n- Maven\n- Run the [MLflow Tracking Server 0.4.2](https://mlflow.org/docs/latest/tracking.html#running-a-tracking-server)\n\n## Build\n\n### Build with tests\n\nThe MLflow Java client tests require that MLflow is on the PATH (to start a local server),\nso it is recommended to run them from within a development conda environment.\n\nTo build a deployable JAR and run tests:\n\n```\nmvn package\n```\n\n## Run\n\nTo run a simple sample.\n\n```\njava -cp target/mlflow-java-client-0.4.2.jar \\\n  com.databricks.mlflow.client.samples.QuickStartDriver http://localhost:5001\n```\n\n## JSON Serialization\n\nMLflow Java client uses [Protobuf](https://developers.google.com/protocol-buffers/) 3.6.0 to serialize the JSON payload.\n\n- [service.proto](../mlflow/protos/service.proto) - Protobuf definition of data objects.\n- [com.databricks.api.proto.mlflow.Service.java](src/main/java/com/databricks/api/proto/mlflow/Service.java) - Generated Java classes of all data objects.\n- [generate_protos.py](generate_protos.py) - One time script to generate Service.java. If service.proto changes you will need to re-run this script.\n- Javadoc can be generated by running `mvn javadoc:javadoc`. The output will be in [target/site/apidocs/index.html](target/site/apidocs/index.html).\n  Here is the javadoc for [Service.java](target/site/apidocs/com/databricks/api/proto/mlflow/Service.html).\n\n## Java Client API\n\nSee [ApiClient.java](src/main/java/org/mlflow/client/ApiClient.java)\nand [Service.java domain objects](src/main/java/org/mlflow/api/proto/mlflow/Service.java).\n\n```\nRun getRun(String runId)\nRunInfo createRun()\nRunInfo createRun(String experimentId)\nRunInfo createRun(String experimentId, String appName)\nRunInfo createRun(CreateRun request)\nList<RunInfo> listRunInfos(String experimentId)\n\n\nList<Experiment> searchExperiments()\nGetExperiment.Response getExperiment(String experimentId)\nOptional<Experiment> getExperimentByName(String experimentName)\nlong createExperiment(String experimentName)\n\nvoid logParam(String runId, String key, String value)\nvoid logMetric(String runId, String key, float value)\nvoid setTerminated(String runId)\nvoid setTerminated(String runId, RunStatus status)\nvoid setTerminated(String runId, RunStatus status, long endTime)\nListArtifacts.Response listArtifacts(String runId, String path)\n```\n\n## Usage\n\n### Java Usage\n\nFor a simple example see [QuickStartDriver.java](src/main/java/org/mlflow/tracking/samples/QuickStartDriver.java).\nFor full examples of API coverage see the [tests](src/test/java/org/mlflow/tracking) such as [MlflowClientTest.java](src/test/java/org/mlflow/tracking/MlflowClientTest.java).\n\n```\npackage org.mlflow.tracking.samples;\n\nimport java.util.List;\nimport java.util.Optional;\n\nimport org.apache.log4j.Level;\nimport org.apache.log4j.LogManager;\n\nimport org.mlflow.api.proto.Service.*;\nimport org.mlflow.tracking.MlflowClient;\n\n/**\n * This is an example application which uses the MLflow Tracking API to create and manage\n * experiments and runs.\n */\npublic class QuickStartDriver {\n  public static void main(String[] args) throws Exception {\n    (new QuickStartDriver()).process(args);\n  }\n\n  void process(String[] args) throws Exception {\n    MlflowClient client;\n    if (args.length < 1) {\n      client = new MlflowClient();\n    } else {\n      client = new MlflowClient(args[0]);\n    }\n\n    boolean verbose = args.length >= 2 && "true".equals(args[1]);\n    if (verbose) {\n      LogManager.getLogger("org.mlflow.client").setLevel(Level.DEBUG);\n    }\n\n    System.out.println("====== createExperiment");\n    String expName ', 'public static void main(String[] args) throws Exception {\n    (new QuickStartDriver()).process(args);\n  }\n\n  void process(String[] args) throws Exception {\n    MlflowClient client;\n    if (args.length < 1) {\n      client = new MlflowClient();\n    } else {\n      client = new MlflowClient(args[0]);\n    }\n\n    boolean verbose = args.length >= 2 && "true".equals(args[1]);\n    if (verbose) {\n      LogManager.getLogger("org.mlflow.client").setLevel(Level.DEBUG);\n    }\n\n    System.out.println("====== createExperiment");\n    String expName = "Exp_" + System.currentTimeMillis();\n    String expId = client.createExperiment(expName);\n    System.out.println("createExperiment: expId=" + expId);\n\n    System.out.println("====== getExperiment");\n    GetExperiment.Response exp = client.getExperiment(expId);\n    System.out.println("getExperiment: " + exp);\n\n    System.out.println("====== searchExperiments");\n    List<Experiment> exps = client.searchExperiments();\n    System.out.println("#experiments: " + exps.size());\n    exps.forEach(e -> System.out.println("  Exp: " + e));\n\n    createRun(client, expId);\n\n    System.out.println("====== getExperiment again");\n    GetExperiment.Response exp2 = client.getExperiment(expId);\n    System.out.println("getExperiment: " + exp2);\n\n    System.out.println("====== getExperiment by name");\n    Optional<Experiment> exp3 = client.getExperimentByName(expName);\n    System.out.println("getExperimentByName: " + exp3);\n  }\n\n  void createRun(MlflowClient client, String expId) {\n    System.out.println("====== createRun");\n\n    // Create run\n    String sourceFile = "MyFile.java";\n    RunInfo runCreated = client.createRun(expId, sourceFile);\n    System.out.println("CreateRun: " + runCreated);\n    String runId = runCreated.getRunUuid();\n\n    // Log parameters\n    client.logParam(runId, "min_samples_leaf", "2");\n    client.logParam(runId, "max_depth", "3");\n\n    // Log metrics\n    client.logMetric(runId, "auc", 2.12F);\n    client.logMetric(runId, "accuracy_score", 3.12F);\n    client.logMetric(runId, "zero_one_loss", 4.12F);\n\n    // Update finished run\n    client.setTerminated(runId, RunStatus.FINISHED);\n\n    // Get run details\n    Run run = client.getRun(runId);\n    System.out.println("GetRun: " + run);\n    client.close();\n  }\n}\n```\n', '# mlflow: R interface for MLflow\n\n[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/mlflow)](https://cran.r-project.org/package=mlflow)\n\n- Install [MLflow](https://mlflow.org/) from R to track experiments\n  locally.\n- Connect to MLflow servers to share experiments with others.\n- Use MLflow to export models that can be served locally and remotely.\n\n## Prerequisites\n\nTo use the MLflow R API, you must install [the MLflow Python package](https://pypi.org/project/mlflow/).\n\n```bash\npip install mlflow\n```\n\nOptionally, you can set the `MLFLOW_PYTHON_BIN` and `MLFLOW_BIN` environment variables to specify\nthe Python and MLflow binaries to use. By default, the R client automatically finds them using\n`Sys.which("python")` and `Sys.which("mlflow")`.\n\n```bash\nexport MLFLOW_PYTHON_BIN=/path/to/bin/python\nexport MLFLOW_BIN=/path/to/bin/mlflow\n```\n\n## Installation\n\nInstall `mlflow` as follows:\n\n```r\ndevtools::install_github("mlflow/mlflow", subdir = "mlflow/R/mlflow")\n```\n\n## Development\n\nInstall the `mlflow` package as follows:\n\n```r\ndevtools::install_github("mlflow/mlflow", subdir = "mlflow/R/mlflow")\n```\n\nThen install the latest released `mlflow` runtime.\n\nHowever, currently, the development runtime of `mlflow` is also\nrequired; which means you also need to download or clone the `mlflow`\nGitHub repo:\n\n```bash\ngit clone https://github.com/mlflow/mlflow\n```\n\nAnd upgrade the runtime to the development version as follows:\n\n```bash\n# Upgrade to the latest development version\npip install -e <local github repo>\n```\n\n## Tracking\n\nMLflow Tracking allows you to logging parameters, code versions,\nmetrics, and output files when running R code and for later visualizing\nthe results.\n\nMLflow allows you to group runs under experiments, which can be useful\nfor comparing runs intended to tackle a particular task. You can create\nand activate a new experiment locally using `mlflow` as follows:\n\n```r\nlibrary(mlflow)\nmlflow_set_experiment("Test")\n```\n\nThen you can list view your experiments from MLflows user interface by\nrunning:\n\n```r\nmlflow_ui()\n```\n\n<img src="tools/readme/mlflow-user-interface.png" class="screenshot" width=520 />\n\nYou can also use a MLflow server to track and share experiments, see\n[running a tracking\nserver](https://www.mlflow.org/docs/latest/tracking.html#running-a-tracking-server),\nand then make use of this server by running:\n\n```r\nmlflow_set_tracking_uri("http://tracking-server:5000")\n```\n\nOnce the tracking url is defined, the experiments will be stored and\ntracked in the specified server which others will also be able to\naccess.\n\n## Projects\n\nAn MLflow Project is a format for packaging data science code in a\nreusable and reproducible way.\n\nMLflow projects can be [explicitly\ncreated](https://www.mlflow.org/docs/latest/projects.html#specifying-projects)\nor implicitly used by running `R` with `mlflow` from the terminal as\nfollows:\n\n```bash\nmlflow run examples/r_wine --entry-point train.R\n```\n\nNotice that is equivalent to running from `examples/r_wine`,\n\n```bash\nRscript -e "mlflow::mlflow_source(\'train.R\')"\n```\n\nand `train.R` performing training and logging as follows:\n\n```r\nlibrary(mlflow)\n\n# read parameters\ncolumn <- mlflow_log_param("column", 1)\n\n# log total rows\nmlflow_log_metric("rows", nrow(iris))\n\n# train model\nmodel <- lm(\n  Sepal.Width ~ x,\n  data.frame(Sepal.Width = iris$Sepal.Width, x = iris[,column])\n)\n\n# log models intercept\nmlflow_log_metric("intercept", model$coefficients[["(Intercept)"]])\n```\n\n### Parameters\n\nYou will often want to parameterize your scripts to support running and\ntracking multiple experiments. You ', 'others will also be able to\naccess.\n\n## Projects\n\nAn MLflow Project is a format for packaging data science code in a\nreusable and reproducible way.\n\nMLflow projects can be [explicitly\ncreated](https://www.mlflow.org/docs/latest/projects.html#specifying-projects)\nor implicitly used by running `R` with `mlflow` from the terminal as\nfollows:\n\n```bash\nmlflow run examples/r_wine --entry-point train.R\n```\n\nNotice that is equivalent to running from `examples/r_wine`,\n\n```bash\nRscript -e "mlflow::mlflow_source(\'train.R\')"\n```\n\nand `train.R` performing training and logging as follows:\n\n```r\nlibrary(mlflow)\n\n# read parameters\ncolumn <- mlflow_log_param("column", 1)\n\n# log total rows\nmlflow_log_metric("rows", nrow(iris))\n\n# train model\nmodel <- lm(\n  Sepal.Width ~ x,\n  data.frame(Sepal.Width = iris$Sepal.Width, x = iris[,column])\n)\n\n# log models intercept\nmlflow_log_metric("intercept", model$coefficients[["(Intercept)"]])\n```\n\n### Parameters\n\nYou will often want to parameterize your scripts to support running and\ntracking multiple experiments. You can define parameters with type under\na `params_example.R` example as follows:\n\n```r\nlibrary(mlflow)\n\n# define parameters\nmy_int <- mlflow_param("my_int", 1, "integer")\nmy_num <- mlflow_param("my_num", 1.0, "numeric")\n\n# log parameters\nmlflow_log_param("param_int", my_int)\nmlflow_log_param("param_num", my_num)\n```\n\nThen run `mlflow run` with custom parameters as\nfollows\n\n    mlflow run tests/testthat/examples/ --entry-point params_example.R -P my_int=10 -P my_num=20.0 -P my_str=XYZ\n\n    === Created directory /var/folders/ks/wm_bx4cn70s6h0r5vgqpsldm0000gn/T/tmpi6d2_wzf for downloading remote URIs passed to arguments of type \'path\' ===\n    === Running command \'source /miniconda2/bin/activate mlflow-da39a3ee5e6b4b0d3255bfef95601890afd80709 && Rscript -e "mlflow::mlflow_source(\'params_example.R\')" --args --my_int 10 --my_num 20.0 --my_str XYZ\' in run with ID \'191b489b2355450a8c3cc9bf96cb1aa3\' ===\n    === Run (ID \'191b489b2355450a8c3cc9bf96cb1aa3\') succeeded ===\n\nRun results that we can view with `mlflow_ui()`.\n\n## Models\n\nAn MLflow Model is a standard format for packaging machine learning\nmodels that can be used in a variety of downstream toolsâ€”for example,\nreal-time serving through a REST API or batch inference on Apache Spark.\nThey provide a convention to save a model in different "flavors" that\ncan be understood by different downstream tools.\n\nTo save a model use `mlflow_save_model()`. For instance, you can add the\nfollowing lines to the previous `train.R` script:\n\n```r\n# train model (...)\n\n# save model\nmlflow_save_model(\n  crate(~ stats::predict(model, .x), model)\n)\n```\n\nAnd trigger a run with that will also save your model as follows:\n\n```bash\nmlflow run train.R\n```\n\nEach MLflow Model is simply a directory containing arbitrary files,\ntogether with an MLmodel file in the root of the directory that can\ndefine multiple flavors that the model can be viewed in.\n\nThe directory containing the model looks as follows:\n\n```r\ndir("model")\n```\n\n    ## [1] "crate.bin" "MLmodel"\n\nand the model definition `model/MLmodel` like:\n\n```r\ncat(paste(readLines("model/MLmodel"), collapse = "\\n"))\n```\n\n   ', 'understood by different downstream tools.\n\nTo save a model use `mlflow_save_model()`. For instance, you can add the\nfollowing lines to the previous `train.R` script:\n\n```r\n# train model (...)\n\n# save model\nmlflow_save_model(\n  crate(~ stats::predict(model, .x), model)\n)\n```\n\nAnd trigger a run with that will also save your model as follows:\n\n```bash\nmlflow run train.R\n```\n\nEach MLflow Model is simply a directory containing arbitrary files,\ntogether with an MLmodel file in the root of the directory that can\ndefine multiple flavors that the model can be viewed in.\n\nThe directory containing the model looks as follows:\n\n```r\ndir("model")\n```\n\n    ## [1] "crate.bin" "MLmodel"\n\nand the model definition `model/MLmodel` like:\n\n```r\ncat(paste(readLines("model/MLmodel"), collapse = "\\n"))\n```\n\n    ## flavors:\n    ##   crate:\n    ##     version: 0.1.0\n    ##     model: crate.bin\n    ## time_created: 18-10-03T22:18:25.25.55\n    ## run_id: 4286a3d27974487b95b19e01b7b3caab\n\nLater on, the R model can be deployed which will perform predictions\nusing\n`mlflow_rfunc_predict()`:\n\n```r\nmlflow_rfunc_predict("model", data = data.frame(x = c(0.3, 0.2)))\n```\n\n    ## Warning in mlflow_snapshot_warning(): Running without restoring the\n    ## packages snapshot may not reload the model correctly. Consider running\n    ## \'mlflow_restore_snapshot()\' or setting the \'restore\' parameter to \'TRUE\'.\n\n    ## 3.400381396714573.40656987651099\n\n    ##        1        2\n    ## 3.400381 3.406570\n\n## Deployment\n\nMLflow provides tools for deployment on a local machine and several\nproduction environments. You can use these tools to easily apply your\nmodels in a production environment.\n\nYou can serve a model by running,\n\n```bash\nmlflow rfunc serve model\n```\n\nwhich is equivalent to\nrunning,\n\n```bash\nRscript -e "mlflow_rfunc_serve(\'model\')"\n```\n\n<img src="tools/readme/mlflow-serve-rfunc.png" class="screenshot" width=520 />\n\nYou can also run:\n\n```bash\nmlflow rfunc predict model data.json\n```\n\nwhich is equivalent to running,\n\n```bash\nRscript -e "mlflow_rfunc_predict(\'model\', \'data.json\')"\n```\n\n## Dependencies\n\nWhen running a project, `mlflow_snapshot()` is automatically called to\ngenerate a `r-dependencies.txt` file which contains a list of required\npackages and versions.\n\nHowever, restoring dependencies is not automatic since it\'s usually an\nexpensive operation. To restore dependencies run:\n\n```r\nmlflow_restore_snapshot()\n```\n\nNotice that the `MLFLOW_SNAPSHOT_CACHE` environment variable can be set\nto a cache directory to improve the time required to restore\ndependencies.\n\n## RStudio\n\nTo enable fast iteration while tracking with MLflow improvements over a\nmodel, [RStudio 1.2.897](https://dailies.rstudio.com/) an ', 'production environment.\n\nYou can serve a model by running,\n\n```bash\nmlflow rfunc serve model\n```\n\nwhich is equivalent to\nrunning,\n\n```bash\nRscript -e "mlflow_rfunc_serve(\'model\')"\n```\n\n<img src="tools/readme/mlflow-serve-rfunc.png" class="screenshot" width=520 />\n\nYou can also run:\n\n```bash\nmlflow rfunc predict model data.json\n```\n\nwhich is equivalent to running,\n\n```bash\nRscript -e "mlflow_rfunc_predict(\'model\', \'data.json\')"\n```\n\n## Dependencies\n\nWhen running a project, `mlflow_snapshot()` is automatically called to\ngenerate a `r-dependencies.txt` file which contains a list of required\npackages and versions.\n\nHowever, restoring dependencies is not automatic since it\'s usually an\nexpensive operation. To restore dependencies run:\n\n```r\nmlflow_restore_snapshot()\n```\n\nNotice that the `MLFLOW_SNAPSHOT_CACHE` environment variable can be set\nto a cache directory to improve the time required to restore\ndependencies.\n\n## RStudio\n\nTo enable fast iteration while tracking with MLflow improvements over a\nmodel, [RStudio 1.2.897](https://dailies.rstudio.com/) an be configured\nto automatically trigger `mlflow_run()` when sourced. This is enabled by\nincluding a `# !source mlflow::mlflow_run` comment at the top of the R\nscript as\nfollows:\n\n<img src="tools/readme/mlflow-source-rstudio.png" class="screenshot" width=520 />\n\n## Contributing\n\nSee the [MLflow contribution guidelines](https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md).\n', '# CLAUDE.md - MLflow Frontend Development\n\nThis file provides guidance to Claude Code when working with the MLflow frontend code in this directory.\n\n**For contribution guidelines, code standards, and additional development information not covered here, please refer to [CONTRIBUTING.md](../../../CONTRIBUTING.md).**\n\n## Consistency is Critical\n\n**IMPORTANT**: Always be consistent with the rest of the repository. This is extremely important!\n\nBefore implementing any feature:\n1. Read through similar files to understand their structure and patterns\n2. Do NOT invent new components if they already exist\n3. Use existing patterns and conventions found in the codebase\n4. Check for similar functionality that already exists\n\n## Development Server\n\n**IMPORTANT**: Always start the development server from the repository root for the best development experience with hot reload:\n\n```bash\n# MUST be run from the repository root\nnohup uv run bash dev/run-dev-server.sh > /tmp/mlflow-dev-server.log 2>&1 &\n\n# Monitor the logs\ntail -f /tmp/mlflow-dev-server.log\n\n# Servers will be available at:\n# - MLflow backend: http://localhost:5000\n# - React frontend: http://localhost:3000 (with hot reload)\n```\n\nThis provides fast edit-refresh for UI development - changes to React components will automatically reload in the browser.\n\n## Available Yarn Scripts\n\nWhen running from the repository root, use this pattern:\n\n```bash\n# Example: Run any yarn command from root\npushd mlflow/server/js && yarn <command>; popd\n```\n\nAvailable scripts:\n\n```bash\n# Development\nyarn start              # Start dev server (port 3000) with hot reload\nyarn build              # Build production bundle\n\n# Testing\nyarn test               # Run Jest tests\nyarn test:watch         # Run tests in watch mode\nyarn test:ci            # Run tests with coverage for CI\n\n# Code Quality\nyarn lint               # Run ESLint\nyarn lint:fix           # Run ESLint with auto-fix\nyarn prettier:check     # Check Prettier formatting\nyarn prettier:fix       # Fix Prettier formatting\nyarn type-check         # Run ', "        # Run tests in watch mode\nyarn test:ci            # Run tests with coverage for CI\n\n# Code Quality\nyarn lint               # Run ESLint\nyarn lint:fix           # Run ESLint with auto-fix\nyarn prettier:check     # Check Prettier formatting\nyarn prettier:fix       # Fix Prettier formatting\nyarn type-check         # Run TypeScript type checking\n\n# Combined Checks\nyarn check-all          # Run all checks (lint, prettier, i18n, type-check)\n\n# Other Commands\nyarn storybook          # Start Storybook for component development\nyarn build-storybook    # Build static Storybook\nyarn i18n:check         # Check i18n translations\n```\n\n### Before Committing\n\n**IMPORTANT**: Always run these checks and fix any remaining issues before committing:\n\n```bash\n# From repository root\npushd mlflow/server/js && yarn check-all; popd\n\n# Fix any issues that are reported\n```\n\n## UI Components and Design System\n\n### Use Databricks Design System Components\n\n**Always use components from `@databricks/design-system` when available.** Do not create custom components if they already exist in the design system.\n\nCommon components include:\n\n- `Button`, `IconButton` - for actions\n- `Input`, `Textarea`, `Select` - for form inputs  \n- `Modal`, `Drawer` - for overlays\n- `Table`, `TableRow`, `TableCell` - for data tables\n- `Tabs`, `TabPane` - for tabbed interfaces\n- `Alert`, `Notification` - for feedback\n- `Spinner`, `Skeleton` - for loading states\n- `Tooltip`, `Popover` - for additional information\n- `Card` - for content containers\n- `Typography` - for text styling\n\nExample import:\n\n```typescript\nimport { Button, Modal, Input } from '@databricks/design-system';\n```\n\n### Theme Usage\n\nUse the design system theme for consistent styling:\n\n```typescript\nimport { useDesignSystemTheme } from '@databricks/design-system';\n\nconst Component = () => {\n  const { theme } = useDesignSystemTheme();\n  \n  return (\n    <div style={{ \n      color: theme.colors.textPrimary,\n      padding: theme.spacing.md,\n      fontSize: theme.typography.fontSizeBase\n    }}>\n  ", 'tabbed interfaces\n- `Alert`, `Notification` - for feedback\n- `Spinner`, `Skeleton` - for loading states\n- `Tooltip`, `Popover` - for additional information\n- `Card` - for content containers\n- `Typography` - for text styling\n\nExample import:\n\n```typescript\nimport { Button, Modal, Input } from \'@databricks/design-system\';\n```\n\n### Theme Usage\n\nUse the design system theme for consistent styling:\n\n```typescript\nimport { useDesignSystemTheme } from \'@databricks/design-system\';\n\nconst Component = () => {\n  const { theme } = useDesignSystemTheme();\n  \n  return (\n    <div style={{ \n      color: theme.colors.textPrimary,\n      padding: theme.spacing.md,\n      fontSize: theme.typography.fontSizeBase\n    }}>\n      Content\n    </div>\n  );\n};\n```\n\n### Spacing Guidelines\n\n**ALWAYS use `theme.spacing` values instead of hard-coded pixel widths.** This ensures consistency and maintainability across the application.\n\n```typescript\n// âœ… GOOD - Use theme spacing\n<div style={{ \n  padding: theme.spacing.md,\n  marginBottom: theme.spacing.lg,\n  gap: theme.spacing.sm \n}} />\n\n// âŒ BAD - Avoid hard-coded pixels\n<div style={{ \n  padding: \'16px\',\n  marginBottom: \'24px\',\n  gap: \'8px\'\n}} />\n```\n\nCommon spacing values:\n- `theme.spacing.xs` - Extra small spacing (4px)\n- `theme.spacing.sm` - Small spacing (8px)\n- `theme.spacing.md` - Medium spacing (16px)\n- `theme.spacing.lg` - Large spacing (24px)\n- `theme.spacing.xl` - Extra large spacing (32px)\n\nFor custom spacing needs, use the spacing function:\n```typescript\n// When you need a specific multiple of the base unit\npadding: theme.spacing(2.5) // 20px (2.5 * 8px base unit)\n```\n\n### Finding the Right Component\n\nWhen looking for a component:\n\n1. First check `@databricks/design-system` imports in existing code\n2. Component names may not be exact (e.g., "dropdown" could be `Select`, `DialogCombobox`, or `DropdownMenu`)\n3. Look at similar UI patterns in the codebase for examples\n4. If multiple matches exist, choose based on the use case\n\n### Discovering Available Components Dynamically\n\nTo see ALL components available in the design system:\n\n```bash\n# From mlflow/server/js directory, check what\'s exported\ncat node_modules/@databricks/design-system/dist/design-system/index.d.ts\n\n# This file lists every component as: export * from \'./ComponentName\';\n# Each line represents a component you can import\n```\n\nThis is the definitive source for available components - more reliable than checking folders since it shows only what\'s publicly exported.\n\n### Viewing Component Documentation in Storybook\n\nYou can use Playwright to view the component documentation and examples in Storybook:\n\n```\nhttps://ui-infra.dev.databricks.com/storybook/js/packages/du-bois/index.html?path=/docs/primitives-<component-name>--docs\n```\n\nFor example:\n- Alert: `https://ui-infra.dev.databricks.com/storybook/js/packages/du-bois/index.html?path=/docs/primitives-alert--docs`\n- Button: `https://ui-infra.dev.databricks.com/storybook/js/packages/du-bois/index.html?path=/docs/primitives-button--docs`\n- Modal: `https://ui-infra.dev.databricks.com/storybook/js/packages/du-bois/index.html?path=/docs/primitives-modal--docs`\n\nUse ', "at similar UI patterns in the codebase for examples\n4. If multiple matches exist, choose based on the use case\n\n### Discovering Available Components Dynamically\n\nTo see ALL components available in the design system:\n\n```bash\n# From mlflow/server/js directory, check what's exported\ncat node_modules/@databricks/design-system/dist/design-system/index.d.ts\n\n# This file lists every component as: export * from './ComponentName';\n# Each line represents a component you can import\n```\n\nThis is the definitive source for available components - more reliable than checking folders since it shows only what's publicly exported.\n\n### Viewing Component Documentation in Storybook\n\nYou can use Playwright to view the component documentation and examples in Storybook:\n\n```\nhttps://ui-infra.dev.databricks.com/storybook/js/packages/du-bois/index.html?path=/docs/primitives-<component-name>--docs\n```\n\nFor example:\n- Alert: `https://ui-infra.dev.databricks.com/storybook/js/packages/du-bois/index.html?path=/docs/primitives-alert--docs`\n- Button: `https://ui-infra.dev.databricks.com/storybook/js/packages/du-bois/index.html?path=/docs/primitives-button--docs`\n- Modal: `https://ui-infra.dev.databricks.com/storybook/js/packages/du-bois/index.html?path=/docs/primitives-modal--docs`\n\nUse Playwright MCP to navigate to these URLs and see live examples, props documentation, and usage patterns.\n\n## Browser Testing with Playwright\n\nFor testing UI changes in a real browser, Claude Code can use the Playwright MCP (Model Context Protocol) integration.\n\n### Checking Playwright MCP Status\n\nTo check if Playwright MCP is available:\n\n- Look for browser testing tools in available MCP functions\n- Try using browser navigation or screenshot capabilities\n\n### Installing Playwright MCP\n\nIf Playwright MCP is not available and you need to test UI changes, you can install it:\n\n```bash\nclaude mcp add playwright npx '@playwright/mcp@latest'\n```\n\n**Note**: After installation, you must restart Claude Code for the integration to be available.\n\n### Using Playwright MCP\n\nOnce installed, you can:\n\n- Navigate to the development server\n- Take screenshots of UI components\n- Interact with forms and buttons\n- Verify UI changes are working correctly\n\nExample workflow:\n\n1. Make changes to React components\n2. Wait for hot reload (automatic)\n3. Use Playwright to navigate to `http://localhost:3000`\n4. Take screenshots or interact with the updated UI\n5. Verify the changes work as expected\n\n## Project Structure\n\n```text\nmlflow/server/js/\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ experiment-tracking/    # Experiment tracking UI\nâ”‚   â”œâ”€â”€ model-registry/         # Model registry UI  \nâ”‚   â”œâ”€â”€ common/                 # Shared components\nâ”‚   â”œâ”€â”€ shared/                 # Shared utilities\nâ”‚   â””â”€â”€ app.tsx          ", "the updated UI\n5. Verify the changes work as expected\n\n## Project Structure\n\n```text\nmlflow/server/js/\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ experiment-tracking/    # Experiment tracking UI\nâ”‚   â”œâ”€â”€ model-registry/         # Model registry UI  \nâ”‚   â”œâ”€â”€ common/                 # Shared components\nâ”‚   â”œâ”€â”€ shared/                 # Shared utilities\nâ”‚   â””â”€â”€ app.tsx                # Main app entry point\nâ”œâ”€â”€ vendor/                     # Third-party dependencies\nâ”œâ”€â”€ package.json               # Dependencies and scripts\nâ”œâ”€â”€ tsconfig.json              # TypeScript configuration\nâ”œâ”€â”€ webpack.config.js          # Webpack bundler config\nâ””â”€â”€ jest.config.js             # Jest test configuration\n```\n\n## Key Technologies\n\n- **React 18**: UI framework\n- **TypeScript**: Type safety\n- **Redux**: State management\n- **Apollo Client**: GraphQL client\n- **Ant Design (antd)**: UI component library\n- **AG-Grid**: Data table component\n- **Jest**: Testing framework\n- **React Testing Library**: Component testing\n- **Webpack**: Module bundler\n\n## Common Tasks\n\n### Adding a New Component\n\n1. Create component file in appropriate directory\n2. Add TypeScript types/interfaces\n3. Write component with hooks (functional components preferred)\n4. Add unit tests in same directory with `.test.tsx` extension\n5. Add to Storybook if it's a reusable component\n\n### Updating GraphQL Queries\n\n1. Modify query in relevant `.graphql` file\n2. Run codegen to update TypeScript types (if configured)\n3. Update components using the query\n\n### Testing Components\n\n```bash\n# Run tests for a specific component\nyarn test ComponentName\n\n# Run tests in watch mode for development\nyarn test --watch\n\n# Update snapshots if needed\nyarn test -u\n```\n\n### Debugging\n\n1. Use React Developer Tools browser extension\n2. Redux DevTools for state debugging\n3. Browser console for network requests\n4. Source maps are enabled in development mode\n\n## Code ", "directory\n2. Add TypeScript types/interfaces\n3. Write component with hooks (functional components preferred)\n4. Add unit tests in same directory with `.test.tsx` extension\n5. Add to Storybook if it's a reusable component\n\n### Updating GraphQL Queries\n\n1. Modify query in relevant `.graphql` file\n2. Run codegen to update TypeScript types (if configured)\n3. Update components using the query\n\n### Testing Components\n\n```bash\n# Run tests for a specific component\nyarn test ComponentName\n\n# Run tests in watch mode for development\nyarn test --watch\n\n# Update snapshots if needed\nyarn test -u\n```\n\n### Debugging\n\n1. Use React Developer Tools browser extension\n2. Redux DevTools for state debugging\n3. Browser console for network requests\n4. Source maps are enabled in development mode\n\n## Code Style\n\n- Use functional components with hooks\n- Prefer TypeScript strict mode\n- Follow existing patterns in the codebase\n- Use meaningful component and variable names\n- Add JSDoc comments for complex logic\n- Keep components small and focused\n\n## Best Practices\n\n### Data Fetching\n\n**Use React Query** for all API calls and data fetching:\n\n```typescript\n// Good: Using React Query\nconst { data, isLoading, error } = useQuery({\n  queryKey: ['experiments', experimentId],\n  queryFn: () => fetchExperiment(experimentId),\n});\n\n// Avoid: Manual fetch in useEffect\n// useEffect(() => { fetch(...) }, [])\n```\n\n### State Management\n\n**Avoid useEffect** when possible. Prefer deriving state with `useMemo`:\n\n```typescript\n// Good: Derive state with useMemo\nconst filteredRuns = useMemo(() => {\n  return runs.filter(run => run.status === 'active');\n}, [runs]);\n\n// Avoid: useEffect to update state\n// useEffect(() => {\n//   setFilteredRuns(runs.filter(run => run.status === 'active'));\n// }, [runs]);\n```\n\nUse `useEffect` only for:\n\n- Side effects (DOM manipulation, subscriptions)\n- Synchronizing with external systems\n- Cleanup operations\n\n## Performance Considerations\n\n- Use React.memo for expensive components\n- Implement virtualization for large lists (AG-Grid handles this)\n- Lazy load routes and heavy components\n", "# Jupter Notebook Trace UI Renderer\n\nThis directory contains a standalone notebook renderer that is built as a separate entry point from the main MLflow application.\n\n## Architecture\n\nThe notebook renderer is configured as a separate webpack entry point that generates its own HTML file and JavaScript bundle, completely independent of the main MLflow application.\n\n### Build Configuration\n\nThe webpack configuration in `craco.config.js` handles the dual-entry setup:\n\n1. **Entry Points**:\n\n   - `main`: The main MLflow application (`src/index.tsx`)\n   - `ml-model-trace-renderer`: The notebook renderer (`src/shared/web-shared/model-trace-explorer/oss-notebook-renderer/index.ts`)\n\n2. **Output Structure**:\n\n   ```\n   build/\n   â”œâ”€â”€ index.html                           # Main app HTML (excludes notebook renderer)\n   â”œâ”€â”€ static/js/main.[hash].js             # Main app bundle\n   â”œâ”€â”€ static/css/main.[hash].css           # Main app styles\n   â””â”€â”€ lib/notebook-trace-renderer/\n       â”œâ”€â”€ index.html                       # Notebook renderer HTML\n       â””â”€â”€ js/ml-model-trace-renderer.[hash].js  # Notebook renderer bundle\n   ```\n\n3. **Path Resolution**:\n   - Main app uses relative paths: `static-files/static/js/...`\n   - Notebook renderer uses absolute paths: `/static-files/lib/notebook-trace-renderer/js/...`\n   - Dynamic chunks use absolute paths: `/static-files/static/...` (via `__webpack_public_path__`)\n\n### Key Configuration Details\n\n#### Separate Entry Configuration\n\n```javascript\nwebpackConfig.entry = {\n  main: webpackConfig.entry, // Preserve original entry as 'main'\n  'ml-model-trace-renderer': path.resolve(\n    __dirname,\n    'src/shared/web-shared/model-trace-explorer/oss-notebook-renderer/index.ts',\n  ),\n};\n```\n\n#### Output Path Functions\n\n```javascript\nwebpackConfig.output = {\n  filename: (pathData) => {\n    return pathData.chunk.name === 'ml-model-trace-renderer'\n      ? 'lib/notebook-trace-renderer/js/[name].[contenthash].js'\n      : 'static/js/[name].[contenthash:8].js';\n  },\n  // ... similar for chunkFilename\n};\n```\n\n#### HTML Plugin Configuration\n\n- **Main app**: Excludes notebook renderer chunks via `excludeChunks: ['ml-model-trace-renderer']`\n- **Notebook renderer**: Includes only its own chunks via `chunks: ['ml-model-trace-renderer']`\n\n#### Runtime Path Override\n\nThe notebook renderer sets `__webpack_public_path__ = '/static-files/'` at ", "Configuration Details\n\n#### Separate Entry Configuration\n\n```javascript\nwebpackConfig.entry = {\n  main: webpackConfig.entry, // Preserve original entry as 'main'\n  'ml-model-trace-renderer': path.resolve(\n    __dirname,\n    'src/shared/web-shared/model-trace-explorer/oss-notebook-renderer/index.ts',\n  ),\n};\n```\n\n#### Output Path Functions\n\n```javascript\nwebpackConfig.output = {\n  filename: (pathData) => {\n    return pathData.chunk.name === 'ml-model-trace-renderer'\n      ? 'lib/notebook-trace-renderer/js/[name].[contenthash].js'\n      : 'static/js/[name].[contenthash:8].js';\n  },\n  // ... similar for chunkFilename\n};\n```\n\n#### HTML Plugin Configuration\n\n- **Main app**: Excludes notebook renderer chunks via `excludeChunks: ['ml-model-trace-renderer']`\n- **Notebook renderer**: Includes only its own chunks via `chunks: ['ml-model-trace-renderer']`\n\n#### Runtime Path Override\n\nThe notebook renderer sets `__webpack_public_path__ = '/static-files/'` at runtime to ensure dynamically loaded chunks use the correct absolute paths.\n\n## Files\n\n- `index.ts`: Entry point that sets webpack public path and bootstraps the renderer\n- `bootstrap.tsx`: Main renderer component\n- `index.html`: HTML template for the standalone renderer\n- `index.css`: Styles for the renderer\n\n## Usage\n\nThe notebook renderer is built automatically as part of the main build process:\n\n```bash\nyarn build\n```\n\nThis generates both the main application and the standalone notebook renderer, accessible at:\n\n- Main app: `/static-files/index.html`\n- Notebook renderer: `/static-files/lib/notebook-trace-renderer/index.html`\n\n## Development Notes\n\n- The renderer is completely independent of the main app - no shared runtime dependencies\n- Uses absolute paths to avoid complex relative path calculations\n- Webpack code splitting works correctly for both entry points\n- CSS extraction is configured separately for each entry point\n", "# MLflow Tracking database migrations\n\nThis directory contains configuration scripts and database migration logic for MLflow tracking\ndatabases, using the Alembic migration library (https://alembic.sqlalchemy.org). To run database\nmigrations, use the `mlflow db upgrade` CLI command. To add and modify database migration logic,\nsee the contributor guide at https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md.\n\nIf you encounter failures while executing migrations, please file a GitHub issue at\nhttps://github.com/mlflow/mlflow/issues.\n\n## Migration descriptions\n\n### 89d4b8295536_create_latest_metrics_table\n\nThis migration creates a `latest_metrics` table and populates it with the latest metric entry for\neach unique `(run_id, metric_key)` tuple. Latest metric entries are computed based on `step`,\n`timestamp`, and `value`.\n\nThis migration may take a long time for databases containing a large number of metric entries. You\ncan determine the total number of metric entries using the following query:\n\n```sql\nSELECT count(*) FROM metrics GROUP BY metrics.key, run_uuid;\n```\n\nAdditionally, query join latency during the migration increases with the number of unique\n`(run_id, metric_key)` tuples. You can determine the total number of unique tuples using\nthe following query:\n\n```sql\nSELECT count(*) FROM (\n   SELECT metrics.key, run_uuid FROM metrics GROUP BY run_uuid, metrics.key\n) unique_metrics;\n```\n\nFor reference, migrating a Tracking database with the following attributes takes roughly\n**three seconds** on MySQL 5.7:\n\n- `3702` unique metrics\n- `466860` total metric entries\n- `186` runs\n- An average of `125` entries per unique metric\n\n#### Recovering from a failed migration\n\nIf the **create_latest_metrics_table** migration fails, simply delete the `latest_metrics`\ntable from your Tracking database as follows:\n\n```sql\nDROP TABLE latest_metrics;\n```\n\nAlembic does not stamp the database with an updated version unless the corresponding migration\ncompletes successfully. Therefore, when this migration fails, the database remains on the\nprevious version, and deleting the `latest_metrics` table is sufficient to restore the database\nto its prior state.\n\nIf the migration fails to complete due to excessive latency, please try executing the\n`mlflow db upgrade` command on the same host machine where the database is running. This will\nreduce the overhead of the migration's queries and batch insert operation.\n", '# Instructions\n\nThis directory contains files to test MLflow tracking operations using the following databases:\n\n- PostgreSQL\n- MySQL\n- Microsoft SQL Server\n- SQLite\n\n## Prerequisites\n\n- Docker\n- Docker Compose V2\n\n## Build Services\n\n```bash\n# Build a service\nservice=mlflow-sqlite\n./tests/db/compose.sh build --build-arg DEPENDENCIES="$(python dev/extract_deps.py)" $service\n\n# Build all services\n./tests/db/compose.sh build --build-arg DEPENDENCIES="$(python dev/extract_deps.py)"\n```\n\n## Run Services\n\n```bash\n# Run a service (`pytest tests/db` is executed by default)\n./tests/db/compose.sh run --rm $service\n\n# Run all services\nfor service in $(./tests/db/compose.sh config --services | grep \'^mlflow-\')\ndo\n  ./tests/db/compose.sh run --rm "$service"\ndone\n\n# Run tests\n./tests/db/compose.sh run --rm $service pytest /path/to/directory/or/script\n\n# Run a python script\n./tests/db/compose.sh run --rm $service python /path/to/script\n```\n\n## Clean Up Services\n\n```bash\n# Clean up containers, networks, and volumes\n./tests/db/compose.sh down --volumes --remove-orphans\n\n# Clean up containers, networks, volumes, and images\n./tests/db/compose.sh down --volumes --remove-orphans --rmi all\n```\n\n## Other Useful Commands\n\n```bash\n# View database logs\n./tests/db/compose.sh logs --follow <database service>\n```\n', '# Adding `examples` unit tests to `pytest` test suite\n\nTwo types of test runs for code in `examples` directory are supported:\n\n- Examples run by `mlflow run`\n- Examples run by another command, such as the `python` interpreter\n\nEach of these types of runs are implemented using `@pytest.mark.parametrize` decorator. Adding a new\nexample to test involves updating the decorator list as described below.\n\nFor purpose of discussion, `new_example_dir` designates the\ndirectory the example code is found, i.e., it is located in `examples/new_example_dir`.\n\n## Examples that utilize `mlflow run` construct\n\nThe `@pytest.mark.mark.parametrize` decorator for `def test_mlflow_run_example(directory, params):`\nis updated.\n\nIf the example is executed by `cd examples/new_example_dir && mlflow run . -P param1=99 -P param2=3`, then\nthis `tuple` is added to the decorator list\n\n```\n("new_example_dir", ["-P", "param1=123", "-P", "param2=99"])\n```\n\nas shown below\n\n```\n@pytest.mark.parametrize(("directory", "params"), [\n    ("sklearn_elasticnet_wine", ["-P", "alpha=0.5"]),\n    (os.path.join("sklearn_elasticnet_diabetes", "linux"), []),\n    ("new_example_dir", ["-P", "param1=123", "-P", "param2=99"]),\n])\ndef test_mlflow_run_example(directory, params):\n```\n\nThe `tuple` for an example requiring no parameters is simply:\n\n```\n("new_example_dir", []),\n```\n\n## Examples that are executed with another command\n\nFor an example that is not run by `mlflow run`, the list in\n`@pytest.mark.parametrize` decorator for `test_command_example(tmpdir, directory, command):` is updated.\n\nExamples invoked by `cd examples/new_example_dir && python train.py` require this tuple added\nto the decorator\'s list\n\n```\n("new_example_dir", ["python", "train.py"]),\n```\n\nas shown below\n\n```\n@pytest.mark.parametrize(("directory", "command"), [\n    (\'sklearn_logistic_regression\', [\'python\', \'train.py\']),\n    (\'h2o\', [\'python\', \'random_forest.py\']),\n    (\'quickstart\', [\'python\', \'mlflow_tracking.py\']),\n    ("new_example_dir", ["python", "train.py"]),\n])\ndef test_command_example(tmpdir, directory, command):\n```\n\nIf the example requires arguments to run, i.e., `python train.py arg1 arg2`, then the\ntuple would look like this\n\n```\n(\'new_example_dir\', [\'python\', \'train.py\', \'arg1\', \'arg2\'])\n```\n', '# Historical Pyfunc Models\n\nThese serialized model files are used in backwards compatibility tests, so we can ensure that models logged with old versions of MLflow are still able to be loaded in newer versions.\n\nThese files were created by running the following:\n\n1. First, install the desired MLflow version with `$ pip install mlflow=={version_number}`\n2. Next, run the following script from MLflow root:\n\n```python\nimport mlflow\n\n\nclass MyModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input):\n        return model_input\n\n\nmodel = MyModel()\n\nmlflow.pyfunc.save_model(\n    python_model=model,\n    path=f"tests/resources/pyfunc_models/{mlflow.__version__}",\n)\n```\n', 'Copyright 2018 Databricks, Inc.  All rights reserved.\n\n\t\t\t\tApache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      "License" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      "Licensor" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      "Legal Entity" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      "control" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      "You" (or "Your") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      "Source" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      "Object" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, ', '  "You" (or "Your") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      "Source" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      "Object" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      "Work" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      "Derivative Works" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      "Contribution" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or ', 'include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      "Contribution" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, "submitted"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as "Not a Contribution."\n\n      "Contributor" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such ', '     on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n  ', '     cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a "NOTICE" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n     ', 'those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a "NOTICE" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n    ', '        notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an "AS IS" ', '6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an "AS IS" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of ', 'to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets "[]"\n      replaced with your own identifying information. (Don\'t include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class ', 'END OF TERMS AND CONDITIONS\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets "[]"\n      replaced with your own identifying information. (Don\'t include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same "printed page" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the "License");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an "AS IS" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n', '# Dev script dependencies\nclick\nruamel.yaml.clib!=0.2.7\nruamel.yaml\nrequests\npackaging\npydantic\npyyaml\ntoml\n', 'https://mlflow.org/docs/latest/auth/index.html\nhttps://mlflow.org/docs/latest/auth/python-api.html\nhttps://mlflow.org/docs/latest/cli.html\nhttps://mlflow.org/docs/latest/deep-learning/keras/quickstart/quickstart_keras.html\nhttps://mlflow.org/docs/latest/deep-learning/pytorch/guide/index.html\nhttps://mlflow.org/docs/latest/deep-learning/tensorflow/guide/index.html\nhttps://mlflow.org/docs/latest/deployment/index.html\nhttps://mlflow.org/docs/latest/getting-started/intro-quickstart/index.html\nhttps://mlflow.org/docs/latest/index.html\nhttps://mlflow.org/docs/latest/introduction/index.html\nhttps://mlflow.org/docs/latest/llms/custom-pyfunc-for-llms/index.html\nhttps://mlflow.org/docs/latest/llms/custom-pyfunc-for-llms/notebooks/custom-pyfunc-advanced-llm.html\nhttps://mlflow.org/docs/latest/llms/custom-pyfunc-for-llms/notebooks/index.html\nhttps://mlflow.org/docs/latest/llms/deployments/guides/index.html\nhttps://mlflow.org/docs/latest/llms/deployments/guides/step1-create-deployments.html\nhttps://mlflow.org/docs/latest/llms/deployments/guides/step2-query-deployments.html\nhttps://mlflow.org/docs/latest/llms/deployments/index.html\nhttps://mlflow.org/docs/latest/llms/deployments/uc_integration.html\nhttps://mlflow.org/docs/latest/llms/index.html\nhttps://mlflow.org/docs/latest/llms/langchain/autologging.html\nhttps://mlflow.org/docs/latest/llms/langchain/guide/index.html\nhttps://mlflow.org/docs/latest/llms/langchain/index.html\nhttps://mlflow.org/docs/latest/llms/langchain/notebooks/langchain-quickstart.html\nhttps://mlflow.org/docs/latest/llms/llama-index/index.html\nhttps://mlflow.org/docs/latest/llms/llm-evaluate/index.html\nhttps://mlflow.org/docs/latest/llms/openai/guide/index.html\nhttps://mlflow.org/docs/latest/llms/openai/index.html\nhttps://mlflow.org/docs/latest/llms/sentence-transformers/guide/index.html\nhttps://mlflow.org/docs/latest/llms/sentence-transformers/index.html\nhttps://mlflow.org/docs/latest/llms/tracing/index.html\nhttps://mlflow.org/docs/latest/llms/tracing/overview.html\nhttps://mlflow.org/docs/latest/llms/transformers/index.html\nhttps://mlflow.org/docs/latest/model-evaluation/index.html\nhttps://mlflow.org/docs/latest/model-registry.html\nhttps://mlflow.org/docs/latest/model/dependencies.html\nhttps://mlflow.org/docs/latest/model/notebooks/signature_examples.html\nhttps://mlflow.org/docs/latest/model/signatures.html\nhttps://mlflow.org/docs/latest/models.html\nhttps://mlflow.org/docs/latest/python_api/index.html\nhttps://mlflow.org/docs/latest/rest-api.html\nhttps://mlflow.org/docs/latest/system-metrics/index.html\nhttps://mlflow.org/docs/latest/tracking.html\nhttps://mlflow.org/docs/latest/tracking/artifacts-stores.html\nhttps://mlflow.org/docs/latest/tracking/autolog.html\nhttps://mlflow.org/docs/latest/tracking/backend-stores.html\nhttps://mlflow.org/docs/latest/tracking/data-api.html\nhttps://mlflow.org/docs/latest/tracking/server.html\nhttps://mlflow.org/docs/latest/tracking/tracking-api.html\nhttps://mlflow.org/docs/latest/tracking/tutorials/local-database.html\nhttps://mlflow.org/docs/latest/tracking/tutorials/remote-server.html', 'promptflow\njinja2\n', '../../LICENSE.txt', '../../LICENSE.txt', '# classification\npyspark.ml.classification.LinearSVCModel\npyspark.ml.classification.DecisionTreeClassificationModel\npyspark.ml.classification.GBTClassificationModel\npyspark.ml.classification.LogisticRegressionModel\npyspark.ml.classification.RandomForestClassificationModel\npyspark.ml.classification.NaiveBayesModel\n\n# clustering\npyspark.ml.clustering.BisectingKMeansModel\npyspark.ml.clustering.KMeansModel\npyspark.ml.clustering.GaussianMixtureModel\n\n# Regression\npyspark.ml.regression.AFTSurvivalRegressionModel\npyspark.ml.regression.DecisionTreeRegressionModel\npyspark.ml.regression.GBTRegressionModel\npyspark.ml.regression.GeneralizedLinearRegressionModel\npyspark.ml.regression.LinearRegressionModel\npyspark.ml.regression.RandomForestRegressionModel\n\n# Featurizer model\npyspark.ml.feature.BucketedRandomProjectionLSHModel\npyspark.ml.feature.ChiSqSelectorModel\npyspark.ml.feature.CountVectorizerModel\npyspark.ml.feature.IDFModel\npyspark.ml.feature.ImputerModel\npyspark.ml.feature.MaxAbsScalerModel\npyspark.ml.feature.MinHashLSHModel\npyspark.ml.feature.MinMaxScalerModel\npyspark.ml.feature.OneHotEncoderModel\npyspark.ml.feature.RobustScalerModel\npyspark.ml.feature.RFormulaModel\npyspark.ml.feature.StandardScalerModel\npyspark.ml.feature.StringIndexerModel\npyspark.ml.feature.VarianceThresholdSelectorModel\npyspark.ml.feature.VectorIndexerModel\npyspark.ml.feature.UnivariateFeatureSelectorModel\n\n# composite model\npyspark.ml.classification.OneVsRestModel\n\n# pipeline model\npyspark.ml.pipeline.PipelineModel\n\n# Hyper-parameter tuning\npyspark.ml.tuning.CrossValidatorModel\npyspark.ml.tuning.TrainValidationSplitModel\n\n# SynapeML models\nsynapse.ml.cognitive.*\nsynapse.ml.exploratory.*\nsynapse.ml.featurize.*\nsynapse.ml.geospatial.*\nsynapse.ml.image.*\nsynapse.ml.io.*\nsynapse.ml.isolationforest.*\nsynapse.ml.lightgbm.*\nsynapse.ml.nn.*\nsynapse.ml.opencv.*\nsynapse.ml.stages.*\nsynapse.ml.vw.*\n', 'pytest==8.4.0\n# transformers 4.51.0 has this issue:\n# https://github.com/huggingface/transformers/issues/37326\ntransformers!=4.51.0\n# https://github.com/BerriAI/litellm/issues/10373\nlitellm!=1.67.4\n# https://github.com/run-llama/llama_index/issues/18587\nllama-index-core!=0.12.34\n# https://github.com/mangiucugna/json_repair/issues/124\njson_repair!=0.45.0\n# https://github.com/huggingface/transformers/issues/38269\ntransformers!=4.52.2\ntransformers!=4.52.1\n# TODO(https://github.com/mlflow/mlflow/issues/15847): Remove this constraint when MLflow is ready for pyspark 4.0.0. Pyspark 3.5.6 has the same issue.\npyspark<3.5.6\n', '-r extra-ml-requirements.txt\n-r test-requirements.txt\n-r lint-requirements.txt\n-r doc-requirements.txt\n', '# Minimum version that works with Python 3.10\nsphinx==4.2.0\njinja2==3.0.3\n# to be compatible with jinja2==3.0.3\nflask<=2.2.5\nsphinx-autobuild\nsphinx-click\n# to be compatible with docutils==0.16\nsphinx-tabs==3.2.0\n# redirect handling\nsphinx-reredirects==0.1.3\n# Pin sphinxcontrib packages. Their newer versions are incompatible with sphinx==4.2.0.\nsphinxcontrib-applehelp<1.0.8\nsphinxcontrib-devhelp<1.0.6\nsphinxcontrib-htmlhelp<2.0.4\nsphinxcontrib-serializinghtml<1.1.10\nsphinxcontrib-qthelp<1.0.7\n', '-r doc-min-requirements.txt\ntensorflow-cpu<=2.12.0; platform_system!="Darwin" or platform_machine!="arm64"\ntensorflow-macos<=2.12.0; platform_system=="Darwin" and platform_machine=="arm64"\npyspark\ndatasets\n# nbsphinx and ipython are required for jupyter notebook rendering\nnbsphinx==0.8.8\n# ipython 8.7.0 is an incompatible release\nipython!=8.7.0\nkeras\ntorch>=1.11.0\ntorchvision>=0.12.0\nlightning>=1.8.1\nscrapy\nipywidgets>=8.1.1\n# incremental==24.7.0 requires setuptools>=61.0, which causes https://github.com/mlflow/mlflow/issues/8635\nincremental<24.7.0\n# this is an extra dependency for the auth app which\n# is not included in the core mlflow requirements\nFlask-WTF<2\n# required for testing polars dataset integration\npolars>=1\n# required for the genai evaluation example\nopenai\n', '## This file describes extra ML library dependencies that you, as an end user,\n## must install in order to use various MLflow Python modules.\n# Required by mlflow.spacy\n# TODO: Remove `<3.8` once we bump the minimim supported python version of MLflow to 3.9.\nspacy>=3.3.0,<3.8\n# Required by mlflow.tensorflow\ntensorflow>=2.10.0; platform_system!="Darwin" or platform_machine!="arm64"\ntensorflow-macos>=2.10.0; platform_system=="Darwin" and platform_machine=="arm64"\n# Required by mlflow.pytorch\ntorch>=1.11.0\ntorchvision>=0.12.0\nlightning>=1.8.1\n# Required by mlflow.xgboost\nxgboost>=0.82\n# Required by mlflow.lightgbm\nlightgbm\n# Required by mlflow.catboost\ncatboost\n# Required by mlflow.statsmodels\nstatsmodels\n# Required by mlflow.h2o\nh2o\n# Required by mlflow.onnx\nonnx>=1.11.0\nonnxruntime\ntf2onnx\n# Required by mlflow.spark and using Delta with MLflow Tracking datasets\npyspark\n# Required by mlflow.paddle\npaddlepaddle\n# Required by mlflow.prophet\n# NOTE: Prophet\'s whl build process will fail with dependencies not being present.\n#   Installation will default to setup.py in order to install correctly.\n#   To install in dev environment, ensure that gcc>=8 is installed to allow pystan\n#   to compile the model binaries. See: https://gcc.gnu.org/install/\n# Avoid 0.25 due to https://github.com/dr-prodigy/python-holidays/issues/1200\nholidays!=0.25\nprophet\n# Required by mlflow.shap\n# and shap evaluation functionality\nshap>=0.42.1\n# Required by mlflow.pmdarima\npmdarima\n# Required by mlflow.diviner\ndiviner\n# Required for using Hugging Face datasets with MLflow Tracking\n# Avoid datasets < 2.19.1 due to an incompatibility issue https://github.com/huggingface/datasets/issues/6737\ndatasets>=2.19.1\n# Required by mlflow.transformers\ntransformers\nsentencepiece\nsetfit\nlibrosa\nffmpeg\naccelerate\n# Required by mlflow.openai\nopenai\ntiktoken\ntenacity\n# Required by mlflow.llama_index\nllama_index\n# Required for an agent example of mlflow.llama_index\nllama-index-agent-openai\n# Required by mlflow.langchain\nlangchain\n# Required by mlflow.promptflow\npromptflow\n# Required by mlflow.sentence_transformers\nsentence-transformers\n# Required by mlflow.anthropic\nanthropic\n# Required by mlflow.ag2\nag2\n# Required by mlflow.dspy\n# In dspy 2.6.9, `dspy.__name__` is not \'dspy\', but \'dspy.__metadata__\',\n# which causes auto-logging tests to fail.\ndspy!=2.6.9\n# Required by mlflow.litellm\nlitellm\n# Required by mlflow.gemini\ngoogle-genai\n# Required by mlflow.groq\ngroq\n# Required by mlflow.mistral\nmistralai\n# Required by mlflow.autogen\nautogen-agentchat\n# Required by mlflow.semantic_kernel\nsemantic-kernel\n# Required by mlflow.agno\nagno\n# Required by mlflow.strands\nstrands-agents\n', 'ruff==0.12.10\nblack[jupyter]==23.7.0\nblacken-docs==1.18.0\npre-commit==4.0.1\ntoml==0.10.2\nmypy==1.17.1\npytest==8.4.0\npydantic==2.11.7\n-e ./dev/clint\n', '## Test-only dependencies\npytest\npytest-cov\n', '## Dependencies required to run tests\n# Required for testing utilities for parsing pip requirements\npip>=20.1\n## Test-only dependencies\npytest\npytest-asyncio\npytest-repeat\npytest-cov\npytest-timeout\npytest-localserver==0.5.0\nmoto>=4.2.0,<5,!=4.2.5\nazure-storage-blob>=12.0.0\nazure-storage-file-datalake>=12.9.1\nazure-identity>=1.6.1\npillow\nplotly\nkaleido\n# Required by tuning tests\nhyperopt\n# Required by evaluator tests\nshap\n# Required to evaluate language models in `mlflow.evaluate`\nevaluate\nnltk\nrouge_score\ntextstat\ntiktoken\n# Required by progress bar tests\nipywidgets\ntqdm\n# Required for LLM eval in `mlflow.evaluate`\nopenai\n# Required for showing pytest stats\npsutil\n# SQLAlchemy == 2.0.25 requires typing_extensions >= 4.6.0\ntyping_extensions>=4.6.0\n# Required for importing boto3 ClientError directly for testing\nbotocore>=1.34\npyspark\n# Required for testing the opentelemetry exporter of tracing\nopentelemetry-exporter-otlp-proto-grpc\nopentelemetry-exporter-otlp-proto-http\n# Required for testing mlflow.server.auth\nFlask-WTF<2\n# required for testing polars dataset integration\npolars>=1\n# required for testing mlflow.genai.optimize_prompt\ndspy\n', 'promptflow[azure]\npromptflow-tools\npython-dotenv\n', 'mlflow\ncloudpickle==2.2.1\nscikit-learn==1.5.2\n', 'bcrypt==3.2.0\ncloudpickle==2.0.0\nconfigparser==5.2.0\ncryptography==39.0.1\ndatabricks-feature-engineering==0.2.1\ndatabricks-rag-studio==0.2.0.dev0\nentrypoints==0.4\ngoogle-cloud-storage==2.11.0\ngrpcio-status==1.48.1\nlangchain==0.1.20\nmlflow[gateway]==2.12.2\nnumpy==1.23.5\npackaging==23.2\npandas==1.5.3\nprotobuf==4.24.0\npsutil==5.9.0\npyarrow==8.0.0\npydantic==1.10.6\npyyaml==6.0\nrequests==2.28.1\ntornado==6.1\n', 'bcrypt==3.2.0\ncloudpickle==2.0.0\nconfigparser==5.2.0\ncryptography==39.0.1\ndatabricks-feature-engineering==0.2.1\nentrypoints==0.4\ngoogle-cloud-storage==2.11.0\ngrpcio-status==1.48.1\nlangchain==0.1.20\nmlflow[gateway]==2.12.2\nnumpy==1.23.5\npackaging==23.2\npandas==1.5.3\nprotobuf==4.24.0\npsutil==5.9.0\npyarrow==8.0.0\npydantic==1.10.6\npyyaml==6.0\nrequests==2.28.1\ntornado==6.1\n', 'mlflow\nscikit-learn==1.4.2\n', 'mlflow\nscikit-learn==1.4.2\n', 'mlflow==2.7.1\ncloudpickle==2.2.1\n', 'mlflow==2.8.1\ncloudpickle==2.2.1\n', "MLflow Contributor Covenant Code of Conduct\n===========================================\n\n.. contents:: **Table of Contents**\n  :local:\n  :depth: 4\n\nOur Pledge\n##########\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\nOur Standards\n#############\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a professional setting\n\nOur Responsibilities\n####################\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\nScope\n#####\n\nThis Code of Conduct applies both within project spaces and in public spaces\nwhen an individual is representing the project or its community. Examples of\nrepresenting a project or community include using an official project e-mail\naddress, posting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event. Representation of a project may be\nfurther defined and clarified by project maintainers.\n\nEnforcement\n###########\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the Technical Steering Committee defined `here <https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#governance>`_.\nAll complaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter ", "individual is representing the project or its community. Examples of\nrepresenting a project or community include using an official project e-mail\naddress, posting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event. Representation of a project may be\nfurther defined and clarified by project maintainers.\n\nEnforcement\n###########\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the Technical Steering Committee defined `here <https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#governance>`_.\nAll complaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\nAttribution\n###########\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq\n", '=========================\nExtra MLflow Dependencies\n=========================\n\nWhen you `install the MLflow Python package <https://mlflow.org/docs/latest/quickstart.html#installing-mlflow>`_,\na set of core dependencies needed to use most MLflow functionality (tracking, projects, models APIs)\nis also installed.\n\nHowever, in order to use certain framework-specific MLflow APIs or configuration options,\nyou need to install additional, "extra" dependencies. For example, the model persistence APIs under\nthe ``mlflow.sklearn`` module require scikit-learn to be installed. Some of the most common MLflow\nextra dependencies can be installed via ``pip install mlflow[extras]``.\n\nThe full set of extra dependencies are documented, along with the modules that depend on them,\nin the following files:\n\n* extra-ml-requirements.txt: ML libraries needed to use model persistence and inference APIs\n* test-requirements.txt: Libraries required to use non-default artifact-logging and tracking server configurations\n', '\nThis document is a hands-on manual for doing issue and pull request triage for `MLflow issues\non GitHub <https://github.com/mlflow/mlflow/issues>`_ .\nThe purpose of triage is to speed up issue management and get community members faster responses.\n\nIssue and pull request triage has three steps:\n\n- assign one or more process labels (e.g. ``needs design`` or ``help wanted``),\n- mark a priority, and\n- label one or more relevant areas, languages, or integrations to help route issues to appropriate contributors or reviewers.\n\nThe remainder of the document describes the labels used in each of these steps and how to apply them.\n\nAssign appropriate process labels\n#######\nAssign at least one process label to every issue you triage.\n\n- ``needs author feedback``: We need input from the author of the issue or PR to proceed.\n- | ``needs design``: This feature is large or tricky enough that we think it warrants a design doc\n  | and review before someone begins implementation.\n- | ``needs committer feedback``: The issue has a design that is ready for committer review, or there is\n  | an issue or pull request that needs feedback from a committer about the approach or appropriateness\n  | of the contribution.\n- | ``needs review``: Use this label for issues that need a more detailed design review or pull\n  | requests ready for review (all questions answered, PR updated if requests have been addressed,\n  | tests passing).\n- ``help wanted``: We would like community help for this issue.\n- ``good first issue``: This would make a good first issue.\n\n\nAssign priority\n#######\n\nYou should assign a priority to each issue you triage. We use `kubernetes-style <https://github.com/\nkubernetes/community/blob/master/contributors/guide/issue-triage.md#define-priority>`_ priority\nlabels.\n\n- | ``priority/critical-urgent``: This is the highest priority and should be worked on by\n  | somebody right now. This should typically be reserved for things like security bugs,\n  | regressions, release blockers.\n- | ``priority/important-soon``: The issue is worked on by the community currently or will\n  | be very soon, ideally in time for the next release.\n- | ``priority/important-longterm``: Important over the long term, but may not be staffed or\n  | may need multiple releases to complete. Also used for things we know are on a\n  ', "priority to each issue you triage. We use `kubernetes-style <https://github.com/\nkubernetes/community/blob/master/contributors/guide/issue-triage.md#define-priority>`_ priority\nlabels.\n\n- | ``priority/critical-urgent``: This is the highest priority and should be worked on by\n  | somebody right now. This should typically be reserved for things like security bugs,\n  | regressions, release blockers.\n- | ``priority/important-soon``: The issue is worked on by the community currently or will\n  | be very soon, ideally in time for the next release.\n- | ``priority/important-longterm``: Important over the long term, but may not be staffed or\n  | may need multiple releases to complete. Also used for things we know are on a\n  | contributor's roadmap in the next few months. We can use this in conjunction with\n  | ``help wanted`` to mark issues we would like to get help with. If someone begins actively\n  | working on an issue with this label and we think it may be merged by the next release, change\n  | the priority to ``priority/important-soon``.\n- | ``priority/backlog``: We believe it is useful but don't see it being prioritized in the\n  | next few months. Use this for issues that are lower priority than ``priority/important-longterm``.\n  | We welcome community members to pick up a ``priority/backlog`` issue, but there may be some\n  | delay in getting support through design review or pull request feedback.\n- | ``priority/awaiting-more-evidence``: Lowest priority. Possibly useful, but not yet enough\n  | support to actually get it done. This is a good place to put issues that could be useful but\n  | require more evidence to demonstrate broad value. Don't use it as a way to say no.\n  | If we think it doesn't fit in MLflow, we should just say that and why.\n\nLabel relevant areas\n#######\n\nAssign one more labels for relevant component or interface surface areas, languages, or\nintegrations. As a principle, we aim to have the minimal set of labels needed to help route issues\nand PRs to appropriate contributors. For example, a ``language/python`` label would not be\nparticularly helpful for routing issues to committers, since most PRs involve Python code.\n``language/java`` and ``language/r`` make sense to have, as the clients ", 'but\n  | require more evidence to demonstrate broad value. Don\'t use it as a way to say no.\n  | If we think it doesn\'t fit in MLflow, we should just say that and why.\n\nLabel relevant areas\n#######\n\nAssign one more labels for relevant component or interface surface areas, languages, or\nintegrations. As a principle, we aim to have the minimal set of labels needed to help route issues\nand PRs to appropriate contributors. For example, a ``language/python`` label would not be\nparticularly helpful for routing issues to committers, since most PRs involve Python code.\n``language/java`` and ``language/r`` make sense to have, as the clients in these languages differ from the Python client and aren\'t maintained by many people. As with process labels, we\ntake inspiration from Kubernetes on naming conventions.\n\nComponents\n""""""""\n- ``area/artifacts``: Artifact stores and artifact logging\n- ``area/build``: Build and test infrastructure for MLflow\n- ``area/docs``: MLflow documentation pages\n- ``area/evaluation``: MLflow model evaluation features, evaluation metrics, and evaluation workflows\n- ``area/examples``: Example code\n- ``area/gateway``: AI Gateway service, Gateway client APIs, third-party Gateway integrations\n- ``area/model-registry``: Model Registry service, APIs, and the fluent client calls for Model Registry\n- ``area/models``: MLmodel format, model serialization/deserialization, flavors\n- ``area/projects``: MLproject format, project execution backends\n- ``area/prompt``: MLflow prompt engineering features, prompt templates, and prompt management\n- ``area/scoring``: MLflow Model server, model deployment tools, Spark UDFs\n- ``area/server-infra``: MLflow Tracking server backend\n- ``area/tracing``: MLflow Tracing features, tracing APIs, and LLM tracing functionality\n- ``area/tracking``: Tracking Service, tracking client APIs, autologging\n\nInterface Surface\n""""""""\n- ``area/uiux``: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- ``area/docker``: Docker use across MLflow\'s components, such as MLflow Projects and MLflow Models\n- ``area/sqlalchemy``: Use of SQLAlchemy in the Tracking Service or Model Registry\n- ``area/windows``: Windows support\n\nLanguage Surface\n""""""""\n- ``language/r``: R APIs and clients\n- ``language/java``: Java APIs and clients\n- ``language/new``: Proposals for new client languages\n\nIntegrations\n""""""""\n- ``integrations/azure``: Azure and Azure ML integrations\n- ``integrations/sagemaker``: SageMaker integrations\n- ``integrations/databricks``: Databricks integrations\n', "Dockerized Model Training with MLflow\n-------------------------------------\nThis directory contains an MLflow project that trains a linear regression model on the UC Irvine\nWine Quality Dataset. The project uses a Docker image to capture the dependencies needed to run\ntraining code. Running a project in a Docker environment (as opposed to Conda) allows for capturing\nnon-Python dependencies, e.g. Java libraries. In the future, we also hope to add tools to MLflow\nfor running Dockerized projects e.g. on a Kubernetes cluster for scale out.\n\nStructure of this MLflow Project\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThis MLflow project contains a ``train.py`` file that trains a scikit-learn model and uses\nMLflow Tracking APIs to log the model and its metadata (e.g., hyperparameters and metrics)\nfor later use and reference. ``train.py`` operates on the Wine Quality Dataset, which is included\nin ``wine-quality.csv``.\n\nMost importantly, the project also includes an ``MLproject`` file, which specifies the Docker\ncontainer environment in which to run the project using the ``docker_env`` field:\n\n.. code-block:: yaml\n\n  docker_env:\n    image:  mlflow-docker-example\n\nHere, ``image`` can be any valid argument to ``docker run``, such as the tag, ID or URL of a Docker\nimage (see `Docker docs <https://docs.docker.com/engine/reference/run/#general-form>`_). The above\nexample references a locally-stored image (``mlflow-docker-example``) by tag.\n\nFinally, the project includes a ``Dockerfile`` that is used to build the image referenced by the\n``MLproject`` file. The ``Dockerfile`` specifies library dependencies required by the project, such\nas ``mlflow`` and ``scikit-learn``.\n\nRunning this Example\n^^^^^^^^^^^^^^^^^^^^\n\nFirst, install MLflow (via ``pip install mlflow``) and install\n`Docker <https://www.docker.com/get-started>`_.\n\nThen, build the image for the project's Docker container environment. You must use the same image\nname that is given by the ``docker_env.image`` field of the MLproject file. In this example, the\nimage name is ``mlflow-docker-example``. Issue the following command to build an image with this\nname:\n\n.. code-block:: bash\n\n  docker build -t mlflow-docker-example -f Dockerfile .\n\nNote that the name if the image used in the ``docker build`` command, ``mlflow-docker-example``,\nmatches the name of the image referenced in the ``MLproject`` file.\n\nFinally, run the example project using ``mlflow run examples/docker -P alpha=0.5``.\n\n.. note::\n    If running this example on a Mac with Apple silicon, ensure that Docker Desktop is running and\n    that you are logged in to the Docker Desktop service.\n    If ", "MLproject file. In this example, the\nimage name is ``mlflow-docker-example``. Issue the following command to build an image with this\nname:\n\n.. code-block:: bash\n\n  docker build -t mlflow-docker-example -f Dockerfile .\n\nNote that the name if the image used in the ``docker build`` command, ``mlflow-docker-example``,\nmatches the name of the image referenced in the ``MLproject`` file.\n\nFinally, run the example project using ``mlflow run examples/docker -P alpha=0.5``.\n\n.. note::\n    If running this example on a Mac with Apple silicon, ensure that Docker Desktop is running and\n    that you are logged in to the Docker Desktop service.\n    If you are modifying the example ``DockerFile`` to specify older versions of ``scikit-learn``,\n    you should enable `Rosetta compatibility <https://docs.docker.com/desktop/settings/mac/#features-in-development>`_\n    in the Docker Desktop configuration settings to ensure that the appropriate ``cython`` compiler is used.\n\nWhat happens when the project is run?\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nRunning ``mlflow run examples/docker`` builds a new Docker image based on ``mlflow-docker-example``\nthat also contains our project code. The resulting image is tagged as\n``mlflow-docker-example-<git-version>`` where ``<git-version>`` is the git commit ID. After the image is\nbuilt, MLflow executes the default (main) project entry point within the container using ``docker run``.\n\nEnvironment variables, such as ``MLFLOW_TRACKING_URI``, are propagated inside the container during\nproject execution. When running against a local tracking URI, MLflow mounts the host system's\ntracking directory (e.g., a local ``mlruns`` directory) inside the container so that metrics and\nparams logged during project execution are accessible afterwards.\n", 'How To Train and Deploy Image Classifier with MLflow and Keras\n--------------------------------------------------------------\n\nIn this example we demonstrate how to train and deploy image classification models with MLflow.\nWe train a VGG16 deep learning model to classify flower species from photos using a `dataset\n<http://download.tensorflow.org/example_images/flower_photos.tgz>`_ available from `tensorflow.org\n<http://www.tensorflow.org>`_. Note that although we use Keras to train the model in this case,\na similar approach can be applied to other deep learning frameworks such as ``PyTorch``.\n\nThe MLflow model produced by running this example can be deployed to any MLflow supported endpoints.\nAll the necessary image preprocessing is packaged with the model. The model can therefore be applied\nto image data directly. All that is required in order to pass new data to the model is to encode the\nimage binary data as base64 encoded string in pandas DataFrame (standard interface for MLflow python\nfunction models). The included Python scripts demonstrate how the model can be deployed to a REST\nAPI endpoint for realtime evaluation or to Spark for batch scoring..\n\nIn order to include custom image pre-processing logic with the model, we define the model as a\ncustom python function model wrapping around the underlying Keras model. The wrapper provides\nnecessary preprocessing to convert input data into multidimensional arrays expected by the\nKeras model. The preprocessing logic is stored with the model as a code dependency. Here is an\nexample of the output model directory layout:\n\n.. code-block:: bash\n\n   tree model\n\n::\n\n   model\n   â”œâ”€â”€ MLmodel\n   â”œâ”€â”€ code\n   â”‚\xa0\xa0 â””â”€â”€ image_pyfunc.py\n   â”œâ”€â”€ data\n   â”‚\xa0\xa0 â””â”€â”€ image_model\n   â”‚\xa0\xa0     â”œâ”€â”€ conf.yaml\n   â”‚\xa0\xa0     â””â”€â”€ keras_model\n   â”‚\xa0\xa0         â”œâ”€â”€ MLmodel\n   â”‚\xa0\xa0         â”œâ”€â”€ conda.yaml\n   â”‚\xa0\xa0         â””â”€â”€ model.h5\n   â””â”€â”€ mlflow_env.yml\n\n\n\nThe example contains the following files:\n\n * MLproject\n   Contains definition of this project. Contains only one entry point to train the model.\n\n * conda.yaml\n   Defines project dependencies. NOTE: You might want to change tensorflow package to ', '  â”‚\xa0\xa0     â”œâ”€â”€ conf.yaml\n   â”‚\xa0\xa0     â””â”€â”€ keras_model\n   â”‚\xa0\xa0         â”œâ”€â”€ MLmodel\n   â”‚\xa0\xa0         â”œâ”€â”€ conda.yaml\n   â”‚\xa0\xa0         â””â”€â”€ model.h5\n   â””â”€â”€ mlflow_env.yml\n\n\n\nThe example contains the following files:\n\n * MLproject\n   Contains definition of this project. Contains only one entry point to train the model.\n\n * conda.yaml\n   Defines project dependencies. NOTE: You might want to change tensorflow package to tensorflow-gpu\n   if you have gpu(s) available.\n\n * train.py\n   Main entry point of the projects. Handles command line arguments and possibly downloads the\n   dataset.\n\n * image_pyfunc.py\n   The implementation of the model train and also of the outputed custom python flavor model. Note\n   that the same preprocessing code that is used during model training is packaged with the output\n   model and is used during scoring.\n\n * score_images_rest.py\n   Score an image or a directory of images using a model deployed to a REST endpoint.\n\n * score_images_spark.py\n   Score an image or a directory of images using model deployed to Spark.\n\n\n\nRunning this Example\n^^^^^^^^^^^^^^^^^^^^\n\nTo train the model, run the example as a standard MLflow project:\n\n\n.. code-block:: bash\n\n    mlflow run examples/flower_classifier\n\nThis will download the training dataset from ``tensorflow.org``, train a classifier using Keras and\nlog results with MLflow.\n\nTo test your model, run the included scoring scripts. For example, say your model was trained with\nrun_id ``101``.\n\n- To test REST api scoring do the following two steps:\n\n  1. Deploy the model as a local REST endpoint by running ``mlflow models serve``:\n\n    .. code-block:: bash\n\n        # deploy the model to local REST api endpoint\n        mlflow models serve --model-uri runs:/101/model --port 54321\n\n  1. Apply the model to new data using the provided score_images_rest.py script:\n\n    .. code-block:: bash\n\n     ', 'your model, run the included scoring scripts. For example, say your model was trained with\nrun_id ``101``.\n\n- To test REST api scoring do the following two steps:\n\n  1. Deploy the model as a local REST endpoint by running ``mlflow models serve``:\n\n    .. code-block:: bash\n\n        # deploy the model to local REST api endpoint\n        mlflow models serve --model-uri runs:/101/model --port 54321\n\n  1. Apply the model to new data using the provided score_images_rest.py script:\n\n    .. code-block:: bash\n\n        # score the deployed model\n        python score_images_rest.py --host http://127.0.0.1 --port 54321 /path/to/images/for/scoring\n\n\n- To test batch scoring in Spark, run score_images_spark.py to score the model in Spark like this:\n\n  .. code-block:: bash\n\n    python score_images_spark.py --model-uri runs:/101/model /path/to/images/for/scoring\n', 'Hyperparameter Tuning Example\n------------------------------\n\nExample of how to do hyperparameter tuning with MLflow and some popular optimization libraries.\n\nThis example tries to optimize the RMSE metric of a Keras deep learning model on a wine quality\ndataset. The Keras model is fitted by the ``train`` entry point and has two hyperparameters that we\ntry to optimize: ``learning-rate`` and ``momentum``. The input dataset is split into three parts: training,\nvalidation, and test. The training dataset is used to fit the model and the validation dataset is used to\nselect the best hyperparameter values, and the test set is used to evaluate expected performance and\nto verify that we did not overfit on the particular training and validation combination. All three\nmetrics are logged with MLflow and you can use the MLflow UI to inspect how they vary between different\nhyperparameter values.\n\nexamples/hyperparam/MLproject has 4 targets:\n  * train:\n    train a simple deep learning model on the wine-quality dataset from our tutorial.\n    It has 2 tunable hyperparameters: ``learning-rate`` and ``momentum``.\n    Contains examples of how Keras callbacks can be used for MLflow integration.\n  * random:\n    perform simple random search over the parameter space.\n  * hyperopt:\n    use `Hyperopt <https://github.com/hyperopt/hyperopt>`_ to optimize hyperparameters.\n\n\nRunning this Example\n^^^^^^^^^^^^^^^^^^^^\n\nYou can run any of the targets as a standard MLflow run.\n\n.. code-block:: bash\n\n    mlflow experiments create -n individual_runs\n\nCreates experiment for individual runs and return its experiment ID.\n\n.. code-block:: bash\n\n    mlflow experiments create -n hyper_param_runs\n\nCreates an experiment for hyperparam runs and return its experiment ID.\n\n.. code-block:: bash\n\n    mlflow run -e train --experiment-id <individual_runs_experiment_id> examples/hyperparam\n\nRuns the Keras deep learning training with default parameters and log it in experiment 1.\n\n.. code-block:: bash\n\n    mlflow run -e random --experiment-id <hyperparam_experiment_id> examples/hyperparam\n\n.. code-block:: bash\n\n    mlflow run -e hyperopt --experiment-id <hyperparam_experiment_id> examples/hyperparam\n\nRuns the hyperparameter tuning with either random search or Hyperopt and log the\nresults under ``hyperparam_experiment_id``.\n\nYou can compare these results by using ``mlflow ui``.\n', "Multistep Workflow Example\n--------------------------\nThis MLproject aims to be a fully self-contained example of how to\nchain together multiple different MLflow runs which each encapsulate\na transformation or training step, allowing a clear definition of the\ninterface between the steps, as well as allowing for caching and reuse\nof the intermediate results.\n\nAt a high level, our goal is to predict users' ratings of movie given\na history of their ratings for other movies. This example is based\non `this webinar <https://databricks.com/blog/2018/07/13/scalable-end-to-end-deep-learning-using-tensorflow-and-databricks-on-demand-webinar-and-faq-now-available.html>`_\nby @brookewenig and @smurching.\n\n.. image:: ../../docs/source/_static/images/tutorial-multistep-workflow.png?raw=true\n\nThere are four steps to this workflow:\n\n- **load_raw_data.py**: Downloads the MovieLens dataset\n  (a set of triples of user id, movie id, and rating) as a CSV and puts\n  it into the artifact store.\n\n- **etl_data.py**: Converts the MovieLens CSV from the\n  previous step into Parquet, dropping unnecessary columns along the way.\n  This reduces the input size from 500 MB to 49 MB, and allows columnar\n  access of the data.\n\n- **als.py**: Runs Alternating Least Squares for collaborative\n  filtering on the Parquet version of MovieLens to estimate the\n  movieFactors and userFactors. This produces a relatively accurate estimator.\n\n- **train_keras.py**: Trains a neural network on the\n  original data, supplemented by the ALS movie/userFactors -- we hope\n  this can improve upon the ALS estimations.\n\nWhile we can run each of these steps manually, here we have a driver\nrun, defined as **main** (main.py). This run will run\nthe steps in order, passing the results of one to the next.\nAdditionally, this run will attempt to determine if a sub-run has\nalready been executed successfully with the same parameters and, if so,\nreuse the cached results.\n\nRunning this Example\n^^^^^^^^^^^^^^^^^^^^\nIn order for the multistep workflow to find the other steps, you must\nexecute ``mlflow run`` from this directory. So, in order to find out if\nthe Keras model does in fact improve upon the ALS model, you can simply\nrun:\n\n.. code-block:: bash\n\n    cd examples/multistep_workflow\n    mlflow run .\n\n\nThis downloads and transforms the MovieLens dataset, trains an ALS\nmodel, and then trains a Keras model -- you can compare the results by\nusing ``mlflow ui``.\n\nYou can also try changing the number of ALS iterations or Keras hidden\nunits:\n\n.. code-block:: bash\n\n  ", 'the same parameters and, if so,\nreuse the cached results.\n\nRunning this Example\n^^^^^^^^^^^^^^^^^^^^\nIn order for the multistep workflow to find the other steps, you must\nexecute ``mlflow run`` from this directory. So, in order to find out if\nthe Keras model does in fact improve upon the ALS model, you can simply\nrun:\n\n.. code-block:: bash\n\n    cd examples/multistep_workflow\n    mlflow run .\n\n\nThis downloads and transforms the MovieLens dataset, trains an ALS\nmodel, and then trains a Keras model -- you can compare the results by\nusing ``mlflow ui``.\n\nYou can also try changing the number of ALS iterations or Keras hidden\nunits:\n\n.. code-block:: bash\n\n    mlflow run . -P als_max_iter=20 -P keras_hidden_units=50\n', 'mlflow REST API Example\n-----------------------\nThis simple example shows how you could use MLflow REST API to create new\nruns inside an experiment to log parameters/metrics.\n\nTo run this example code do the following:\n\nOpen a terminal and navigate to the ``/tmp`` directory and start the mlflow tracking server::\n\n  mlflow server\n\nIn another terminal window navigate to the ``mlflow/examples/rest_api`` directory.  Run the example code\nwith this command::\n\n  python mlflow_tracking_rest_api.py\n\nProgram options::\n\n  usage: mlflow_tracking_rest_api.py [-h] [--hostname HOSTNAME] [--port PORT]\n                                   [--experiment-id EXPERIMENT_ID]\n\n  MLflow REST API Example\n\n  optional arguments:\n    -h, --help            show this help message and exit\n    --hostname HOSTNAME   MLflow server hostname/ip (default: localhost)\n    --port PORT           MLflow server port number (default: 5000)\n    --experiment-id EXPERIMENT_ID\n                            Experiment ID (default: 0)\n', 'Python Package Anti-Tampering with MLflow\n-----------------------------------------\nThis directory contains an MLflow project showing how to harden the ML supply chain, and in particular\nhow to protect against Python package tampering by enforcing\n`hash checks <https://pip.pypa.io/en/latest/cli/pip_install/#hash-checking-mode>`_ on packages.\n\nRunning this Example\n^^^^^^^^^^^^^^^^^^^^\n\nFirst, install MLflow (via ``pip install mlflow``).\n\nThe model is trained locally by running:\n\n.. code-block:: bash\n\n  mlflow run .\n\nAt the end of the training, note the run ID (say ``e651fcd4dab140a2bd4d3745a32370ac``).\n\nThe model is served locally by running:\n\n.. code-block:: bash\n\n  mlflow models serve -m runs:/e651fcd4dab140a2bd4d3745a32370ac/model\n\nInference is performed by sending JSON POST requests to http://localhost:5000/invocations:\n\n.. code-block:: bash\n\n  curl -X POST -d "{\\"dataframe_split\\": {\\"data\\":[[0.0199132142,0.0506801187,0.1048086895,0.0700725447,-0.0359677813,-0.0266789028,-0.0249926566,-0.002592262,0.0037117382,0.0403433716]]}}" -H "Content-Type: application/json" http://localhost:5000/invocations\n\nWhich returns ``[235.11371081266924]``.\n\nStructure of this MLflow Project\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. code-block:: yaml\n\n  name: mlflow-supply-chain-security\n  channels:\n  - nodefaults\n  dependencies:\n  - python=3.9\n  - pip\n  - pip:\n    - --require-hashes\n    - -r requirements.txt\n\nThis ensures that all the package requirements referenced in ``requirements.txt`` have been pinned through both version and hash:\n\n.. code-block:: text\n\n  mlflow==1.20.2 \\\n      --hash=sha256:963c22532e82a93450674ab97d62f9e528ed0906b580fadb7c003e696197557c \\\n      --hash=sha256:b15ff0c7e5e64f864a0b40c99b9a582227315eca2065d9f831db9aeb8f24637b\n  numpy==1.21.4 \\\n      --hash=sha256:0b78ecfa070460104934e2caf51694ccd00f37d5e5dbe76f021b1b0b0d221823 \\\n  ...\n\nThat same conda environment is referenced when logging the model in ``train.py`` so the environment matches during inference:\n\n.. code-block:: python\n\n  mlflow.sklearn.log_model(\n      model,\n      name="model",\n      signature=mlflow.models.infer_signature(X_train[:10], y_train[:10]),\n      input_example=X_train[:10],\n      conda_env="conda.yaml",\n  )\n\nThe package requirements are managed in ``requirements.in``:\n\n.. code-block:: text\n\n  pandas==1.3.2\n  scikit-learn==0.24.2\n  mlflow==1.20.2\n\nThey are compiled using ``pip-tools`` to resolve all the package dependencies, their versions, and their hashes:\n\n.. code-block:: bash\n\n  pip install pip-tools\n  pip-compile --generate-hashes --output-file=requirements.txt requirements.in\n', '{\n  "kube-context": "docker-for-desktop",\n  "kube-job-template-path": "examples/docker/kubernetes_job_template.yaml",\n  "repository-uri": "username/mlflow-kubernetes-example"\n}\n', '[[4.6, 3.1, 1.5, 0.2]]\n', '{\n  "name": "mlflow-typescript",\n  "private": true,\n  "description": "TypeScript implementation of MLflow Tracing SDK. This is the root workspace package that includes all the public packages as sub-directories.",\n  "workspaces": [\n    "core",\n    "integrations/*"\n  ],\n  "scripts": {\n    "build": "npm run build:subpackages",\n    "build:subpackages": "npm run build:core && npm run build:integrations",\n    "build:core": "cd core && npm run build",\n    "build:integrations": "cd integrations/openai && npm run build",\n    "test": "npm run test:subpackages",\n    "test:subpackages": "npm run test:core && npm run test:integrations",\n    "test:core": "cd core && npm run test",\n    "test:integrations": "cd integrations/openai && npm run test",\n    "lint": "eslint . --ext .ts",\n    "lint:fix": "eslint . --ext .ts --fix",\n    "format": "prettier --write .",\n    "format:check": "prettier --check .",\n    "prepare": "npm run build"\n  },\n  "devDependencies": {\n    "typedoc": "^0.28.0"\n  }\n}\n', '{\n  "name": "mlflow-tracing",\n  "version": "0.1.0",\n  "description": "TypeScript implementation of MLflow Tracing SDK for LLM observability",\n  "repository": {\n    "type": "git",\n    "url": "https://github.com/mlflow/mlflow.git"\n  },\n  "homepage": "https://mlflow.org/",\n  "author": {\n    "name": "MLflow",\n    "url": "https://mlflow.org/"\n  },\n  "bugs": {\n    "url": "https://github.com/mlflow/mlflow/issues"\n  },\n  "license": "Apache-2.0",\n  "keywords": [\n    "mlflow",\n    "tracing",\n    "observability",\n    "opentelemetry",\n    "llm",\n    "javascript",\n    "typescript"\n  ],\n  "main": "dist/index.js",\n  "types": "dist/index.d.ts",\n  "scripts": {\n    "build": "tsc",\n    "test": "jest",\n    "lint": "eslint . --ext .ts",\n    "lint:fix": "eslint . --ext .ts --fix",\n    "format": "prettier --write .",\n    "format:check": "prettier --check ."\n  },\n  "dependencies": {\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/sdk-node": "^0.201.1",\n    "@types/json-bigint": "^1.0.4",\n    "fast-safe-stringify": "^2.1.1",\n    "ini": "^5.0.0",\n    "json-bigint": "^1.0.0"\n  },\n  "devDependencies": {\n    "@types/ini": "^4.1.1",\n    "@types/jest": "^29.5.3",\n    "@types/node": "^20.4.5",\n    "@typescript-eslint/eslint-plugin": "^6.21.0",\n    "@typescript-eslint/parser": "^6.21.0",\n    "openai": "^4.0.0",\n    "eslint": "^8.57.1",\n    "jest": "^29.6.2",\n    "msw": "^2.10.3",\n    "prettier": "^3.5.3",\n    "ts-jest": "^29.1.1",\n    "tsx": "^4.7.0",\n    "typescript": "^5.8.3",\n    "whatwg-fetch": "^3.6.20"\n  },\n  "engines": {\n    "node": ">=18"\n  },\n  "files": [\n    "dist/"\n  ]\n}\n', '{\n  "name": "mlflow-openai",\n  "version": "0.1.0",\n  "description": "OpenAI integration package for MLflow Tracing",\n  "repository": {\n    "type": "git",\n    "url": "https://github.com/mlflow/mlflow.git"\n  },\n  "homepage": "https://mlflow.org/",\n  "author": {\n    "name": "MLflow",\n    "url": "https://mlflow.org/"\n  },\n  "bugs": {\n    "url": "https://github.com/mlflow/mlflow/issues"\n  },\n  "license": "Apache-2.0",\n  "keywords": [\n    "mlflow",\n    "tracing",\n    "observability",\n    "opentelemetry",\n    "llm",\n    "openai",\n    "javascript",\n    "typescript"\n  ],\n  "main": "dist/index.js",\n  "types": "dist/index.d.ts",\n  "scripts": {\n    "build": "tsc",\n    "test": "jest",\n    "lint": "eslint . --ext .ts",\n    "lint:fix": "eslint . --ext .ts --fix",\n    "format": "prettier --write .",\n    "format:check": "prettier --check ."\n  },\n  "peerDependencies": {\n    "mlflow-tracing": "^0.1.0-rc.0",\n    "openai": "^4.0.0"\n  },\n  "devDependencies": {\n    "jest": "^29.6.2",\n    "typescript": "^5.8.3"\n  },\n  "engines": {\n    "node": ">=18"\n  },\n  "files": [\n    "dist/"\n  ]\n}\n', '{\n  "name": "@mlflow/mlflow",\n  "version": "0.1.0",\n  "scripts": {\n    "start": "craco start",\n    "storybook": "start-storybook -p 6006 -s public",\n    "build-storybook": "build-storybook -s public",\n    "test": "craco --max_old_space_size=8192 test --env=jsdom --colors --watchAll=false",\n    "test:watch": "yarn test --watch",\n    "test:ci": "CI=true craco test --env=jsdom --colors --forceExit --ci --coverage",\n    "lint": "eslint --ext js,jsx,ts,tsx src",\n    "lint:fix": "eslint --ext js,jsx,ts,tsx src --fix",\n    "type-check": "tsc --noEmit",\n    "prettier": "prettier",\n    "prettier:fix": "prettier . --write",\n    "prettier:check": "prettier . --check",\n    "i18n:check": "yarn i18n --lint",\n    "i18n": "node scripts/extract-i18n.js",\n    "check-all": "yarn lint && yarn prettier:check && yarn i18n:check && yarn type-check",\n    "knip": "knip --reporter markdown --preprocessor ./knip-preprocessor.ts",\n    "build": "craco --max_old_space_size=8192 build",\n    "graphql-codegen": "python ../../../dev/proto_to_graphql/code_generator.py && yarn graphql-codegen:clean && yarn graphql-codegen:base",\n    "graphql-codegen:base": "graphql-codegen --config ./src/graphql/graphql-codegen.ts",\n    "graphql-codegen:clean": "find . -path \'**/__generated__/*.ts\' | xargs rm"\n  },\n  "dependencies": {\n    "@ag-grid-community/client-side-row-model": "^27.2.1",\n    "@ag-grid-community/core": "^27.2.1",\n    "@ag-grid-community/react": "^27.2.1",\n    "@apollo/client": "^3.6.9",\n    "@craco/craco": "7.0.0-alpha.0",\n    "@databricks/design-system": "^1.12.20",\n    "@emotion/cache": "^11.11.0",\n    "@emotion/react": "^11.11.3",\n    "@tanstack/react-query": "^4.29.17",\n    "@tanstack/react-table": "^8.8.2",\n    "@tanstack/react-virtual": "^3.8.1",\n    "@types/react-virtualized": "^9.21.9",\n    "babel-jest": "^27.5.1",\n    "buffer": "^6.0.3",\n    "bytes": "3.0.0",\n    "classnames": "^2.2.6",\n    "cookie": "0.3.1",\n    "cronstrue": "^1.94.0",\n    "d3-array": "^3.2.4",\n    "d3-scale": "^2.1.0",\n    "dateformat": "3.0.3",\n    "diff": "5.1.0",\n    "file-saver": "^2.0.5",\n    "font-awesome": "4.7.0",\n    "graphql": "^15.5.0",\n    "http-proxy-middleware": "^1.0.3",\n    "immutable": "3.8.1",\n    "invariant": "^2.2.4",\n    "js-yaml": "^3.14.0",\n    "json-bigint": "databricks/json-bigint#a1defaf9cd8dd749f0fd4d5f83a22cd846789658",\n    "leaflet": "^1.5.1",\n    "lodash": "^4.17.21",\n    "moment": "^2.29.4",\n    ', '"buffer": "^6.0.3",\n    "bytes": "3.0.0",\n    "classnames": "^2.2.6",\n    "cookie": "0.3.1",\n    "cronstrue": "^1.94.0",\n    "d3-array": "^3.2.4",\n    "d3-scale": "^2.1.0",\n    "dateformat": "3.0.3",\n    "diff": "5.1.0",\n    "file-saver": "^2.0.5",\n    "font-awesome": "4.7.0",\n    "graphql": "^15.5.0",\n    "http-proxy-middleware": "^1.0.3",\n    "immutable": "3.8.1",\n    "invariant": "^2.2.4",\n    "js-yaml": "^3.14.0",\n    "json-bigint": "databricks/json-bigint#a1defaf9cd8dd749f0fd4d5f83a22cd846789658",\n    "leaflet": "^1.5.1",\n    "lodash": "^4.17.21",\n    "moment": "^2.29.4",\n    "pako": "0.2.7",\n    "papaparse": "^5.3.2",\n    "parcoord-es": "^2.2.10",\n    "pdfjs-dist": "^5.3.31",\n    "plotly.js": "2.5.1",\n    "prop-types": "^15.8.1",\n    "qs": "6.10.5",\n    "rc-image": "~5.2.4",\n    "react": "^18.2.0",\n    "react-dnd": "^15.1.1",\n    "react-dnd-html5-backend": "^15.1.2",\n    "react-dom": "^18.2.0",\n    "react-draggable": "^4.4.6",\n    "react-error-boundary": "^4.0.2",\n    "react-hook-form": "^7.36.0",\n    "react-iframe": "1.8.0",\n    "react-intl": "^6.0.4",\n    "react-markdown-10": "npm:react-markdown@10",\n    "react-mde": "^11.0.0",\n    "react-pdf": "^10.0.1",\n    "react-plotly.js": "^2.5.1",\n    "react-redux": "^7.2.5",\n    "react-resizable": "^3.0.4",\n    "react-router": "^6.4.0",\n    "react-router-dom": "^6.4.0",\n    "react-syntax-highlighter": "^15.4.5",\n    "react-transition-group": "^4.4.1",\n    "react-treebeard": "2.1.0",\n    "react-vega": "^7.6.0",\n    "react-virtual": "^2.10.4",\n    "react-virtualized": "^9.21.2",\n    "redux": "^4.1.1",\n    "redux-promise-middleware": "^5.1.1",\n    "redux-thunk": "^2.3.0",\n    "remark-gfm-4": "npm:remark-gfm@4",\n    "sanitize-html": "^1.18.5",\n    "showdown": "^1.8.6",\n    "stream-browserify": "^3.0.0",\n    "stylis": "^4.0.10",\n    "url": "^0.11.0",\n    "use-clipboard-copy": "^0.2.0",\n    "use-debounce": "^10.0.4",\n    "use-sync-external-store": "^1.2.0",\n    "wavesurfer.js": "^7.8.8",\n    "yup": "^1.6.1"\n  },\n  "devDependencies": {\n    "@babel/core": "^7.27.3",\n    "@babel/eslint-parser": "^7.22.15",\n    "@babel/preset-env": "^7.27.2",\n    "@babel/preset-react": "^7.27.1",\n    ', '"react-virtualized": "^9.21.2",\n    "redux": "^4.1.1",\n    "redux-promise-middleware": "^5.1.1",\n    "redux-thunk": "^2.3.0",\n    "remark-gfm-4": "npm:remark-gfm@4",\n    "sanitize-html": "^1.18.5",\n    "showdown": "^1.8.6",\n    "stream-browserify": "^3.0.0",\n    "stylis": "^4.0.10",\n    "url": "^0.11.0",\n    "use-clipboard-copy": "^0.2.0",\n    "use-debounce": "^10.0.4",\n    "use-sync-external-store": "^1.2.0",\n    "wavesurfer.js": "^7.8.8",\n    "yup": "^1.6.1"\n  },\n  "devDependencies": {\n    "@babel/core": "^7.27.3",\n    "@babel/eslint-parser": "^7.22.15",\n    "@babel/preset-env": "^7.27.2",\n    "@babel/preset-react": "^7.27.1",\n    "@babel/preset-typescript": "^7.27.1",\n    "@emotion/babel-plugin": "^11.11.0",\n    "@emotion/babel-preset-css-prop": "^11.11.0",\n    "@emotion/eslint-plugin": "^11.7.0",\n    "@formatjs/cli": "^4.2.15",\n    "@graphql-codegen/cli": "^5.0.0",\n    "@graphql-codegen/typescript": "^4.0.1",\n    "@graphql-codegen/typescript-operations": "^4.0.1",\n    "@jest/globals": "^30.0.2",\n    "@storybook/addon-actions": "^6.5.5",\n    "@storybook/addon-docs": "^6.5.5",\n    "@storybook/addon-essentials": "^6.5.5",\n    "@storybook/addon-links": "^6.5.5",\n    "@storybook/builder-webpack5": "6.5.5",\n    "@storybook/manager-webpack5": "6.5.5",\n    "@storybook/node-logger": "^6.5.5",\n    "@storybook/preset-create-react-app": "^4.1.0",\n    "@storybook/react": "^6.5.5",\n    "@testing-library/dom": "^10.4.0",\n    "@testing-library/jest-dom": "^6.4.2",\n    "@testing-library/react": "^16.1.0",\n    "@testing-library/user-event": "^14.5.2",\n    "@types/d3-array": "^3.2.1",\n    "@types/d3-scale": "^2.1.0",\n    "@types/d3-selection": "^1.3.0",\n    "@types/diff": "^5.1.0",\n    "@types/file-saver": "^2.0.3",\n    "@types/invariant": "^2.2.35",\n    "@types/jest": "^29.5.14",\n    "@types/pako": "^2.0.0",\n    "@types/plotly.js": "^1.54.21",\n    "@types/react": "^17.0.50",\n    "@types/react-dom": "^17.0.17",\n    "@types/react-plotly.js": "^2.5.0",\n    "@types/react-resizable": "^3.0.3",\n    "@types/react-router": "^5.1.20",\n    "@types/react-router-dom": "^5.3.3",\n    "@types/react-transition-group": "^4.4.4",\n    "@types/stylis": "^4.0.1",\n    "@types/use-sync-external-store": "^0.0.3",\n    "@typescript-eslint/eslint-plugin": "^5.28.0",\n    "@typescript-eslint/parser": "^5.28.0",\n    "@wojtekmaj/enzyme-adapter-react-17": "^0.6.3",\n    "argparse": "^2.0.1",\n    "babel-plugin-formatjs": "^10.2.14",\n    "babel-plugin-react-require": "^3.1.3",\n    "confusing-browser-globals": "^1.0.11",\n    "enzyme": "^3.11.0",\n    "eslint": "^8.25.0",\n    "eslint-config-prettier": "^8.5.0",\n    ', '"@types/plotly.js": "^1.54.21",\n    "@types/react": "^17.0.50",\n    "@types/react-dom": "^17.0.17",\n    "@types/react-plotly.js": "^2.5.0",\n    "@types/react-resizable": "^3.0.3",\n    "@types/react-router": "^5.1.20",\n    "@types/react-router-dom": "^5.3.3",\n    "@types/react-transition-group": "^4.4.4",\n    "@types/stylis": "^4.0.1",\n    "@types/use-sync-external-store": "^0.0.3",\n    "@typescript-eslint/eslint-plugin": "^5.28.0",\n    "@typescript-eslint/parser": "^5.28.0",\n    "@wojtekmaj/enzyme-adapter-react-17": "^0.6.3",\n    "argparse": "^2.0.1",\n    "babel-plugin-formatjs": "^10.2.14",\n    "babel-plugin-react-require": "^3.1.3",\n    "confusing-browser-globals": "^1.0.11",\n    "enzyme": "^3.11.0",\n    "eslint": "^8.25.0",\n    "eslint-config-prettier": "^8.5.0",\n    "eslint-config-standard": "10.2.1",\n    "eslint-import-resolver-webpack": "0.8.4",\n    "eslint-loader": "2.1.1",\n    "eslint-plugin-chai-expect": "1.1.1",\n    "eslint-plugin-chai-friendly": "^0.7.2",\n    "eslint-plugin-cypress": "^2.12.1",\n    "eslint-plugin-flowtype": "^8.0.3",\n    "eslint-plugin-formatjs": "^3.1.5",\n    "eslint-plugin-import": "^2.26.0",\n    "eslint-plugin-jest": "^26.5.3",\n    "eslint-plugin-jsx-a11y": "^6.7.1",\n    "eslint-plugin-no-lookahead-lookbehind-regexp": "^0.1.0",\n    "eslint-plugin-no-only-tests": "^2.6.0",\n    "eslint-plugin-node": "5.2.1",\n    "eslint-plugin-prettier": "^4.0.0",\n    "eslint-plugin-promise": "3.6.0",\n    "eslint-plugin-react": "^7.30.0",\n    "eslint-plugin-react-hooks": "^4.6.0",\n    "eslint-plugin-standard": "3.0.1",\n    "eslint-plugin-testing-library": "^6.1.0",\n    "fast-glob": "^3.2.11",\n    "graphql-codegen-typescript-operation-types": "^2.0.1",\n    "jest-canvas-mock": "^2.2.0",\n    "jest-localstorage-mock": "^2.3.0",\n    "knip": "^5.30.2",\n    "msw": "^1.2.3",\n    "prettier": "^2.8.0",\n    "react-17": "npm:react@^17.0.2",\n    "react-dom-17": "npm:react-dom@^17.0.2",\n    "react-scripts": "5.0.0",\n    "react-test-renderer-17": "npm:react-test-renderer@^17.0.2",\n    "redux-mock-store": "^1.5.3",\n    "resolve": "^1.22.1",\n    "stream-browserify": "^3.0.0",\n    "tsconfig-paths-webpack-plugin": "^4.0.1",\n    "typescript": "^5.8.3",\n    "webpack": "^5.69.0",\n    "whatwg-fetch": "^3.6.17"\n  },\n  "private": true,\n  "engines": {\n    "node": "^22.16.0"\n  },\n  "resolutions": {\n    "@floating-ui/dom@^0.5.3": "patch:@floating-ui/dom@npm%3A0.5.4#yarn/patches/@floating-ui-dom-0.5.4.diff",\n    "@types/react": "^17.0.50",\n    "@emotion/react": "11.11.0",\n    "@types/react-plotly.js/@types/plotly.js": "^1.54.6",\n    "d3-transition": "3.0.1",\n    "react-dev-utils/fork-ts-checker-webpack-plugin": "6.5.3",\n    "postcss-preset-env/autoprefixer": "10.4.5",\n    "rc-virtual-list@^3.2.0": "patch:rc-virtual-list@npm%3A3.2.0#yarn/patches/rc-virtual-list-npm-3.2.0-5efaefc12e.patch",\n    "rc-virtual-list@^3.0.3": "patch:rc-virtual-list@npm%3A3.2.0#yarn/patches/rc-virtual-list-npm-3.2.0-5efaefc12e.patch",\n ', '"react-test-renderer-17": "npm:react-test-renderer@^17.0.2",\n    "redux-mock-store": "^1.5.3",\n    "resolve": "^1.22.1",\n    "stream-browserify": "^3.0.0",\n    "tsconfig-paths-webpack-plugin": "^4.0.1",\n    "typescript": "^5.8.3",\n    "webpack": "^5.69.0",\n    "whatwg-fetch": "^3.6.17"\n  },\n  "private": true,\n  "engines": {\n    "node": "^22.16.0"\n  },\n  "resolutions": {\n    "@floating-ui/dom@^0.5.3": "patch:@floating-ui/dom@npm%3A0.5.4#yarn/patches/@floating-ui-dom-0.5.4.diff",\n    "@types/react": "^17.0.50",\n    "@emotion/react": "11.11.0",\n    "@types/react-plotly.js/@types/plotly.js": "^1.54.6",\n    "d3-transition": "3.0.1",\n    "react-dev-utils/fork-ts-checker-webpack-plugin": "6.5.3",\n    "postcss-preset-env/autoprefixer": "10.4.5",\n    "rc-virtual-list@^3.2.0": "patch:rc-virtual-list@npm%3A3.2.0#yarn/patches/rc-virtual-list-npm-3.2.0-5efaefc12e.patch",\n    "rc-virtual-list@^3.0.3": "patch:rc-virtual-list@npm%3A3.2.0#yarn/patches/rc-virtual-list-npm-3.2.0-5efaefc12e.patch",\n    "rc-virtual-list@^3.0.1": "patch:rc-virtual-list@npm%3A3.2.0#yarn/patches/rc-virtual-list-npm-3.2.0-5efaefc12e.patch"\n  },\n  "//": "homepage is hard to configure without resorting to env variables and doesn\'t play nicely with other webpack settings. This field should be removed.",\n  "homepage": "static-files",\n  "browserslist": [\n    "defaults"\n  ],\n  "babel": {\n    "env": {\n      "test": {\n        "plugins": [\n          [\n            "babel-plugin-formatjs",\n            {\n              "idInterpolationPattern": "[sha512:contenthash:base64:6]",\n              "removeDefaultMessage": false\n            }\n          ]\n        ]\n      }\n    }\n  }\n}\n', '{\n  "short_name": "React App",\n  "name": "Create React App Sample",\n  "icons": [\n    {\n      "src": "favicon.ico",\n      "sizes": "64x64 32x32 24x24 16x16",\n      "type": "image/x-icon"\n    }\n  ],\n  "start_url": "./index.html",\n  "display": "standalone",\n  "theme_color": "#000000",\n  "background_color": "#ffffff"\n}\n', '{\n  "columns": [\n    "company_name",\n    "company_goal",\n    "prompt",\n    "output",\n    "MLFLOW_model",\n    "MLFLOW_route_type",\n    "MLFLOW_latency"\n  BROKEN', '{\n  "columns": [\n    "company_name",\n    "company_goal",\n    "prompt",\n    "output",\n    "MLFLOW_model",\n    "MLFLOW_route_type",\n    "MLFLOW_latency"\n  ],\n  "data": [\n    [\n      "Abc",\n      "bottom line revenue",\n      "You are a marketing consultant for a technology company. Develop a marketing strategy report for Abc aiming to bottom line revenue",\n      " Here is an outline for a marketing strategy report aimed at increasing bottom line revenue for the technology company Abc:\\n\\nIntroduction \\n- Brief background on Abc - products/services, target markets, competitive landscape\\n- Objective of report: Provide recommendations to increase bottom line revenue through marketing strategies\\n\\nMarket Analysis\\n- Trends in Abc\'s industry and target markets\\n- Customer analysis - demographics, psychographics, buying behavior \\n- Competitor analysis - positioning, pricing,",\n      "",\n      "",\n      "7788.29999999702"\n    ],\n    [\n      "XYZ Company",\n      "Increase top-line revenue",\n      "You are a marketing consultant for a technology company. Develop a marketing strategy report for XYZ Company aiming to Increase top-line revenue",\n      " Here is an outline for a marketing strategy report aimed at increasing top-line revenue for XYZ Company:\\n\\nXYZ Company \\nMarketing Strategy Report\\n\\nExecutive Summary\\n- Brief overview of key recommendations to increase revenue \\n\\nCurrent Situation Analysis\\n- Background on XYZ Company\'s products/services, target customers, competitive landscape\\n- Analysis of current marketing efforts and sales performance \\n\\nOpportunities for Growth \\n- New customer segments to target\\n- Additional products/services to meet customer needs",\n      "claude-2",\n      "llm/v1/completions",\n      "11563.60000000149"\n    ]\n  ]\n}\n', '{\n  "columns": [0, 1],\n  "data": [\n    ["a", "b"],\n    [1, 2]\n  ]\n}\n', '{\n  "current": {\n    "path": "/static/lib/ml-model-trace-renderer/index.html"\n  },\n  "2": {\n    "path": "/static/lib/ml-model-trace-renderer/2/index.html",\n    "commit": "93d4afc7a2e876f74cbba56c2db3d05edc91e872"\n  },\n  "oss": {\n    "path": "/static/lib/ml-model-trace-renderer/oss/index.html",\n    "commit": "b5595f5c6263c1c8e3614d85eb1d233d28789bb9"\n  },\n  "3": {\n    "path": "/static/lib/ml-model-trace-renderer/3/index.html",\n    "commit": "93d4afc7a2e876f74cbba56c2db3d05edc91e872"\n  }\n}\n', '{\n  "bd263e2b04b04460a40c1acae72a18ae": {\n    "metric_1": -2.5797830282214,\n    "metric_0": -2.434975966906267,\n    "metric_3": -0.9688077263066934,\n    "metric_2": -1.4438003481072212\n  },\n  "55461e2180fb40338072c04ff86fd0f9": {\n    "metric_1": -2.2857451912150792,\n    "metric_0": 3.4173603047073176,\n    "metric_3": -0.24019895935855473,\n    "metric_2": -0.7097425052930393\n  },\n  "123810810b234e9b8b97fb1e00abd9aa": {\n    "metric_1": 0.7308706035999548,\n    "metric_0": 3.0994891921059544,\n    "metric_3": 1.9819820891007573,\n    "metric_2": 0.48569560278784785\n  },\n  "66fbc3c813944c1a80d2336849c6e72f": {\n    "metric_1": -2.672718621393841,\n    "metric_0": -1.7902590711838267,\n    "metric_3": 2.477982822663786,\n    "metric_2": 1.273023064731822\n  },\n  "83698fafa8714bd3929b9c38bf6cdee8": {\n    "metric_1": -2.292131339453693,\n    "metric_0": 3.3064205155126096,\n    "metric_3": -0.46183104891365634,\n    "metric_2": 0.8465206209214458\n  },\n  "79461e9d7aa24e18a626f61b047315c8": {\n    "metric_1": -0.006443567706641673,\n    "metric_0": 1.8136489475666746,\n    "metric_3": 2.6700440103809013,\n    "metric_2": 0.1556304999295106\n  },\n  "6870761df41f4350adbd37f2a18eb641": {\n    "metric_1": -0.24614682477958416,\n    "metric_0": -1.5486858485543848,\n    "metric_3": -2.742733532695466,\n    "metric_2": 3.0344898094132358\n  },\n  "eb124e3ef9c04109a65372aab4222307": {\n    "metric_1": -1.9754772100389224,\n    "metric_0": -0.6234240819980461,\n    "metric_3": -2.4020972270978844,\n    "metric_2": 2.7962318576455436\n  },\n  "39c152b25fb04c08b3cf4a4c8ebccb7c": {\n    "metric_1": -2.149649101035413,\n    "metric_0": 0.7310583410679579,\n    "metric_3": 1.1244662209560552,\n    "metric_2": -2.0257045260054656\n  },\n  "1b92583de96c4fd88ff5b04e866aff8c": {\n    "metric_1": -0.02959618784025775,\n    "metric_0": -0.2931365711894838,\n    "metric_3": 3.1567281048311546,\n    "metric_2": 2.9203148639651495\n  },\n  "3c28149e5ab44efb8d7f4b3c11ab0cb1": {\n    "metric_1": -2.0944626407865288,\n    "metric_0": 3.779234079600906,\n    "metric_3": -2.9467929849482024,\n    "metric_2": -1.4704105222912913\n  },\n  "1afafa10e69a4083bb88a96b23547b7b": {\n    "metric_1": -0.10343236992763893,\n    "metric_0": 2.28372065230554,\n    "metric_3": 2.0481770225881615,\n    "metric_2": -1.1601122975618\n  },\n  "5f9c5e48ff844c0498199b390ca9c1a4": {\n    "metric_1": 2.4422641299267553,\n    "metric_0": -1.8107502356154743,\n    "metric_3": 3.6679213677343423,\n    "metric_2": -1.8061767363300147\n  },\n  "0ffbde81192e43a48482434219cc4458": {\n    "metric_1": -1.5559004670240322,\n    "metric_0": 0.8721310864910716,\n    "metric_3": 2.5822778193072846,\n    "metric_2": 0.6969033758109711\n  ', '},\n  "3c28149e5ab44efb8d7f4b3c11ab0cb1": {\n    "metric_1": -2.0944626407865288,\n    "metric_0": 3.779234079600906,\n    "metric_3": -2.9467929849482024,\n    "metric_2": -1.4704105222912913\n  },\n  "1afafa10e69a4083bb88a96b23547b7b": {\n    "metric_1": -0.10343236992763893,\n    "metric_0": 2.28372065230554,\n    "metric_3": 2.0481770225881615,\n    "metric_2": -1.1601122975618\n  },\n  "5f9c5e48ff844c0498199b390ca9c1a4": {\n    "metric_1": 2.4422641299267553,\n    "metric_0": -1.8107502356154743,\n    "metric_3": 3.6679213677343423,\n    "metric_2": -1.8061767363300147\n  },\n  "0ffbde81192e43a48482434219cc4458": {\n    "metric_1": -1.5559004670240322,\n    "metric_0": 0.8721310864910716,\n    "metric_3": 2.5822778193072846,\n    "metric_2": 0.6969033758109711\n  },\n  "f9b1cb0a470e4f9b98bd4d36a51e4d31": {\n    "metric_1": 2.9364188950633814,\n    "metric_0": -0.2344822065779013,\n    "metric_3": 3.8849138907360397,\n    "metric_2": -2.524561921321326\n  },\n  "99d7cdc4d77f4103937bbb9a70c5d4c8": {\n    "metric_1": 0.8548586864241012,\n    "metric_0": 0.9891059238008051,\n    "metric_3": 1.7255056515257134,\n    "metric_2": -0.958042549612474\n  },\n  "f662d9fdd072422899f1a91dca132e45": {\n    "metric_eq_ts_step": 4.7\n  },\n  "8c27fb8d3d734fc988eafc7e130af2c1": {\n    "metric_1": 0.2568792149591985,\n    "metric_0": -2.9251778063945633,\n    "metric_3": 3.5795601350695536,\n    "metric_2": 2.2358743640336654\n  },\n  "46830b05f4914ce980ee960921474308": {\n    "metric_1": -2.4512019495254855,\n    "metric_0": -1.7363712773941686,\n    "metric_3": -0.6764695293332332,\n    "metric_2": 0.902757467400038\n  },\n  "2bf6a4001dda47a89bd0dd1b638900c8": {\n    "metric_1": 1.4881757006157033,\n    "metric_0": -2.184521948675489,\n    "metric_3": 1.5024111371215891,\n    "metric_2": -0.9992038125180369\n  },\n  "bf3f29f1c16741e8b2de46b8af7f26db": {\n    "metric_1": -2.856136683397856,\n    "metric_0": 1.7351206329209488,\n    "metric_3": 3.3574106252540243,\n    "metric_2": 1.898369877601997\n  },\n  "8a56867ed3af4ead84ac5c215c43d2f2": {\n    "metric_1": -1.6845465954407057,\n    "metric_0": 2.799113454790449,\n    "metric_3": -1.9554854183785264,\n    "metric_2": 1.2596708758008042\n  },\n  "0e77b745f78c4985825c66aafb15f45e": {\n    "metric_1": -0.5966921249245285,\n    "metric_0": 0.38303481722942134,\n    "metric_3": 1.4751754578912797,\n    "metric_2": -2.935488516752223\n  },\n  "f8a6361f177a40b291751d743ec0cd52": {\n    "metric_1": -2.52066603748033,\n    "metric_0": -0.7281695813441598,\n    "metric_3": 0.3529736105779344,\n    "metric_2": -1.603741841379303\n  },\n  "8c6d7c99dc014b15b5ccec784110b83f": {\n    "metric_1": -1.2231009149094496,\n    "metric_0": 1.4079626574511428,\n  ', '  "metric_3": 3.3574106252540243,\n    "metric_2": 1.898369877601997\n  },\n  "8a56867ed3af4ead84ac5c215c43d2f2": {\n    "metric_1": -1.6845465954407057,\n    "metric_0": 2.799113454790449,\n    "metric_3": -1.9554854183785264,\n    "metric_2": 1.2596708758008042\n  },\n  "0e77b745f78c4985825c66aafb15f45e": {\n    "metric_1": -0.5966921249245285,\n    "metric_0": 0.38303481722942134,\n    "metric_3": 1.4751754578912797,\n    "metric_2": -2.935488516752223\n  },\n  "f8a6361f177a40b291751d743ec0cd52": {\n    "metric_1": -2.52066603748033,\n    "metric_0": -0.7281695813441598,\n    "metric_3": 0.3529736105779344,\n    "metric_2": -1.603741841379303\n  },\n  "8c6d7c99dc014b15b5ccec784110b83f": {\n    "metric_1": -1.2231009149094496,\n    "metric_0": 1.4079626574511428,\n    "metric_3": 2.5892028641452907,\n    "metric_2": 3.643033981543657\n  },\n  "9af4b84d78524c4ab08161e7b5f7f2dc": {\n    "metric_1": 0.2702968551842675,\n    "metric_0": 0.11248586282952555,\n    "metric_3": -1.533809108651962,\n    "metric_2": 1.678493181317803\n  }\n}\n', '{\n  "kube-context": "docker-for-desktop",\n  "kube-job-template-path": "examples/docker/kubernetes_job_template.yaml",\n  "repository-uri": "username/mlflow-kubernetes-example"\n}\n', '{ "messages": [{ "role": "user", "content": "What is Retrieval-augmented Generation?" }] }\n', 'build_dependencies:\n  - pip\ndependencies:\n  - diviner\n  - mlflow>=1.24.1\n', 'apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: "{replaced with MLflow Project name}"\n  namespace: mlflow\nspec:\n  ttlSecondsAfterFinished: 100\n  backoffLimit: 0\n  template:\n    spec:\n      containers:\n        - name: "{replaced with MLflow Project name}"\n          image: "{replaced with URI of Docker image created during Project execution}"\n          command: ["{replaced with MLflow Project entry point command}"]\n          resources:\n            limits:\n              memory: 512Mi\n            requests:\n              memory: 256Mi\n      restartPolicy: Never\n', 'build_dependencies:\n  - pip==22.2.2\ndependencies:\n  - mlflow>=1.6\n  - pandas==1.5.0\n  - scikit-learn==1.1.3\n  - tensorflow==2.10.0\n  - pillow==9.2.0\n', 'endpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: ai21labs\n      name: j2-mid\n      config:\n        ai21labs_api_key: $AI21LABS_API_KEY\n', 'endpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: anthropic\n      name: claude-1.3-100k\n      config:\n        anthropic_api_key: $ANTHROPIC_API_KEY\n', 'endpoints:\n  - name: chat\n    endpoint_type: llm/v1/chat\n    model:\n      provider: openai\n      name: gpt-4o-mini\n      config:\n        openai_api_type: "azure"\n        openai_api_key: $OPENAI_API_KEY\n        openai_deployment_name: "{your_deployment_name}"\n        openai_api_base: "https://{your_resource_name}-azureopenai.openai.azure.com/"\n        openai_api_version: "2023-05-15"\n\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: openai\n      name: gpt-4o-mini\n      config:\n        openai_api_type: "azuread"\n        openai_api_key: $AZURE_AAD_TOKEN\n        openai_deployment_name: "{your_deployment_name}"\n        openai_api_base: "https://{your_resource_name}-azureopenai.openai.azure.com/"\n        openai_api_version: "2023-05-15"\n\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: openai\n      name: text-embedding-ada-002\n      config:\n        openai_api_type: "azure"\n        openai_api_key: $OPENAI_API_KEY\n        openai_deployment_name: "{your_deployment_name}"\n        openai_api_base: "https://{your_resource_name}-azureopenai.openai.azure.com/"\n        openai_api_version: "2023-05-15"\n', 'endpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: amazon-bedrock\n      name: amazon.titan-tg1-large\n      config:\n        aws_config:\n          aws_region: us-east-1\n          aws_access_key_id: $AWS_ACCESS_KEY_ID\n          aws_secret_access_key: $AWS_SECRET_ACCESS_KEY\n', 'endpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: cohere\n      name: command\n      config:\n        cohere_api_key: $COHERE_API_KEY\n\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: cohere\n      name: embed-english-light-v2.0\n      config:\n        cohere_api_key: $COHERE_API_KEY\n', 'endpoints:\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: gemini\n      name: gemini-embedding-exp-03-07\n      config:\n        gemini_api_key: $GEMINI_API_KEY\n\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: gemini\n      name: gemini-2.0-flash\n      config:\n        gemini_api_key: $GEMINI_API_KEY\n\n  - name: chat\n    endpoint_type: llm/v1/chat\n    model:\n      provider: gemini\n      name: gemini-2.0-flash\n      config:\n        gemini_api_key: $GEMINI_API_KEY\n', 'endpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: "huggingface-text-generation-inference"\n      name: falcon-7b-instruct\n      config:\n        hf_server_url: http://127.0.0.1:8080\n', 'endpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: mistral\n      name: mistral-tiny\n      config:\n        mistral_api_key: $MISTRAL_API_KEY\n\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: mistral\n      name: mistral-embed\n      config:\n        mistral_api_key: $MISTRAL_API_KEY\n', 'endpoints:\n  - name: fillmask\n    endpoint_type: llm/v1/completions\n    model:\n      provider: mlflow-model-serving\n      name: mask-fill\n      config:\n        model_server_url: http://127.0.0.1:9010\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: mlflow-model-serving\n      name: sentence-transformer\n      config:\n        model_server_url: http://127.0.0.1:9020\n', 'endpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: mosaicml\n      name: mpt-7b-instruct\n      config:\n        mosaicml_api_key: $MOSAICML_API_KEY\n\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: mosaicml\n      name: instructor-xl\n      config:\n        mosaicml_api_key: $MOSAICML_API_KEY\n\n  - name: chat\n    endpoint_type: llm/v1/chat\n    model:\n      provider: mosaicml\n      name: llama2-70b-chat\n      config:\n        mosaicml_api_key: $MOSAICML_API_KEY\n', 'endpoints:\n  - name: chat\n    endpoint_type: llm/v1/chat\n    model:\n      provider: openai\n      name: gpt-4o-mini\n      config:\n        openai_api_key: $OPENAI_API_KEY\n    limit:\n      renewal_period: minute\n      calls: 10\n\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: openai\n      name: gpt-4o-mini\n      config:\n        openai_api_key: $OPENAI_API_KEY\n\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: openai\n      name: text-embedding-ada-002\n      config:\n        openai_api_key: $OPENAI_API_KEY\n', 'endpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: palm\n      name: text-bison-001\n      config:\n        palm_api_key: $PALM_API_KEY\n\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: palm\n      name: embedding-gecko-001\n      config:\n        palm_api_key: $PALM_API_KEY\n\n  - name: chat\n    endpoint_type: llm/v1/chat\n    model:\n      provider: palm\n      name: chat-bison-001\n      config:\n        palm_api_key: $PALM_API_KEY\n', 'endpoints:\n  - name: chat\n    endpoint_type: llm/v1/chat\n    model:\n      provider: my_llm\n      name: my-model-0.1.2\n      config:\n        my_llm_api_key: $MY_LLM_API_KEY\n', 'endpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: togetherai\n      name: mistralai/Mixtral-8x7B-v0.1\n      config:\n        togetherai_api_key: $TOGETHERAI_API_KEY\n\n  - name: chat\n    endpoint_type: llm/v1/chat\n    model:\n      provider: togetherai\n      name: mistralai/Mixtral-8x7B-Instruct-v0.1\n      config:\n        togetherai_api_key: $TOGETHERAI_API_KEY\n\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: togetherai\n      name: togethercomputer/m2-bert-80M-8k-retrieval\n      config:\n        togetherai_api_key: $TOGETHERAI_API_KEY\n', 'build_dependencies:\n  - pip\ndependencies:\n  - h2o\n  - mlflow>=1.0\n  - numpy\n  - pandas\n', 'build_dependencies:\n  - pip\ndependencies:\n  - numpy\n  - click\n  - pandas\n  - scipy\n  - scikit-learn\n  - tensorflow==2.10.0\n  - matplotlib\n  - mlflow>=1.6\n  - hyperopt\n  - protobuf<4.0.0\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow>=1.6.0\n  - matplotlib\n  - lightgbm\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow>=1.6.0\n  - matplotlib\n  - lightgbm\n  - cloudpickle>=2.0.0\n', 'python: "3.10"\nbuild_dependencies:\n  - pip\ndependencies:\n  - openai>=0.27.2\n  - tiktoken>=0.4.0\n  - tenacity>=8.2.2\n  - mlflow>=2.4.0\n', 'python: "3.10"\nbuild_dependencies:\n  - pip\ndependencies:\n  - langchain>=0.0.244\n  - openai>=0.27.2\n  - evaluate>=0.4.0\n  - mlflow>=2.4.0\n  - tiktoken>=0.4.0\n', 'build_dependencies:\n  - pip\ndependencies:\n  - tensorflow==1.15.2\n  - keras==2.2.4\n  - mlflow>=1.0\n  - pyspark\n  - requests\n  - click\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - paddlepaddle==2.1.0\n  - cloudpickle==1.6.0\n', 'build_dependencies:\n  - pip\ndependencies:\n  - pmdarima\n  - mlflow>=1.23.1\n', '$schema: https://azuremlschemas.azureedge.net/promptflow/latest/Flow.schema.json\n\ninputs:\n  text:\n    type: string\n    default: Hello World!\n\noutputs:\n  output:\n    type: string\n    reference: ${llm.output}\n\nnodes:\n  - name: hello_prompt\n    type: python\n    source:\n      type: code\n      path: render_template.py\n    inputs:\n      text: ${inputs.text}\n      template: |\n        system:\n        Your task is to generate what I ask.\n        user:\n        Write a simple {{text}} program that displays the greeting message.\n  - name: llm\n    type: python\n    source:\n      type: code\n      path: hello.py\n    inputs:\n      prompt: ${hello_prompt.output}\n      deployment_name: gpt-4o-mini\n      max_tokens: "120"\nenvironment:\n  image: mcr.microsoft.com/azureml/promptflow/promptflow-runtime:latest\n  python_requirements_txt: requirements.txt\n', 'build_dependencies:\n  - pip\ndependencies:\n  - prophet>=1.0.1\n', 'build_dependencies:\n  - pip\ndependencies:\n  - torch\n  - torchvision\n  - mlflow\n  - tensorboardX\n', 'channels:\n  - conda-forge\ndependencies:\n  - python=3.8.2\n  - pip\n  - pip:\n      - mlflow\n      - lightning==2.0.0\n      - jsonargparse[signatures]>=4.17.0\n      # typeguard is used for type validation in the ax-platform code base. 3.0.0 release has\n      # breaking changes that need to be resolved in ax. Remove this pin when\n      # https://github.com/facebook/Ax/issues/1509 is addressed\n      - typeguard<3.0.0\n      - ax-platform\n      - torchvision>=0.15.1\n      - torch>=2.0\n      # gyptorch 1.9.x is incompatible with the versions of botorch\n      # required by many versions of pytorch\n      - gpytorch<1.9.0\n      - protobuf<4.0.0\n      # Pinning pandas version less than 1.4.4 due to https://github.com/facebook/Ax/issues/1153\n      - pandas<=1.4.4\n      # Numpy>=2 is not compatible with pandas<=1.4.4\n      - numpy<2\n      # TODO: Remove this requirement once ax-platform achieves compatibility with SQLAlchemy 2.x\n      - sqlalchemy<2\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - lightning==2.0.0\n  - jsonargparse[signatures]>=4.17.0\n  # typeguard is used for type validation in the ax-platform code base. 3.0.0 release has\n  # breaking changes that need to be resolved in ax. Remove this pin when\n  # https://github.com/facebook/Ax/issues/1509 is addressed\n  - typeguard<3.0.0\n  - ax-platform\n  - torchvision>=0.15.1\n  - torch>=2.0\n  # gyptorch 1.9.x is incompatible with the versions of botorch\n  # required by many versions of pytorch\n  - gpytorch<1.9.0\n  - protobuf<4.0.0\n  # Pinning pandas version less than 1.4.4 due to https://github.com/facebook/Ax/issues/1153\n  - pandas<=1.4.4\n  # Numpy>=2 is not compatible with pandas<=1.4.4\n  - numpy<2\n  # TODO: Remove this requirement once ax-platform achieves compatibility with SQLAlchemy 2.x\n  - sqlalchemy<2\n', 'channels:\n  - conda-forge\ndependencies:\n  - python=3.8.2\n  - pip\n  - pip:\n      - mlflow\n      - scikit-learn\n      - cloudpickle==1.6.0\n      - boto3\n      - transformers>=4.0.0\n      - pandas\n      - numpy<2.0\n      - torch>=2.0.0\n      - torchdata>=0.6.0\n      - torchtext==0.15.1\n      - lightning==2.0.0\n      - jsonargparse[signatures]>=4.17.0\n      - protobuf<4.0.0\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - scikit-learn\n  - cloudpickle==1.6.0\n  - boto3\n  - transformers>=4.0.0\n  - pandas\n  - numpy<2.0\n  - torch>=2.0\n  - torchdata\n  - torchtext==0.16.2\n  - lightning\n  - jsonargparse[signatures]>=4.17.0\n  - protobuf<4.0.0\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - pandas\n  - scipy\n  - captum\n  - boto3\n  - scikit-learn\n  - prettytable\n  - ipython\n  - torch\n', 'channels:\n  - conda-forge\ndependencies:\n  - python=3.8.2\n  - pip\n  - pip:\n      - mlflow\n      - torchvision>=0.15.1\n      - cloudpickle==1.6.0\n      - lightning==2.0.0\n      - jsonargparse[signatures]>=4.17.0\n      # typeguard is used for type validation in the ax-platform code base. 3.0.0 release has\n      # breaking changes that need to be resolved in ax. Remove this pin when\n      # https://github.com/facebook/Ax/issues/1509 is addressed\n      - typeguard<3.0.0\n      - ax-platform\n      - prettytable\n      - torch>=2.0\n      - protobuf<4.0.0\n      # gyptorch 1.9.x is incompatible with the versions of botorch\n      # required by many versions of pytorch\n      - gpytorch<1.9.0\n      # Pinning pandas version less than 1.4.4 due to https://github.com/facebook/Ax/issues/1153\n      - pandas<=1.4.4\n      # Numpy>=2 is not compatible with pandas<=1.4.4\n      - numpy<2\n      # ax-platform 0.2.x is not yet compatible with SQLAlchemy 2.x\n      # TODO: Remove this requirement once ax-platform achieves compatibility with SQLAlchemy 2.x\n      - sqlalchemy<2\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - torchvision>=0.15.1\n  - cloudpickle==1.6.0\n  - lightning==2.0.0\n  - jsonargparse[signatures]>=4.17.0\n  # typeguard is used for type validation in the ax-platform code base. 3.0.0 release has\n  # breaking changes that need to be resolved in ax. Remove this pin when\n  # https://github.com/facebook/Ax/issues/1509 is addressed\n  - typeguard<3.0.0\n  - ax-platform\n  - prettytable\n  - torch>=2.0\n  # gyptorch 1.9.x is incompatible with the versions of botorch\n  # required by many versions of pytorch\n  - gpytorch<1.9.0\n  - protobuf<4.0.0\n  # Pinning pandas version less than 1.4.4 due to https://github.com/facebook/Ax/issues/1153\n  - pandas<=1.4.4\n  # Numpy>=2 is not compatible with pandas<=1.4.4\n  - numpy<2\n  # ax-platform 0.2.x is not yet compatible with SQLAlchemy 2.x\n  # TODO: Remove this requirement once ax-platform achieves compatibility with SQLAlchemy 2.x\n  - sqlalchemy<2\n', 'channels:\n  - conda-forge\ndependencies:\n  - python=3.8.2\n  - pip\n  - pip:\n      - mlflow\n      - torchvision>=0.15.1\n      - torch>=2.0\n      - lightning==2.0.0\n      - jsonargparse[signatures]>=4.17.0\n      - protobuf<4.0.0\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - torchvision>=0.15.1\n  - torch>=2.0\n  - lightning==2.0.0\n  - jsonargparse[signatures]>=4.17.0\n  - protobuf<4.0.0\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - torch==1.8.0\n  - torchvision==0.9.1\n  - pytorch-lightning==1.0.2\n', 'build_dependencies:\n  - pip\ndependencies:\n  - scikit-learn\n  - cloudpickle==1.6.0\n  - boto3\n  - torchvision>=0.9.1\n  - torch>=1.9.0\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - cloudpickle==1.6.0\n  - boto3\n  - torchvision>=0.9.1\n  - torch>=1.9.0\n', 'name: mlflow\nchannels:\n  - rapidsai\n  - nvidia\n  - conda-forge\n  - defaults\ndependencies:\n  - _libgcc_mutex=0.1=conda_forge\n  - _openmp_mutex=4.5=1_llvm\n  - arrow-cpp=0.15.0=py37h090bef1_2\n  - bokeh=2.1.0=py37hc8dfbb8_0\n  - boost-cpp=1.70.0=h8e57a91_2\n  - brotli=1.0.7=he1b5a44_1002\n  - bzip2=1.0.8=h516909a_2\n  - c-ares=1.15.0=h516909a_1001\n  - ca-certificates=2020.4.5.2=hecda079_0\n  - certifi=2020.4.5.2=py37hc8dfbb8_0\n  - click=7.1.2=pyh9f0ad1d_0\n  - cloudpickle=1.4.1=py_0\n  - cudatoolkit=10.2.89=h6bb024c_0\n  - cudf=0.14.0=py37_0\n  - cudnn=7.6.5=cuda10.2_0\n  - cuml=0.14.0=cuda10.2_py37_0\n  - cupy=7.5.0=py37h940342b_0\n  - cytoolz=0.10.1=py37h516909a_0\n  - dask=2.18.1=py_0\n  - dask-core=2.18.1=py_0\n  - dask-cudf=0.14.0=py37_0\n  - distributed=2.18.0=py37hc8dfbb8_0\n  - dlpack=0.2=he1b5a44_1\n  - double-conversion=3.1.5=he1b5a44_2\n  - fastavro=0.23.4=py37h8f50634_0\n  - fastrlock=0.5=py37h3340039_0\n  - freetype=2.10.2=he06d7ca_0\n  - fsspec=0.7.4=py_0\n  - gflags=2.2.2=he1b5a44_1002\n  - glog=0.4.0=h49b9bf7_3\n  - grpc-cpp=1.23.0=h18db393_0\n  - heapdict=1.0.1=py_0\n  - icu=64.2=he1b5a44_1\n  - jinja2=2.11.2=pyh9f0ad1d_0\n  - joblib=0.15.1=py_0\n  - jpeg=9d=h516909a_0\n  - ld_impl_linux-64=2.33.1=h53a641e_7\n  - libblas=3.8.0=16_openblas\n  - libcblas=3.8.0=16_openblas\n  - libcudf=0.14.0=cuda10.2_0\n  - libcuml=0.14.0=cuda10.2_0\n  - libcumlprims=0.14.1=cuda10.2_0\n  - libedit=3.1.20181209=hc058e9b_0\n  - libevent=2.1.10=h72c5cf5_0\n  - libffi=3.3=he6710b0_1\n  - libgcc-ng=9.2.0=h24d8f2e_2\n  - libgfortran-ng=7.5.0=hdf63c60_6\n  - libhwloc=2.1.0=h3c4fd83_0\n  - libiconv=1.15=h516909a_1006\n  - liblapack=3.8.0=16_openblas\n  - libllvm8=8.0.1=hc9558a2_0\n  - libnvstrings=0.14.0=cuda10.2_0\n  - libopenblas=0.3.9=h5ec1e0e_0\n  - libpng=1.6.37=hed695b0_1\n  - libprotobuf=3.8.0=h8b12597_0\n  - librmm=0.14.0=cuda10.2_0\n  - libstdcxx-ng=9.1.0=hdf63c60_0\n  - libtiff=4.1.0=hfc65ed5_0\n  - libxml2=2.9.10=hee79883_0\n  - llvm-openmp=10.0.0=hc9558a2_0\n  - llvmlite=0.32.1=py37h5202443_0\n  - locket=0.2.0=py_2\n  - lz4-c=1.8.3=he1b5a44_1001\n  - markupsafe=1.1.1=py37h8f50634_1\n  - msgpack-python=1.0.0=py37h99015e2_1\n  - nccl=2.6.4.1=hc6a2c23_0\n  - ncurses=6.2=he6710b0_1\n  - numba=0.49.1=py37h0da4684_0\n  - numpy=1.17.5=py37h95a1406_0\n  - nvstrings=0.14.0=py37_0\n  - olefile=0.46=py_0\n  - openssl=1.1.1g=h516909a_0\n  - packaging=20.4=pyh9f0ad1d_0\n  - pandas=0.25.3=py37hb3f55d8_0\n  - parquet-cpp=1.5.1=2\n  - partd=1.1.0=py_0\n  - pillow=5.3.0=py37h00a061d_1000\n  - pip=20.1.1=py37_1\n  - psutil=5.7.0=py37h8f50634_1\n  - pyarrow=0.15.0=py37h8b68381_1\n  - pyparsing=2.4.7=pyh9f0ad1d_0\n  - python=3.8.13=h12debd9_0\n  - python-dateutil=2.8.1=py_0\n  - python_abi=3.8=2_cp38\n  - pytz=2020.1=pyh9f0ad1d_0\n  - pyyaml=5.3.1=py37h8f50634_0\n  - re2=2020.04.01=he1b5a44_0\n  - readline=8.0=h7b6447c_0\n  - rmm=0.14.0=py37_0\n  - setuptools=47.3.0=py37_0\n  - six=1.15.0=pyh9f0ad1d_0\n  - snappy=1.1.8=he1b5a44_2\n  - sortedcontainers=2.2.2=pyh9f0ad1d_0\n  - spdlog=1.6.1=hc9558a2_0\n  - sqlite=3.31.1=h62c20be_1\n  - tblib=1.6.0=py_0\n  - thrift-cpp=0.12.0=hf3afdfd_1004\n  - tk=8.6.8=hbc83047_0\n  - toolz=0.10.0=py_0\n  - tornado=6.0.4=py37h8f50634_1\n  - typing_extensions=3.7.4.2=py_0\n  - ucx=1.8.0+gf6ec8d4=cuda10.2_20\n  - ucx-py=0.14.0+gf6ec8d4=py37_0\n  - uriparser=0.9.3=he1b5a44_1\n  - wheel=0.34.2=py37_0\n  - xz=5.2.5=h7b6447c_0\n  - yaml=0.2.5=h516909a_0\n  - zict=2.0.0=py_0\n  - zlib=1.2.11=h7b6447c_3\n  - zstd=1.4.3=h3b9ef0a_0\n  - pip:\n    ', 'psutil=5.7.0=py37h8f50634_1\n  - pyarrow=0.15.0=py37h8b68381_1\n  - pyparsing=2.4.7=pyh9f0ad1d_0\n  - python=3.8.13=h12debd9_0\n  - python-dateutil=2.8.1=py_0\n  - python_abi=3.8=2_cp38\n  - pytz=2020.1=pyh9f0ad1d_0\n  - pyyaml=5.3.1=py37h8f50634_0\n  - re2=2020.04.01=he1b5a44_0\n  - readline=8.0=h7b6447c_0\n  - rmm=0.14.0=py37_0\n  - setuptools=47.3.0=py37_0\n  - six=1.15.0=pyh9f0ad1d_0\n  - snappy=1.1.8=he1b5a44_2\n  - sortedcontainers=2.2.2=pyh9f0ad1d_0\n  - spdlog=1.6.1=hc9558a2_0\n  - sqlite=3.31.1=h62c20be_1\n  - tblib=1.6.0=py_0\n  - thrift-cpp=0.12.0=hf3afdfd_1004\n  - tk=8.6.8=hbc83047_0\n  - toolz=0.10.0=py_0\n  - tornado=6.0.4=py37h8f50634_1\n  - typing_extensions=3.7.4.2=py_0\n  - ucx=1.8.0+gf6ec8d4=cuda10.2_20\n  - ucx-py=0.14.0+gf6ec8d4=py37_0\n  - uriparser=0.9.3=he1b5a44_1\n  - wheel=0.34.2=py37_0\n  - xz=5.2.5=h7b6447c_0\n  - yaml=0.2.5=h516909a_0\n  - zict=2.0.0=py_0\n  - zlib=1.2.11=h7b6447c_3\n  - zstd=1.4.3=h3b9ef0a_0\n  - pip:\n      - alembic==1.4.2\n      - attrs==19.3.0\n      - backcall==0.2.0\n      - bleach==3.1.5\n      - chardet==3.0.4\n      - cycler==0.10.0\n      - databricks-cli==0.11.0\n      - decorator==4.4.2\n      - defusedxml==0.6.0\n      - docker==4.2.1\n      - entrypoints==0.3\n      - flask==1.1.2\n      - future==0.18.2\n      - gitdb==4.0.5\n      - gitpython==3.1.3\n      - gorilla==0.3.0\n      - gunicorn==20.0.4\n      - hyperopt==0.2.4\n      - idna==2.9\n      - importlib-metadata==1.6.1\n      - ipykernel==5.3.0\n      - ipython==7.15.0\n      - ipython-genutils==0.2.0\n      - ipywidgets==7.5.1\n      - itsdangerous==1.1.0\n      - jedi==0.17.0\n      - json5==0.9.5\n      - jsonschema==3.2.0\n      - jupyter==1.0.0\n      - jupyter-client==6.1.3\n      - jupyter-console==6.1.0\n      - jupyter-core==4.6.3\n      - jupyterlab==2.1.4\n      - jupyterlab-server==1.1.5\n      - kiwisolver==1.2.0\n      - lab==6.0\n  ', 'ipython==7.15.0\n      - ipython-genutils==0.2.0\n      - ipywidgets==7.5.1\n      - itsdangerous==1.1.0\n      - jedi==0.17.0\n      - json5==0.9.5\n      - jsonschema==3.2.0\n      - jupyter==1.0.0\n      - jupyter-client==6.1.3\n      - jupyter-console==6.1.0\n      - jupyter-core==4.6.3\n      - jupyterlab==2.1.4\n      - jupyterlab-server==1.1.5\n      - kiwisolver==1.2.0\n      - lab==6.0\n      - mako==1.1.3\n      - matplotlib==3.2.2\n      - mistune==0.8.4\n      - mlflow==1.8.0\n      - nbconvert==5.6.1\n      - nbformat==5.0.7\n      - networkx==2.4\n      - notebook==6.0.3\n      - pandocfilters==1.4.2\n      - parso==0.7.0\n      - pexpect==4.8.0\n      - pickleshare==0.7.5\n      - prometheus-client==0.8.0\n      - prometheus-flask-exporter==0.14.1\n      - prompt-toolkit==3.0.5\n      - protobuf==3.12.2\n      - ptyprocess==0.6.0\n      - pygments==2.6.1\n      - pyrsistent==0.16.0\n      - python-editor==1.0.4\n      - pyzmq==19.0.1\n      - qtconsole==4.7.4\n      - qtpy==1.9.0\n      - querystring-parser==1.2.4\n      - requests==2.24.0\n      - scikit-learn==0.23.1\n      - scipy==1.4.1\n      - send2trash==1.5.0\n      - simplejson==3.17.0\n      - sklearn==0.0\n      - smmap==3.0.4\n      - sqlalchemy==1.3.13\n      - sqlparse==0.4.2\n      - tabulate==0.8.7\n      - terminado==0.8.3\n      - ', ' - qtconsole==4.7.4\n      - qtpy==1.9.0\n      - querystring-parser==1.2.4\n      - requests==2.24.0\n      - scikit-learn==0.23.1\n      - scipy==1.4.1\n      - send2trash==1.5.0\n      - simplejson==3.17.0\n      - sklearn==0.0\n      - smmap==3.0.4\n      - sqlalchemy==1.3.13\n      - sqlparse==0.4.2\n      - tabulate==0.8.7\n      - terminado==0.8.3\n      - testpath==0.4.4\n      - threadpoolctl==2.1.0\n      - tqdm==4.46.1\n      - traitlets==4.3.3\n      - txt2tags==3.7\n      - urllib3==1.25.9\n      - wcwidth==0.2.4\n      - webencodings==0.5.1\n      - websocket-client==0.57.0\n      - werkzeug==1.0.1\n      - widgetsnbextension==3.5.1\n      - zipp==3.1.0\n', 'name: mlflow\nchannels:\n  - rapidsai\n  - nvidia\n  - conda-forge\n  - defaults\ndependencies:\n  - _libgcc_mutex=0.1=conda_forge\n  - _openmp_mutex=4.5=1_llvm\n  - arrow-cpp=0.15.0=py37h090bef1_2\n  - bokeh=2.1.0=py37hc8dfbb8_0\n  - boost-cpp=1.70.0=h8e57a91_2\n  - brotli=1.0.7=he1b5a44_1002\n  - bzip2=1.0.8=h516909a_2\n  - c-ares=1.15.0=h516909a_1001\n  - ca-certificates=2020.4.5.2=hecda079_0\n  - certifi=2020.4.5.2=py37hc8dfbb8_0\n  - click=7.1.2=pyh9f0ad1d_0\n  - cloudpickle=1.4.1=py_0\n  - cudatoolkit=10.2.89=h6bb024c_0\n  - cudf=0.14.0=py37_0\n  - cudnn=7.6.5=cuda10.2_0\n  - cuml=0.14.0=cuda10.2_py37_0\n  - cupy=7.5.0=py37h940342b_0\n  - cytoolz=0.10.1=py37h516909a_0\n  - dask=2.18.1=py_0\n  - dask-core=2.18.1=py_0\n  - dask-cudf=0.14.0=py37_0\n  - distributed=2.18.0=py37hc8dfbb8_0\n  - dlpack=0.2=he1b5a44_1\n  - double-conversion=3.1.5=he1b5a44_2\n  - fastavro=0.23.4=py37h8f50634_0\n  - fastrlock=0.5=py37h3340039_0\n  - freetype=2.10.2=he06d7ca_0\n  - fsspec=0.7.4=py_0\n  - gflags=2.2.2=he1b5a44_1002\n  - glog=0.4.0=h49b9bf7_3\n  - grpc-cpp=1.23.0=h18db393_0\n  - heapdict=1.0.1=py_0\n  - icu=64.2=he1b5a44_1\n  - jinja2=2.11.2=pyh9f0ad1d_0\n  - joblib=0.15.1=py_0\n  - jpeg=9d=h516909a_0\n  - ld_impl_linux-64=2.33.1=h53a641e_7\n  - libblas=3.8.0=16_openblas\n  - libcblas=3.8.0=16_openblas\n  - libcudf=0.14.0=cuda10.2_0\n  - libcuml=0.14.0=cuda10.2_0\n  - libcumlprims=0.14.1=cuda10.2_0\n  - libedit=3.1.20181209=hc058e9b_0\n  - libevent=2.1.10=h72c5cf5_0\n  - libffi=3.3=he6710b0_1\n  - libgcc-ng=9.2.0=h24d8f2e_2\n  - libgfortran-ng=7.5.0=hdf63c60_6\n  - libhwloc=2.1.0=h3c4fd83_0\n  - libiconv=1.15=h516909a_1006\n  - liblapack=3.8.0=16_openblas\n  - libllvm8=8.0.1=hc9558a2_0\n  - libnvstrings=0.14.0=cuda10.2_0\n  - libopenblas=0.3.9=h5ec1e0e_0\n  - libpng=1.6.37=hed695b0_1\n  - libprotobuf=3.8.0=h8b12597_0\n  - librmm=0.14.0=cuda10.2_0\n  - libstdcxx-ng=9.1.0=hdf63c60_0\n  - libtiff=4.1.0=hfc65ed5_0\n  - libxml2=2.9.10=hee79883_0\n  - llvm-openmp=10.0.0=hc9558a2_0\n  - llvmlite=0.32.1=py37h5202443_0\n  - locket=0.2.0=py_2\n  - lz4-c=1.8.3=he1b5a44_1001\n  - markupsafe=1.1.1=py37h8f50634_1\n  - msgpack-python=1.0.0=py37h99015e2_1\n  - nccl=2.6.4.1=hc6a2c23_0\n  - ncurses=6.2=he6710b0_1\n  - numba=0.49.1=py37h0da4684_0\n  - numpy=1.17.5=py37h95a1406_0\n  - nvstrings=0.14.0=py37_0\n  - olefile=0.46=py_0\n  - openssl=1.1.1g=h516909a_0\n  - packaging=20.4=pyh9f0ad1d_0\n  - pandas=0.25.3=py37hb3f55d8_0\n  - parquet-cpp=1.5.1=2\n  - partd=1.1.0=py_0\n  - pillow=5.3.0=py37h00a061d_1000\n  - pip=20.1.1=py37_1\n  - psutil=5.7.0=py37h8f50634_1\n  - pyarrow=0.15.0=py37h8b68381_1\n  - pyparsing=2.4.7=pyh9f0ad1d_0\n  - python=3.8.13=h12debd9_0\n  - python-dateutil=2.8.1=py_0\n  - python_abi=3.8=2_cp38\n  - pytz=2020.1=pyh9f0ad1d_0\n  - pyyaml=5.3.1=py37h8f50634_0\n  - re2=2020.04.01=he1b5a44_0\n  - readline=8.0=h7b6447c_0\n  - rmm=0.14.0=py37_0\n  - setuptools=47.3.0=py37_0\n  - six=1.15.0=pyh9f0ad1d_0\n  - snappy=1.1.8=he1b5a44_2\n  - sortedcontainers=2.2.2=pyh9f0ad1d_0\n  - spdlog=1.6.1=hc9558a2_0\n  - sqlite=3.31.1=h62c20be_1\n  - tblib=1.6.0=py_0\n  - thrift-cpp=0.12.0=hf3afdfd_1004\n  - tk=8.6.8=hbc83047_0\n  - toolz=0.10.0=py_0\n  - tornado=6.0.4=py37h8f50634_1\n  - typing_extensions=3.7.4.2=py_0\n  - ucx=1.8.0+gf6ec8d4=cuda10.2_20\n  - ucx-py=0.14.0+gf6ec8d4=py37_0\n  - uriparser=0.9.3=he1b5a44_1\n  - wheel=0.34.2=py37_0\n  - xz=5.2.5=h7b6447c_0\n  - yaml=0.2.5=h516909a_0\n  - zict=2.0.0=py_0\n  - zlib=1.2.11=h7b6447c_3\n  - zstd=1.4.3=h3b9ef0a_0\n  - pip:\n    ', 'psutil=5.7.0=py37h8f50634_1\n  - pyarrow=0.15.0=py37h8b68381_1\n  - pyparsing=2.4.7=pyh9f0ad1d_0\n  - python=3.8.13=h12debd9_0\n  - python-dateutil=2.8.1=py_0\n  - python_abi=3.8=2_cp38\n  - pytz=2020.1=pyh9f0ad1d_0\n  - pyyaml=5.3.1=py37h8f50634_0\n  - re2=2020.04.01=he1b5a44_0\n  - readline=8.0=h7b6447c_0\n  - rmm=0.14.0=py37_0\n  - setuptools=47.3.0=py37_0\n  - six=1.15.0=pyh9f0ad1d_0\n  - snappy=1.1.8=he1b5a44_2\n  - sortedcontainers=2.2.2=pyh9f0ad1d_0\n  - spdlog=1.6.1=hc9558a2_0\n  - sqlite=3.31.1=h62c20be_1\n  - tblib=1.6.0=py_0\n  - thrift-cpp=0.12.0=hf3afdfd_1004\n  - tk=8.6.8=hbc83047_0\n  - toolz=0.10.0=py_0\n  - tornado=6.0.4=py37h8f50634_1\n  - typing_extensions=3.7.4.2=py_0\n  - ucx=1.8.0+gf6ec8d4=cuda10.2_20\n  - ucx-py=0.14.0+gf6ec8d4=py37_0\n  - uriparser=0.9.3=he1b5a44_1\n  - wheel=0.34.2=py37_0\n  - xz=5.2.5=h7b6447c_0\n  - yaml=0.2.5=h516909a_0\n  - zict=2.0.0=py_0\n  - zlib=1.2.11=h7b6447c_3\n  - zstd=1.4.3=h3b9ef0a_0\n  - pip:\n      - alembic==1.4.2\n      - attrs==19.3.0\n      - backcall==0.2.0\n      - bleach==3.1.5\n      - chardet==3.0.4\n      - cycler==0.10.0\n      - databricks-cli==0.11.0\n      - decorator==4.4.2\n      - defusedxml==0.6.0\n      - docker==4.2.1\n      - entrypoints==0.3\n      - flask==1.1.2\n      - future==0.18.2\n      - gitdb==4.0.5\n      - gitpython==3.1.3\n      - gorilla==0.3.0\n      - gunicorn==20.0.4\n      - hyperopt==0.2.4\n      - idna==2.9\n      - importlib-metadata==1.6.1\n      - ipykernel==5.3.0\n      - ipython==7.15.0\n      - ipython-genutils==0.2.0\n      - ipywidgets==7.5.1\n      - itsdangerous==1.1.0\n      - jedi==0.17.0\n      - json5==0.9.5\n      - jsonschema==3.2.0\n      - jupyter==1.0.0\n      - jupyter-client==6.1.3\n      - jupyter-console==6.1.0\n      - jupyter-core==4.6.3\n      - jupyterlab==2.1.4\n      - jupyterlab-server==1.1.5\n      - kiwisolver==1.2.0\n      - lab==6.0\n  ', 'ipython==7.15.0\n      - ipython-genutils==0.2.0\n      - ipywidgets==7.5.1\n      - itsdangerous==1.1.0\n      - jedi==0.17.0\n      - json5==0.9.5\n      - jsonschema==3.2.0\n      - jupyter==1.0.0\n      - jupyter-client==6.1.3\n      - jupyter-console==6.1.0\n      - jupyter-core==4.6.3\n      - jupyterlab==2.1.4\n      - jupyterlab-server==1.1.5\n      - kiwisolver==1.2.0\n      - lab==6.0\n      - mako==1.1.3\n      - matplotlib==3.2.2\n      - mistune==0.8.4\n      - mlflow==1.8.0\n      - nbconvert==5.6.1\n      - nbformat==5.0.7\n      - networkx==2.4\n      - notebook==6.0.3\n      - pandocfilters==1.4.2\n      - parso==0.7.0\n      - pexpect==4.8.0\n      - pickleshare==0.7.5\n      - prometheus-client==0.8.0\n      - prometheus-flask-exporter==0.14.1\n      - prompt-toolkit==3.0.5\n      - protobuf==3.12.2\n      - ptyprocess==0.6.0\n      - pygments==2.6.1\n      - pyrsistent==0.16.0\n      - python-editor==1.0.4\n      - pyzmq==19.0.1\n      - qtconsole==4.7.4\n      - qtpy==1.9.0\n      - querystring-parser==1.2.4\n      - requests==2.24.0\n      - scikit-learn==0.23.1\n      - scipy==1.4.1\n      - send2trash==1.5.0\n      - simplejson==3.17.0\n      - sklearn==0.0\n      - smmap==3.0.4\n      - sqlalchemy==1.3.13\n      - sqlparse==0.4.2\n      - tabulate==0.8.7\n      - terminado==0.8.3\n      - ', ' - qtconsole==4.7.4\n      - qtpy==1.9.0\n      - querystring-parser==1.2.4\n      - requests==2.24.0\n      - scikit-learn==0.23.1\n      - scipy==1.4.1\n      - send2trash==1.5.0\n      - simplejson==3.17.0\n      - sklearn==0.0\n      - smmap==3.0.4\n      - sqlalchemy==1.3.13\n      - sqlparse==0.4.2\n      - tabulate==0.8.7\n      - terminado==0.8.3\n      - testpath==0.4.4\n      - threadpoolctl==2.1.0\n      - tqdm==4.46.1\n      - traitlets==4.3.3\n      - txt2tags==3.7\n      - urllib3==1.25.9\n      - wcwidth==0.2.4\n      - webencodings==0.5.1\n      - websocket-client==0.57.0\n      - werkzeug==1.0.1\n      - widgetsnbextension==3.5.1\n      - zipp==3.1.0\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - numpy\n  - matplotlib\n  - pandas\n  - scipy\n  - scikit-learn\n  - cloudpickle\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow>=1.0\n  - cloudpickle\n  - numpy\n  - matplotlib\n  - pandas\n  - scikit-learn\n', 'name: tutorial\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.9\n  - pip\n  - pip:\n      - scikit-learn==1.4.2\n      - mlflow>=1.0\n      - pandas\n', 'build_dependencies:\n  - pip\ndependencies:\n  - scikit-learn==1.4.2\n  - mlflow>=1.0\n  - pandas\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow>=1.0\n  - scipy\n  - scikit-learn\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow<3,>=2.1\n  - sktime==0.16.0\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow>=1.0\n  - spacy==3.8.2\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - statsmodels\n  - scikit-learn\n', 'python: "3.10"\nbuild_dependencies:\n  - pip\ndependencies:\n  - numpy\n  - pandas\n  - scipy\n  - scikit-learn\n  - mlflow\n', 'name: tutorial\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.8\n  - pip\n  - pip:\n      - mlflow>=2.0\n      - tensorflow>=2.8\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow>=2.0\n  - tensorflow>=2.8\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - scikit-learn\n  - matplotlib\n  - xgboost\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - pandas\n  - scikit-learn\n  - xgboost\n', '# These are the core requirements for the complete MLflow platform, which augments\n# the skinny client functionality with support for running the MLflow Tracking\n# Server & UI. It also adds project backends such as Docker and Kubernetes among\n# other capabilities. When we release a new major/minor version, this file is\n# automatically updated as a part of the release process.\n\nalembic:\n  pip_release: alembic\n  max_major_version: 1\n  # alembic 1.10.0 contains a regression: https://github.com/sqlalchemy/alembic/issues/1195\n  unsupported: ["1.10.0"]\n\ndocker:\n  pip_release: docker\n  minimum: "4.0.0"\n  max_major_version: 7\n\nflask:\n  pip_release: Flask\n  max_major_version: 3\n\nnumpy:\n  pip_release: numpy\n  max_major_version: 2\n\nscipy:\n  pip_release: scipy\n  max_major_version: 1\n\npandas:\n  pip_release: pandas\n  max_major_version: 2\n\nsqlalchemy:\n  pip_release: sqlalchemy\n  minimum: "1.4.0"\n  max_major_version: 2\n\ncryptography:\n  pip_release: cryptography\n  minimum: "43.0.0"\n  max_major_version: 45\n\ngunicorn:\n  pip_release: gunicorn\n  max_major_version: 23\n  markers: "platform_system != \'Windows\'"\n\nwaitress:\n  pip_release: waitress\n  max_major_version: 3\n  markers: "platform_system == \'Windows\'"\n\nscikit-learn:\n  pip_release: scikit-learn\n  max_major_version: 1\n\npyarrow:\n  pip_release: pyarrow\n  minimum: "4.0.0"\n  max_major_version: 21\n\nmatplotlib:\n  pip_release: matplotlib\n  max_major_version: 3\n\ngraphene:\n  pip_release: graphene\n  max_major_version: 3\n\nfastmcp:\n  pip_release: fastmcp\n  minimum: "2.0.0"\n  max_major_version: 2\n', '# These are the extra requirements for MLflow Gateway, which can be installed\n# on top of the core requirements using `pip install mlflow[gateway]`.\n# When we release a new major/minor version, this file is automatically updated\n# as a part of the release process.\n\nfastapi:\n  pip_release: fastapi\n  max_major_version: 0\n\nuvicorn:\n  pip_release: uvicorn\n  extras:\n    - standard\n  max_major_version: 0\n\nwatchfiles:\n  pip_release: watchfiles\n  max_major_version: 1\n\naiohttp:\n  pip_release: aiohttp\n  max_major_version: 3\n\nboto3:\n  pip_release: boto3\n  minimum: "1.28.56"\n  max_major_version: 1\n\ntiktoken:\n  pip_release: tiktoken\n  max_major_version: 0\n\nslowapi:\n  pip_release: slowapi\n  max_major_version: 0\n  minimum: "0.1.9"\n', '# Minimal requirements for the skinny MLflow client which provides a limited\n# subset of functionality such as: RESTful client functionality for Tracking and\n# Model Registry, as well as support for Project execution against local backends\n# and Databricks. When we release a new major/minor version, this file is automatically\n# updated as a part of the release process.\n\nclick:\n  pip_release: click\n  minimum: "7.0"\n  max_major_version: 8\n\ncloudpickle:\n  pip_release: cloudpickle\n  max_major_version: 3\n\npython-dotenv:\n  pip_release: python-dotenv\n  minimum: "0.19.0"\n  max_major_version: 1\n\ngitpython:\n  pip_release: gitpython\n  minimum: "3.1.9"\n  max_major_version: 3\n\npyyaml:\n  pip_release: pyyaml\n  minimum: "5.1"\n  max_major_version: 6\n\nprotobuf:\n  pip_release: protobuf\n  minimum: "3.12.0"\n  max_major_version: 6\n\nrequests:\n  pip_release: requests\n  minimum: "2.17.3"\n  max_major_version: 2\n\npackaging:\n  pip_release: packaging\n  max_major_version: 25\n\nimportlib_metadata:\n  pip_release: importlib_metadata\n  # Automated dependency detection in MLflow Models relies on\n  # `importlib_metadata.packages_distributions` to resolve a module name to its package name\n  # (e.g. \'sklearn\' -> \'scikit-learn\'). importlib_metadata 3.7.0 or newer supports this function:\n  # https://github.com/python/importlib_metadata/blob/main/CHANGES.rst#v370\n  minimum: "3.7.0"\n  max_major_version: 8\n  unsupported: ["4.7.0"]\n\nsqlparse:\n  pip_release: sqlparse\n  # Lower bound sqlparse for: https://github.com/andialbrecht/sqlparse/pull/567\n  minimum: "0.4.0"\n  max_major_version: 0\n\n# Required for tracing\ncachetools:\n  pip_release: cachetools\n  minimum: "5.0.0"\n  max_major_version: 6\n\n# 1.9.0 is the minimum supported version as NoOpTracer was introduced in 1.9.0\nopentelemetry-api:\n  pip_release: opentelemetry-api\n  minimum: "1.9.0"\n  max_major_version: 2\n\nopentelemetry-sdk:\n  pip_release: opentelemetry-sdk\n  minimum: "1.9.0"\n  max_major_version: 2\n\nopentelemetry-proto:\n  pip_release: opentelemetry-proto\n  minimum: "1.9.0"\n  max_major_version: 2\n\ndatabricks-sdk:\n  pip_release: databricks-sdk\n  minimum: "0.20.0"\n  max_major_version: 0\n\npydantic:\n  pip_release: pydantic\n  minimum: "1.10.8"\n  max_major_version: 2\n\ntyping-extensions:\n  pip_release: typing-extensions\n  minimum: "4.0.0"\n  max_major_version: 4\n\nfastapi:\n  pip_release: fastapi\n  max_major_version: 0\n\nuvicorn:\n  pip_release: uvicorn\n  max_major_version: 0\n', '# Minimal requirements for the MLflow Tracing package. It is a lightweight\n# package that only includes the minimum set of dependencies and functionality\n# to instrument code/models/agents with MLflow Tracing.\n# When we release a new major/minor version, this file is automatically\n# updated as a part of the release process.\n\nprotobuf:\n  pip_release: protobuf\n  minimum: "3.12.0"\n  max_major_version: 6\n\npackaging:\n  pip_release: packaging\n  max_major_version: 25\n\n# Required for tracing\ncachetools:\n  pip_release: cachetools\n  minimum: "5.0.0"\n  max_major_version: 6\n\n# 1.9.0 is the minimum supported version as NoOpTracer was introduced in 1.9.0\nopentelemetry-api:\n  pip_release: opentelemetry-api\n  minimum: "1.9.0"\n  max_major_version: 2\n\nopentelemetry-sdk:\n  pip_release: opentelemetry-sdk\n  minimum: "1.9.0"\n  max_major_version: 2\n\nopentelemetry-proto:\n  pip_release: opentelemetry-proto\n  minimum: "1.9.0"\n  max_major_version: 2\n\ndatabricks-sdk:\n  pip_release: databricks-sdk\n  minimum: "0.20.0"\n  max_major_version: 0\n\npydantic:\n  pip_release: pydantic\n  minimum: "1.10.8"\n  max_major_version: 2\n', 'llm:\n  model_name: "gpt-4o-mini"\n  temperature: 0.7\n', 'embedding_model_query_instructions: "Represent this sentence for searching relevant passages:"\nllm_model: "databricks-dbrx-instruct"\nllm_prompt_template: "You are a trustful assistant."\nretriever_config:\n  k: 5\n  use_mmr: false\nllm_parameters:\n  temperature: 0.01\n  max_tokens: 500\nllm_prompt_template_variables:\n  - "chat_history"\n  - "context"\n  - "question"\n', 'embedding_model_query_instructions: "Represent this sentence for searching relevant passages:"\nllm_model: "databricks-dbrx-instruct"\nllm_prompt_template: "You are a trustful assistant. Answer concisely and clearly."\nretriever_config:\n  k: 5\n  use_mmr: false\nllm_parameters:\n  temperature: 0.01\n  max_tokens: 200\nllm_prompt_template_variables:\n  - "chat_history"\n  - "context"\n  - "question"\n', '$schema: https://azuremlschemas.azureedge.net/promptflow/latest/Flow.schema.json\n\ninputs:\n  text:\n    type: string\n    default: Hello World!\n\noutputs:\n  output:\n    type: string\n    reference: ${llm.output}\n\nnodes:\n  - name: hello_prompt\n    type: python\n    source:\n      type: code\n      path: render_template.py\n    inputs:\n      text: ${inputs.text}\n      template: |\n        system:\n        Your task is to generate what I ask.\n        user:\n        Write a simple {{text}} program that displays the greeting message.\n  - name: llm\n    type: python\n    source:\n      type: code\n      path: echo.py\n    inputs:\n      prompt: ${hello_prompt.output}\nenvironment:\n  image: mcr.microsoft.com/azureml/promptflow/promptflow-runtime:latest\n  python_requirements_txt: requirements.txt\nadditional_includes:\n  - ../additional_file\n', 'channels:\n  - conda-forge\ndependencies:\n  - python=3.10.16\n  - pip<=23.0.1\n  - pip:\n      - mlflow\n      - cloudpickle==2.2.1\n      - scikit-learn==1.5.2\nname: mlflow-env\n', 'python: 3.10.16\nbuild_dependencies:\n  - pip==23.0.1\n  - setuptools==58.1.0\n  - wheel==0.42.0\ndependencies:\n  - -r requirements.txt\n', 'channels:\n  - conda-forge\ndependencies:\n  - python=3.10.12\n  - pip<=22.3.1\n  - pip:\n      - bcrypt==3.2.0\n      - cloudpickle==2.0.0\n      - configparser==5.2.0\n      - cryptography==39.0.1\n      - databricks-feature-engineering==0.2.1\n      - entrypoints==0.4\n      - google-cloud-storage==2.11.0\n      - grpcio-status==1.48.1\n      - langchain==0.1.20\n      - mlflow[gateway]==2.12.2\n      - numpy==1.23.5\n      - packaging==23.2\n      - pandas==1.5.3\n      - protobuf==4.24.0\n      - psutil==5.9.0\n      - pyarrow==8.0.0\n      - pydantic==1.10.6\n      - pyyaml==6.0\n      - requests==2.28.1\n      - tornado==6.1\nname: mlflow-env\n', 'python: 3.10.12\nbuild_dependencies:\n  - pip==22.3.1\n  - setuptools==65.6.3\n  - wheel==0.38.4\ndependencies:\n  - -r requirements.txt\n', 'channels:\n  - conda-forge\ndependencies:\n  - python=3.10.12\n  - pip<=22.3.1\n  - pip:\n      - bcrypt==3.2.0\n      - cloudpickle==2.0.0\n      - configparser==5.2.0\n      - cryptography==39.0.1\n      - databricks-feature-engineering==0.2.1\n      - entrypoints==0.4\n      - google-cloud-storage==2.11.0\n      - grpcio-status==1.48.1\n      - langchain==0.1.20\n      - mlflow[gateway]==2.12.2\n      - numpy==1.23.5\n      - packaging==23.2\n      - pandas==1.5.3\n      - protobuf==4.24.0\n      - psutil==5.9.0\n      - pyarrow==8.0.0\n      - pydantic==1.10.6\n      - pyyaml==6.0\n      - requests==2.28.1\n      - tornado==6.1\nname: mlflow-env\n', 'python: 3.10.12\nbuild_dependencies:\n  - pip==22.3.1\n  - setuptools==65.6.3\n  - wheel==0.38.4\ndependencies:\n  - -r requirements.txt\n', '# Adding a comment here to distinguish the hash of this conda.yaml from the hashes of existing\n# conda.yaml files in the test environment.\nname: tutorial\nchannels:\n  - defaults\ndependencies:\n  - python=3.10\n  - pip:\n      - -e ../../../\n      - psutil\n', 'name: virtualenv-conda\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.10.16\n  - numpy\n  - pip\n  - pip:\n      - mlflow\n      - scikit-learn==1.4.2\n', 'python: "3.10.16"\nbuild_dependencies:\n  - pip\ndependencies:\n  - -r requirements.txt\n', 'python: "3.10.16"\nbuild_dependencies:\n  - pip\ndependencies:\n  - -r requirements.txt\n', 'channels:\n  - conda-forge\ndependencies:\n  - python=3.9.13\n  - pip<=23.3\n  - pip:\n      - mlflow==2.7.1\n      - cloudpickle==2.2.1\nname: mlflow-env\n', 'python: 3.9.13\nbuild_dependencies:\n  - pip==23.3\n  - setuptools\n  - wheel==0.41.2\ndependencies:\n  - -r requirements.txt\n', 'channels:\n  - conda-forge\ndependencies:\n  - python=3.9.13\n  - pip<=23.3\n  - pip:\n      - mlflow==2.8.1\n      - cloudpickle==2.2.1\nname: mlflow-env\n', 'python: 3.9.13\nbuild_dependencies:\n  - pip==23.3\n  - setuptools\n  - wheel==0.41.2\ndependencies:\n  - -r requirements.txt\n', 'volumes:\n  pgdata:\n  minio-data:\n\nservices:\n  postgres:\n    image: postgres:15\n    container_name: mlflow-postgres\n    environment:\n      POSTGRES_USER: ${POSTGRES_USER}\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n      POSTGRES_DB: ${POSTGRES_DB}\n    volumes:\n      - pgdata:/var/lib/postgresql/data\n    ports:\n      - "5432:5432"\n    healthcheck:\n      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]\n      interval: 5s\n      timeout: 3s\n      retries: 10\n\n  minio:\n    image: minio/minio:latest\n    container_name: mlflow-minio\n    environment:\n      MINIO_ROOT_USER: ${MINIO_ROOT_USER}\n      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}\n    volumes:\n      - minio-data:/data\n    command: server /data --console-address ":9001"\n    ports:\n      - "9000:9000"\n      - "9001:9001"\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${MINIO_PORT}/minio/health/live"]\n      interval: 5s\n      timeout: 3s\n      retries: 20\n\n  create-bucket:\n    image: minio/mc:latest\n    container_name: mlflow-create-bucket\n    depends_on:\n      minio:\n        condition: service_healthy\n    entrypoint: >\n      /bin/sh -c \'\n        mc alias set myminio http://${MINIO_HOST}:${MINIO_PORT} \\\n          ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD} &&\n        mc mb --ignore-existing myminio/${MINIO_BUCKET:-mlflow}\n      \'\n    restart: "no"\n\n  mlflow:\n    image: ghcr.io/mlflow/mlflow:${MLFLOW_VERSION}\n    container_name: mlflow-server\n    depends_on:\n      postgres:\n        condition: service_healthy\n      minio:\n        condition: service_healthy\n      create-bucket:\n       ', '  mc alias set myminio http://${MINIO_HOST}:${MINIO_PORT} \\\n          ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD} &&\n        mc mb --ignore-existing myminio/${MINIO_BUCKET:-mlflow}\n      \'\n    restart: "no"\n\n  mlflow:\n    image: ghcr.io/mlflow/mlflow:${MLFLOW_VERSION}\n    container_name: mlflow-server\n    depends_on:\n      postgres:\n        condition: service_healthy\n      minio:\n        condition: service_healthy\n      create-bucket:\n        condition: service_completed_successfully\n    environment:\n      # Backend store URI built from vars\n      MLFLOW_BACKEND_STORE_URI: ${MLFLOW_BACKEND_STORE_URI}\n\n      # S3/MinIO settings\n      MLFLOW_S3_ENDPOINT_URL: ${MLFLOW_S3_ENDPOINT_URL}\n      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}\n      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}\n      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION}\n      MLFLOW_S3_IGNORE_TLS: "true"\n\n      # Server host/port\n      MLFLOW_HOST: ${MLFLOW_HOST}\n      MLFLOW_PORT: ${MLFLOW_PORT}\n\n    command: >\n      /bin/bash -c "\n        pip install --no-cache-dir psycopg2-binary boto3 &&\n        mlflow server \\\n          --backend-store-uri ${MLFLOW_BACKEND_STORE_URI} \\\n          --default-artifact-root ${MLFLOW_DEFAULT_ARTIFACT_ROOT} \\\n          --host ${MLFLOW_HOST} \\\n          --port ${MLFLOW_PORT}\n      "\n    ports:\n      - "${MLFLOW_PORT}:${MLFLOW_PORT}"\n    healthcheck:\n      test:\n        [\n          "CMD",\n          "python",\n          "-c",\n          "import urllib.request; ', '\\\n          --host ${MLFLOW_HOST} \\\n          --port ${MLFLOW_PORT}\n      "\n    ports:\n      - "${MLFLOW_PORT}:${MLFLOW_PORT}"\n    healthcheck:\n      test:\n        [\n          "CMD",\n          "python",\n          "-c",\n          "import urllib.request; urllib.request.urlopen(\'http://localhost:${MLFLOW_PORT}/health\')",\n        ]\n      interval: 10s\n      timeout: 5s\n      retries: 30\n\nnetworks:\n  default:\n    name: mlflow-network\n', 'version: "3"\nservices:\n  minio:\n    image: minio/minio\n    expose:\n      - "9000"\n    ports:\n      - "9000:9000"\n      # MinIO Console is available at http://localhost:9001\n      - "9001:9001"\n    environment:\n      MINIO_ROOT_USER: "user"\n      MINIO_ROOT_PASSWORD: "password"\n    healthcheck:\n      test: timeout 5s bash -c \':> /dev/tcp/127.0.0.1/9000\' || exit 1\n      interval: 1s\n      timeout: 10s\n      retries: 5\n    # Note there is no bucket by default\n    command: server /data --console-address ":9001"\n\n  minio-create-bucket:\n    image: minio/mc\n    depends_on:\n      minio:\n        condition: service_healthy\n    entrypoint: >\n      bash -c "\n      mc alias set minio http://minio:9000 user password &&\n      if ! mc ls minio/bucket; then\n        mc mb minio/bucket\n      else\n        echo \'bucket already exists\'\n      fi\n      "\n\n  artifacts-server:\n    build:\n      context: .\n      dockerfile: "${DOCKERFILE:-Dockerfile}"\n    depends_on:\n      - minio-create-bucket\n    expose:\n      - "5500"\n    ports:\n      - "5500:5500"\n    environment:\n      MLFLOW_S3_ENDPOINT_URL: http://minio:9000\n      AWS_ACCESS_KEY_ID: "user"\n      AWS_SECRET_ACCESS_KEY: "password"\n    command: >\n      mlflow server\n      --host 0.0.0.0\n      --port 5500\n      --artifacts-destination s3://bucket\n      --gunicorn-opts "--log-level debug"\n   ', '  depends_on:\n      - minio-create-bucket\n    expose:\n      - "5500"\n    ports:\n      - "5500:5500"\n    environment:\n      MLFLOW_S3_ENDPOINT_URL: http://minio:9000\n      AWS_ACCESS_KEY_ID: "user"\n      AWS_SECRET_ACCESS_KEY: "password"\n    command: >\n      mlflow server\n      --host 0.0.0.0\n      --port 5500\n      --artifacts-destination s3://bucket\n      --gunicorn-opts "--log-level debug"\n      --artifacts-only\n\n  postgres:\n    image: postgres\n    restart: always\n    environment:\n      POSTGRES_DB: db\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n\n  tracking-server:\n    build:\n      context: .\n      dockerfile: "${DOCKERFILE:-Dockerfile}"\n    depends_on:\n      - postgres\n      - artifacts-server\n    expose:\n      - "5000"\n    ports:\n      # MLflow UI is available at http://localhost:5000\n      - "5000:5000"\n    command: >\n      mlflow server\n      --host 0.0.0.0\n      --port 5000\n      --backend-store-uri postgresql://user:password@postgres:5432/db\n      --default-artifact-root http://artifacts-server:5500/api/2.0/mlflow-artifacts/artifacts/experiments\n      --gunicorn-opts "--log-level debug"\n\n  client:\n    build:\n      context: .\n      dockerfile: "${DOCKERFILE:-Dockerfile}"\n    depends_on:\n      - tracking-server\n    environment:\n      MLFLOW_TRACKING_URI: http://tracking-server:5000\n', 'services:\n  base:\n    image: mlflow-base\n    build:\n      context: .\n    volumes:\n      - ${PWD}:/mlflow/home\n    working_dir: /mlflow/home\n    entrypoint: /mlflow/home/tests/db/entrypoint.sh\n    command: pytest tests/db\n    environment:\n      DISABLE_RESET_MLFLOW_URI_FIXTURE: "true"\n\n  postgresql:\n    image: postgres\n    restart: always\n    environment:\n      POSTGRES_DB: mlflowdb\n      POSTGRES_USER: mlflowuser\n      POSTGRES_PASSWORD: mlflowpassword\n\n  mlflow-postgresql:\n    depends_on:\n      - postgresql\n    extends:\n      service: base\n    environment:\n      MLFLOW_TRACKING_URI: postgresql://mlflowuser:mlflowpassword@postgresql:5432/mlflowdb\n      INSTALL_MLFLOW_FROM_REPO: true\n\n  migration-postgresql:\n    depends_on:\n      - postgresql\n    extends:\n      service: base\n    environment:\n      MLFLOW_TRACKING_URI: postgresql://mlflowuser:mlflowpassword@postgresql:5432/mlflowdb\n    command: tests/db/check_migration.sh\n\n  mysql:\n    image: mysql\n    restart: always\n    environment:\n      MYSQL_ROOT_PASSWORD: root-password\n      MYSQL_DATABASE: mlflowdb\n      MYSQL_USER: mlflowuser\n      MYSQL_PASSWORD: mlflowpassword\n\n  mlflow-mysql:\n    extends:\n      service: base\n    depends_on:\n      - mysql\n    environment:\n      MLFLOW_TRACKING_URI: mysql://mlflowuser:mlflowpassword@mysql:3306/mlflowdb?charset=utf8mb4\n      INSTALL_MLFLOW_FROM_REPO: true\n\n  migration-mysql:\n    extends:\n      service: base\n    depends_on:\n      - mysql\n    environment:\n      MLFLOW_TRACKING_URI: mysql://mlflowuser:mlflowpassword@mysql:3306/mlflowdb?charset=utf8mb4\n    command: tests/db/check_migration.sh\n\n  mssql:\n    image: mcr.microsoft.com/mssql/server\n    restart: always\n    environment:\n      ACCEPT_EULA: Y\n      SA_PASSWORD: "1Secure*Password1"\n\n  mlflow-mssql:\n    depends_on:\n      - mssql\n    extends:\n  ', '   MLFLOW_TRACKING_URI: mysql://mlflowuser:mlflowpassword@mysql:3306/mlflowdb?charset=utf8mb4\n      INSTALL_MLFLOW_FROM_REPO: true\n\n  migration-mysql:\n    extends:\n      service: base\n    depends_on:\n      - mysql\n    environment:\n      MLFLOW_TRACKING_URI: mysql://mlflowuser:mlflowpassword@mysql:3306/mlflowdb?charset=utf8mb4\n    command: tests/db/check_migration.sh\n\n  mssql:\n    image: mcr.microsoft.com/mssql/server\n    restart: always\n    environment:\n      ACCEPT_EULA: Y\n      SA_PASSWORD: "1Secure*Password1"\n\n  mlflow-mssql:\n    depends_on:\n      - mssql\n    extends:\n      service: base\n    platform: linux/amd64\n    image: mlflow-mssql\n    build:\n      context: .\n      dockerfile: Dockerfile.mssql\n    environment:\n      MLFLOW_TRACKING_URI: mssql+pyodbc://mlflowuser:Mlfl*wpassword1@mssql/mlflowdb?driver=ODBC+Driver+17+for+SQL+Server\n      INSTALL_MLFLOW_FROM_REPO: true\n\n  migration-mssql:\n    depends_on:\n      - mssql\n    extends:\n      service: base\n    platform: linux/amd64\n    image: mlflow-mssql\n    build:\n      context: .\n      dockerfile: Dockerfile.mssql\n    environment:\n      # We could try using ODBC Driver 18 and append `LongAsMax=Yes` to fix error for sqlalchemy<2.0:\n      # [ODBC Driver 17 for SQL Server][SQL Server]The data types varchar and ntext are incompatible in the equal to operator\n      # https://docs.sqlalchemy.org/en/20/dialects/mssql.html#avoiding-sending-large-string-parameters-as-text-ntext\n      MLFLOW_TRACKING_URI: mssql+pyodbc://mlflowuser:Mlfl*wpassword1@mssql/mlflowdb?driver=ODBC+Driver+17+for+SQL+Server\n    command: tests/db/check_migration.sh\n\n  mlflow-sqlite:\n    extends:\n      service: base\n    environment:\n      MLFLOW_TRACKING_URI: "sqlite:////tmp/mlflowdb"\n      INSTALL_MLFLOW_FROM_REPO: true\n\n  migration-sqlite:\n    extends:\n      service: base\n    environment:\n      MLFLOW_TRACKING_URI: "sqlite:////tmp/mlflowdb"\n    command: tests/db/check_migration.sh\n', 'prompt_with_history_str: "Here is a history between you and a human: {chat_history}\\nNow, please answer this question: {question}"\n', 'llm_prompt_template: "Answer the following question based on the context: {context}\\nQuestion: {question}"\nembedding_size: 5\nresponse: "Databricks"\nnot_used_array:\n  - 1\n  - 2\n  - 3\n', 'use_gpu: True\ntemperature: 0.9\ntimeout: 300\n', 'llm_prompt_template: "Answer the following question based on the context: {context}\\nQuestion: {question}"\nembedding_size: 5\nresponse: "Databricks"\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-3-small', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-3-small', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'BatchEmbedder', '_init_args': {'embedder': None, 'batch_size': 500}, 'batch_size': 500}}
2025-09-05 20:09:09,659 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict Embedder, {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': None, '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://api.openai.com/v1', 'chat_completion_parser': <function get_first_message_content at 0x000001AF3C58AAC0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-3-small', 'dimensions': 256, 'encoding_format': 'float', 'input': ['import { render } from \'@testing-library/react\';\nimport { IntlProvider } from \'react-intl\';\n\nimport { TagAssignmentRow } from \'./TagAssignmentRow\';\n\ndescribe(\'TagAssignmentRow\', () => {\n  it(\'should throw an error if more than 3 children are passed\', () => {\n    const children = Array(4)\n      .fill(null)\n      .map((_, i) => <div key={i} />);\n\n    const renderComponent = () =>\n      render(\n        <IntlProvider locale="en">\n          <TagAssignmentRow>{children}</TagAssignmentRow>\n        </IntlProvider>,\n      );\n\n    expect(renderComponent).toThrow(\'TagAssignmentRow must have 3 children or less\');\n  });\n\n  it(\'should render children\', () => {\n    const children = Array(3)\n      .fill(null)\n      .map((_, i) => <div key={i} />);\n\n    const { container } = render(\n      <IntlProvider locale="en">\n        <TagAssignmentRow>{children}</TagAssignmentRow>\n      </IntlProvider>,\n    );\n\n    expect(container).toMatchSnapshot();\n  });\n});\n', "import invariant from 'invariant';\nimport React from 'react';\n\nimport { useDesignSystemTheme } from '@databricks/design-system';\n\nexport function TagAssignmentRow({ children }: { children: React.ReactNode }) {\n  const { theme } = useDesignSystemTheme();\n\n  const stableChildren = React.Children.toArray(children);\n  invariant(stableChildren.length <= 3, 'TagAssignmentRow must have 3 children or less');\n\n  const parsedChildren = Array(3)\n    .fill(null)\n    .map((_, i) => stableChildren[i] ?? <span key={i} style={{ width: theme.general.heightSm }} />); // Sync width with only icon button width\n\n  return (\n    <div css={{ display: 'grid', gridTemplateColumns: '1fr 1fr min-content', gap: theme.spacing.sm }}>\n      {parsedChildren}\n    </div>\n  );\n}\n", '// Do not modify this file\n\nimport type { ControllerProps, FieldValues, Path } from \'react-hook-form\';\nimport { Controller } from \'react-hook-form\';\n\nimport { TagAssignmentInput } from \'./TagAssignmentField/TagAssignmentInput\';\nimport { useTagAssignmentContext } from \'../context/TagAssignmentContextProvider\';\n\ninterface TagAssignmentValueProps<T extends FieldValues> {\n  rules?: ControllerProps<T>[\'rules\'];\n  index: number;\n  render?: ControllerProps<T>[\'render\'];\n}\n\nexport function TagAssignmentValue<T extends FieldValues>({ rules, index, render }: TagAssignmentValueProps<T>) {\n  const { name, valueProperty } = useTagAssignmentContext<T>();\n\n  return (\n    <Controller\n      rules={rules}\n      name={`${name}.${index}.${valueProperty}` as Path<T>}\n      render={({ field, fieldState, formState }) => {\n        if (render) {\n          return render({ field, fieldState, formState });\n        }\n\n        return (\n          <TagAssignmentInput\n            componentId="TagAssignmentValue.Default.Input"\n            errorMessage={fieldState.error?.message}\n            {...field}\n          />\n        );\n      }}\n    />\n  );\n}\n', 'import { forwardRef } from \'react\';\n\nimport type { InputProps, InputRef } from \'@databricks/design-system\';\nimport { FormUI, Input } from \'@databricks/design-system\';\n\ninterface TagAssignmentInputProps extends InputProps {\n  errorMessage?: string;\n}\n\nexport const TagAssignmentInput: React.ForwardRefExoticComponent<\n  TagAssignmentInputProps & React.RefAttributes<InputRef>\n> = forwardRef<InputRef, TagAssignmentInputProps>(({ errorMessage, ...otherProps }: TagAssignmentInputProps, ref) => {\n  return (\n    <div css={{ flex: 1 }}>\n      <Input validationState={errorMessage ? \'error\' : \'info\'} {...otherProps} ref={ref} />\n      {errorMessage && <FormUI.Message message={errorMessage} type="error" />}\n    </div>\n  );\n});\n', 'import { render, screen } from \'@testing-library/react\';\nimport userEvent from \'@testing-library/user-event\';\n\nimport { TagAssignmentRemoveButtonUI } from \'./TagAssignmentRemoveButtonUI\';\n\ndescribe(\'TagAssignmentRemoveButtonUI\', () => {\n  it(\'should render a button\', async () => {\n    const handleClick = jest.fn();\n    render(<TagAssignmentRemoveButtonUI componentId="test" onClick={handleClick} />);\n\n    const button = screen.getByRole(\'button\');\n    await userEvent.click(button);\n\n    expect(handleClick).toHaveBeenCalledTimes(1);\n  });\n});\n', "import type { ButtonProps } from '@databricks/design-system';\nimport { Button, TrashIcon } from '@databricks/design-system';\n\nexport function TagAssignmentRemoveButtonUI(props: Omit<ButtonProps, 'icon'>) {\n  return <Button icon={<TrashIcon />} {...props} />;\n}\n", "import { render, screen } from '@testing-library/react';\n\nimport { TagAssignmentRowContainer } from './TagAssignmentRowContainer';\n\ndescribe('TagAssignmentRowContainer', () => {\n  it('should render children', () => {\n    render(\n      <TagAssignmentRowContainer>\n        <div>child</div>\n      </TagAssignmentRowContainer>,\n    );\n\n    expect(screen.getByText('child')).toBeInTheDocument();\n  });\n});\n", "import { useDesignSystemTheme } from '@databricks/design-system';\n\nexport function TagAssignmentRowContainer({ children }: { children: React.ReactNode }) {\n  const { theme } = useDesignSystemTheme();\n  return <div css={{ display: 'flex', flexDirection: 'column', gap: theme.spacing.sm }}>{children}</div>;\n}\n", "import invariant from 'invariant';\nimport { createContext, useContext } from 'react';\nimport type { FieldValues, ArrayPath, FieldArray } from 'react-hook-form';\n\nimport type { UseTagAssignmentFormReturn } from '../hooks/useTagAssignmentForm';\n\nexport const TagAssignmentContext = createContext<UseTagAssignmentFormReturn | null>(null);\n\nexport function TagAssignmentContextProvider<\n  T extends FieldValues = FieldValues,\n  K extends ArrayPath<T> = ArrayPath<T>,\n  V extends FieldArray<T, K> = FieldArray<T, K>,\n>({ children, ...props }: { children: React.ReactNode } & UseTagAssignmentFormReturn<T, K, V>) {\n  return <TagAssignmentContext.Provider value={props as any}>{children}</TagAssignmentContext.Provider>;\n}\n\nexport function useTagAssignmentContext<\n  T extends FieldValues = FieldValues,\n  K extends ArrayPath<T> = ArrayPath<T>,\n  V extends FieldArray<T, K> = FieldArray<T, K>,\n>() {\n  const context = useContext(TagAssignmentContext as React.Context<UseTagAssignmentFormReturn<T, K, V> | null>);\n  invariant(context, 'useTagAssignmentContext must be used within a TagAssignmentRoot');\n  return context;\n}\n", 'import { renderHook } from \'@testing-library/react\';\nimport { useForm, FormProvider } from \'react-hook-form\';\nimport { IntlProvider } from \'react-intl\';\n\nimport { useTagAssignmentFieldArray } from \'./useTagAssignmentFieldArray\';\n\nconst DefaultWrapper = ({ children }: { children: React.ReactNode }) => {\n  return <IntlProvider locale="en">{children}</IntlProvider>;\n};\n\ndescribe(\'useTagAssignmentFieldArray\', () => {\n  it(\'should use passed form as prop\', () => {\n    const { result: formResult } = renderHook(() =>\n      useForm<{ input: string; tags: { key: string; value: string }[] }>({\n        defaultValues: { input: \'test_input\', tags: [{ key: \'key1\', value: \'value1\' }] },\n      }),\n    );\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentFieldArray({\n          name: \'tags\',\n          emptyValue: { key: \'\', value: \'\' },\n          form: formResult.current,\n          keyProperty: \'key\',\n        }),\n      { wrapper: DefaultWrapper },\n    );\n    result.current.appendIfPossible({ key: \'foo\', value: \'bar\' }, {});\n\n    const values = result.current.form.getValues();\n    expect(values.tags).toStrictEqual([\n      { key: \'key1\', value: \'value1\' },\n      { key: \'foo\', value: \'bar\' },\n    ]);\n    expect(values.input).toBe(\'test_input\');\n  });\n\n  it(\'should use context form if no form prop is passed\', () => {\n    const Wrapper = ({ children }: { children: React.ReactNode }) => {\n      const methods = useForm<{ input: string; tags: { key: string; value: string }[] }>({\n        defaultValues: { input: \'test_input\', tags: [{ key: \'key1\', value: \'value1\' }] },\n      });\n      return (\n        <IntlProvider locale="en">\n         ', 'expect(values.input).toBe(\'test_input\');\n  });\n\n  it(\'should use context form if no form prop is passed\', () => {\n    const Wrapper = ({ children }: { children: React.ReactNode }) => {\n      const methods = useForm<{ input: string; tags: { key: string; value: string }[] }>({\n        defaultValues: { input: \'test_input\', tags: [{ key: \'key1\', value: \'value1\' }] },\n      });\n      return (\n        <IntlProvider locale="en">\n          c<FormProvider {...methods}>{children}</FormProvider>\n        </IntlProvider>\n      );\n    };\n\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentFieldArray({\n          name: \'tags\',\n          emptyValue: { key: \'\', value: \'\' },\n          keyProperty: \'key\',\n        }),\n      { wrapper: Wrapper },\n    );\n    result.current.appendIfPossible({ key: \'foo\', value: \'bar\' }, {});\n\n    const values = result.current.form.getValues();\n    expect(values[\'tags\']).toStrictEqual([\n      { key: \'key1\', value: \'value1\' },\n      { key: \'foo\', value: \'bar\' },\n    ]);\n    expect(values[\'input\']).toBe(\'test_input\');\n  });\n\n  it(\'should throw an error if no form is passed and not in a form context\', () => {\n    expect(() =>\n      renderHook(\n        () =>\n          useTagAssignmentFieldArray({\n            name: \'tags\',\n            emptyValue: { key: \'\', value: undefined },\n            keyProperty: \'key\',\n ', "]);\n    expect(values['input']).toBe('test_input');\n  });\n\n  it('should throw an error if no form is passed and not in a form context', () => {\n    expect(() =>\n      renderHook(\n        () =>\n          useTagAssignmentFieldArray({\n            name: 'tags',\n            emptyValue: { key: '', value: undefined },\n            keyProperty: 'key',\n          }),\n        { wrapper: DefaultWrapper },\n      ),\n    ).toThrow('Nest your component on a FormProvider or pass a form prop');\n  });\n\n  it('should not add the empty value to the form via appendIfPossible if maxLength is reached', () => {\n    const { result: formResult } = renderHook(() =>\n      useForm<{ tags: { key: string; value: string }[] }>({\n        defaultValues: {\n          tags: [\n            { key: 'key1', value: 'value1' },\n            { key: 'key2', value: 'value2' },\n          ],\n        },\n      }),\n    );\n\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentFieldArray<{ tags: { key: string; value: string }[] }>({\n          name: 'tags',\n          emptyValue: { key: '', value: '' },\n          maxLength: 2,\n          form: formResult.current,\n ", "],\n        },\n      }),\n    );\n\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentFieldArray<{ tags: { key: string; value: string }[] }>({\n          name: 'tags',\n          emptyValue: { key: '', value: '' },\n          maxLength: 2,\n          form: formResult.current,\n          keyProperty: 'key',\n        }),\n      { wrapper: DefaultWrapper },\n    );\n\n    result.current.appendIfPossible({ key: 'not-added', value: 'not-added' }, {});\n    expect(result.current.getTagsValues()).toStrictEqual([\n      { key: 'key1', value: 'value1' },\n      { key: 'key2', value: 'value2' },\n    ]);\n  });\n\n  it('should remove tag when removeOrUpdate is called for tag not at the end of the array', () => {\n    const { result: formResult } = renderHook(() =>\n      useForm<{ tags: { key: string; value: string }[] }>({\n        defaultValues: {\n          tags: [\n            { key: 'key1', value: 'value1' },\n            { key: 'key2', value: 'value2' },\n            { key: '', value: '' },\n          ],\n        },\n      }),\n    );\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentFieldArray({\n         ", "{ key: 'key1', value: 'value1' },\n            { key: 'key2', value: 'value2' },\n            { key: '', value: '' },\n          ],\n        },\n      }),\n    );\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentFieldArray({\n          name: 'tags',\n          emptyValue: { key: '', value: '' },\n          maxLength: 5,\n          form: formResult.current,\n          keyProperty: 'key',\n        }),\n      { wrapper: DefaultWrapper },\n    );\n\n    result.current.removeOrUpdate(0);\n\n    expect(result.current.getTagsValues()).toStrictEqual([\n      { key: 'key2', value: 'value2' },\n      { key: '', value: '' },\n    ]);\n  });\n\n  it('should set last tag to the empty value when removeOrUpdate is called for last tag', () => {\n    const { result: formResult } = renderHook(() =>\n      useForm<{ tags: { key: string; value: string }[] }>({\n        defaultValues: {\n          tags: [\n            { key: 'key1', value: 'value1' },\n            { key: 'key2', value: 'value2' },\n          ],\n        },\n      }),\n    );\n    const { result } = renderHook(\n      () ", "string }[] }>({\n        defaultValues: {\n          tags: [\n            { key: 'key1', value: 'value1' },\n            { key: 'key2', value: 'value2' },\n          ],\n        },\n      }),\n    );\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentFieldArray({\n          name: 'tags',\n          emptyValue: { key: '', value: '' },\n          maxLength: 2,\n          form: formResult.current,\n          keyProperty: 'key',\n        }),\n      { wrapper: DefaultWrapper },\n    );\n\n    result.current.removeOrUpdate(1);\n\n    expect(result.current.getTagsValues()).toStrictEqual([\n      { key: 'key1', value: 'value1' },\n      { key: '', value: '' },\n    ]);\n  });\n\n  it('should add an empty tag to the end of the array when removeOrUpdate is called when the max number of tags are present', () => {\n    const { result: formResult } = renderHook(() =>\n      useForm<{ tags: { key: string; value: string }[] }>({\n        defaultValues: {\n          tags: [\n            { key: 'key1', value: 'value1' },\n            { key: 'key2', value: 'value2' },\n          ],\n        },\n ", "() => {\n    const { result: formResult } = renderHook(() =>\n      useForm<{ tags: { key: string; value: string }[] }>({\n        defaultValues: {\n          tags: [\n            { key: 'key1', value: 'value1' },\n            { key: 'key2', value: 'value2' },\n          ],\n        },\n      }),\n    );\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentFieldArray({\n          name: 'tags',\n          emptyValue: { key: '', value: '' },\n          maxLength: 2,\n          form: formResult.current,\n          keyProperty: 'key',\n        }),\n      { wrapper: DefaultWrapper },\n    );\n\n    result.current.removeOrUpdate(0);\n\n    expect(result.current.getTagsValues()).toStrictEqual([\n      { key: 'key2', value: 'value2' },\n      { key: '', value: '' },\n    ]);\n  });\n});\n", 'import { renderHook } from \'@testing-library/react\';\nimport { useForm, FormProvider } from \'react-hook-form\';\nimport { IntlProvider } from \'react-intl\';\n\nimport { useTagAssignmentForm } from \'./useTagAssignmentForm\';\n\nconst DefaultWrapper = ({ children }: { children: React.ReactNode }) => {\n  return <IntlProvider locale="en">{children}</IntlProvider>;\n};\n\ndescribe(\'useTagAssignmentForm\', () => {\n  it(\'should use passed form as prop\', () => {\n    const { result: formResult } = renderHook(() =>\n      useForm<{ input: string; tags: { key: string; value: undefined }[] }>({ defaultValues: { input: \'test_input\' } }),\n    );\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm({\n          name: \'tags\',\n          emptyValue: { key: \'\', value: undefined },\n          form: formResult.current,\n          keyProperty: \'key\',\n          valueProperty: \'value\',\n        }),\n      { wrapper: DefaultWrapper },\n    );\n    const values = result.current.form.getValues();\n    expect(values.tags).toStrictEqual([{ key: \'\', value: undefined }]);\n    expect(values.input).toBe(\'test_input\');\n  });\n\n  it(\'should use context form if no form prop is passed\', () => {\n    const Wrapper = ({ children }: { children: React.ReactNode }) => {\n      const methods = useForm<{ input: string; tags: { key: string; value: undefined }[] }>({\n        defaultValues: { input: \'test_input\' },\n      });\n      return (\n        <FormProvider {...methods}>\n          <IntlProvider locale="en">{children}</IntlProvider>\n        </FormProvider>\n      );\n    };\n\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm({\n  ', '= useForm<{ input: string; tags: { key: string; value: undefined }[] }>({\n        defaultValues: { input: \'test_input\' },\n      });\n      return (\n        <FormProvider {...methods}>\n          <IntlProvider locale="en">{children}</IntlProvider>\n        </FormProvider>\n      );\n    };\n\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm({\n          name: \'tags\',\n          emptyValue: { key: \'\', value: undefined },\n          keyProperty: \'key\',\n          valueProperty: \'value\',\n        }),\n      { wrapper: Wrapper },\n    );\n\n    const values = result.current.form.getValues();\n    expect(values[\'tags\']).toStrictEqual([{ key: \'\', value: undefined }]);\n    expect(values[\'input\']).toBe(\'test_input\');\n  });\n\n  it(\'should add an empty value on default values provided by form context\', () => {\n    const Wrapper = ({ children }: { children: React.ReactNode }) => {\n      const methods = useForm<{ input: string; tags: { key: string; value: string | undefined }[] }>({\n        defaultValues: { input: \'test_input\', tags: [{ key: \'defaultKey\', value: \'defaultValue\' }] },\n      });\n      return (\n        <FormProvider {...methods}>\n          <IntlProvider locale="en">{children}</IntlProvider>\n        </FormProvider>\n      );\n    };\n\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm({\n          name: \'tags\',\n   ', 'defaultValues: { input: \'test_input\', tags: [{ key: \'defaultKey\', value: \'defaultValue\' }] },\n      });\n      return (\n        <FormProvider {...methods}>\n          <IntlProvider locale="en">{children}</IntlProvider>\n        </FormProvider>\n      );\n    };\n\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm({\n          name: \'tags\',\n          emptyValue: { key: \'\', value: undefined },\n          keyProperty: \'key\',\n          valueProperty: \'value\',\n        }),\n      { wrapper: Wrapper },\n    );\n\n    const values = result.current.form.getValues();\n    expect(values[\'tags\']).toStrictEqual([\n      { key: \'defaultKey\', value: \'defaultValue\' },\n      { key: \'\', value: undefined },\n    ]);\n    expect(values[\'input\']).toBe(\'test_input\');\n  });\n\n  it(\'should throw an error if no form is passed and not in a form context\', () => {\n    expect(() =>\n      renderHook(\n        () =>\n          useTagAssignmentForm({\n            name: \'tags\',\n            emptyValue: { key: \'\', value: undefined },\n            keyProperty: \'key\',\n            valueProperty: \'value\',\n          }),\n        { wrapper: DefaultWrapper },\n      ),\n    ).toThrow(\'Nest your component on a FormProvider or pass a form prop\');\n  });\n\n  ', '          name: \'tags\',\n            emptyValue: { key: \'\', value: undefined },\n            keyProperty: \'key\',\n            valueProperty: \'value\',\n          }),\n        { wrapper: DefaultWrapper },\n      ),\n    ).toThrow(\'Nest your component on a FormProvider or pass a form prop\');\n  });\n\n  it(\'should throw an error if default values are passed and in a form context\', () => {\n    const Wrapper = ({ children }: { children: React.ReactNode }) => {\n      const methods = useForm<{ input: string; tags: { key: string; value: string | undefined }[] }>({\n        defaultValues: { input: \'test_input\' },\n      });\n      return (\n        <FormProvider {...methods}>\n          <IntlProvider locale="en">{children}</IntlProvider>\n        </FormProvider>\n      );\n    };\n\n    expect(() =>\n      renderHook(\n        () =>\n          useTagAssignmentForm({\n            name: \'tags\',\n            emptyValue: { key: \'\', value: undefined },\n            keyProperty: \'key\',\n            valueProperty: \'value\',\n            defaultValues: [{ key: \'defaultKey\', value: \'defaultValue\' }],\n          }),\n        { wrapper: Wrapper },\n      ),\n   ', "       name: 'tags',\n            emptyValue: { key: '', value: undefined },\n            keyProperty: 'key',\n            valueProperty: 'value',\n            defaultValues: [{ key: 'defaultKey', value: 'defaultValue' }],\n          }),\n        { wrapper: Wrapper },\n      ),\n    ).toThrow('Define defaultValues at form context level');\n  });\n\n  it('should use empty value if no default values are passed', () => {\n    const { result: formResult } = renderHook(() => useForm());\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm({\n          name: 'tags',\n          emptyValue: { key: '', value: undefined },\n          keyProperty: 'key',\n          valueProperty: 'value',\n          form: formResult.current,\n        }),\n      { wrapper: DefaultWrapper },\n    );\n\n    const values = result.current.form.getValues();\n    expect(values['tags']).toStrictEqual([{ key: '', value: undefined }]);\n  });\n\n  it('should use default values + empty value if default values are passed', () => {\n    const { result: formResult } = renderHook(() => useForm<{ tags: { key: string; value: string | undefined }[] }>());\n    const defaultValues = [{ key: 'defaultKey', value: 'defaultValue' }];\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm<{ tags: { key: string; value: string | undefined }[] }>({\n         ", "expect(values['tags']).toStrictEqual([{ key: '', value: undefined }]);\n  });\n\n  it('should use default values + empty value if default values are passed', () => {\n    const { result: formResult } = renderHook(() => useForm<{ tags: { key: string; value: string | undefined }[] }>());\n    const defaultValues = [{ key: 'defaultKey', value: 'defaultValue' }];\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm<{ tags: { key: string; value: string | undefined }[] }>({\n          name: 'tags',\n          emptyValue: { key: '', value: undefined },\n          defaultValues,\n          form: formResult.current,\n          keyProperty: 'key',\n          valueProperty: 'value',\n        }),\n      { wrapper: DefaultWrapper },\n    );\n\n    const values = result.current.form.getValues();\n    expect(values.tags).toStrictEqual([\n      { key: 'defaultKey', value: 'defaultValue' },\n      { key: '', value: undefined },\n    ]);\n  });\n\n  it('should not add the empty value to the form if maxLength is reached', () => {\n    const { result: formResult } = renderHook(() => useForm<{ tags: { key: string; value: string | undefined }[] }>());\n    const defaultValues = [\n      { key: 'key1', value: 'value1' },\n      { key: 'key2', value: 'value2' },\n    ];\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm<{ tags: { key: string; value: string | undefined }[] }>({\n          name: 'tags',\n          emptyValue: { ", "useForm<{ tags: { key: string; value: string | undefined }[] }>());\n    const defaultValues = [\n      { key: 'key1', value: 'value1' },\n      { key: 'key2', value: 'value2' },\n    ];\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm<{ tags: { key: string; value: string | undefined }[] }>({\n          name: 'tags',\n          emptyValue: { key: '', value: undefined },\n          defaultValues,\n          maxLength: 2,\n          form: formResult.current,\n          keyProperty: 'key',\n          valueProperty: 'value',\n        }),\n      { wrapper: DefaultWrapper },\n    );\n\n    const values = result.current.form.getValues();\n    expect(values.tags).toStrictEqual([\n      { key: 'key1', value: 'value1' },\n      { key: 'key2', value: 'value2' },\n    ]);\n  });\n\n  it('should wait for loading before resetting', () => {\n    const { result: formResult } = renderHook(() => useForm());\n    const { result, rerender } = renderHook(\n      ({ loading }) =>\n        useTagAssignmentForm({\n          name: 'tags',\n          emptyValue: { key: '', value: undefined },\n          loading,\n          form: formResult.current,\n          keyProperty: 'key',\n          valueProperty: 'value',\n        }),\n     ", "renderHook(\n      ({ loading }) =>\n        useTagAssignmentForm({\n          name: 'tags',\n          emptyValue: { key: '', value: undefined },\n          loading,\n          form: formResult.current,\n          keyProperty: 'key',\n          valueProperty: 'value',\n        }),\n      { initialProps: { loading: true }, wrapper: DefaultWrapper },\n    );\n\n    const initialValues = result.current.form.getValues();\n    expect(initialValues['tags']).toStrictEqual([]);\n\n    rerender({ loading: false });\n\n    const values = result.current.form.getValues();\n    expect(values['tags']).toStrictEqual([{ key: '', value: undefined }]);\n  });\n\n  it('should not override the other filled values when setting default values', () => {\n    const { result: formResult } = renderHook(() =>\n      useForm<{ tags: { key: string; value: string | undefined }[]; input1: string; input2: string }>({\n        defaultValues: {\n          input1: 'test_input1',\n          input2: 'test_input2',\n        },\n      }),\n    );\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm({\n          name: 'tags',\n          emptyValue: { key: '', value: undefined },\n          form: formResult.current,\n          keyProperty: 'key',\n          valueProperty: 'value',\n        }),\n      { wrapper: DefaultWrapper },\n    );\n\n ", "result } = renderHook(\n      () =>\n        useTagAssignmentForm({\n          name: 'tags',\n          emptyValue: { key: '', value: undefined },\n          form: formResult.current,\n          keyProperty: 'key',\n          valueProperty: 'value',\n        }),\n      { wrapper: DefaultWrapper },\n    );\n\n    const values = result.current.form.getValues();\n    expect(values.tags).toStrictEqual([{ key: '', value: undefined }]);\n    expect(values.input1).toBe('test_input1');\n    expect(values.input2).toBe('test_input2');\n  });\n});\n", "import invariant from 'invariant';\nimport { useEffect, useState } from 'react';\nimport type { ArrayPath, FieldArray, FieldValues, Path, PathValue, UseFormReturn } from 'react-hook-form';\nimport { useFormContext } from 'react-hook-form';\n\nimport { useTagAssignmentFieldArray } from './useTagAssignmentFieldArray';\n\nexport interface UseTagAssignmentProps<\n  T extends FieldValues,\n  K extends ArrayPath<T> = ArrayPath<T>,\n  V extends FieldArray<T, K> = FieldArray<T, K>,\n> {\n  name: K;\n  maxLength?: number;\n  emptyValue: V;\n  loading?: boolean;\n  defaultValues?: V[];\n  form?: UseFormReturn<T>;\n  keyProperty: keyof V extends string ? keyof V : never;\n  valueProperty: keyof V extends string ? keyof V : never;\n}\n\nexport function useTagAssignmentForm<\n  T extends FieldValues,\n  K extends ArrayPath<T> = ArrayPath<T>,\n  V extends FieldArray<T, K> = FieldArray<T, K>,\n>({\n  name,\n  maxLength,\n  emptyValue,\n  defaultValues,\n  loading,\n  form,\n  keyProperty,\n  valueProperty,\n}: UseTagAssignmentProps<T, K, V>) {\n  const [_emptyValue] = useState(emptyValue);\n\n  const formCtx = useFormContext<T>();\n  const shouldUseFormContext = Boolean(formCtx) && !form;\n  const internalForm = shouldUseFormContext ? formCtx : form;\n\n  invariant(internalForm, 'Nest your component on a FormProvider or pass a form prop');\n  invariant(!(defaultValues && shouldUseFormContext), 'Define defaultValues at form context level');\n\n  const { setValue } = internalForm;\n\n  const fieldArrayMethods = useTagAssignmentFieldArray({\n    name,\n    maxLength,\n    emptyValue,\n    form: internalForm,\n    keyProperty,\n  });\n  const getTagsValues = fieldArrayMethods.getTagsValues;\n\n  useEffect(() => {\n    if (loading) return;\n    if (defaultValues) {\n      const newValues = [...defaultValues];\n      if (!maxLength || (maxLength && newValues.length < maxLength)) {\n        newValues.push(_emptyValue);\n      }\n      setValue(name as Path<T>, newValues as PathValue<T, Path<T>>);\n      return;\n    }\n\n    if (shouldUseFormContext) {\n      const existentValues = getTagsValues() ?? [];\n      if (!maxLength || (maxLength && existentValues.length < maxLength)) {\n        existentValues.push(_emptyValue);\n      }\n      setValue(name as Path<T>, existentValues ", ' if (!maxLength || (maxLength && newValues.length < maxLength)) {\n        newValues.push(_emptyValue);\n      }\n      setValue(name as Path<T>, newValues as PathValue<T, Path<T>>);\n      return;\n    }\n\n    if (shouldUseFormContext) {\n      const existentValues = getTagsValues() ?? [];\n      if (!maxLength || (maxLength && existentValues.length < maxLength)) {\n        existentValues.push(_emptyValue);\n      }\n      setValue(name as Path<T>, existentValues as PathValue<T, Path<T>>);\n      return;\n    }\n\n    setValue(name as Path<T>, [_emptyValue] as PathValue<T, Path<T>>);\n  }, [defaultValues, setValue, loading, maxLength, name, _emptyValue, shouldUseFormContext, getTagsValues]);\n\n  return {\n    ...fieldArrayMethods,\n    form: internalForm,\n    maxLength,\n    emptyValue,\n    name,\n    keyProperty,\n    valueProperty,\n  };\n}\n\nexport type UseTagAssignmentFormReturn<\n  T extends FieldValues = FieldValues,\n  K extends ArrayPath<T> = ArrayPath<T>,\n  V extends FieldArray<T, K> = FieldArray<T, K>,\n> = ReturnType<typeof useTagAssignmentForm<T, K, V>>;\n', "import { FormProvider, useForm } from 'react-hook-form';\n\nimport { TagAssignmentContext } from '../context/TagAssignmentContextProvider';\nimport { useTagAssignmentForm } from '../hooks/useTagAssignmentForm';\n\ninterface TestFormI {\n  tags: {\n    key: string;\n    value: string;\n  }[];\n}\n\ninterface TestTagAssignmentContextProviderProps extends Partial<ReturnType<typeof useTagAssignmentForm<TestFormI>>> {\n  children: React.ReactNode;\n}\n\nexport function TestTagAssignmentContextProvider({ children, ...props }: TestTagAssignmentContextProviderProps) {\n  const form = useForm<TestFormI>();\n  const tagForm = useTagAssignmentForm<TestFormI, 'tags', { key: string; value: string }>({\n    form,\n    name: 'tags',\n    emptyValue: { key: '', value: '' },\n    keyProperty: 'key',\n    valueProperty: 'value',\n  });\n  return (\n    <FormProvider {...form}>\n      <TagAssignmentContext.Provider\n        value={{\n          ...(tagForm as any),\n          ...props,\n        }}\n      >\n        {children}\n      </TagAssignmentContext.Provider>\n    </FormProvider>\n  );\n}\n", 'import { GenericSkeleton, ParagraphSkeleton, Typography, useDesignSystemTheme } from \'@databricks/design-system\';\nimport type { ReactNode } from \'react\';\nimport { useRef } from \'react\';\nimport { FormattedMessage } from \'react-intl\';\nimport useResponsiveContainer from \'./useResponsiveContainer\';\n\nexport interface AsideSectionProps {\n  id: string;\n  title?: ReactNode;\n  content: ReactNode;\n  isTitleLoading?: boolean;\n}\n\nexport type MaybeAsideSection = AsideSectionProps | null;\nexport type AsideSections = Array<MaybeAsideSection>;\n\nconst SIDEBAR_WIDTHS = {\n  sm: 316,\n  lg: 480,\n} as const;\nconst VERTICAL_MARGIN_PX = 16;\nconst DEFAULT_MAX_WIDTH = 450;\n\nexport const OverviewLayout = ({\n  isLoading,\n  asideSections,\n  children,\n  isTabLayout = true,\n  sidebarSize = \'sm\',\n  verticalStackOrder,\n}: {\n  isLoading?: boolean;\n  asideSections: AsideSections;\n  children: ReactNode;\n  isTabLayout?: boolean;\n  sidebarSize?: \'sm\' | \'lg\';\n  verticalStackOrder?: \'main-first\' | \'aside-first\';\n}) => {\n  const { theme } = useDesignSystemTheme();\n  const containerRef = useRef<HTMLDivElement>(null);\n\n  const stackVertically = useResponsiveContainer(containerRef, { small: theme.responsive.breakpoints.lg }) === \'small\';\n\n  // Determine vertical stack order, i.e. should the main content be on top or bottom\n  const verticalDisplayPrimaryContentOnTop = verticalStackOrder === \'main-first\';\n\n  const totalSidebarWidth = SIDEBAR_WIDTHS[sidebarSize];\n  const innerSidebarWidth = totalSidebarWidth - VERTICAL_MARGIN_PX;\n\n  const secondaryStackedStyles = stackVertically\n    ? verticalDisplayPrimaryContentOnTop\n      ? { width: \'100%\' }\n      : { borderBottom: `1px solid ${theme.colors.border}`, width: \'100%\' }\n    : verticalDisplayPrimaryContentOnTop\n    ? {\n        width: innerSidebarWidth,\n      }\n    : {\n        paddingBottom: theme.spacing.sm,\n        width: innerSidebarWidth,\n      };\n\n  return (\n    <div\n      data-testid="entity-overview-container"\n      ref={containerRef}\n      css={{\n        display: \'flex\',\n        flexDirection: stackVertically ? (verticalDisplayPrimaryContentOnTop ? \'column\' : \'column-reverse\') : \'row\',\n        gap: theme.spacing.lg,\n      }}\n    >\n      <div\n        css={{\n      ', '   width: innerSidebarWidth,\n      };\n\n  return (\n    <div\n      data-testid="entity-overview-container"\n      ref={containerRef}\n      css={{\n        display: \'flex\',\n        flexDirection: stackVertically ? (verticalDisplayPrimaryContentOnTop ? \'column\' : \'column-reverse\') : \'row\',\n        gap: theme.spacing.lg,\n      }}\n    >\n      <div\n        css={{\n          display: \'flex\',\n          flexGrow: 1,\n          flexDirection: \'column\',\n          gap: theme.spacing.md,\n          width: stackVertically ? \'100%\' : `calc(100% - ${totalSidebarWidth}px)`,\n        }}\n      >\n        {isLoading ? <GenericSkeleton /> : children}\n      </div>\n      <div\n        style={{\n          display: \'flex\',\n          ...(isTabLayout && { marginTop: -theme.spacing.md }), // remove the gap between tab list and sidebar content\n        }}\n      >\n        <div\n          css={{\n            display: \'flex\',\n            flexDirection: \'column\',\n            gap: theme.spacing.lg,\n            ...secondaryStackedStyles,\n          }}\n        >\n          {isLoading ', "  >\n        <div\n          css={{\n            display: 'flex',\n            flexDirection: 'column',\n            gap: theme.spacing.lg,\n            ...secondaryStackedStyles,\n          }}\n        >\n          {isLoading && <GenericSkeleton />}\n          {!isLoading && <SidebarWrapper secondarySections={asideSections} />}\n        </div>\n      </div>\n    </div>\n  );\n};\n\nconst SidebarWrapper = ({ secondarySections }: { secondarySections: AsideSections }) => {\n  return (\n    <div>\n      {secondarySections\n        .filter((section) => section !== null)\n        .filter((section) => section?.content !== null)\n        .map(({ title, isTitleLoading, content, id }, index) => (\n          <AsideSection title={title} isTitleLoading={isTitleLoading} content={content} key={id} index={index} />\n        ))}\n    </div>\n  );\n};\n\nexport const AsideSectionTitle = ({ children }: { children: ReactNode }) => {\n  const { theme } = useDesignSystemTheme();\n  return (\n    <Typography.Title\n      level={4}\n      style={{\n        whiteSpace: 'nowrap',\n        marginRight: theme.spacing.lg,\n        marginTop: 0,\n      }}\n    >\n      {children}\n    </Typography.Title>\n  );\n};\n\nconst AsideSection = ({\n  title,\n  content,\n  index,\n  isTitleLoading = false,\n}: Omit<AsideSectionProps, 'id'> & {\n  index: number;\n}) => {\n  const { theme } = useDesignSystemTheme();\n\n  const titleComponent ", ' return (\n    <Typography.Title\n      level={4}\n      style={{\n        whiteSpace: \'nowrap\',\n        marginRight: theme.spacing.lg,\n        marginTop: 0,\n      }}\n    >\n      {children}\n    </Typography.Title>\n  );\n};\n\nconst AsideSection = ({\n  title,\n  content,\n  index,\n  isTitleLoading = false,\n}: Omit<AsideSectionProps, \'id\'> & {\n  index: number;\n}) => {\n  const { theme } = useDesignSystemTheme();\n\n  const titleComponent = isTitleLoading ? (\n    <ParagraphSkeleton\n      label={\n        <FormattedMessage\n          defaultMessage="Section title loading"\n          description="Loading skeleton label for overview page section title in Catalog Explorer"\n        />\n      }\n    />\n  ) : title ? (\n    <AsideSectionTitle>{title}</AsideSectionTitle>\n  ) : null;\n\n  const compactStyles = { padding: `${theme.spacing.md}px 0 ${theme.spacing.md}px 0` };\n\n  return (\n    <div\n      css={{\n        ...compactStyles,\n        ...(index === 0 ? {} : { borderTop: `1px solid ${theme.colors.border}` }),\n      }}\n    >\n      {titleComponent}\n      {content}\n    </div>\n  );\n};\n\nexport const KeyValueProperty = ({\n  keyValue,\n  value,\n  maxWidth,\n}: {\n  keyValue: string;\n  value: React.ReactNode;\n  maxWidth?: number | string;\n}) => {\n  const { theme } = useDesignSystemTheme();\n  return (\n    <div\n      css={{\n        display: \'flex\',\n        alignItems: \'center\',\n        \'&:has(+ div)\': {\n          marginBottom: theme.spacing.xs,\n   ', '  {titleComponent}\n      {content}\n    </div>\n  );\n};\n\nexport const KeyValueProperty = ({\n  keyValue,\n  value,\n  maxWidth,\n}: {\n  keyValue: string;\n  value: React.ReactNode;\n  maxWidth?: number | string;\n}) => {\n  const { theme } = useDesignSystemTheme();\n  return (\n    <div\n      css={{\n        display: \'flex\',\n        alignItems: \'center\',\n        \'&:has(+ div)\': {\n          marginBottom: theme.spacing.xs,\n        },\n        maxWidth: maxWidth ?? DEFAULT_MAX_WIDTH,\n        wordBreak: \'break-word\',\n        lineHeight: theme.typography.lineHeightLg,\n      }}\n    >\n      <div\n        css={{\n          color: theme.colors.textSecondary,\n          flex: 0.5,\n          alignSelf: \'start\',\n        }}\n      >\n        {keyValue}\n      </div>\n      <div\n        css={{\n          flex: 1,\n          alignSelf: \'start\',\n          overflow: \'hidden\',\n        }}\n      >\n        {value}\n      </div>\n    </div>\n  );\n};\n\nexport const NoneCell = () => {\n  return (\n    <Typography.Text color="secondary">\n      <FormattedMessage defaultMessage="None" description="Cell value when there\'s no content" />\n    </Typography.Text>\n  );\n};\n', '<head>\n  <link\n    rel="stylesheet"\n    href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/xcode.min.css"\n  />\n  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>\n  <script>\n    hljs.highlightAll();\n  </script>\n  <style>\n    body {\n      margin: 0;\n      font-family: -apple-system, BlinkMacSystemFont, Segoe UI, Roboto,\n        Helvetica Neue, Arial, Noto Sans, sans-serif, Apple Color Emoji,\n        Segoe UI Emoji, Segoe UI Symbol, Noto Color Emoji;\n      -webkit-tap-highlight-color: rgba(0, 0, 0, 0);\n      margin: 0;\n      font-weight: 400;\n      font-size: 13px;\n      line-height: 18px;\n      color: rgb(17, 23, 28);\n    }\n    code {\n      line-height: 18px;\n      font-size: 11px;\n      background: rgb(250, 250, 250) !important;\n    }\n    pre {\n      background: rgb(250, 250, 250);\n      margin: 0;\n      display: none;\n    }\n    pre.active {\n      display: unset;\n    }\n    button {\n      white-space: nowrap;\n      text-align: center;\n      position: relative;\n      cursor: pointer;\n      background: rgba(34, 114, 180, 0) !important;\n      color: rgb(34, 114, 180) !important;\n      border-color: rgba(34, 114, 180, 0) !important;\n      padding: 4px 6px !important;\n      text-decoration: none !important;\n      line-height: 20px !important;\n      box-shadow: none !important;\n      height: 32px !important;\n      display: inline-flex !important;\n      -webkit-box-align: center !important;\n      align-items: center !important;\n      -webkit-box-pack: center ', ' background: rgba(34, 114, 180, 0) !important;\n      color: rgb(34, 114, 180) !important;\n      border-color: rgba(34, 114, 180, 0) !important;\n      padding: 4px 6px !important;\n      text-decoration: none !important;\n      line-height: 20px !important;\n      box-shadow: none !important;\n      height: 32px !important;\n      display: inline-flex !important;\n      -webkit-box-align: center !important;\n      align-items: center !important;\n      -webkit-box-pack: center !important;\n      justify-content: center !important;\n      vertical-align: middle !important;\n    }\n    p {\n      margin: 0;\n      padding: 0;\n    }\n    button:hover {\n      background: rgba(34, 114, 180, 0.08) !important;\n      color: rgb(14, 83, 139) !important;\n    }\n    button:active {\n      background: rgba(34, 114, 180, 0.16) !important;\n      color: rgb(4, 53, 93) !important;\n    }\n    h1 {\n      margin-top: 4px;\n      font-size: 22px;\n    }\n    .info {\n      font-size: 12px;\n      font-weight: 500;\n      line-height: 16px;\n      color: rgb(95, 114, 129);\n    }\n    .tabs {\n      margin-top: 10px;\n      border-bottom: 1px solid rgb(209, 217, 225) !important;\n      display: flex;\n      line-height: 24px;\n    }\n    .tab {\n      font-size: 13px;\n      font-weight: 600 !important;\n      cursor: pointer;\n      margin: 0 24px 0 2px;\n      ', ' line-height: 16px;\n      color: rgb(95, 114, 129);\n    }\n    .tabs {\n      margin-top: 10px;\n      border-bottom: 1px solid rgb(209, 217, 225) !important;\n      display: flex;\n      line-height: 24px;\n    }\n    .tab {\n      font-size: 13px;\n      font-weight: 600 !important;\n      cursor: pointer;\n      margin: 0 24px 0 2px;\n      padding-left: 2px;\n    }\n    .tab:hover {\n      color: rgb(14, 83, 139) !important;\n    }\n    .tab.active {\n      border-bottom: 3px solid rgb(34, 114, 180) !important;\n    }\n    .link {\n      margin-left: 12px;\n      display: inline-block;\n      text-decoration: none;\n      color: rgb(34, 114, 180) !important;\n      font-size: 13px;\n      font-weight: 400;\n    }\n    .link:hover {\n      color: rgb(14, 83, 139) !important;\n    }\n    .link-content {\n      display: flex;\n      gap: 6px;\n      align-items: center;\n    }\n    .caret-up {\n      transform: rotate(180deg);\n    }\n  </style>\n</head>\n<body>\n  <div style="display: flex; align-items: center">\n    The logged model is compatible with the Mosaic AI Agent Framework.\n    <button onclick="toggleCode()">\n      See how to evaluate the model&nbsp;\n      <span\n        role="img"\n        id="caret"\n        aria-hidden="true"\n        class="anticon css-6xix1i"\n        style="font-size: ', '  .caret-up {\n      transform: rotate(180deg);\n    }\n  </style>\n</head>\n<body>\n  <div style="display: flex; align-items: center">\n    The logged model is compatible with the Mosaic AI Agent Framework.\n    <button onclick="toggleCode()">\n      See how to evaluate the model&nbsp;\n      <span\n        role="img"\n        id="caret"\n        aria-hidden="true"\n        class="anticon css-6xix1i"\n        style="font-size: 14px"\n        ><svg\n          xmlns="http://www.w3.org/2000/svg"\n          width="1em"\n          height="1em"\n          fill="none"\n          viewBox="0 0 16 16"\n          aria-hidden="true"\n          focusable="false"\n          class=""\n        >\n          <path\n            fill="currentColor"\n            fill-rule="evenodd"\n            d="M8 8.917 10.947 6 12 7.042 8 11 4 7.042 5.053 6z"\n            clip-rule="evenodd"\n          ></path>\n        </svg>\n      </span>\n    </button>\n  </div>\n  <div id="code" style="display: none">\n    <h1>\n      Agent evaluation\n      <a\n        class="link"\n        href="https://docs.databricks.com/en/generative-ai/agent-evaluation/synthesize-evaluation-set.html?utm_source=mlflow.log_model&utm_medium=notebook"\n        target="_blank"\n      ', '7.042 8 11 4 7.042 5.053 6z"\n            clip-rule="evenodd"\n          ></path>\n        </svg>\n      </span>\n    </button>\n  </div>\n  <div id="code" style="display: none">\n    <h1>\n      Agent evaluation\n      <a\n        class="link"\n        href="https://docs.databricks.com/en/generative-ai/agent-evaluation/synthesize-evaluation-set.html?utm_source=mlflow.log_model&utm_medium=notebook"\n        target="_blank"\n      >\n        <span class="link-content">\n          Learn more\n          <span role="img" aria-hidden="true" class="anticon css-6xix1i"\n            ><svg\n              xmlns="http://www.w3.org/2000/svg"\n              width="1em"\n              height="1em"\n              fill="none"\n              viewBox="0 0 16 16"\n              aria-hidden="true"\n              focusable="false"\n              class=""\n            >\n              <path\n                fill="currentColor"\n                d="M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z"\n              ></path>\n             ', '           class=""\n            >\n              <path\n                fill="currentColor"\n                d="M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z"\n              ></path>\n              <path\n                fill="currentColor"\n                d="M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z"\n              ></path></svg></span></span\n      ></a>\n    </h1>\n    <p class="info">\n      Copy the following code snippet in a notebook cell (right click â†’ copy)\n    </p>\n    <div class="tabs">\n      <div class="tab active" onclick="tabClicked(0)">Using synthetic data</div>\n      <div class="tab" onclick="tabClicked(1)">Using your own dataset</div>\n    </div>\n    <div style="height: 472px">\n      <pre\n        class="active"\n      ><code class="language-python">{{eval_with_synthetic_code}}</code></pre>\n\n      <pre><code class="language-python">{{eval_with_dataset_code}}</code></pre>\n    </div>\n  </div>\n  <script>\n    var codeShown = false;\n    function clip(el) {\n      var range = document.createRange();\n      range.selectNodeContents(el);\n      var sel = window.getSelection();\n      sel.removeAllRanges();\n      sel.addRange(range);\n    }\n\n    function toggleCode() {\n      if (codeShown) {\n        document.getElementById("code").style.display = "none";\n    ', '   ><code class="language-python">{{eval_with_synthetic_code}}</code></pre>\n\n      <pre><code class="language-python">{{eval_with_dataset_code}}</code></pre>\n    </div>\n  </div>\n  <script>\n    var codeShown = false;\n    function clip(el) {\n      var range = document.createRange();\n      range.selectNodeContents(el);\n      var sel = window.getSelection();\n      sel.removeAllRanges();\n      sel.addRange(range);\n    }\n\n    function toggleCode() {\n      if (codeShown) {\n        document.getElementById("code").style.display = "none";\n        codeShown = false;\n      } else {\n        document.getElementById("code").style.display = "block";\n        clip(document.querySelector("pre.active"));\n        codeShown = true;\n      }\n      document.getElementById("caret").classList.toggle("caret-up");\n    }\n\n    function tabClicked(tabIndex) {\n      document.querySelectorAll(".tab").forEach((tab, index) => {\n        if (index === tabIndex) {\n          tab.classList.add("active");\n        } else {\n          tab.classList.remove("active");\n        }\n      });\n      document.querySelectorAll("pre").forEach((pre, index) => {\n        if (index === tabIndex) {\n          pre.classList.add("active");\n        } else {\n          pre.classList.remove("active");\n        }\n      });\n      clip(document.querySelector("pre.active"));\n    }\n  </script>\n</body>\n', '<!DOCTYPE html>\n<html lang="en">\n  <head>\n    <meta charset="utf-8" />\n    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />\n    <link rel="shortcut icon" href="./static-files/favicon.ico" />\n    <meta name="theme-color" content="#000000" />\n    <!--\n      manifest.json provides metadata used when your web app is added to the\n      homescreen on Android. See https://developers.google.com/web/fundamentals/engage-and-retain/web-app-manifest/\n    -->\n    <link rel="manifest" href="./static-files/manifest.json" crossorigin="use-credentials" />\n    <title>MLflow</title>\n  </head>\n\n  <body>\n    <noscript> You need to enable JavaScript to run this app. </noscript>\n    <div id="root" class="mlflow-ui-container"></div>\n    <div id="modal" class="mlflow-ui-container"></div>\n  </body>\n</html>\n', '<!DOCTYPE html>\n<html>\n  <head>\n    <title>Test HTML</title>\n  </head>\n  <body>\n    <h1>Test HTML</h1>\n    <p>This is a test HTML file.</p>\n  </body>\n</html>', '<html>\n  <head></head>\n  <body>\n    <div id="root"></div>\n  </body>\n</html>\n', '<head>\n  <link\n    rel="stylesheet"\n    href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/xcode.min.css"\n  />\n  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>\n  <script>\n    hljs.highlightAll();\n  </script>\n  <style>\n    body {\n      margin: 0;\n      font-family: -apple-system, BlinkMacSystemFont, Segoe UI, Roboto,\n        Helvetica Neue, Arial, Noto Sans, sans-serif, Apple Color Emoji,\n        Segoe UI Emoji, Segoe UI Symbol, Noto Color Emoji;\n      -webkit-tap-highlight-color: rgba(0, 0, 0, 0);\n      margin: 0;\n      font-weight: 400;\n      font-size: 13px;\n      line-height: 18px;\n      color: rgb(17, 23, 28);\n    }\n    code {\n      line-height: 18px;\n      font-size: 11px;\n      background: rgb(250, 250, 250) !important;\n    }\n    pre {\n      background: rgb(250, 250, 250);\n      margin: 0;\n      display: none;\n    }\n    pre.active {\n      display: unset;\n    }\n    button {\n      white-space: nowrap;\n      text-align: center;\n      position: relative;\n      cursor: pointer;\n      background: rgba(34, 114, 180, 0) !important;\n      color: rgb(34, 114, 180) !important;\n      border-color: rgba(34, 114, 180, 0) !important;\n      padding: 4px 6px !important;\n      text-decoration: none !important;\n      line-height: 20px !important;\n      box-shadow: none !important;\n      height: 32px !important;\n      display: inline-flex !important;\n      -webkit-box-align: center !important;\n      align-items: center !important;\n      -webkit-box-pack: center ', ' background: rgba(34, 114, 180, 0) !important;\n      color: rgb(34, 114, 180) !important;\n      border-color: rgba(34, 114, 180, 0) !important;\n      padding: 4px 6px !important;\n      text-decoration: none !important;\n      line-height: 20px !important;\n      box-shadow: none !important;\n      height: 32px !important;\n      display: inline-flex !important;\n      -webkit-box-align: center !important;\n      align-items: center !important;\n      -webkit-box-pack: center !important;\n      justify-content: center !important;\n      vertical-align: middle !important;\n    }\n    p {\n      margin: 0;\n      padding: 0;\n    }\n    button:hover {\n      background: rgba(34, 114, 180, 0.08) !important;\n      color: rgb(14, 83, 139) !important;\n    }\n    button:active {\n      background: rgba(34, 114, 180, 0.16) !important;\n      color: rgb(4, 53, 93) !important;\n    }\n    h1 {\n      margin-top: 4px;\n      font-size: 22px;\n    }\n    .info {\n      font-size: 12px;\n      font-weight: 500;\n      line-height: 16px;\n      color: rgb(95, 114, 129);\n    }\n    .tabs {\n      margin-top: 10px;\n      border-bottom: 1px solid rgb(209, 217, 225) !important;\n      display: flex;\n      line-height: 24px;\n    }\n    .tab {\n      font-size: 13px;\n      font-weight: 600 !important;\n      cursor: pointer;\n      margin: 0 24px 0 2px;\n      ', ' line-height: 16px;\n      color: rgb(95, 114, 129);\n    }\n    .tabs {\n      margin-top: 10px;\n      border-bottom: 1px solid rgb(209, 217, 225) !important;\n      display: flex;\n      line-height: 24px;\n    }\n    .tab {\n      font-size: 13px;\n      font-weight: 600 !important;\n      cursor: pointer;\n      margin: 0 24px 0 2px;\n      padding-left: 2px;\n    }\n    .tab:hover {\n      color: rgb(14, 83, 139) !important;\n    }\n    .tab.active {\n      border-bottom: 3px solid rgb(34, 114, 180) !important;\n    }\n    .link {\n      margin-left: 12px;\n      display: inline-block;\n      text-decoration: none;\n      color: rgb(34, 114, 180) !important;\n      font-size: 13px;\n      font-weight: 400;\n    }\n    .link:hover {\n      color: rgb(14, 83, 139) !important;\n    }\n    .link-content {\n      display: flex;\n      gap: 6px;\n      align-items: center;\n    }\n    .caret-up {\n      transform: rotate(180deg);\n    }\n  </style>\n</head>\n<body>\n  <div style="display: flex; align-items: center">\n    The logged model is compatible with the Mosaic AI Agent Framework.\n    <button onclick="toggleCode()">\n      See how to evaluate the model&nbsp;\n      <span\n        role="img"\n        id="caret"\n        aria-hidden="true"\n        class="anticon css-6xix1i"\n        style="font-size: ', '  .caret-up {\n      transform: rotate(180deg);\n    }\n  </style>\n</head>\n<body>\n  <div style="display: flex; align-items: center">\n    The logged model is compatible with the Mosaic AI Agent Framework.\n    <button onclick="toggleCode()">\n      See how to evaluate the model&nbsp;\n      <span\n        role="img"\n        id="caret"\n        aria-hidden="true"\n        class="anticon css-6xix1i"\n        style="font-size: 14px"\n        ><svg\n          xmlns="http://www.w3.org/2000/svg"\n          width="1em"\n          height="1em"\n          fill="none"\n          viewBox="0 0 16 16"\n          aria-hidden="true"\n          focusable="false"\n          class=""\n        >\n          <path\n            fill="currentColor"\n            fill-rule="evenodd"\n            d="M8 8.917 10.947 6 12 7.042 8 11 4 7.042 5.053 6z"\n            clip-rule="evenodd"\n          ></path>\n        </svg>\n      </span>\n    </button>\n  </div>\n  <div id="code" style="display: none">\n    <h1>\n      Agent evaluation\n      <a\n        class="link"\n        href="https://docs.databricks.com/en/generative-ai/agent-evaluation/synthesize-evaluation-set.html?utm_source=mlflow.log_model&utm_medium=notebook"\n        target="_blank"\n      ', '7.042 8 11 4 7.042 5.053 6z"\n            clip-rule="evenodd"\n          ></path>\n        </svg>\n      </span>\n    </button>\n  </div>\n  <div id="code" style="display: none">\n    <h1>\n      Agent evaluation\n      <a\n        class="link"\n        href="https://docs.databricks.com/en/generative-ai/agent-evaluation/synthesize-evaluation-set.html?utm_source=mlflow.log_model&utm_medium=notebook"\n        target="_blank"\n      >\n        <span class="link-content">\n          Learn more\n          <span role="img" aria-hidden="true" class="anticon css-6xix1i"\n            ><svg\n              xmlns="http://www.w3.org/2000/svg"\n              width="1em"\n              height="1em"\n              fill="none"\n              viewBox="0 0 16 16"\n              aria-hidden="true"\n              focusable="false"\n              class=""\n            >\n              <path\n                fill="currentColor"\n                d="M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z"\n              ></path>\n             ', '           class=""\n            >\n              <path\n                fill="currentColor"\n                d="M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z"\n              ></path>\n              <path\n                fill="currentColor"\n                d="M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z"\n              ></path></svg></span></span\n      ></a>\n    </h1>\n    <p class="info">\n      Copy the following code snippet in a notebook cell (right click â†’ copy)\n    </p>\n    <div class="tabs">\n      <div class="tab active" onclick="tabClicked(0)">Using synthetic data</div>\n      <div class="tab" onclick="tabClicked(1)">Using your own dataset</div>\n    </div>\n    <div style="height: 472px">\n      <pre\n        class="active"\n      ><code class="language-python">%pip install -U databricks-agents\ndbutils.library.restartPython()\n## Run the above in a separate cell ##\n\nfrom databricks.agents.evals import generate_evals_df\nimport mlflow\n\nagent_description = &quot;A chatbot that answers questions about Databricks.&quot;\nquestion_guidelines = &quot;&quot;&quot;\n# User personas\n- A developer new to the Databricks platform\n# Example questions\n- What API lets me parallelize operations over rows of a delta table?\n&quot;&quot;&quot;\n# TODO: Spark/Pandas DataFrame with &quot;content&quot; and &quot;doc_uri&quot; columns.\ndocs = spark.table(&quot;catalog.schema.my_table_of_docs&quot;)\nevals = generate_evals_df(\n    docs=docs,\n    num_evals=25,\n    agent_description=agent_description,\n    question_guidelines=question_guidelines,\n)\neval_result = mlflow.evaluate(data=evals, model=&quot;runs:/1/model&quot;, model_type=&quot;databricks-agent&quot;)\n</code></pre>\n\n      <pre><code class="language-python">%pip install -U databricks-agents\ndbutils.library.restartPython()\n## Run the above in a ', '   ><code class="language-python">%pip install -U databricks-agents\ndbutils.library.restartPython()\n## Run the above in a separate cell ##\n\nfrom databricks.agents.evals import generate_evals_df\nimport mlflow\n\nagent_description = &quot;A chatbot that answers questions about Databricks.&quot;\nquestion_guidelines = &quot;&quot;&quot;\n# User personas\n- A developer new to the Databricks platform\n# Example questions\n- What API lets me parallelize operations over rows of a delta table?\n&quot;&quot;&quot;\n# TODO: Spark/Pandas DataFrame with &quot;content&quot; and &quot;doc_uri&quot; columns.\ndocs = spark.table(&quot;catalog.schema.my_table_of_docs&quot;)\nevals = generate_evals_df(\n    docs=docs,\n    num_evals=25,\n    agent_description=agent_description,\n    question_guidelines=question_guidelines,\n)\neval_result = mlflow.evaluate(data=evals, model=&quot;runs:/1/model&quot;, model_type=&quot;databricks-agent&quot;)\n</code></pre>\n\n      <pre><code class="language-python">%pip install -U databricks-agents\ndbutils.library.restartPython()\n## Run the above in a separate cell ##\n\nimport pandas as pd\nimport mlflow\n\nevals = [\n    {\n        &quot;request&quot;: {\n            &quot;messages&quot;: [\n                {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;How do I convert a Spark DataFrame to Pandas?&quot;}\n            ],\n        },\n        # Optional, needed for judging correctness.\n        &quot;expected_facts&quot;: [\n            &quot;To convert a Spark DataFrame to Pandas, you can use the toPandas() method.&quot;\n        ],\n    }\n]\neval_result = mlflow.evaluate(\n    data=pd.DataFrame.from_records(evals), model=&quot;runs:/1/model&quot;, model_type=&quot;databricks-agent&quot;\n)\n</code></pre>\n    </div>\n  </div>\n  <script>\n    var codeShown = false;\n    function clip(el) {\n      var range = document.createRange();\n      range.selectNodeContents(el);\n      var sel = window.getSelection();\n      sel.removeAllRanges();\n      sel.addRange(range);\n    }\n\n    function toggleCode() {\n      if (codeShown) {\n        document.getElementById("code").style.display = "none";\n        codeShown = false;\n      } else ', ' </div>\n  </div>\n  <script>\n    var codeShown = false;\n    function clip(el) {\n      var range = document.createRange();\n      range.selectNodeContents(el);\n      var sel = window.getSelection();\n      sel.removeAllRanges();\n      sel.addRange(range);\n    }\n\n    function toggleCode() {\n      if (codeShown) {\n        document.getElementById("code").style.display = "none";\n        codeShown = false;\n      } else {\n        document.getElementById("code").style.display = "block";\n        clip(document.querySelector("pre.active"));\n        codeShown = true;\n      }\n      document.getElementById("caret").classList.toggle("caret-up");\n    }\n\n    function tabClicked(tabIndex) {\n      document.querySelectorAll(".tab").forEach((tab, index) => {\n        if (index === tabIndex) {\n          tab.classList.add("active");\n        } else {\n          tab.classList.remove("active");\n        }\n      });\n      document.querySelectorAll("pre").forEach((pre, index) => {\n        if (index === tabIndex) {\n          pre.classList.add("active");\n        } else {\n          pre.classList.remove("active");\n        }\n      });\n      clip(document.querySelector("pre.active"));\n    }\n  </script>\n</body>\n', "@import 'reset.css';\n@import 'common/components/EditableNote.css';\n@import 'model-registry/index.css';\n\na {\n  color: #2374bb;\n}\na:hover,\na:focus {\n  color: #005580;\n}\n\nbody {\n  margin: 0;\n  padding: 0;\n}\n\n#root {\n  height: 100%;\n  display: flex;\n  flex-direction: column;\n}\n", "[class^=ant-]::-ms-clear,\n[class*= ant-]::-ms-clear,\n[class^=ant-] input::-ms-clear,\n[class*= ant-] input::-ms-clear,\n[class^=ant-] input::-ms-reveal,\n[class*= ant-] input::-ms-reveal {\n  display: none;\n}\nhtml,\nbody {\n  width: 100%;\n  height: 100%;\n}\ninput::-ms-clear,\ninput::-ms-reveal {\n  display: none;\n}\n*,\n*::before,\n*::after {\n  box-sizing: border-box;\n}\nhtml {\n  font-family: sans-serif;\n  line-height: 1.15;\n  -webkit-text-size-adjust: 100%;\n  -ms-text-size-adjust: 100%;\n  -ms-overflow-style: scrollbar;\n  -webkit-tap-highlight-color: rgba(0, 0, 0, 0);\n}\n@-ms-viewport {\n  width: device-width;\n}\nbody {\n  margin: 0;\n  color: rgba(0, 0, 0, 0.85);\n  font-size: 14px;\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, 'Noto Sans', sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n  font-variant: tabular-nums;\n  line-height: 1.5715;\n  background-color: #fff;\n  font-feature-settings: 'tnum';\n}\n[tabindex='-1']:focus {\n  outline: none !important;\n}\nhr {\n  box-sizing: content-box;\n  height: 0;\n  overflow: visible;\n}\nh1,\nh2,\nh3,\nh4,\nh5,\nh6 {\n  margin-top: 0;\n  margin-bottom: 0.5em;\n  font-weight: 500;\n}\np {\n  margin-top: 0;\n  margin-bottom: 1em;\n}\nabbr[title],\nabbr[data-original-title] {\n  text-decoration: underline;\n  -webkit-text-decoration: underline dotted;\n          text-decoration: underline dotted;\n  border-bottom: 0;\n  cursor: help;\n}\naddress {\n  margin-bottom: 1em;\n  font-style: normal;\n  line-height: inherit;\n}\ninput[type='text'],\ninput[type='password'],\ninput[type='number'],\ntextarea {\n  -webkit-appearance: none;\n}\nol,\nul,\ndl {\n  margin-top: 0;\n  margin-bottom: 1em;\n}\nol ol,\nul ul,\nol ul,\nul ol {\n  margin-bottom: 0;\n}\ndt {\n  font-weight: 500;\n}\ndd {\n  margin-bottom: 0.5em;\n  margin-left: 0;\n}\nblockquote {\n  margin: 0 0 1em;\n}\ndfn {\n  font-style: italic;\n}\nb,\nstrong {\n  font-weight: bolder;\n}\nsmall {\n  font-size: 80%;\n}\nsub,\nsup {\n  position: relative;\n  font-size: 75%;\n  line-height: 0;\n  vertical-align: baseline;\n}\nsub {\n  bottom: -0.25em;\n}\nsup {\n  top: -0.5em;\n}\na {\n  color: #1890ff;\n  text-decoration: none;\n  background-color: transparent;\n  outline: none;\n  cursor: pointer;\n  transition: color 0.3s;\n  -webkit-text-decoration-skip: objects;\n}\na:hover {\n  color: #40a9ff;\n}\na:active {\n  color: #096dd9;\n}\na:active,\na:hover {\n  text-decoration: none;\n  outline: 0;\n}\na:focus {\n  text-decoration: none;\n  outline: 0;\n}\na[disabled] {\n  color: rgba(0, 0, 0, 0.25);\n  cursor: not-allowed;\n}\npre,\ncode,\nkbd,\nsamp {\n  font-size: 1em;\n  font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;\n}\npre {\n  margin-top: 0;\n  margin-bottom: 1em;\n  overflow: auto;\n}\nfigure {\n  margin: 0 0 1em;\n}\nimg {\n  vertical-align: middle;\n  border-style: none;\n}\nsvg:not(:root) {\n  overflow: hidden;\n}\na,\narea,\nbutton,\n[role='button'],\ninput:not([type='range']),\nlabel,\nselect,\nsummary,\ntextarea {\n  touch-action: manipulation;\n}\ntable {\n  border-collapse: collapse;\n}\ncaption {\n  padding-top: 0.75em;\n  padding-bottom: 0.3em;\n ", ' outline: none;\n  cursor: pointer;\n  transition: color 0.3s;\n  -webkit-text-decoration-skip: objects;\n}\na:hover {\n  color: #40a9ff;\n}\na:active {\n  color: #096dd9;\n}\na:active,\na:hover {\n  text-decoration: none;\n  outline: 0;\n}\na:focus {\n  text-decoration: none;\n  outline: 0;\n}\na[disabled] {\n  color: rgba(0, 0, 0, 0.25);\n  cursor: not-allowed;\n}\npre,\ncode,\nkbd,\nsamp {\n  font-size: 1em;\n  font-family: \'SFMono-Regular\', Consolas, \'Liberation Mono\', Menlo, Courier, monospace;\n}\npre {\n  margin-top: 0;\n  margin-bottom: 1em;\n  overflow: auto;\n}\nfigure {\n  margin: 0 0 1em;\n}\nimg {\n  vertical-align: middle;\n  border-style: none;\n}\nsvg:not(:root) {\n  overflow: hidden;\n}\na,\narea,\nbutton,\n[role=\'button\'],\ninput:not([type=\'range\']),\nlabel,\nselect,\nsummary,\ntextarea {\n  touch-action: manipulation;\n}\ntable {\n  border-collapse: collapse;\n}\ncaption {\n  padding-top: 0.75em;\n  padding-bottom: 0.3em;\n  color: rgba(0, 0, 0, 0.45);\n  text-align: left;\n  caption-side: bottom;\n}\ninput,\nbutton,\nselect,\noptgroup,\ntextarea {\n  margin: 0;\n  color: inherit;\n  font-size: inherit;\n  font-family: inherit;\n  line-height: inherit;\n}\nbutton,\ninput {\n  overflow: visible;\n}\nbutton,\nselect {\n  text-transform: none;\n}\nbutton,\nhtml [type="button"],\n[type="reset"],\n[type="submit"] {\n  -webkit-appearance: button;\n}\nbutton::-moz-focus-inner,\n[type=\'button\']::-moz-focus-inner,\n[type=\'reset\']::-moz-focus-inner,\n[type=\'submit\']::-moz-focus-inner {\n  padding: 0;\n  border-style: none;\n}\ninput[type=\'radio\'],\ninput[type=\'checkbox\'] {\n  box-sizing: border-box;\n  padding: 0;\n}\ninput[type=\'date\'],\ninput[type=\'time\'],\ninput[type=\'datetime-local\'],\ninput[type=\'month\'] {\n  -webkit-appearance: listbox;\n}\ntextarea {\n  overflow: auto;\n  resize: vertical;\n}\nfieldset {\n  min-width: 0;\n  margin: 0;\n  padding: 0;\n  border: 0;\n}\nlegend {\n  display: block;\n  width: 100%;\n  max-width: 100%;\n  margin-bottom: 0.5em;\n  padding: 0;\n  color: inherit;\n  font-size: 1.5em;\n  line-height: inherit;\n  white-space: normal;\n}\nprogress {\n  vertical-align: baseline;\n}\n[type=\'number\']::-webkit-inner-spin-button,\n[type=\'number\']::-webkit-outer-spin-button {\n  height: auto;\n}\n[type=\'search\'] {\n  outline-offset: -2px;\n  -webkit-appearance: none;\n}\n[type=\'search\']::-webkit-search-cancel-button,\n[type=\'search\']::-webkit-search-decoration {\n  -webkit-appearance: none;\n}\n::-webkit-file-upload-button {\n  font: inherit;\n  -webkit-appearance: button;\n}\noutput {\n  display: inline-block;\n}\nsummary {\n  display: list-item;\n}\ntemplate {\n  display: none;\n}\n[hidden] {\n  display: none !important;\n}\nmark {\n  padding: 0.2em;\n  background-color: #feffe6;\n}\n::-moz-selection {\n  color: #fff;\n  background: #1890ff;\n}\n::selection {\n  color: #fff;\n  background: #1890ff;\n}\n.clearfix::before {\n  display: table;\n  content: \'\';\n}\n.clearfix::after {\n  display: table;\n  clear: both;\n  content: \'\';\n}\n', '.mlflow-editable-note-actions {\n  margin-top: 16px;\n}\n\n.mlflow-editable-note-actions button + button {\n  margin-left: 16px;\n}\n\n.mde-header {\n  background: none;\n}\n', '.mlflow-center {\n  text-align: center;\n}\n\n.mlflow-error-image {\n  margin: 12% auto 60px;\n  display: block;\n}\n', '/* Styles for antd `copyable` code snippets */\n\n.mlflow-ui-container .code-keyword {\n  color: rgb(204, 120, 50);\n}\n.mlflow-ui-container .code {\n  color: rgb(100, 110, 120);\n}\n.mlflow-ui-container .code-comment {\n  color: rgb(140, 140, 140);\n}\n.mlflow-ui-container .code-string {\n  color: rgb(106, 165, 89);\n}\n.mlflow-ui-container .code-number {\n  color: rgb(104, 151, 187);\n}\n', 'div.mlflow-artifact-view {\n  display: flex;\n  overflow: hidden;\n}\n\n.mlflow-artifact-left {\n  min-width: 200px;\n  max-width: 400px;\n  flex: 1;\n}\n\n.mlflow-artifact-left li {\n  white-space: nowrap;\n}\n\n.mlflow-artifact-right {\n  flex: 3;\n  min-width: 400px;\n  max-width: calc(100% - 200px); /* 200px is the min-width of .mlflow-artifact-left */\n\n  overflow: hidden;\n  display: flex;\n  flex-direction: column;\n  height: 100%;\n}\n\n.mlflow-artifact-info-left {\n  flex: 1;\n  max-width: 75%;\n}\n.mlflow-artifact-info-right {\n  margin-left: auto;\n}\n\n.mlflow-artifact-info-path {\n  display: flex;\n  align-items: center;\n}\n\n.mlflow-artifact-info-text {\n  min-width: 0;\n}\n\n.mlflow-artifact-info-size {\n  overflow: hidden;\n  text-overflow: ellipsis;\n}\n\n.mlflow-loading-spinner {\n  height: 20px;\n  opacity: 0;\n  -webkit-animation: spin 3s linear infinite;\n  -moz-animation: spin 3s linear infinite;\n  animation: spin 3s linear infinite;\n}\n\n.mlflow-artifact-info-right .model-version-link {\n  display: flex;\n  align-items: baseline;\n  max-width: 140px;\n  padding-top: 1px;\n  padding-left: 4px;\n}\n\n.mlflow-artifact-info-right .model-version-link .model-name {\n  overflow: hidden;\n  text-overflow: ellipsis;\n}\n\n.mlflow-artifact-info-right .model-version-info {\n  font-size: 12px;\n}\n\n.mlflow-artifact-info-right .model-version-info .model-version-link-section {\n  display: flex;\n  align-items: center;\n}\n\n.mlflow-artifact-info-right .model-version-info .model-version-status-text {\n  overflow: hidden;\n  max-width: 160px;\n  text-overflow: ellipsis;\n}\n', '.mlflow-sticky-header {\n  position: sticky;\n  position: -webkit-sticky;\n  left: 0;\n}\n\n.mlflow-compare-run-table {\n  display: block;\n  overflow: auto;\n  width: 100%;\n}\n\n.mlflow-compare-table th.inter-title {\n  padding: 20px 0 0;\n  background: transparent;\n}\n\n.mlflow-compare-table .head-value {\n  overflow: hidden;\n  overflow-wrap: break-word;\n}\n\n.mlflow-compare-table td.data-value,\n.mlflow-compare-table th.data-value {\n  overflow: hidden;\n  max-width: 120px;\n  text-overflow: ellipsis;\n}\n\n.mlflow-responsive-table-container {\n  width: 100%;\n  overflow-x: auto;\n}\n\n.mlflow-compare-table .diff-row .data-value {\n  background-color: rgba(249, 237, 190, 0.5);\n  color: #555;\n}\n\n.mlflow-compare-table .diff-row .head-value {\n  background-color: rgba(249, 237, 190, 1);\n  color: #555;\n}\n\n.mlflow-compare-table .diff-row:hover {\n  background-color: rgba(249, 237, 190, 1);\n  color: #555;\n}\n\n/* Overrides to make it look more like antd */\n.mlflow-compare-table {\n  width: 100%;\n  max-width: 100%;\n  margin-bottom: 20px;\n}\n.mlflow-compare-table th,\n.mlflow-compare-table td {\n  padding: 12px 8px;\n  border-bottom: 1px solid #e8e8e8;\n}\n.mlflow-compare-table th {\n  color: rgba(0, 0, 0, 0.85);\n  font-weight: 500;\n  background-color: rgb(250, 250, 250);\n  text-align: left;\n}\n.mlflow-compare-table > tbody > tr:hover:not(.diff-row) > td:not(.highlight-data) {\n  background-color: rgb(250, 250, 250);\n}\n', '/* Overriding the table styles since antd tables take the full screen by default.\nWe would like to change it to auto to automatically grow based on the columns */\n\n.mlflow-html-table-view table {\n  width: auto;\n  min-width: 400px;\n}\n\n.mlflow-html-table-view th {\n  width: auto;\n  min-width: 200px;\n  margin-right: 80px;\n  font-size: 13px;\n  color: #888;\n}\n', '.mlflow-metrics-plot-container {\n  display: flex;\n  width: 100%;\n  align-items: flex-start;\n}\n\n.mlflow-metrics-plot-container .plot-controls {\n  display: flex;\n  flex-direction: column;\n  min-height: 500px;\n}\n\n.mlflow-metrics-plot-container .plot-controls .inline-control {\n  margin-top: 25px;\n  display: flex;\n  align-items: center;\n}\n\n.mlflow-metrics-plot-container .plot-controls .inline-control .control-label {\n  margin-right: 10px;\n}\n\n.mlflow-metrics-plot-container .plot-controls .block-control {\n  margin-top: 25px;\n}\n\n.mlflow-metrics-plot-container .metrics-plot-data {\n  flex: 1;\n  display: flex;\n  flex-direction: column;\n}\n\n.mlflow-metrics-plot-container .metrics-plot-view-container {\n  min-height: 500px;\n  flex: 1;\n}\n\n.mlflow-metrics-plot-container .metrics-summary {\n  margin: 20px 20px 20px 60px;\n}\n\n.mlflow-metrics-plot-container .metrics-summary .mlflow-html-table-view {\n  margin-bottom: 25px;\n  /* Shrink to fit, so that scroll bars are aligned with the edge of the table */\n  display: inline-block;\n}\n\n/* Reset min-width which is overridden in HtmlTableView as this breaks the\n   table layout when scrolling is enabled and widths are specified */\n.mlflow-metrics-plot-container .metrics-summary .mlflow-html-table-view th {\n  min-width: auto;\n}\n', '.mlflow-html-iframe {\n  border: none;\n}\n\n.mlflow-artifact-html-view {\n  width: 100%;\n  height: 100%;\n  overflow: auto;\n}\n', '.mlflow-show-artifact-logged-model-view {\n  width: 100%;\n  height: 100%;\n  overflow: auto;\n}\n', '.mlflow-ui-container .map-container {\n  height: 100%;\n  width: 100%;\n}\n\n.mlflow-ui-container .leaflet-container {\n  height: 100%;\n  width: 100%;\n}\n', '.mlflow-pdf-outer-container {\n  display: flex;\n  justify-content: center;\n  align-items: center;\n  height: 100%;\n  width: 100%;\n  padding-left: 16px;\n  overflow: hidden;\n}\n.mlflow-pdf-viewer {\n  display: flex;\n  flex-direction: column;\n  overflow-y: scroll;\n  height: 100%;\n}\n\n.mlflow-paginator {\n  display: flex;\n  justify-content: center;\n  align-items: center;\n  position: sticky;\n  z-index: 1001;\n  top: 0;\n  padding-bottom: 15px;\n  background-color: rgba(250, 250, 250, 0.6);\n  padding-top: 10px;\n}\n\n.mlflow-document {\n  display: flex;\n  justify-content: center;\n  align-items: center;\n}\n', '.mlflow-ShowArtifactPage .text-area {\n  box-sizing: border-box;\n  width: 100%;\n  height: 100%;\n  font-family: Menlo, Consolas, monospace;\n}\n\n.mlflow-ShowArtifactPage,\n.mlflow-ShowArtifactPage .text-area-border-box {\n  width: 100%;\n  height: 100%;\n  overflow: hidden;\n}\n', '.mlflow-ui-container .parcoords > svg,\n.mlflow-ui-container .parcoords > canvas {\n  overflow: visible;\n}\n\n.mlflow-ui-container .parcoords svg text.label {\n  cursor: pointer;\n}\n\n.mlflow-ui-container .parcoords svg g.axis-label-tooltip rect {\n  outline: 1px solid black;\n}\n\n.mlflow-ui-container .parcoords svg g.axis-label-tooltip {\n  visibility: hidden;\n  pointer-events: none;\n}\n.mlflow-ui-container .parcoords svg text.label:hover:not(:active) + g.axis-label-tooltip {\n  visibility: visible;\n}\n\n.mlflow-ui-container .parcoords svg g.tick-label-tooltip rect {\n  outline: 1px solid black;\n}\n\n.mlflow-ui-container .parcoords svg g.tick-label-tooltip {\n  visibility: hidden;\n  pointer-events: none;\n}\n.mlflow-ui-container .parcoords svg text:hover:not(:active) + g.tick-label-tooltip {\n  visibility: visible;\n}\n', "@import 'components/ModelVersionTable.css';\n@import 'components/ModelVersionView.css';\n@import 'components/ModelStageTransitionDropdown.css';\n\n/** TODO(Zangr) migrate globally common components and styles into src/common folder */\n.mlflow-metadata-container {\n  display: flex;\n  flex-wrap: wrap;\n  align-items: center;\n}\n\n.mlflow-metadata-entry {\n  margin-bottom: 16px;\n}\n\n.mlflow-icon-fail {\n  color: red;\n}\n", '.sticky-header {\n  position: sticky;\n  left: 0;\n}\n\n.compare-model-table {\n  display: block;\n  overflow: auto;\n  width: 100%;\n}\n\n.compare-table-row {\n  display: inline-flex;\n}\n\n.compare-table .head-value {\n  overflow: hidden;\n  overflow-wrap: break-word;\n  z-index: 10;\n}\n\n.compare-table .diff-row .data-value {\n  background-color: rgba(249, 237, 190, 0.5) ;\n  color: #555;\n}\n\n.compare-table .diff-row:hover,\n.compare-table .diff-row .head-value,\n.compare-table .diff-row .head-value > span {\n  background-color: rgba(249, 237, 190, 1.0) ;\n  color: #555;\n}\n', '.mlflow-pagination-section {\n  padding-bottom: 30px;\n}\n\n.mlflow-ui-container .ant-alert-info .ant-alert-icon {\n  color: #00b379;\n}\n\n.mlflow-search-input-tooltip .du-bois-light-popover-inner .du-bois-light-popover-inner-content {\n  background-color: rgba(0, 0, 0, 0.75);\n  color: white;\n  border-radius: 4px;\n}\n\n.mlflow-search-input-tooltip .du-bois-light-popover-arrow-content {\n  background-color: rgba(0, 0, 0, 0.75);\n}\n', '.mlflow-stage-transition-dropdown .ant-tag {\n  cursor: pointer;\n  border-radius: 4px;\n}\n', '.mlflow-table-endpoint-text {\n  white-space: nowrap;\n  text-overflow: ellipsis;\n  display: block;\n  overflow: hidden;\n}\n', '/* >>> Extract to our own Alert wrapper component */\n.mlflow-status-alert {\n  margin-bottom: 16px;\n  border-radius: 2px;\n}\n\n.mlflow-status-alert .model-version-status-icon {\n  margin-left: -3px;\n}\n\n.mlflow-status-alert.mlflow-status-alert-info {\n  border-left: 2px solid #3895d3;\n}\n\n.mlflow-status-alert.mlflow-status-alert-error {\n  border-left: 2px solid red;\n}\n\n.mlflow-version-follow-icon {\n  margin-left: auto;\n}\n\n.ant-popover-content {\n  max-width: 500px;\n}\n/* <<< Extract to our own Alert wrapper component */\n', '.mlflow-model-select-dropdown .ant-select-dropdown-menu-item-group-title {\n  color: #666;\n  font-weight: bold;\n}\n\n.mlflow-model-select-dropdown .mlflow-create-new-model-option {\n  border-top: 1px solid #ccc;\n}\n\n.mlflow-register-model-form .modal-explanatory-text {\n  color: rgba(0, 0, 0, 0.52);\n  font-size: 13px;\n}\n', "/* Replaceing AntD Image */\n.mlflow-ui-container .rc-image-preview {\n  height: 100%;\n  pointer-events: none;\n  text-align: center;\n}\n\n.mlflow-ui-container .rc-image-preview-mask {\n  position: fixed;\n  top: 0;\n  left: 0;\n  right: 0;\n  bottom: 0;\n  height: 100%;\n  background-color: rgba(0, 0, 0, 0.45);\n  z-index: 1000;\n}\n\n.mlflow-ui-container .rc-image-preview-mask img {\n  max-width: 100%;\n  max-height: 100%;\n  position: absolute;\n  top: 50%;\n  left: 50%;\n  transform: translate(-50%, -50%);\n}\n\n.mlflow-ui-container .rc-image-preview-mask {\n  background-color: rgba(0, 0, 0, 0.45);\n  bottom: 0;\n  height: 100%;\n  left: 0;\n  position: fixed;\n  right: 0;\n  top: 0;\n  z-index: 1000;\n}\n\n.mlflow-ui-container .rc-image-preview-mask-hidden {\n  display: none;\n}\n\n.mlflow-ui-container .rc-image-preview-wrap {\n  -webkit-overflow-scrolling: touch;\n  bottom: 0;\n  left: 0;\n  outline: 0;\n  overflow: auto;\n  position: fixed;\n  right: 0;\n  top: 0px;\n}\n\n.mlflow-ui-container .rc-image-preview-body {\n  bottom: 0;\n  left: 0;\n  overflow: hidden;\n  position: absolute;\n  right: 0;\n  top: 0;\n}\n\n.mlflow-ui-container .rc-image-preview-img {\n  cursor: grab;\n  max-height: 100%;\n  max-width: 100%;\n  pointer-events: auto;\n  transform: scaleX(1);\n  -webkit-user-select: none;\n  -moz-user-select: none;\n  -ms-user-select: none;\n  user-select: none;\n  vertical-align: middle;\n}\n\n.mlflow-ui-container .rc-image-preview-img,\n.mlflow-ui-container .rc-image-preview-img-wrapper {\n  transition: transform 0.3s cubic-bezier(0.215, 0.61, 0.355, 1) 0s;\n}\n\n.mlflow-ui-container .rc-image-preview-img-wrapper {\n  bottom: 0;\n  left: 0;\n  position: absolute;\n  right: 0;\n  top: 0;\n}\n\n.mlflow-ui-container .rc-image-preview-img-wrapper:before {\n  content: '';\n  display: inline-block;\n  height: 50%;\n  margin-right: -1px;\n  width: 1px;\n}\n\n.mlflow-ui-container .rc-image-preview-moving .mlflow-ui-container .rc-image-preview-img {\n  cursor: grabbing;\n}\n\n.mlflow-ui-container .rc-image-preview-moving .mlflow-ui-container .rc-image-preview-img-wrapper {\n  transition-duration: 0s;\n}\n\n.mlflow-ui-container .rc-image-preview-wrap {\n  z-index: 1080;\n}\n\n.mlflow-ui-container .rc-image-preview-operations {\n  font-feature-settings: 'tnum', 'tnum';\n  align-items: center;\n  background: rgba(0, 0, 0, 0.1);\n  box-sizing: border-box;\n  color: rgba(0, 0, 0, 0.85);\n  color: hsla(0, 0%, 100%, 0.85);\n  display: flex;\n  flex-direction: row-reverse;\n  font-size: 14px;\n  font-variant: tabular-nums;\n  line-height: 1.5715;\n  list-style: none;\n  margin: 0;\n  padding: 0;\n  pointer-events: auto;\n  position: absolute;\n  right: 0;\n  top: 0;\n  width: 100%;\n  z-index: 1;\n}\n\n.mlflow-ui-container .rc-image-preview-operations-operation {\n  cursor: pointer;\n  margin-left: 12px;\n  padding: 12px;\n}\n\n.mlflow-ui-container .rc-image-preview-operations-operation-disabled {\n  color: hsla(0, 0%, 100%, 0.25);\n  pointer-events: none;\n}\n\n.mlflow-ui-container .rc-image-preview-operations-operation:last-of-type {\n  margin-left: 0;\n}\n\n.mlflow-ui-container .rc-image-preview-operations-icon {\n  font-size: 18px;\n}\n\n.mlflow-ui-container .rc-image-preview-switch-left,\n.mlflow-ui-container .rc-image-preview-switch-right ", ' align-items: center;\n  background: rgba(0, 0, 0, 0.1);\n  box-sizing: border-box;\n  color: rgba(0, 0, 0, 0.85);\n  color: hsla(0, 0%, 100%, 0.85);\n  display: flex;\n  flex-direction: row-reverse;\n  font-size: 14px;\n  font-variant: tabular-nums;\n  line-height: 1.5715;\n  list-style: none;\n  margin: 0;\n  padding: 0;\n  pointer-events: auto;\n  position: absolute;\n  right: 0;\n  top: 0;\n  width: 100%;\n  z-index: 1;\n}\n\n.mlflow-ui-container .rc-image-preview-operations-operation {\n  cursor: pointer;\n  margin-left: 12px;\n  padding: 12px;\n}\n\n.mlflow-ui-container .rc-image-preview-operations-operation-disabled {\n  color: hsla(0, 0%, 100%, 0.25);\n  pointer-events: none;\n}\n\n.mlflow-ui-container .rc-image-preview-operations-operation:last-of-type {\n  margin-left: 0;\n}\n\n.mlflow-ui-container .rc-image-preview-operations-icon {\n  font-size: 18px;\n}\n\n.mlflow-ui-container .rc-image-preview-switch-left,\n.mlflow-ui-container .rc-image-preview-switch-right {\n  align-items: center;\n  background: rgba(0, 0, 0, 0.1);\n  border-radius: 50%;\n  color: hsla(0, 0%, 100%, 0.85);\n  cursor: pointer;\n  display: flex;\n  height: 44px;\n  justify-content: center;\n  margin-top: -22px;\n  pointer-events: auto;\n  position: absolute;\n  right: 10px;\n  top: 50%;\n  width: 44px;\n  z-index: 1;\n}\n\n.mlflow-ui-container .rc-image-preview-switch-left-disabled,\n.mlflow-ui-container .rc-image-preview-switch-right-disabled {\n  color: hsla(0, 0%, 100%, 0.25);\n  cursor: not-allowed;\n}\n\n.mlflow-ui-container .rc-image-preview-switch-left-disabled > .anticon,\n.mlflow-ui-container .rc-image-preview-switch-right-disabled > .anticon {\n  cursor: not-allowed;\n}\n\n.mlflow-ui-container .rc-image-preview-switch-left > .anticon,\n.mlflow-ui-container .rc-image-preview-switch-right > .anticon {\n  font-size: 18px;\n}\n\n.mlflow-ui-container .rc-image-preview-switch-left {\n  left: 10px;\n}\n\n.mlflow-ui-container .rc-image-preview-switch-right {\n  right: 10px;\n}\n\n.mlflow-ui-container .fade-enter,\n.mlflow-ui-container .fade-appear {\n  animation-duration: 0.3s;\n  animation-fill-mode: both;\n  animation-play-state: paused;\n}\n.mlflow-ui-container .fade-leave {\n  animation-duration: 0.3s;\n  animation-fill-mode: both;\n  animation-play-state: paused;\n}\n.mlflow-ui-container .fade-enter.fade-enter-active,\n.mlflow-ui-container .fade-appear.fade-appear-active {\n  animation-name: mlflow-rcImageFadeIn;\n  animation-play-state: running;\n}\n.mlflow-ui-container .fade-leave.fade-leave-active {\n  animation-name: mlflow-rcImageFadeOut;\n  animation-play-state: running;\n  pointer-events: none;\n}\n.mlflow-ui-container .fade-enter,\n.mlflow-ui-container .fade-appear {\n  opacity: 0;\n  animation-timing-function: linear;\n}\n.mlflow-ui-container .fade-leave {\n  animation-timing-function: linear;\n}\n\n@keyframes mlflow-rcImageFadeIn {\n  0% {\n    opacity: 0;\n  }\n  100% {\n    opacity: 1;\n  }\n}\n\n@keyframes mlflow-rcImageFadeOut {\n  0% {\n    opacity: 1;\n  }\n  100% {\n    opacity: 0;\n  }\n}\n\n.mlflow-ui-container .zoom-enter,\n.mlflow-ui-container .zoom-appear {\n  animation-duration: 0.3s;\n  animation-fill-mode: both;\n  animation-play-state: paused;\n}\n.mlflow-ui-container .zoom-leave {\n  animation-duration: 0.3s;\n  animation-fill-mode: both;\n  animation-play-state: paused;\n}\n.mlflow-ui-container .zoom-enter.zoom-enter-active,\n.mlflow-ui-container .zoom-appear.zoom-appear-active {\n  animation-name: mlflow-rcImageZoomIn;\n  animation-play-state: running;\n}\n.mlflow-ui-container .zoom-leave.zoom-leave-active {\n  animation-name: mlflow-rcImageZoomOut;\n  animation-play-state: running;\n  pointer-events: none;\n}\n.mlflow-ui-container .zoom-enter,\n.mlflow-ui-container .zoom-appear {\n  transform: scale(0);\n  opacity: 0;\n  animation-timing-function: cubic-bezier(0.08, ', 'linear;\n}\n\n@keyframes mlflow-rcImageFadeIn {\n  0% {\n    opacity: 0;\n  }\n  100% {\n    opacity: 1;\n  }\n}\n\n@keyframes mlflow-rcImageFadeOut {\n  0% {\n    opacity: 1;\n  }\n  100% {\n    opacity: 0;\n  }\n}\n\n.mlflow-ui-container .zoom-enter,\n.mlflow-ui-container .zoom-appear {\n  animation-duration: 0.3s;\n  animation-fill-mode: both;\n  animation-play-state: paused;\n}\n.mlflow-ui-container .zoom-leave {\n  animation-duration: 0.3s;\n  animation-fill-mode: both;\n  animation-play-state: paused;\n}\n.mlflow-ui-container .zoom-enter.zoom-enter-active,\n.mlflow-ui-container .zoom-appear.zoom-appear-active {\n  animation-name: mlflow-rcImageZoomIn;\n  animation-play-state: running;\n}\n.mlflow-ui-container .zoom-leave.zoom-leave-active {\n  animation-name: mlflow-rcImageZoomOut;\n  animation-play-state: running;\n  pointer-events: none;\n}\n.mlflow-ui-container .zoom-enter,\n.mlflow-ui-container .zoom-appear {\n  transform: scale(0);\n  opacity: 0;\n  animation-timing-function: cubic-bezier(0.08, 0.82, 0.17, 1);\n}\n.mlflow-ui-container .zoom-leave {\n  animation-timing-function: cubic-bezier(0.78, 0.14, 0.15, 0.86);\n}\n\n@keyframes mlflow-rcImageZoomIn {\n  0% {\n    transform: scale(0.2);\n    opacity: 0;\n  }\n  100% {\n    transform: scale(1);\n    opacity: 1;\n  }\n}\n\n@keyframes mlflow-rcImageZoomOut {\n  0% {\n    transform: scale(1);\n  }\n  100% {\n    transform: scale(0.2);\n    opacity: 0;\n  }\n}\n', "input,\nselect,\noption,\nbutton,\ntextarea,\nbody,\nthead {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, 'Noto Sans', sans-serif,\n    'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n}\n\nbody,\nthead {\n  font-size: 13px;\n  line-height: 18px;\n  font-weight: 400;\n  box-shadow: none;\n}\n\nhtml,\nbody,\npre,\ncode {\n  margin: 0;\n  padding: 0;\n}\n\nbody {\n  min-height: 100vh;\n}\n\nhtml {\n  overflow-y: hidden;\n}\n", '# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n**For contribution guidelines, code standards, and additional development information not covered here, please refer to [CONTRIBUTING.md](./CONTRIBUTING.md).**\n\n## Repository Overview\n\nMLflow is an open-source platform for managing the end-to-end machine learning lifecycle. It provides tools for:\n\n- Experiment tracking\n- Model versioning and deployment\n- LLM observability and tracing\n- Model evaluation\n- Prompt management\n\n## Quick Start: Development Server\n\n### Start the Full Development Environment (Recommended)\n\n```bash\n# Kill any existing servers\npkill -f "mlflow server" || true; pkill -f "yarn start" || true\n\n# Start both MLflow backend and React frontend dev servers\nnohup uv run bash dev/run-dev-server.sh > /tmp/mlflow-dev-server.log 2>&1 &\n\n# Monitor the logs\ntail -f /tmp/mlflow-dev-server.log\n\n# Servers will be available at:\n# - MLflow backend: http://localhost:5000\n# - React frontend: http://localhost:3000\n```\n\nThis uses `uv` (fast Python package manager) to automatically manage dependencies and run the development environment.\n\n### Start Development Server with Databricks Backend\n\nTo run the MLflow dev server that proxies requests to a Databricks workspace:\n\n```bash\n# IMPORTANT: All four environment variables below are REQUIRED for proper Databricks backend operation\n# Set them in this exact order:\nexport DATABRICKS_HOST="https://your-workspace.databricks.com"  # Your Databricks workspace URL\nexport DATABRICKS_TOKEN="your-databricks-token"                # Your Databricks personal access token\nexport MLFLOW_TRACKING_URI="databricks"                        # Must be set to "databricks"\nexport MLFLOW_REGISTRY_URI="databricks-uc"                     # Use "databricks-uc" for Unity Catalog, or "databricks" for workspace model registry\n\n# Start the dev server with these environment variables\nnohup uv run bash dev/run-dev-server.sh > /tmp/mlflow-dev-server.log 2>&1 &\n\n# Monitor the logs\ntail -f /tmp/mlflow-dev-server.log\n\n# The MLflow server will now proxy tracking and model registry requests to Databricks\n# Access the UI at http://localhost:3000 to see your Databricks experiments and models\n```\n\n**Note**: The MLflow server acts as a proxy, forwarding API requests to your Databricks workspace while serving the local React frontend. This allows you to develop and test UI changes against real Databricks data.\n\n## Development Commands\n\n### Testing\n\n```bash\n# First-time setup: ', '    # Use "databricks-uc" for Unity Catalog, or "databricks" for workspace model registry\n\n# Start the dev server with these environment variables\nnohup uv run bash dev/run-dev-server.sh > /tmp/mlflow-dev-server.log 2>&1 &\n\n# Monitor the logs\ntail -f /tmp/mlflow-dev-server.log\n\n# The MLflow server will now proxy tracking and model registry requests to Databricks\n# Access the UI at http://localhost:3000 to see your Databricks experiments and models\n```\n\n**Note**: The MLflow server acts as a proxy, forwarding API requests to your Databricks workspace while serving the local React frontend. This allows you to develop and test UI changes against real Databricks data.\n\n## Development Commands\n\n### Testing\n\n```bash\n# First-time setup: Install test dependencies\nuv sync\nuv pip install -r requirements/test-requirements.txt\n\n# Run Python tests\nuv run pytest tests/\n\n# Run specific test file\nuv run pytest tests/test_version.py\n\n# Run JavaScript tests\nyarn --cwd mlflow/server/js test\n```\n\n### Code Quality\n\n```bash\n# Python linting and formatting with Ruff\nuv run ruff check . --fix         # Lint with auto-fix\nuv run ruff format .              # Format code\n\n# Check for MLflow spelling typos\nuv run bash dev/mlflow-typo.sh .\n\n# JavaScript linting and formatting\nyarn --cwd mlflow/server/js lint\nyarn --cwd mlflow/server/js prettier:check\nyarn --cwd mlflow/server/js prettier:fix\n\n# Type checking\nyarn --cwd mlflow/server/js type-check\n\n# Run all checks\nyarn --cwd mlflow/server/js check-all\n```\n\n### Special Testing\n\n```bash\n# Run tests with minimal dependencies (skinny client)\nuv run bash dev/run-python-skinny-tests.sh\n\n# Test in Docker container\nuv run bash dev/run-test-container.sh\n```\n\n### Documentation\n\n```bash\n# Build documentation site (needs gateway extras for API doc generation)\nuv run --all-extras bash dev/build-docs.sh --build-api-docs\n\n# Build with R docs included\nuv run --all-extras bash dev/build-docs.sh --build-api-docs --with-r-docs\n\n# Serve documentation locally (after building)\ncd docs && yarn serve --port 8080\n```\n\n## Important Files\n\n- `pyproject.toml`: Package configuration and tool settings\n- `.python-version`: Minimum Python version (3.10)\n- `requirements/`: Dependency specifications\n- `mlflow/ml-package-versions.yml`: Supported ML framework versions\n\n## Common Development Tasks\n\n### Modifying the UI\n\nSee `mlflow/server/js/` for frontend development.\n\n## Language-Specific Style Guides\n\n- [Python](/dev/guides/python.md)\n\n## Git Workflow\n\n### Committing Changes\n\n**IMPORTANT**: After making your commits, run pre-commit hooks on your PR changes to ensure code quality:\n\n```bash\n# Make your commit first (with DCO sign-off)\ngit commit -s -m "Your commit message"\n\n# Then check all files changed in your PR\nuv run pre-commit run --from-ref origin/master --to-ref HEAD\n\n# Fix any issues and amend your ', '--with-r-docs\n\n# Serve documentation locally (after building)\ncd docs && yarn serve --port 8080\n```\n\n## Important Files\n\n- `pyproject.toml`: Package configuration and tool settings\n- `.python-version`: Minimum Python version (3.10)\n- `requirements/`: Dependency specifications\n- `mlflow/ml-package-versions.yml`: Supported ML framework versions\n\n## Common Development Tasks\n\n### Modifying the UI\n\nSee `mlflow/server/js/` for frontend development.\n\n## Language-Specific Style Guides\n\n- [Python](/dev/guides/python.md)\n\n## Git Workflow\n\n### Committing Changes\n\n**IMPORTANT**: After making your commits, run pre-commit hooks on your PR changes to ensure code quality:\n\n```bash\n# Make your commit first (with DCO sign-off)\ngit commit -s -m "Your commit message"\n\n# Then check all files changed in your PR\nuv run pre-commit run --from-ref origin/master --to-ref HEAD\n\n# Fix any issues and amend your commit if needed\ngit add <fixed files>\ngit commit --amend -s\n\n# Re-run pre-commit to verify fixes\nuv run pre-commit run --from-ref origin/master --to-ref HEAD\n\n# Only push once all checks pass\ngit push origin <your-branch>\n```\n\nThis workflow ensures you only check files you\'ve actually modified in your PR, avoiding false positives from unrelated files.\n\n**IMPORTANT**: You MUST sign all commits with DCO (Developer Certificate of Origin). Always use the `-s` flag:\n\n```bash\n# REQUIRED: Always use -s flag when committing\ngit commit -s -m "Your commit message"\n\n# This will NOT work - missing -s flag\n# git commit -m "Your commit message"  âŒ\n```\n\nCommits without DCO sign-off will be rejected by CI.\n\n**Frontend Changes**: If your PR touches any code in `mlflow/server/js/`, you MUST run `yarn check-all` before committing:\n\n```bash\nyarn --cwd mlflow/server/js check-all\n```\n\n### Creating Pull Requests\n\nFollow [the PR template](./.github/pull_request_template.md) when creating pull requests. The template will automatically appear when you create a PR on GitHub.\n\n### Checking CI Status\n\nUse GitHub CLI to check for failing CI:\n\n```bash\n# Check workflow runs for current branch\ngh run list --branch $(git branch --show-current)\n\n# View details of a specific run\ngh run view <run-id>\n\n# Watch a run in progress\ngh run watch\n```\n\n## Pre-commit Hooks\n\nThe repository uses pre-commit for code quality. Install hooks with:\n\n```bash\nuv run pre-commit install --install-hooks\n```\n\nRun pre-commit manually:\n\n```bash\n# Run on all files\nuv run pre-commit run --all-files\n\n# Run on all files, skipping hooks that require external tools\nSKIP=taplo,typos,conftest uv run pre-commit run --all-files\n\n# Run on specific files\nuv run pre-commit run --files path/to/file.py\n\n# Run a specific hook\nuv run pre-commit run ruff --all-files\n```\n\nThis runs Ruff, typos checker, and other tools automatically before commits.\n\n**Note about external ', "failing CI:\n\n```bash\n# Check workflow runs for current branch\ngh run list --branch $(git branch --show-current)\n\n# View details of a specific run\ngh run view <run-id>\n\n# Watch a run in progress\ngh run watch\n```\n\n## Pre-commit Hooks\n\nThe repository uses pre-commit for code quality. Install hooks with:\n\n```bash\nuv run pre-commit install --install-hooks\n```\n\nRun pre-commit manually:\n\n```bash\n# Run on all files\nuv run pre-commit run --all-files\n\n# Run on all files, skipping hooks that require external tools\nSKIP=taplo,typos,conftest uv run pre-commit run --all-files\n\n# Run on specific files\nuv run pre-commit run --files path/to/file.py\n\n# Run a specific hook\nuv run pre-commit run ruff --all-files\n```\n\nThis runs Ruff, typos checker, and other tools automatically before commits.\n\n**Note about external tools**: Some pre-commit hooks require external tools that aren't Python packages:\n\n- `taplo` - TOML formatter\n- `typos` - Spell checker\n- `conftest` - Policy testing tool\n\nTo install these tools:\n\n```bash\n# Install all tools at once (recommended)\nuv run bin/install.py\n```\n\nThis automatically downloads and installs the correct versions of all external tools to the `bin/` directory. The tools work on both Linux and ARM Macs.\n\nThese tools are optional. Use `SKIP=taplo,typos,conftest` if they're not installed.\n\n**Note**: If the typos hook fails, you only need to fix typos in code that was changed by your PR, not pre-existing typos in the codebase.\n", '### Evaluation Criteria\n\nWhen evaluating potential new MLflow committers, the following criteria will be considered:\n\n- **Code Contributions**: Should have multiple non-trivial code contributions accepted and committed to the MLflow codebase. This demonstrates the ability to produce quality code aligned with the project\'s standards.\n- **Technical Expertise**: Should demonstrate a deep understanding of MLflow\'s architecture and design principles, evidenced by making appropriate design choices and technical recommendations. History of caring about code quality, testing, maintainability, and ability to critically evaluate technical artifacts (PRs, designs, etc.) and provide constructive suggestions for improvement.\n- **Subject Matter Breadth**: Contributions and learnings span multiple areas of the codebase, APIs, and integration points rather than a narrow niche.\n- **Community Participation**: Active participation for at least 3 months prior to nomination by authoring code contributions and engaging in the code review process. Involvement in mailing lists, Slack channels, Stack Overflow, and GitHub issues is valued but not strictly required.\n- **Communication**: Should maintain a constructive tone in communications, be receptive to feedback, and collaborate well with existing committers and other community members.\n- **Project Commitment**: Demonstrate commitment to MLflow\'s long-term success, uphold project principles and values, and willingness to pitch in for "unglamorous" work.\n\n### Committership Nomination\n\n- Any current MLflow committer can nominate a contributor for committership by emailing MLflow\'s TSC members with a nomination packet.\n- The nomination packet should provide details on the nominee\'s salient contributions, as well as justification on how they meet the evaluation criteria. Links to GitHub activity, mailing list threads, and other artifacts should be included.\n- In addition to the nominator, every nomination must have a seconder -- a separate committer who advocates for the nominee. The seconder should be a more senior committer (active committer for >1 year) familiar with the nominee\'s work.\n- It is the nominator\'s responsibility to identify a willing seconder and include their recommendation in the nomination packet.\n- If no eligible seconder is available or interested, it may indicate insufficient support to proceed with the nomination at that time. This ensures there are two supporting committers invested in each nomination - the nominator and the seconder. The seconder\'s seniority and familiarity with the situation ', "to the nominator, every nomination must have a seconder -- a separate committer who advocates for the nominee. The seconder should be a more senior committer (active committer for >1 year) familiar with the nominee's work.\n- It is the nominator's responsibility to identify a willing seconder and include their recommendation in the nomination packet.\n- If no eligible seconder is available or interested, it may indicate insufficient support to proceed with the nomination at that time. This ensures there are two supporting committers invested in each nomination - the nominator and the seconder. The seconder's seniority and familiarity with the situation also help build more consensus among the TSC members during evaluation.\n\n### Evaluation Process\n\n- When a committer nomination is made, the TSC members closely review the proposal and evaluate the nominee's qualifications.\n- Throughout the review, the nominator is responsible for addressing any questions from the TSC, and providing clarification or additional evidence as requested by TSC members.\n- After adequate discussion (~1 week), the nominator calls for a formal consensus check among the TSC.\n- A positive consensus requires at least 2 TSC +1 binding votes and no vetoes.\n- Any vetoes must be accompanied by a clear rationale that can be debated.\n- If consensus is not achieved, the nomination is rejected at that time.\n- If consensus fails, the nominator summarizes substantive feedback and remaining gaps to the nominee for their growth and potential re-nomination later. Nomination can be tried again in 3 months after addressing any gaps identified.\n\n### Onboarding a new committer\n\n- Upon a positive consensus being reached, one of the TSC members will extend the formal invitation to the nominee to become a committer. They also field the private initial response from the nominee on willingness to accept.\n- If the proposal is accepted, the nominator grants them the commit access and the new committer will be:\n  - Added to the committer list in the README.md\n  - Announced on the MLflow mailing lists, Slack channels, and the MLflow website\n  - Spotlighted through a post on the MLflow LinkedIn and X handles\n- The nominator will work with the new committer to ", 'a positive consensus being reached, one of the TSC members will extend the formal invitation to the nominee to become a committer. They also field the private initial response from the nominee on willingness to accept.\n- If the proposal is accepted, the nominator grants them the commit access and the new committer will be:\n  - Added to the committer list in the README.md\n  - Announced on the MLflow mailing lists, Slack channels, and the MLflow website\n  - Spotlighted through a post on the MLflow LinkedIn and X handles\n- The nominator will work with the new committer to identify well-scoped initial areas for the new committer to focus on, such as improvements to a specific component.\n- The nominator will also set up periodic 1:1 mentorship check-ins with the new committer over their first month to provide guidance where needed.\n', '# Issue Policy\n\nThe MLflow Issue Policy outlines the categories of MLflow GitHub issues and discusses the guidelines & processes\nassociated with each type of issue.\n\nBefore filing an issue, make sure to [search for related issues](https://github.com/mlflow/mlflow/issues) and check if\nthey address yours.\n\nFor support (ex. "How do I do X?"), please ask on [Stack Overflow](https://stackoverflow.com/questions/tagged/mlflow).\n\n## Issue Categories\n\nOur policy is that GitHub issues fall into one of the following categories:\n\n1. Feature Requests\n2. Bug reports\n3. Documentation fixes\n4. Installation issues\n\nEach category has its own GitHub issue template. Please do not delete the issue template unless you are certain your\nissue is outside its scope.\n\n### Feature Requests\n\n#### Guidelines\n\nFeature requests that are likely to be accepted:\n\n- Are minimal in scope (note that it\'s always easier to add additional functionality later than remove functionality)\n- Are extensible (e.g. if adding an integration with an ML framework, is it possible to add similar integrations with other frameworks?)\n- Have user impact & value that justifies the maintenance burden of supporting the feature moving forwards. The\n  [JQuery contributor guide](https://contribute.jquery.org/open-source/#contributing-something-new) has an excellent discussion on this.\n\n#### Lifecycle\n\nFeature requests typically go through the following lifecycle:\n\n1. A feature request GitHub Issue is submitted, which contains a high-level description of the proposal and its motivation.\n   We encourage requesters to provide an overview of the feature\'s implementation as well, if possible.\n2. The [issue is triaged](ISSUE_TRIAGE.rst) to identify whether more information is needed from the author, give an indication of priority, and route feature requests to appropriate committers.\n3. The feature request is discussed with a committer. The committer will provide input on the implementation overview or\n   ask for a more detailed design, if applicable.\n4. After discussion & agreement on the feature request and its implementation, an implementation owner is identified.\n5. The implementation owner begins developing the feature and ultimately files associated pull requests against the\n   MLflow Repository or packages the feature as an MLflow Plugin.\n\n### Bug reports\n\n#### Guidelines\n\nIn order to ensure that maintainers are able to assist in any reported bug:\n\n- Ensure that the bug report template is filled out in its entirety with appropriate levels of detail, particularly in the `Code to reproduce ', "will provide input on the implementation overview or\n   ask for a more detailed design, if applicable.\n4. After discussion & agreement on the feature request and its implementation, an implementation owner is identified.\n5. The implementation owner begins developing the feature and ultimately files associated pull requests against the\n   MLflow Repository or packages the feature as an MLflow Plugin.\n\n### Bug reports\n\n#### Guidelines\n\nIn order to ensure that maintainers are able to assist in any reported bug:\n\n- Ensure that the bug report template is filled out in its entirety with appropriate levels of detail, particularly in the `Code to reproduce issue` section.\n- Verify that the bug you are reporting meets one of the following criteria:\n  - A recent release of MLflow does not support the operation you are doing that an earlier release did (a regression).\n  - A [documented feature](https://mlflow.org/docs/latest/index.html) or functionality does not work properly by executing a provided example from the docs.\n  - Any exception raised is directly from MLflow and is not the result of an underlying package's exception (e.g., don't file an issue that MLflow can't log a model that can't be trained due to a tensorflow Exception)\n- Make a best effort to diagnose and troubleshoot the issue prior to filing.\n- Verify that the environment that you're experiencing the bug in is supported as defined in the docs.\n- Validate that MLflow supports the functionality that you're having an issue with. _A lack of a feature does not constitute a bug_.\n- Read the docs on the feature for the issue that you're reporting. If you're certain that you're following documented guidelines, please file a bug report.\n\nBug reports typically go through the following lifecycle:\n\n1. A bug report GitHub Issue is submitted, which contains a high-level description of the bug and information required to reproduce it.\n2. The [bug report is triaged](ISSUE_TRIAGE.rst) to identify whether more information is needed from the author, give an indication of priority, and route to request appropriate committers.\n3. An MLflow committer reproduces the bug and provides feedback about how to implement a fix.\n4. After an approach has been agreed upon, an owner ", "on the feature for the issue that you're reporting. If you're certain that you're following documented guidelines, please file a bug report.\n\nBug reports typically go through the following lifecycle:\n\n1. A bug report GitHub Issue is submitted, which contains a high-level description of the bug and information required to reproduce it.\n2. The [bug report is triaged](ISSUE_TRIAGE.rst) to identify whether more information is needed from the author, give an indication of priority, and route to request appropriate committers.\n3. An MLflow committer reproduces the bug and provides feedback about how to implement a fix.\n4. After an approach has been agreed upon, an owner for the fix is identified. MLflow committers may choose to adopt\n   ownership of severe bugs to ensure a timely fix.\n5. The fix owner begins implementing the fix and ultimately files associated pull requests.\n\n### Documentation fixes\n\nDocumentation issues typically go through the following lifecycle:\n\n1. A documentation GitHub Issue is submitted, which contains a description of the issue and its location(s) in the MLflow documentation.\n2. The [issue is triaged](ISSUE_TRIAGE.rst) to identify whether more information is needed from the author, give an indication of priority, and route the request to appropriate committers.\n3. An MLflow committer confirms the documentation issue and provides feedback about how to implement a fix.\n4. After an approach has been agreed upon, an owner for the fix is identified. MLflow committers may choose to adopt\n   ownership of severe documentation issues to ensure a timely fix.\n5. The fix owner begins implementing the fix and ultimately files associated pull requests.\n\n### Installation issues\n\nInstallation issues typically go through the following lifecycle:\n\n1. An installation GitHub Issue is submitted, which contains a description of the issue and the platforms its affects.\n2. The [issue is triaged](ISSUE_TRIAGE.rst) to identify whether more information is needed from the author, give an indication of priority, and route the issue to appropriate committers.\n3. An MLflow committer confirms the installation issue and provides feedback about how to implement a fix.\n4. After an approach has been agreed upon, an owner for the fix is identified. MLflow committers may choose to adopt\n   ownership of severe installation issues to ensure a ", 'requests.\n\n### Installation issues\n\nInstallation issues typically go through the following lifecycle:\n\n1. An installation GitHub Issue is submitted, which contains a description of the issue and the platforms its affects.\n2. The [issue is triaged](ISSUE_TRIAGE.rst) to identify whether more information is needed from the author, give an indication of priority, and route the issue to appropriate committers.\n3. An MLflow committer confirms the installation issue and provides feedback about how to implement a fix.\n4. After an approach has been agreed upon, an owner for the fix is identified. MLflow committers may choose to adopt\n   ownership of severe installation issues to ensure a timely fix.\n5. The fix owner begins implementing the fix and ultimately files associated pull requests.\n', '<h1 align="center" style="border-bottom: none">\n    <a href="https://mlflow.org/">\n        <img alt="MLflow logo" src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/logo.svg" width="200" />\n    </a>\n</h1>\n<h2 align="center" style="border-bottom: none">Open-Source Platform for Productionizing AI</h2>\n\nMLflow is an open-source developer platform to build AI/LLM applications and models with confidence. Enhance your AI applications with end-to-end **experiment tracking**, **observability**, and **evaluations**, all in one integrated platform.\n\n<div align="center">\n\n[![Python SDK](https://img.shields.io/pypi/v/mlflow)](https://pypi.org/project/mlflow/)\n[![PyPI Downloads](https://img.shields.io/pypi/dm/mlflow)](https://pepy.tech/projects/mlflow)\n[![License](https://img.shields.io/github/license/mlflow/mlflow)](https://github.com/mlflow/mlflow/blob/main/LICENSE)\n<a href="https://twitter.com/intent/follow?screen_name=mlflow" target="_blank">\n<img src="https://img.shields.io/twitter/follow/mlflow?logo=X&color=%20%23f5f5f5"\n      alt="follow on X(Twitter)"></a>\n<a href="https://www.linkedin.com/company/mlflow-org/" target="_blank">\n<img src="https://custom-icon-badges.demolab.com/badge/LinkedIn-0A66C2?logo=linkedin-white&logoColor=fff"\n      alt="follow on LinkedIn"></a>\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/mlflow/mlflow)\n\n</div>\n\n<div align="center">\n   <div>\n      <a href="https://mlflow.org/"><strong>Website</strong></a> Â·\n      <a href="https://mlflow.org/docs/latest/index.html"><strong>Docs</strong></a> Â·\n      <a href="https://github.com/mlflow/mlflow/issues/new/choose"><strong>Feature Request</strong></a> Â·\n      <a href="https://mlflow.org/blog"><strong>News</strong></a> Â·\n      <a href="https://www.youtube.com/@mlflowoss"><strong>YouTube</strong></a> Â·\n      <a href="https://lu.ma/mlflow?k=c"><strong>Events</strong></a>\n   </div>\n</div>\n\n<br>\n\n## ðŸš€ Installation\n\nTo install the MLflow Python package, run the following command:\n\n```\npip install mlflow\n```\n\n## ðŸ“¦ Core Components\n\nMLflow is **the only platform that provides a unified solution for all your AI/ML needs**, including LLMs, Agents, Deep Learning, and traditional machine learning.\n\n### ðŸ’¡ For LLM / GenAI Developers\n\n<table>\n  <tr>\n    <td>\n    <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-tracing.png" alt="Tracing" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/llms/tracing/index.html"><strong>ðŸ” Tracing / Observability</strong></a>\n        <br><br>\n        <div>Trace the internal states of your LLM/agentic applications for debugging quality issues and monitoring performance with ease.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/tracing/quickstart/python-openai/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n    <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-llm-eval.png" alt="LLM Evaluation" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/genai/eval-monitor/"><strong>ðŸ“Š LLM Evaluation</strong></a>\n        <br><br>\n        <div>A suite of automated model evaluation tools, seamlessly integrated with experiment tracking to compare ', 'debugging quality issues and monitoring performance with ease.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/tracing/quickstart/python-openai/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n    <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-llm-eval.png" alt="LLM Evaluation" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/genai/eval-monitor/"><strong>ðŸ“Š LLM Evaluation</strong></a>\n        <br><br>\n        <div>A suite of automated model evaluation tools, seamlessly integrated with experiment tracking to compare across multiple versions.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/eval-monitor/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-prompt.png" alt="Prompt Management">\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/prompt-registry/"><strong>ðŸ¤– Prompt Management</strong></a>\n        <br><br>\n        <div>Version, track, and reuse prompts across your organization, helping maintain consistency and improve collaboration in prompt development.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-registry/create-and-edit-prompts/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-logged-model.png" alt="MLflow Hero">\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/version-tracking/"><strong>ðŸ“¦ App Version Tracking</strong></a>\n        <br><br>\n        <div>MLflow keeps track of many moving parts in your AI applications, such as models, prompts, tools, and code, with end-to-end lineage.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/version-tracking/quickstart/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n</table>\n\n### ðŸŽ“ For Data Scientists\n\n<table>\n  <tr>\n    <td colspan="2" align="center" >\n     ', '     <br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/version-tracking/"><strong>ðŸ“¦ App Version Tracking</strong></a>\n        <br><br>\n        <div>MLflow keeps track of many moving parts in your AI applications, such as models, prompts, tools, and code, with end-to-end lineage.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/version-tracking/quickstart/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n</table>\n\n### ðŸŽ“ For Data Scientists\n\n<table>\n  <tr>\n    <td colspan="2" align="center" >\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-experiment.png" alt="Tracking" width=50%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/ml/tracking/"><strong>ðŸ“ Experiment Tracking</strong></a>\n        <br><br>\n        <div>Track your models, parameters, metrics, and evaluation results in ML experiments and compare them using an interactive UI.</div><br>\n        <a href="https://mlflow.org/docs/latest/ml/tracking/quickstart/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-model-registry.png" alt="Model Registry" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/ml/model-registry/"><strong>ðŸ’¾ Model Registry</strong></a>\n        <br><br>\n        <div> A centralized model store designed to collaboratively manage the full lifecycle and deployment of machine learning models.</div><br>\n        <a href="https://mlflow.org/docs/latest/ml/model-registry/tutorial/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-deployment.png" alt="Deployment" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/ml/deployment/"><strong>ðŸš€ Deployment</strong></a>\n        <br><br>\n        <div> Tools for seamless model deployment to batch ', 'store designed to collaboratively manage the full lifecycle and deployment of machine learning models.</div><br>\n        <a href="https://mlflow.org/docs/latest/ml/model-registry/tutorial/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-deployment.png" alt="Deployment" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/ml/deployment/"><strong>ðŸš€ Deployment</strong></a>\n        <br><br>\n        <div> Tools for seamless model deployment to batch and real-time scoring on platforms like Docker, Kubernetes, Azure ML, and AWS SageMaker.</div><br>\n        <a href="https://mlflow.org/docs/latest/ml/deployment/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n</table>\n\n## ðŸŒ Hosting MLflow Anywhere\n\n<div align="center" >\n  <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-providers.png" alt="Providers" width=100%>\n</div>\n\nYou can run MLflow in many different environments, including local machines, on-premise servers, and cloud infrastructure.\n\nTrusted by thousands of organizations, MLflow is now offered as a managed service by most major cloud providers:\n\n- [Amazon SageMaker](https://aws.amazon.com/sagemaker-ai/experiments/)\n- [Azure ML](https://learn.microsoft.com/en-us/azure/machine-learning/concept-mlflow?view=azureml-api-2)\n- [Databricks](https://www.databricks.com/product/managed-mlflow)\n- [Nebius](https://nebius.com/services/managed-mlflow)\n\nFor hosting MLflow on your own infrastructure, please refer to [this guidance](https://mlflow.org/docs/latest/ml/tracking/#tracking-setup).\n\n## ðŸ—£ï¸ Supported Programming Languages\n\n- [Python](https://pypi.org/project/mlflow/)\n- [TypeScript / JavaScript](https://www.npmjs.com/package/mlflow-tracing)\n- [Java](https://mvnrepository.com/artifact/org.mlflow/mlflow-client)\n- [R](https://cran.r-project.org/web/packages/mlflow/readme/README.html)\n\n## ðŸ”— Integrations\n\nMLflow is natively integrated with many popular machine learning frameworks and GenAI libraries.\n\n![Integrations](https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-integrations.png)\n\n## Usage Examples\n\n### Experiment Tracking ([Doc](https://mlflow.org/docs/latest/ml/tracking/))\n\nThe following examples trains a simple regression model with scikit-learn, while enabling MLflow\'s [autologging](https://mlflow.org/docs/latest/tracking/autolog.html) feature for experiment tracking.\n\n```python\nimport mlflow\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Enable MLflow\'s automatic experiment tracking for scikit-learn\nmlflow.sklearn.autolog()\n\n# Load the training dataset\ndb = load_diabetes()\nX_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n\nrf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\n# MLflow triggers logging automatically upon model fitting\nrf.fit(X_train, y_train)\n```\n\nOnce the above code finishes, run the following command in a separate terminal and access the MLflow UI via the printed URL. An MLflow **Run** should be automatically created, which tracks the training dataset, hyper parameters, performance metrics, the trained model, dependencies, and even more.\n\n```\nmlflow ui\n```\n\n### Evaluating Models ([Doc](https://mlflow.org/docs/latest/model-evaluation/index.html))\n\nThe following example runs automatic evaluation for question-answering tasks with several built-in metrics.\n\n```python\nimport mlflow\nimport ', 'import train_test_split\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Enable MLflow\'s automatic experiment tracking for scikit-learn\nmlflow.sklearn.autolog()\n\n# Load the training dataset\ndb = load_diabetes()\nX_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n\nrf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\n# MLflow triggers logging automatically upon model fitting\nrf.fit(X_train, y_train)\n```\n\nOnce the above code finishes, run the following command in a separate terminal and access the MLflow UI via the printed URL. An MLflow **Run** should be automatically created, which tracks the training dataset, hyper parameters, performance metrics, the trained model, dependencies, and even more.\n\n```\nmlflow ui\n```\n\n### Evaluating Models ([Doc](https://mlflow.org/docs/latest/model-evaluation/index.html))\n\nThe following example runs automatic evaluation for question-answering tasks with several built-in metrics.\n\n```python\nimport mlflow\nimport pandas as pd\n\n# Evaluation set contains (1) input question (2) model outputs (3) ground truth\ndf = pd.DataFrame(\n    {\n        "inputs": ["What is MLflow?", "What is Spark?"],\n        "outputs": [\n            "MLflow is an innovative fully self-driving airship powered by AI.",\n            "Sparks is an American pop and rock duo formed in Los Angeles.",\n        ],\n        "ground_truth": [\n            "MLflow is an open-source platform for productionizing AI.",\n            "Apache Spark is an open-source, distributed computing system.",\n        ],\n    }\n)\neval_dataset = mlflow.data.from_pandas(\n    df, predictions="outputs", targets="ground_truth"\n)\n\n# Start an MLflow Run to record the evaluation results to\nwith mlflow.start_run(run_name="evaluate_qa"):\n    # Run automatic evaluation with a set of built-in metrics for question-answering models\n    results = mlflow.evaluate(\n        data=eval_dataset,\n        model_type="question-answering",\n    )\n\nprint(results.tables["eval_results_table"])\n```\n\n### Observability ([Doc](https://mlflow.org/docs/latest/llms/tracing/index.html))\n\nMLflow Tracing provides LLM observability for various GenAI libraries such as OpenAI, LangChain, LlamaIndex, DSPy, AutoGen, and more. To enable auto-tracing, call `mlflow.xyz.autolog()` before running your models. Refer to the documentation for customization ', '],\n    }\n)\neval_dataset = mlflow.data.from_pandas(\n    df, predictions="outputs", targets="ground_truth"\n)\n\n# Start an MLflow Run to record the evaluation results to\nwith mlflow.start_run(run_name="evaluate_qa"):\n    # Run automatic evaluation with a set of built-in metrics for question-answering models\n    results = mlflow.evaluate(\n        data=eval_dataset,\n        model_type="question-answering",\n    )\n\nprint(results.tables["eval_results_table"])\n```\n\n### Observability ([Doc](https://mlflow.org/docs/latest/llms/tracing/index.html))\n\nMLflow Tracing provides LLM observability for various GenAI libraries such as OpenAI, LangChain, LlamaIndex, DSPy, AutoGen, and more. To enable auto-tracing, call `mlflow.xyz.autolog()` before running your models. Refer to the documentation for customization and manual instrumentation.\n\n```python\nimport mlflow\nfrom openai import OpenAI\n\n# Enable tracing for OpenAI\nmlflow.openai.autolog()\n\n# Query OpenAI LLM normally\nresponse = OpenAI().chat.completions.create(\n    model="gpt-4o-mini",\n    messages=[{"role": "user", "content": "Hi!"}],\n    temperature=0.1,\n)\n```\n\nThen navigate to the "Traces" tab in the MLflow UI to find the trace records OpenAI query.\n\n## ðŸ’­ Support\n\n- For help or questions about MLflow usage (e.g. "how do I do X?") visit the [documentation](https://mlflow.org/docs/latest/index.html).\n- In the documentation, you can ask the question to our AI-powered chat bot. Click on the **"Ask AI"** button at the right bottom.\n- Join the [virtual events](https://lu.ma/mlflow?k=c) like office hours and meetups.\n- To report a bug, file a documentation issue, or submit a feature request, please [open a GitHub issue](https://github.com/mlflow/mlflow/issues/new/choose).\n- For release announcements and other discussions, please subscribe to our mailing list (mlflow-users@googlegroups.com)\n  or join us on [Slack](https://mlflow.org/slack).\n\n## ðŸ¤ Contributing\n\nWe happily welcome contributions to MLflow!\n\n- Submit [bug reports](https://github.com/mlflow/mlflow/issues/new?template=bug_report_template.yaml) and [feature requests](https://github.com/mlflow/mlflow/issues/new?template=feature_request_template.yaml)\n- Contribute for [good-first-issues](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) and [help-wanted](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22)\n- Writing about MLflow and sharing your experience\n\nPlease see our [contribution guide](CONTRIBUTING.md) to learn more about contributing to MLflow.\n\n## â­ï¸ Star History\n\n<a href="https://star-history.com/#mlflow/mlflow&Date">\n <picture>\n   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date&theme=dark" />\n   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date" />\n   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date" />\n </picture>\n</a>\n\n## âœï¸ Citation\n\nIf you use MLflow in your research, please cite it using the "Cite this repository" button at the top of the [GitHub repository page](https://github.com/mlflow/mlflow), which will provide you with citation formats including APA and BibTeX.\n\n## ðŸ‘¥ Core Members\n\nMLflow is currently maintained by the following core members with significant contributions from ', '[good-first-issues](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) and [help-wanted](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22)\n- Writing about MLflow and sharing your experience\n\nPlease see our [contribution guide](CONTRIBUTING.md) to learn more about contributing to MLflow.\n\n## â­ï¸ Star History\n\n<a href="https://star-history.com/#mlflow/mlflow&Date">\n <picture>\n   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date&theme=dark" />\n   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date" />\n   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date" />\n </picture>\n</a>\n\n## âœï¸ Citation\n\nIf you use MLflow in your research, please cite it using the "Cite this repository" button at the top of the [GitHub repository page](https://github.com/mlflow/mlflow), which will provide you with citation formats including APA and BibTeX.\n\n## ðŸ‘¥ Core Members\n\nMLflow is currently maintained by the following core members with significant contributions from hundreds of exceptionally talented community members.\n\n- [Ben Wilson](https://github.com/BenWilson2)\n- [Corey Zumar](https://github.com/dbczumar)\n- [Daniel Lok](https://github.com/daniellok-db)\n- [Gabriel Fu](https://github.com/gabrielfu)\n- [Harutaka Kawamura](https://github.com/harupy)\n- [Serena Ruan](https://github.com/serena-ruan)\n- [Tomu Hirata](https://github.com/TomeHirata)\n- [Weichen Xu](https://github.com/WeichenXu123)\n- [Yuki Watanabe](https://github.com/B-Step62)\n', '# Security Policy\n\nMLflow and its community take security bugs seriously. We appreciate efforts to improve the security of MLflow\nand follow the [GitHub coordinated disclosure of security vulnerabilities](https://docs.github.com/en/code-security/security-advisories/about-coordinated-disclosure-of-security-vulnerabilities#about-reporting-and-disclosing-vulnerabilities-in-projects-on-github)\nfor responsible disclosure and prompt mitigation. We are committed to working with security researchers to\nresolve the vulnerabilities they discover.\n\n## Supported Versions\n\nThe latest version of MLflow has continued support. If a critical vulnerability is found in the current version\nof MLflow, we may opt to backport patches to previous versions.\n\n## Reporting a Vulnerability\n\nWhen finding a security vulnerability in MLflow, please perform the following actions:\n\n- [Open an issue](https://github.com/mlflow/mlflow/issues/new?assignees=&labels=bug&template=bug_report_template.md&title=%5BBUG%5D%20Security%20Vulnerability) on the MLflow repository. Ensure that you use `[BUG] Security Vulnerability` as the title and _do not_ mention any vulnerability details in the issue post.\n- Send a notification [email](mailto:mlflow-oss-maintainers@googlegroups.com) to `mlflow-oss-maintainers@googlegroups.com` that contains, at a minimum:\n  - The link to the filed issue stub.\n  - Your GitHub handle.\n  - Detailed information about the security vulnerability, evidence that supports the relevance of the finding and any reproducibility instructions for independent confirmation.\n\nThis first stage of reporting is to ensure that a rapid validation can occur without wasting the time and effort of a reporter. Future communication and vulnerability resolution will be conducted after validating\nthe veracity of the reported issue.\n\nAn MLflow maintainer will, after validating the report:\n\n- Acknowledge the [bug](ISSUE_POLICY.md#bug-reports) during [triage](ISSUE_TRIAGE.rst)\n- Mark the issue as `priority/critical-urgent`\n- Open a draft [GitHub Security Advisory](https://docs.github.com/en/code-security/security-advisories/creating-a-security-advisory)\n  to discuss the vulnerability details in private.\n\nThe private Security Advisory will be used to confirm the issue, prepare a fix, and publicly disclose it after the fix has been released.\n', '## MLflow Dev Scripts\n\nThis directory contains automation scripts for MLflow developers and the build infrastructure.\n\n## Job Statuses\n\n[![Examples Action Status](https://img.shields.io/github/actions/workflow/status/mlflow/dev/examples.yml.svg?branch=master&event=schedule&label=Examples&style=for-the-badge&logo=github)](https://github.com/mlflow/dev/actions/workflows/examples.yml?query=workflow%3AExamples+event%3Aschedule)\n[![Cross Version Tests Status](https://img.shields.io/github/actions/workflow/status/mlflow/dev/cross-version-tests.yml.svg?branch=master&event=schedule&label=Cross%20version%20tests&style=for-the-badge&logo=github)](https://github.com/mlflow/dev/actions/workflows/cross-version-tests.yml?query=workflow%3A%22Cross+version+tests%22+event%3Aschedule)\n[![R-devel Action Status](https://img.shields.io/github/actions/workflow/status/mlflow/dev/r.yml.svg?branch=master&event=schedule&label=r-devel&style=for-the-badge&logo=github)](https://github.com/mlflow/dev/actions/workflows/r.yml?query=workflow%3AR+event%3Aschedule)\n[![Test Requirements Status](https://img.shields.io/github/actions/workflow/status/mlflow/dev/requirements.yml.svg?branch=master&event=schedule&label=test%20requirements&logo=github&style=for-the-badge)](https://github.com/mlflow/dev/actions/workflows/requirements.yml?query=workflow%3A%22Test+requirements%22+event%3Aschedule)\n[![Push Images Status](https://img.shields.io/github/actions/workflow/status/mlflow/mlflow/push-images.yml.svg?event=release&label=push-images&logo=github&style=for-the-badge)](https://github.com/mlflow/mlflow/actions/workflows/push-images.yml?query=event%3Arelease)\n[![Slow Tests Status](https://img.shields.io/github/actions/workflow/status/mlflow/dev/slow-tests.yml.svg?branch=master&event=schedule&label=slow-tests&logo=github&style=for-the-badge)](https://github.com/mlflow/dev/actions/workflows/slow-tests.yml?query=event%3Aschedule)\n[![Website E2E Tests Status](https://img.shields.io/github/actions/workflow/status/mlflow/mlflow-website/e2e.yml.svg?branch=main&event=schedule&label=website-e2e&logo=github&style=for-the-badge)](https://github.com/mlflow/mlflow-website/actions/workflows/e2e.yml?query=event%3Aschedule)\n', '# Typos\n\nA quick guide on how to use [`typos`](https://github.com/crate-ci/typos) to find, fix, and ignore typos.\n\n## Installation\n\n```sh\n# Replace `<version>` with the version installed in `dev/install-typos.sh`.\nbrew install typos-cli@<version>\n\n```\n\nSee https://github.com/crate-ci/typos?tab=readme-ov-file#install for other installation methods.\n\n## Finding typos\n\n```sh\npre-commit run --all-files typos\n```\n\n## Fixing typos\n\nYou can fix typos either manually or by running the following command:\n\n```sh\ntypos --write-changes [PATH]\n```\n\n## Ignoring false positives\n\nThere are two ways to ignore false positives:\n\n### Option 1: Ignore a line/block containing false positives\n\nThis option is preferred if the false positive is a one-off.\n\n```python\n# Ignore a line containing a typo:\n\n"<false_positive>"  # spellchecker: disable-line\n\n# Ignore a block containing typos:\n\n# spellchecker: off\n"<false_positive>"\n"<another_false_positive>"\n# spellchecker: on\n```\n\n### Option 2: Extend the ignore list in [`pyproject.toml`](../pyproject.toml)\n\nThis option is preferred if the false positive is common across multiple files/lines.\n\n```toml\n# pyproject.toml\n\n[tool.typos.default]\nextend-ignore-re = [\n  ...,\n  "false_positive",\n]\n```\n\n## Found a typo, but `typos` doesn\'t recognize it?\n\n`typos` only recognizes typos that are in its dictionary.\nIf you find a typo that `typos` doesn\'t recognize,\nyou can extend the `extend-words` list in [`pyproject.toml`](../pyproject.toml).\n\n```toml\n# pyproject.toml\n\n[tool.typos.default.extend-words]\n...\nmflow = "mlflow"\n```\n', '# Clint\n\nA custom linter for mlflow to enforce rules that ruff doesn\'t cover.\n\n## Installation\n\n```\npip install -e dev/clint\n```\n\n## Usage\n\n```bash\nclint file.py ...\n```\n\n## Integrating with Visual Studio Code\n\n1. Install [the Pylint extension](https://marketplace.visualstudio.com/items?itemName=ms-python.pylint)\n2. Add the following setting in your `settings.json` file:\n\n```json\n{\n  "pylint.path": ["${interpreter}", "-m", "clint"]\n}\n```\n\n## Ignoring Rules for Specific Files or Lines\n\n**To ignore a rule on a specific line (recommended):**\n\n```python\nfoo()  # clint: disable=<rule_name>\n```\n\nReplace `<rule_name>` with the actual rule you want to disable.\n\n**To ignore a rule for an entire file:**\n\nAdd the file path to the `exclude` list in your `pyproject.toml`:\n\n```toml\n[tool.clint]\nexclude = [\n  # ...existing entries...\n  "path/to/file.py",\n]\n```\n\n## Testing\n\n```bash\npytest --confcutdir dev/clint dev/clint\n```\n', '# Python Style Guide\n\nThis guide documents Python coding conventions that go beyond what [ruff](https://docs.astral.sh/ruff/) and [clint](../../dev/clint/) can enforce. The practices below require human judgment to implement correctly and improve code readability, maintainability, and testability across the MLflow codebase.\n\n## Avoid Redundant Test Docstrings\n\nOmit docstrings that merely echo the function name without adding value. Test names should be self-documenting.\n\n```python\n# Bad\ndef test_foo():\n    """Test foo"""\n    ...\n\n\n# Good\ndef test_foo():\n    ...\n```\n\n## Use Type Hints for All Functions\n\nAdd type hints to all function parameters and return values. This enables better IDE support, catches bugs early, and serves as inline documentation.\n\n```python\n# Bad\ndef foo(s):\n    return len(s)\n\n\n# Good\ndef foo(s: str) -> int:\n    return len(s)\n```\n\n### Exceptions\n\n**Test functions:** The `-> None` return type can be omitted for test functions since they implicitly return `None` and the return value is not used.\n\n```python\n# Acceptable\ndef test_foo(s: str):\n    ...\n\n\n# Also acceptable (but not required)\ndef test_foo(s: str) -> None:\n    ...\n```\n\n**`__init__` methods:** The `-> None` return type can be omitted for `__init__` methods since they always return `None` by definition.\n\n```python\n# Acceptable\nclass Foo:\n    def __init__(self, s: str):\n        ...\n\n\n# Also acceptable (but not required)\nclass Foo:\n    def __init__(self, s: str) -> None:\n        ...\n```\n\n## Minimize Try-Catch Block Scope\n\nWrap only the specific operations that can raise exceptions. Keep safe operations outside the try block to improve debugging and avoid masking unexpected errors.\n\n```python\n# Bad\ntry:\n    never_fails()\n    can_fail()\nexcept ...:\n    handle_error()\n\n# Good\nnever_fails()\ntry:\n    can_fail()\nexcept ...:\n    handle_error()\n```\n\n## Use Dataclasses Instead of Complex Tuples\n\nReplace tuples with 3+ elements with named dataclasses. This improves code clarity, prevents positional argument errors, and enables type checking on individual fields.\n\n```python\n# Bad\ndef get_user() -> tuple[str, int, str]:\n    return "Alice", 30, "Engineer"\n\n\n# Good\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass User:\n    name: str\n    age: int\n    occupation: str\n\n\ndef get_user() -> User:\n    return User(name="Alice", age=30, occupation="Engineer")\n```\n\n## Use next() to Find First ', 'Bad\ntry:\n    never_fails()\n    can_fail()\nexcept ...:\n    handle_error()\n\n# Good\nnever_fails()\ntry:\n    can_fail()\nexcept ...:\n    handle_error()\n```\n\n## Use Dataclasses Instead of Complex Tuples\n\nReplace tuples with 3+ elements with named dataclasses. This improves code clarity, prevents positional argument errors, and enables type checking on individual fields.\n\n```python\n# Bad\ndef get_user() -> tuple[str, int, str]:\n    return "Alice", 30, "Engineer"\n\n\n# Good\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass User:\n    name: str\n    age: int\n    occupation: str\n\n\ndef get_user() -> User:\n    return User(name="Alice", age=30, occupation="Engineer")\n```\n\n## Use next() to Find First Match Instead of Loop-and-Break\n\nUse the `next()` builtin function with a generator expression to find the first item that matches a condition. This is more concise and functional than manually looping with break statements.\n\n```python\n# Bad\nresult = None\nfor item in items:\n    if item.name == "target":\n        result = item\n        break\n\n# Good\nresult = next((item for item in items if item.name == "target"), None)\n```\n\n## Always Verify Mock Calls with Assertions\n\nEvery mocked function must have an assertion (`assert_called`, `assert_called_once`, etc.) to verify it was invoked correctly. Without assertions, tests may pass even when the mocked code isn\'t executed.\n\n```python\nfrom unittest import mock\n\n\n# Bad\ndef test_foo():\n    with mock.patch("foo.bar"):\n        calls_bar()\n\n\n# Good\ndef test_bar():\n    with mock.patch("foo.bar") as mock_bar:\n        calls_bar()\n        mock_bar.assert_called_once()\n```\n\n## Set Mock Behaviors in Patch Declaration\n\nDefine `return_value` and `side_effect` directly in the `patch()` call rather than assigning them afterward. This keeps mock configuration explicit and reduces setup code.\n\n```python\nfrom unittest import mock\n\n\n# Bad\ndef test_foo():\n    with mock.patch("foo.bar") as mock_bar:\n        mock_bar.return_value = 42\n        calls_bar()\n\n    with mock.patch("foo.bar") as mock_bar:\n        mock_bar.side_effect = Exception("Error")\n        calls_bar()\n\n\n# Good\ndef test_foo():\n    with mock.patch("foo.bar", return_value=42) as mock_bar:\n      ', ' mock_bar.assert_called_once()\n```\n\n## Set Mock Behaviors in Patch Declaration\n\nDefine `return_value` and `side_effect` directly in the `patch()` call rather than assigning them afterward. This keeps mock configuration explicit and reduces setup code.\n\n```python\nfrom unittest import mock\n\n\n# Bad\ndef test_foo():\n    with mock.patch("foo.bar") as mock_bar:\n        mock_bar.return_value = 42\n        calls_bar()\n\n    with mock.patch("foo.bar") as mock_bar:\n        mock_bar.side_effect = Exception("Error")\n        calls_bar()\n\n\n# Good\ndef test_foo():\n    with mock.patch("foo.bar", return_value=42) as mock_bar:\n        calls_bar()\n\n    with mock.patch("foo.bar", side_effect=Exception("Error")) as mock_bar:\n        calls_bar()\n```\n\n## Use Pytest\'s Monkeypatch for Directory Changes\n\nUse `monkeypatch.chdir()` instead of manual `os.chdir()` with try/finally blocks. Pytest automatically restores the original directory after the test, preventing side effects.\n\n```python\nimport os\nimport pytest\n\n\n# Bad\ndef test_foo():\n    cwd = os.getcwd()\n    try:\n        os.chdir("some/directory")\n    finally:\n        os.chdir(cwd)\n\n\n# Good\ndef test_foo(monkeypatch: pytest.MonkeyPatch):\n    monkeypatch.chdir("some/directory")\n```\n\n## Parametrize Tests with Multiple Input Cases\n\nUse `@pytest.mark.parametrize` to test multiple inputs instead of repeating assertions. This creates separate test cases for each input, making failures easier to diagnose and tests more maintainable.\n\n```python\n# Bad\ndef test_foo():\n    assert foo("a") == 0\n    assert foo("b") == 1\n    assert foo("c") == 2\n\n\n# Good\n@pytest.mark.parametrize(\n    ("input", "expected"),\n    [\n        ("a", 0),\n        ("b", 1),\n        ("c", 2),\n    ],\n)\ndef test_foo(input: str, expected: int):\n    assert foo(input) == expected\n```\n\n## Use Pytest\'s Monkeypatch for Mocking Environment Variables\n\nUse `monkeypatch.setenv()` and `monkeypatch.delenv()` instead of `mock.patch.dict()` for environment variables. Pytest\'s monkeypatch fixture automatically restores the original environment after the test, providing cleaner and more reliable test isolation.\n\n```python\n# Bad - Setting environment variables\ndef test_foo():\n    with mock.patch.dict("os.environ", {"FOO": "True"}):\n        ...\n\n\n# Bad - ', '[\n        ("a", 0),\n        ("b", 1),\n        ("c", 2),\n    ],\n)\ndef test_foo(input: str, expected: int):\n    assert foo(input) == expected\n```\n\n## Use Pytest\'s Monkeypatch for Mocking Environment Variables\n\nUse `monkeypatch.setenv()` and `monkeypatch.delenv()` instead of `mock.patch.dict()` for environment variables. Pytest\'s monkeypatch fixture automatically restores the original environment after the test, providing cleaner and more reliable test isolation.\n\n```python\n# Bad - Setting environment variables\ndef test_foo():\n    with mock.patch.dict("os.environ", {"FOO": "True"}):\n        ...\n\n\n# Bad - Removing environment variables\ndef test_bar():\n    with mock.patch.dict("os.environ", {}, clear=True):\n        ...\n\n\n# Good - Setting environment variables\ndef test_foo(monkeypatch: pytest.MonkeyPatch):\n    monkeypatch.setenv("FOO", "True")\n    ...\n\n\n# Good - Removing environment variables\ndef test_bar(monkeypatch: pytest.MonkeyPatch):\n    # raising=False prevents KeyError if FOO doesn\'t exist\n    monkeypatch.delenv("FOO", raising=False)\n    ...\n```\n\n## Use Pytest\'s tmp_path Fixture for Temporary Files\n\nUse `tmp_path` fixture instead of manual `tempfile.TemporaryDirectory()` for handling temporary files and directories in tests. Pytest automatically cleans up the temporary directory after the test, provides better test isolation, and integrates seamlessly with pytest\'s fixture system.\n\n```python\n# Bad\nimport tempfile\n\n\ndef test_foo():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        ...\n\n\n# Good\nfrom pathlib import Path\n\n\ndef test_foo(tmp_path: Path):\n    ...\n```\n', '# MLflow Proto To GraphQL Autogeneration\n\n## What is this\n\nThe system in `dev/proto_to_graphql` parses proto rpc definitions and generates graphql schema based on the proto rpc definition. The goal of this system is to quickly generate base GraphQL schema and resolver code so that we can easily take advantage of the data joining functionalities of GraphQL.\n\nThe autogenerated schema and resolver are in the following file: `mlflow/server/graphql/autogenerated_graphql_schema.py`\n\nThe autogenerated schema and resolvers are referenced and can be extended in this file `mlflow/server/graphql/graphql_schema_extensions.py`\n\nYou can run `python ./dev/proto_to_graphql/code_generator.py` or `./dev/generate-protos.sh` to trigger the codegen process.\n\n## FAQs\n\n### How to onboard a new rpc to GraphQL\n\n- In your proto rpc definition, add `option (graphql) = {};` and re-run `./dev/generate-protos.sh`. You should see the changes in the generated schema. [Example](https://github.com/mlflow/mlflow/pull/11215/files#diff-8ab2ad3109b67a713e147edf557d4da88853563398ce354cc895bb5930950dc5R175).\n- In `mlflow/server/handlers.py`, identify the handler function for your rpc, for example `_get_run`, make sure there exists a corresponding `get_run_impl` function that takes in a `request_message` and returns a response messages that is of the generated service_pb proto type. If no such function exists, you can easily extract it out like in this [example](https://github.com/mlflow/mlflow/pull/11215/files#diff-5c10a4e2ca47745f06fa9e7201087acfc102849756cb8d85e774a5ac468cb037R1779-R1795).\n- Test manually with a localhost server, as well as adding a unit test in `tests/tracking/test_rest_tracking.py`. [Example](https://github.com/mlflow/mlflow/pull/11215/files#diff-2ec8756f67a20ecbaeec2d2c5e7bf33310a50c015fc3aa487e27100fc4c2f9a7R1771-R1802).\n\n### How to customize a generated query/mutation to join multiple rpc endpoints\n\nThe proto to graphql autogeneration only supports 1 to 1 mapping from proto rpc to graphql operation. However, the power of GraphQL is to join multiple rpc endpoints together as one query. So we often would like to customize or extend the autogenerated operations to join these multiple endpoints.\n\nFor example, we would like to query data about `Experiment`, `ModelVersions` and `Run` in one query by extending the `MlflowRun` object.\n\n```\nquery testQuery {\n    mlflowGetRun(input: {runId: "my-id"}) {\n        run {\n            experiment {\n                name\n            }\n            modelVersions {\n         ', 'example, we would like to query data about `Experiment`, `ModelVersions` and `Run` in one query by extending the `MlflowRun` object.\n\n```\nquery testQuery {\n    mlflowGetRun(input: {runId: "my-id"}) {\n        run {\n            experiment {\n                name\n            }\n            modelVersions {\n                name\n            }\n        }\n    }\n}\n```\n\nTo achieve joins, follow the steps below:\n\n- Make sure the rpcs you would like to join are already onboarded to GraphQL by following the `How to onboard a new rpc to GraphQL` section\n- Identify the class you would like to extend in `autogenerated_graphql_schema.py` and create a new class that inherits the target class, put it in `graphql_schema_extensions.py`. Add the new fields and the resolver function as you intended. [Example](https://github.com/mlflow/mlflow/pull/11173/files#diff-9e4f7bdf4d7f9d362338bed9ce6607a51b8f520ee605e2fd4c9bda5e43cb617cR21-R31)\n- Run `python ./dev/proto_to_graphql/code_generator.py` or `./dev/generate-protos.sh`, you should see the autogenerated schema being updated to reference the extension class you just created.\n- Add a test case in `tests/tracking/test_rest_tracking.py` [Example](https://github.com/mlflow/mlflow/pull/11173/files#diff-2ec8756f67a20ecbaeec2d2c5e7bf33310a50c015fc3aa487e27100fc4c2f9a7R1771-R1795)\n\n### How to generate typescript types for a GraphQL operation\n\nTo generate typescript types, first make sure the generated schema is up-to-date by running `python ./dev/proto_to_graphql/code_generator.py`\n\nThen write your new query or mutation in the mlflow/server/js/src folder, after that run the following commands:\n\n- cd mlflow/server/js\n- yarn graphql-codegen\n\nYou should be able to see the generated types in `mlflow/server/js/src/graphql/__generated__/`\n', "# MLflow with Docker Compose (PostgreSQL + MinIO)\n\nThis directory provides a **Docker Compose** setup for running **MLflow** locally with a **PostgreSQL** backend store and **MinIO** (S3-compatible) artifact storage. It's intended for quick evaluation and local development.\n\n---\n\n## Overview\n\n- **MLflow Tracking Server** â€” exposed on your host (default `http://localhost:5000`).\n- **PostgreSQL** â€” persists MLflow's metadata (experiments, runs, params, metrics).\n- **MinIO** â€” stores run artifacts via an S3-compatible API.\n\nCompose automatically reads configuration from a local `.env` file in this directory.\n\n---\n\n## Prerequisites\n\n- **Git**\n- **Docker** and **Docker Compose**\n  - Windows/macOS: [Docker Desktop](https://www.docker.com/products/docker-desktop/)\n  - Linux: Docker Engine + the `docker compose` plugin\n\nVerify your setup:\n\n```bash\ndocker --version\ndocker compose version\n```\n\n---\n\n## 1. Clone the Repository\n\n```bash\ngit clone https://github.com/mlflow/mlflow.git\ncd docker-compose\n```\n\n---\n\n## 2. Configure Environment\n\nCopy the example environment file and modify as needed:\n\n```bash\ncp .env.dev.example .env\n```\n\nThe `.env` file defines container image tags, ports, credentials, and storage configuration. Open it and review values before starting the stack.\n\n**Common variables** :\n\n- **MLflow**\n  - `MLFLOW_PORT=5000` â€” host port for the MLflow UI/API\n  - `MLFLOW_DEFAULT_ARTIFACT_ROOT=s3://mlflow/` â€” artifact store URI\n  - `MLFLOW_S3_ENDPOINT_URL=http://minio:9000` â€” S3 endpoint (inside the Compose network)\n- **PostgreSQL**\n  - `POSTGRES_USER=mlflow`\n  - `POSTGRES_PASSWORD=mlflow`\n  - `POSTGRES_DB=mlflow`\n- **MinIO (S3-compatible)**\n  - `MINIO_ROOT_USER=minio`\n  - `MINIO_ROOT_PASSWORD=minio123`\n  - `MINIO_HOST=minio`\n  - `MINIO_PORT=9000`\n  - `MINIO_BUCKET=mlflow`\n\n---\n\n## 3. Launch the Stack\n\n```bash\ndocker compose up -d\n```\n\nThis:\n\n- Builds/pulls images as needed\n- Creates a user-defined network\n- Starts **postgres**, **minio**, and **mlflow** containers\n\nCheck status:\n\n```bash\ndocker compose ps\n```\n\nView logs (useful on first run):\n\n```bash\ndocker compose logs -f\n```\n\n---\n\n## 4. Access MLflow\n\nOpen the MLflow UI:\n\n- **URL**: `http://localhost:5000` (or the port set in `.env`)\n\nYou can now create experiments, run training scripts, and log metrics, parameters, and artifacts to this local MLflow instance.\n\n---\n\n## 5. Shutdown\n\nTo stop and remove the containers and network:\n\n```bash\ndocker compose down\n```\n\n> Data is preserved in Docker **volumes**. To remove volumes as well (irreversible), run:\n>\n> ```bash\n> docker compose down -v\n> ```\n\n---\n\n## Tips & Troubleshooting\n\n- **Verify connectivity**  \n  If MLflow can't write artifacts, confirm your S3 settings:\n\n  - `MLFLOW_DEFAULT_ARTIFACT_ROOT` points to your MinIO bucket (e.g., `s3://mlflow/`)\n  - `MLFLOW_S3_ENDPOINT_URL` is reachable from the MLflow container (often `http://minio:9000`)\n\n- **Resetting the environment**  \n  If you want a clean slate, stop the stack and remove volumes:\n\n  ", "training scripts, and log metrics, parameters, and artifacts to this local MLflow instance.\n\n---\n\n## 5. Shutdown\n\nTo stop and remove the containers and network:\n\n```bash\ndocker compose down\n```\n\n> Data is preserved in Docker **volumes**. To remove volumes as well (irreversible), run:\n>\n> ```bash\n> docker compose down -v\n> ```\n\n---\n\n## Tips & Troubleshooting\n\n- **Verify connectivity**  \n  If MLflow can't write artifacts, confirm your S3 settings:\n\n  - `MLFLOW_DEFAULT_ARTIFACT_ROOT` points to your MinIO bucket (e.g., `s3://mlflow/`)\n  - `MLFLOW_S3_ENDPOINT_URL` is reachable from the MLflow container (often `http://minio:9000`)\n\n- **Resetting the environment**  \n  If you want a clean slate, stop the stack and remove volumes:\n\n  ```bash\n  docker compose down -v\n  docker compose up -d\n  ```\n\n- **Logs**\n\n  - MLflow server: `docker compose logs -f mlflow`\n  - PostgreSQL: `docker compose logs -f postgres`\n  - MinIO: `docker compose logs -f minio`\n\n- **Port conflicts**  \n  If `5000` (or any other port) is in use, change it in `.env` and restart:\n  ```bash\n  docker compose down\n  docker compose up -d\n  ```\n\n---\n\n## How It Works (at a Glance)\n\n- MLflow uses **PostgreSQL** as the _backend store_ for experiment/run metadata.\n- MLflow uses **MinIO** as the _artifact store_ via S3 APIs.\n- Docker Compose wires services on a shared network; MLflow talks to PostgreSQL and MinIO by container name (e.g., `postgres`, `minio`).\n\n---\n\n## Next Steps\n\n- Point your training scripts to this server:\n  ```bash\n  export MLFLOW_TRACKING_URI=http://localhost:5000\n  ```\n- Start logging runs with `mlflow.start_run()` (Python) or the MLflow CLI.\n- Customize the `.env` and `docker-compose.yml` to fit your local workflow (e.g., change image tags, add volumes, etc.).\n\n---\n\n**You now have a fully local MLflow stack with persistent metadata and artifact storageâ€”ideal for development and experimentation.**\n", "## MLflow examples\n\n### Quick Start example\n\n- `quickstart/mlflow_tracking.py` is a basic example to introduce MLflow concepts.\n\n## Tutorials\n\nVarious examples that depict MLflow tracking, project, and serving use cases.\n\n- `h2o` depicts how MLflow can be use to track various random forest architectures to train models\n  for predicting wine quality.\n- `hyperparam` shows how to do hyperparameter tuning with MLflow and some popular optimization libraries.\n- `keras` modifies\n  [a Keras classification example](https://github.com/keras-team/keras/blob/ed07472bc5fc985982db355135d37059a1f887a9/examples/reuters_mlp.py)\n  and uses MLflow's `mlflow.tensorflow.autolog()` API to automatically log metrics and parameters\n  to MLflow during training.\n- `multistep_workflow` is an end-to-end of a data ETL and ML training pipeline built as an MLflow\n  project. The example shows how parts of the workflow can leverage from previously run steps.\n- `pytorch` uses CNN on MNIST dataset for character recognition. The example logs TensorBoard events\n  and stores (logs) them as MLflow artifacts.\n- `remote_store` has a usage example of REST based backed store for tracking.\n- `r_wine` demonstrates how to log parameters, metrics, and models from R.\n- `sklearn_elasticnet_diabetes` uses the sklearn diabetes dataset to predict diabetes progression\n  using ElasticNet.\n- `sklearn_elasticnet_wine_quality` is an example for MLflow projects. This uses the Wine\n  Quality dataset and Elastic Net to predict quality. The example uses `MLproject` to set up a\n  Conda environment, define parameter types and defaults, entry point for training, etc.\n- `sklearn_logistic_regression` is a simple MLflow example with hooks to log training data to MLflow\n  tracking server.\n- `supply_chain_security` shows how to strengthen the security of ML projects against supply-chain attacks by enforcing hash checks on Python packages.\n- `tensorflow` contains end-to-end one run examples from train to predict for TensorFlow 2.8+ It includes usage of MLflow's\n  `mlflow.tensorflow.autolog()` API, which captures TensorBoard data and logs to MLflow with no code change.\n- `docker` demonstrates how to create and run an MLflow project using docker (rather than conda)\n  to manage project dependencies\n- `johnsnowlabs` gives you access to [20.000+ state-of-the-art enterprise NLP models in 200+ languages](https://nlp.johnsnowlabs.com/models) for medical, finance, legal and many more domains.\n\n## Demos\n\n- `demos` folder contains notebooks used during MLflow presentations.\n", "# Basic authentication example\n\nThis example demonstrates the authentication and authorization feature of MLflow.\n\nTo run this example,\n\n1. Start the tracking server\n   ```shell\n   mlflow ui --app-name=basic-auth\n   ```\n2. Go to `http://localhost:5000/signup` and register two users:\n   - `(user_a, password_a)`\n   - `(user_b, password_b)`\n3. Run the script\n   ```shell\n   python auth.py\n   ```\n   Expected output:\n   ```\n   2023/05/02 14:03:58 INFO mlflow.tracking.fluent: Experiment with name 'experiment_a' does not exist. Creating a new experiment.\n   {}\n   API request to endpoint /api/2.0/mlflow/runs/create failed with error code 403 != 200. Response body: 'Permission denied'\n   ```\n", '# MLflow 3 Examples\n\n## Pre-requisites\n\nBefore running the examples, run the following command to install mlflow 3.0:\n\n```sh\npip install git+https://github.com/mlflow/mlflow.git@mlflow-3\n```\n', '# MLflow Deployments\n\nThe examples provided within this directory show how to get started with MLflow Deployments using:\n\n- Databricks (see the `databricks` subdirectory)\n', '### MLflow evaluation Examples\n\nThe examples in this directory demonstrate how to use the `mlflow.evaluate()` API. Specifically,\nthey show how to evaluate a PyFunc model on a specified dataset using the builtin default evaluator\nand specified extra metrics, where the resulting metrics & artifacts are logged to MLflow Tracking.\nThey also show how to specify validation thresholds for the resulting metrics to validate the quality\nof your model. See full list of examples below:\n\n- Example `evaluate_on_binary_classifier.py` evaluates an xgboost `XGBClassifier` model on dataset loaded by\n  `shap.datasets.adult`.\n- Example `evaluate_on_multiclass_classifier.py` evaluates a scikit-learn `LogisticRegression` model on dataset\n  generated by `sklearn.datasets.make_classification`.\n- Example `evaluate_on_regressor.py` evaluate as scikit-learn `LinearRegression` model on dataset loaded by\n  `sklearn.datasets.fetch_california_housing`\n- Example `evaluate_with_custom_metrics.py` evaluates a scikit-learn `LinearRegression`\n  model with a custom metric function on dataset loaded by `sklearn.datasets.fetch_california_housing`\n- Example `evaluate_with_custom_metrics_comprehensive.py` evaluates a scikit-learn `LinearRegression` model\n  with a comprehensive list of custom metric functions on dataset loaded by `sklearn.datasets.fetch_california_housing`\n- Example `evaluate_with_model_validation.py` trains both a candidate xgboost `XGBClassifier` model\n  and a baseline `DummyClassifier` model on dataset loaded by `shap.datasets.adult`. Then, it validates\n  the candidate model against specified thresholds on both builtin and extra metrics and the dummy model.\n\n#### Prerequisites\n\n```\npip install scikit-learn xgboost shap>=0.40 matplotlib\n```\n\n#### How to run the examples\n\nRun in this directory with Python.\n\n```sh\npython evaluate_on_binary_classifier.py\npython evaluate_on_multiclass_classifier.py\npython evaluate_on_regressor.py\npython evaluate_with_custom_metrics.py\npython evaluate_with_custom_metrics_comprehensive.py\npython evaluate_with_model_vaidation.py\n```\n', '# MLflow AI Gateway\n\nThe examples provided within this directory show how to get started with individual providers and at least\none of the supported endpoint types. When configuring an instance of the MLflow AI Gateway, multiple providers,\ninstances of endpoint types, and model versions can be specified for each query endpoint on the server.\n\n## Example configuration files\n\nWithin this directory are example config files for each of the supported providers. If using these as a guide\nfor configuring a large number of endpoints, ensure that the placeholder names (i.e., "completions", "chat", "embeddings")\nare modified to prevent collisions. These names are provided for clarity only for the examples and real-world\nuse cases should define a relevant and meaningful endpoint name to eliminate ambiguity and minimize the chances of name collisions.\n\n# Getting Started with MLflow AI Gateway for OpenAI\n\nThis guide will walk you through the installation and basic setup of the MLflow AI Gateway.\nWithin sub directories of this examples section, you can find specific executable examples\nthat can be used to validate a given provider\'s configuration through the MLflow AI Gateway.\nLet\'s get started.\n\n## Step 1: Installing the MLflow AI Gateway\n\nThe MLflow AI Gateway is best installed from PyPI. Open your terminal and use the following pip command:\n\n```sh\n# Installation from PyPI\npip install \'mlflow[genai]\'\n```\n\nFor those interested in development or in using the most recent build of the MLflow AI Gateway, you may choose to install from the fork of the repository:\n\n```sh\n# Installation from the repository\npip install -e \'.[genai]\'\n```\n\n## Step 2: Configuring Endpoints\n\nEach provider has a distinct set of allowable endpoint types (i.e., chat, completions, etc) and\nspecific requirements for the initialization of the endpoints to interface with their services.\nFor full examples of configurations and supported endpoint types, see:\n\n- [OpenAI](openai/config.yaml)\n- [MosaicML](mosaicml/config.yaml)\n- [Anthropic](anthropic/config.yaml)\n- [Cohere](cohere/config.yaml)\n- [AI21 Labs](ai21labs/config.yaml)\n- [PaLM](palm/config.yaml)\n- [AzureOpenAI](azure_openai/config.yaml)\n- [Mistral](mistral/config.yaml)\n- [TogetherAI](togetherai/config.yaml)\n\n## Step 3: Setting Access Keys\n\nSee information on specific methods of obtaining and setting the access keys within the provider-specific documentation within this directory.\n\n## Step 4: Starting the MLflow AI Gateway\n\nWith the MLflow configuration file in place and access key(s) set, you can now start the MLflow AI Gateway.\nReplace `<provider>` with the actual path to the MLflow configuration file for the provider of your choice:\n\n```sh\nmlflow ', "(i.e., chat, completions, etc) and\nspecific requirements for the initialization of the endpoints to interface with their services.\nFor full examples of configurations and supported endpoint types, see:\n\n- [OpenAI](openai/config.yaml)\n- [MosaicML](mosaicml/config.yaml)\n- [Anthropic](anthropic/config.yaml)\n- [Cohere](cohere/config.yaml)\n- [AI21 Labs](ai21labs/config.yaml)\n- [PaLM](palm/config.yaml)\n- [AzureOpenAI](azure_openai/config.yaml)\n- [Mistral](mistral/config.yaml)\n- [TogetherAI](togetherai/config.yaml)\n\n## Step 3: Setting Access Keys\n\nSee information on specific methods of obtaining and setting the access keys within the provider-specific documentation within this directory.\n\n## Step 4: Starting the MLflow AI Gateway\n\nWith the MLflow configuration file in place and access key(s) set, you can now start the MLflow AI Gateway.\nReplace `<provider>` with the actual path to the MLflow configuration file for the provider of your choice:\n\n```sh\nmlflow gateway start --config-path examples/gateway/<provider>/config.yaml --port 7000\n\n# For example:\nmlflow gateway start --config-path examples/gateway/openai/config.yaml --port 7000\n```\n\n## Step 5: Accessing the Interactive API Documentation\n\nWith the MLflow AI Gateway up and running, access its interactive API documentation by navigating to the following URL:\n\nhttp://127.0.0.1:7000/docs\n\n## Step 6: Sending Test Requests\n\nAfter successfully setting up the MLflow AI Gateway, you can send a test request using the provided Python script.\nReplace <provider> with the name of the provider example test script that you'd like to use:\n\n```sh\npython examples/gateway/<provider>/example.py\n```\n", "## Example endpoint configuration for AI21 Labs\n\nTo set up your MLflow configuration file, include a single endpoint for the completions endpoint as shown in the [AI21 labs configuration](config.yaml) YAML file.\n\n## Obtaining and Setting the AI21 Labs API Key\n\nTo obtain an AI21 Labs API key, you need to create an account and subscribe to the service at [AI21 Labs](https://studio.ai21.com/account/api-key?source=docs).\n\nAfter obtaining the key, you can export it to your environment variables. Make sure to replace the '...' with your actual API key:\n\n```sh\nexport AI21LABS_API_KEY=...\n```\n", "## Example endpoint configuration for Anthropic\n\nTo set up your MLflow configuration file, include a single endpoint for the completions endpoint as shown in the [anthropic configuration](config.yaml) YAML file.\n\n## Obtaining and Setting the Anthropic API Key\n\nTo obtain an Anthropic API key, you need to create an account and subscribe to the service at [Anthropic](https://docs.anthropic.com/claude/docs/getting-access-to-claude).\n\nAfter obtaining the key, you can export it to your environment variables. Make sure to replace the '...' with your actual API key:\n\n```sh\nexport ANTHROPIC_API_KEY=...\n```\n", "## Example endpoint configuration for Azure OpenAI\n\nThe following example configuration shows the 3 supported endpoints for Azure OpenAI: chat, completions, and embeddings.\nAdditionally, it illustrates the two separate api types that are supported for this service.\n\n- `azure` api type: uses a generated token that is applied by setting the API token key directly to an environment variable\n- `azuread` api type: uses Azure Active Directory for supplying the active directory key to be used to an environment variable\n\nDepending on how your users will be interacting with the MLflow AI Gateway, a single access paradigm (either `azure` **or** `azuread` is recommended, not a mix of both).\n\nSee the [Azure OpenAI configuration](config.yaml) YAML file for example configurations showing all supported endpoint types and the different token access types.\n\n## Setting the Azure OpenAI API Key\n\nIn order to get access to the Azure OpenAI service, [see the documentation](https://azure.microsoft.com/en-us/products/cognitive-services/openai-service) guidance in the cognitive services portal.\nWith the key, export it to your environment variables.\n\nReplace the '...' with your actual API key:\n\n```sh\nexport OPENAI_API_KEY=...\n```\n\n## Validating the Azure OpenAI endpoint\n\nSee the [OpenAI Example](../openai/example.py) for testing the Azure OpenAI endpoints. The usage is identical to the standard OpenAI integration from an API perspective.\n", '## Example endpoint configuration for Amazon Bedrock\n\nTo view an example of a Bedrock endpoint configuration, see [the configuration example](config.yaml) YAML file.\n\n## Credentials\n\nValid AWS credentials are required for this example. Set `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` to valid credentials, or run in an environment with those variables set.\n', "## Example endpoint configuration for Cohere\n\nTo see an example of specifying both the completions and the embeddings endpoints for Cohere, see [the configuration](config.yaml) YAML file.\n\nThis configuration file specifies two endpoints: 'completions' and 'embeddings', both using Cohere's models 'command' and 'embed-english-light-v2.0', respectively.\n\n## Setting a Cohere API Key\n\nThis example requires a [Cohere API key](https://docs.cohere.com/docs/going-live):\n\n```sh\nexport COHERE_API_KEY=...\n```\n", "## Example endpoint configuration for GEMINI\n\nTo see an example of specifying both the completions and embeddings endpoints for Gemini, see [the configuration](config.yaml) YAML file.\n\nThis configuration file specifies three endpoints: 'completions', 'embeddings', and 'chat', using Gemini's model gemini-2.0-flash for completions and chat and gemini-embedding-exp-03-07 for embeddings.\n\n## Setting a GEMINI API Key\n\nThis example requires a [GEMINI API key](https://ai.google.dev/gemini-api/docs/api-key):\n\n```sh\nexport GEMINI_API_KEY=...\n```\n", '## Example endpoint configuration for Huggingface Text Generation Inference\n\n[Huggingface Text Generation Inference (TGI)](https://huggingface.co/docs/text-generation-inference/index) is a comprehensive toolkit designed for deploying and serving Large Language Models (LLMs) efficiently. It offers optimized support for various popular open-source LLMs such as Llama, Falcon, StarCoder, BLOOM, and GPT-Neo. TGI comes with various built-in optimizations and features, such as:\n\n- Simple launcher to serve most popular LLMs\n- Tensor Parallelism for faster inference on multiple GPUs\n- Safetensors weight loading\n- Optimized transformers code for inference using Flash Attention and Paged Attention on the most popular architectures\n\nIt should be noted that only a [selection of models](https://huggingface.co/docs/text-generation-inference/supported_models) are optimized for TGI, which uses custom CUDA kernels for faster inference. You can add the flag `--disable-custom-kernels`` at the end of the docker run command if you wish to disable them. If the above list lacks the model you would like to serve, or in the case you created a custom created model, you can try to initialize and serve the model anyways. However, since the model is not optimized for TGI, performance is not guaranteed.\n\nFor a more detailed description of all features, please go to the [documentation](https://huggingface.co/docs/text-generation-inference/index).\n\n## Getting Started\n\n> **NOTE** This example is tested on a Linux Machine (Debian 11) with a NVIDIA A100 GPU.\n\nTo configure the MLflow AI Gateway with Huggingface Text Generation Inference, a few additional steps need to be followed. The initial step involves deploying a Huggingface model on the TGI server, which is illustrated in the next section.\n\nThe recommended approach for deploying the TGI server is by utilizing the [official Docker container](ghcr.io/huggingface/text-generation-inference:1.1.1). Docker is an open-source platform that provides a streamlined solution for automating the deployment, scaling, and management of applications through containers. These containers encompass all the essential dependencies required for seamless execution, including libraries, binaries, and configuration files. To install Docker, please refer to the [installation guide](https://docs.docker.com/get-docker/).\n\nBefore proceeding, it is important to verify that your machine has the appropriate hardware to initiate the server. TGI optimized models are compatible with NVIDIA A100, A10G, and T4 GPUs. While other GPU hardware may still provide performance advantages, certain operations such as flash attention and paged attention will ', 'utilizing the [official Docker container](ghcr.io/huggingface/text-generation-inference:1.1.1). Docker is an open-source platform that provides a streamlined solution for automating the deployment, scaling, and management of applications through containers. These containers encompass all the essential dependencies required for seamless execution, including libraries, binaries, and configuration files. To install Docker, please refer to the [installation guide](https://docs.docker.com/get-docker/).\n\nBefore proceeding, it is important to verify that your machine has the appropriate hardware to initiate the server. TGI optimized models are compatible with NVIDIA A100, A10G, and T4 GPUs. While other GPU hardware may still provide performance advantages, certain operations such as flash attention and paged attention will not be executed. If you intend to run the container on a machine lacking GPUs or CUDA support, you can eliminate the `--gpus all` flag and include `--disable-custom-kernels`. However, please note that the CPU is not the intended platform for the server, and this choice significantly impacts performance.\n\n#### Installing the NVIDIA Container Toolkit\n\nTo begin, the installation of the NVIDIA container toolkit is necessary. This toolkit is essential for running GPU-accelerated containers. Execute the following command to acquire all the requisite packages [ref the code]:\n\n```sh\ncurl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\\n  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\\n    sed \'s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g\' | \\\n    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list \\\n  && \\\n    sudo apt-get update\n```\n\nInstall the NVIDIA Container toolkit by running the following command.\n\n```\nsudo apt-get install -y nvidia-container-toolkit\n```\n\n#### Running the TGI server.\n\nAfter you installed the NVIDIA Container toolkit, you can run the following Docker command to to start a TGI server on your local machine on port `8000`. This will load a [falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) model on the TGI server.\n\n```\nmodel=tiiuae/falcon-7b-instruct\nvolume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run\ndocker run --gpus all --shm-size 1g -p 8000:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:1.1.1 --model-id $model\n```\n\nAfter the TGI server is deployed, run the following script to verify that it is working correctly:\n\n```\nimport requests\nheaders = {\n    "Content-Type": "application/json",\n}\ndata = {\n    \'inputs\': \'What is Deep Learning?\',\n    \'parameters\': {\n      ', 'you can run the following Docker command to to start a TGI server on your local machine on port `8000`. This will load a [falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) model on the TGI server.\n\n```\nmodel=tiiuae/falcon-7b-instruct\nvolume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run\ndocker run --gpus all --shm-size 1g -p 8000:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:1.1.1 --model-id $model\n```\n\nAfter the TGI server is deployed, run the following script to verify that it is working correctly:\n\n```\nimport requests\nheaders = {\n    "Content-Type": "application/json",\n}\ndata = {\n    \'inputs\': \'What is Deep Learning?\',\n    \'parameters\': {\n        \'max_new_tokens\': 20,\n    },\n}\nresponse = requests.post(\'http://127.0.0.1:8000/generate\', headers=headers, json=data)\nprint(response.json())\n# {\'generated_text\': \'\\nDeep learning is a branch of machine learning that uses artificial neural networks to learn and make decisions.\'}\n```\n\n## Update the config.yaml to add a new embeddings endpoint\n\nAfter you started the server, update the MLflow AI Gateway configuration file [config.yaml](config.yaml) and add the server as a new endpoint:\n\n```\nendpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: "huggingface-text-generation-inference"\n      name: llm\n      config:\n        hf_server_url: http://127.0.0.1:8000/generate\n```\n\n## Starting the MLflow AI Gateway\n\nAfter the configuration file is created, you can start the MLflow AI Gateway by running the following command:\n\n```\nmlflow gateway start --config-path examples/gateway/huggingface/config.yaml --port 7000\n```\n\n## Querying the endpoint\n\nSee the [example script](example.py) within this directory to see how to query the `falcon-7b-instruct` model that is served.\n\n## Setting the parameters of TGI\n\nWhen you make a request to the MLflow Depoyments server, the information you provide in the request body will be sent to TGI. This gives you more control over the output you receive from TGI. However, it\'s important to note that you cannot turn off `details` and `decoder_input_details`, as they are necessary for TGI endpoints to work correctly.\n', "## Example endpoint configuration for Mistral\n\nTo see an example of specifying both the completions and the embeddings endpoints for Mistral, see [the configuration](config.yaml) YAML file.\n\nThis configuration file specifies two endpoints: 'completions' and 'embeddings', both using Mistral's models 'mistral-tiny' and 'mistral-embed', respectively.\n\n## Setting a Mistral API Key\n\nThis example requires a [Mistral API key](https://docs.mistral.ai/):\n\n```sh\nexport MISTRAL_API_KEY=...\n```\n", '# Guide to using an MLflow served model with MLflow Deployments\n\nIn order to utilize MLflow Deployments with MLflow model serving, a few steps must be taken\nin addition to those for configuring access to SaaS models (such as Anthropic and OpenAI). The first and most obvious\nstep that must be taken prior to interfacing with an MLflow served model is that a model needs to be logged to the\nMLflow tracking server.\n\nAn important consideration for deciding whether to interface MLflow Deployments with a specific model is to evaluate the PyFunc interface that the model will\nreturn after being called for inference. Due to the fact that the MLflow AI Gateway defines a specific response signature, expectations for each endpoint type\'s payload contents\nmust be met in order for a endpoint to be valid.\n\nFor example, an embeddings endpoint (llm/v1/embeddings endpoint type) is designed to return embeddings data as a collection (a list) of floats that correspond to each of the\ninput strings that are sent for embeddings inference to a service. The expectation that the embeddings endpoint definition has is that the data is in a particular format. Specifically one that\nis capable of having the embeddings data extractable from a service response. Therefore, an MLflow model that returns data in the format below is perfectly valid.\n\n```json\n{\n  "predictions": [\n    [0.0, 0.1],\n    [1.0, 0.0]\n  ]\n}\n```\n\nHowever, a return value from a serving endpoint via a custom PyFunc of the form below will not work.\n\n```json\n{\n  "predictions": [\n    {\n      "embedding": [0.0, 0.1]\n    },\n    {\n      "embedding": [1.0, 0.0]\n    }\n  ]\n}\n```\n\nIt is important to note that the MLflow AI Gateway does not perform validation on a configured endpoint until the point of querying. Creating a endpoint that interfaces with the\nMLflow model server that is returning a payload that is incompatible with the configured endpoint type definition will raise 502 exceptions only when queried.\n\n> **NOTE:** It is important to validate the output response of a model served by MLflow to ensure compatibility with ', '    "embedding": [0.0, 0.1]\n    },\n    {\n      "embedding": [1.0, 0.0]\n    }\n  ]\n}\n```\n\nIt is important to note that the MLflow AI Gateway does not perform validation on a configured endpoint until the point of querying. Creating a endpoint that interfaces with the\nMLflow model server that is returning a payload that is incompatible with the configured endpoint type definition will raise 502 exceptions only when queried.\n\n> **NOTE:** It is important to validate the output response of a model served by MLflow to ensure compatibility with the MLflow Deployments endpoint definitions. Not all model outputs are compatible with given endpoint types.\n\n## Creating and logging an embeddings model\n\nTo start, we need a model that is capable of generating embeddings. For this example, we\'ll use\nthe `sentence_transformers` library and the corresponding MLflow flavor.\n\n```python\nfrom sentence_transformers import SentenceTransformer\nimport mlflow\n\n\nmodel = SentenceTransformer(model_name_or_path="all-MiniLM-L6-v2")\nartifact_path = "embeddings_model"\n\nwith mlflow.start_run():\n    model_info = mlflow.sentence_transformers.log_model(\n        model,\n        name=artifact_path,\n    )\n```\n\n## Generate the cli command for starting a local MLflow Model Serving endpoint for this embeddings model\n\n```python\nprint(f"mlflow models serve -m {model_info.model_uri} -h 127.0.0.1 -p 9020 --no-conda")\n```\n\nCopy the output from the print statement to the clipboard.\n\n## Starting the model server for the embeddings model\n\nWith the printed string from running the above command copied to the clipboard, open a new terminal\nand paste the string. Leave the terminal window open and running.\n\n```commandline\nmlflow models serve -m file:///Users/me/demos/mlruns/0/2bfcdcb66eaf4c88abe8e0c7bcab639e/artifacts/embeddings_model -h 127.0.0.1 -p 9020 --no-conda\n```\n\n## Update the config.yaml to add a new embeddings endpoint\n\nAfter assigning a valid port and ensuring that the model server starts correctly:\n\n```commandline\n2023/08/08 17:36:44 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor \'python_function\'\n2023/08/08 17:36:44 INFO mlflow.pyfunc.backend: === Running command \'exec uvicorn --host 127.0.0.1 --port 9020 --workers 1 mlflow.pyfunc.scoring_server.app:app\'\nINFO:     Started server process [6992]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:9020\n```\n\nThe scoring server is ready to receive traffic.\n\nUpdate the MLflow AI Gateway configuration file (config.yaml) with the new endpoint:\n\n```yaml\nendpoints:\n  ', 'file:///Users/me/demos/mlruns/0/2bfcdcb66eaf4c88abe8e0c7bcab639e/artifacts/embeddings_model -h 127.0.0.1 -p 9020 --no-conda\n```\n\n## Update the config.yaml to add a new embeddings endpoint\n\nAfter assigning a valid port and ensuring that the model server starts correctly:\n\n```commandline\n2023/08/08 17:36:44 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor \'python_function\'\n2023/08/08 17:36:44 INFO mlflow.pyfunc.backend: === Running command \'exec uvicorn --host 127.0.0.1 --port 9020 --workers 1 mlflow.pyfunc.scoring_server.app:app\'\nINFO:     Started server process [6992]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:9020\n```\n\nThe scoring server is ready to receive traffic.\n\nUpdate the MLflow AI Gateway configuration file (config.yaml) with the new endpoint:\n\n```yaml\nendpoints:\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: mlflow-model-serving\n      name: sentence-transformer\n      config:\n        model_server_url: http://127.0.0.1:9020\n```\n\nThe key component here is the `model_server_url`. For serving an MLflow LLM, this url must match to the service that you are specifying for the\nModel Serving server.\n\n> **NOTE:** The MLflow Model Server does not have to be running in order to update the configuration file or to start the MLflow AI Gateway. In order to respond to submitted queries, it is required to be running.\n\n## Creating and logging a fill mask model\n\nTo support an additional endpoint for generating a mask fill response from masked input text, we need to log an appropriate model.\nFor this tutorial example, we\'ll use a `transformers` `Pipeline` wrapping a `BertForMaskedLM` torch model and will log this pipeline using the MLflow `transformers` flavor.\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nimport mlflow\n\n\nlm_architecture = "bert-base-cased"\nartifact_path = "mask_fill_model"\n\ntokenizer = AutoTokenizer.from_pretrained(lm_architecture)\nmodel = AutoModelForMaskedLM.from_pretrained(lm_architecture)\n\ncomponents = {"model": model, "tokenizer": tokenizer}\n\nwith mlflow.start_run():\n    model_info = mlflow.transformers.log_model(\n        transformers_model=components,\n        name=artifact_path,\n    )\n```\n\n## Generate the cli command for starting a local MLflow Model Serving endpoint for this fill mask model\n\n```python\nprint(f"mlflow models serve -m {model_info.model_uri} -h 127.0.0.1 -p 9010 --no-conda")\n```\n\n## Starting the model server for the fill mask model\n\nUsing the command printed to stdout from above, open a new terminal (do not close ', 'pipeline using the MLflow `transformers` flavor.\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nimport mlflow\n\n\nlm_architecture = "bert-base-cased"\nartifact_path = "mask_fill_model"\n\ntokenizer = AutoTokenizer.from_pretrained(lm_architecture)\nmodel = AutoModelForMaskedLM.from_pretrained(lm_architecture)\n\ncomponents = {"model": model, "tokenizer": tokenizer}\n\nwith mlflow.start_run():\n    model_info = mlflow.transformers.log_model(\n        transformers_model=components,\n        name=artifact_path,\n    )\n```\n\n## Generate the cli command for starting a local MLflow Model Serving endpoint for this fill mask model\n\n```python\nprint(f"mlflow models serve -m {model_info.model_uri} -h 127.0.0.1 -p 9010 --no-conda")\n```\n\n## Starting the model server for the fill mask model\n\nUsing the command printed to stdout from above, open a new terminal (do not close the terminal that is currently running the embeddings model being served!)\nand paste the command.\n\n```commandline\nmlflow models serve -m file:///Users/me/demos/mlruns/0/bc8bdb7fb90c406eb95603a97742cef8/artifacts/mask_fill_model -h 127.0.0.1 -p 9010 --no-conda\n```\n\n## Update the config.yaml to add a new completions endpoint\n\nEnsure that the MLflow serving endpoint starts and is ready for traffic.\n\n```commandline\n2023/08/08 17:39:14 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor \'python_function\'\n2023/08/08 17:39:14 INFO mlflow.pyfunc.backend: === Running command \'exec uvicorn --host 127.0.0.1 --port 9010 --workers 1 mlflow.pyfunc.scoring_server.app:app\'\nINFO:     Started server process [6992]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:9010\n```\n\nAdd the entry to the MLflow AI Gateway configuration file. The final file should match [the config file](config.yaml)\n\n## Create a completions model using MPT-7B-instruct (optional, see notes below)\n\n> **NOTE:** If your system does not have a CUDA-compatible GPU and you have not installed torch with the appropriate CUDA libraries, it is not recommended to attempt to run this portion of the example.\n> The inference performance of the MPT-7B-instruct model running on CPU is very slow.\n> It is also not recommended to add this model to an MLflow model serving environment that does not have a sufficiently powerful GPU available.\n\n### Download the MPT-7B instruct model and tokenizer to a local directory cache\n\n```python\nfrom huggingface_hub import snapshot_download\n\nsnapshot_location = snapshot_download(\n    repo_id="mosaicml/mpt-7b-instruct", local_dir="mpt-7b"\n)\n```\n\n### Define the PyFunc model that will be used for the completions endpoint\n\n```python\nimport transformers\nimport mlflow\nimport torch\n\n\nclass MPT(mlflow.pyfunc.PythonModel):\n    def load_context(self, context):\n        """\n    ', 'attempt to run this portion of the example.\n> The inference performance of the MPT-7B-instruct model running on CPU is very slow.\n> It is also not recommended to add this model to an MLflow model serving environment that does not have a sufficiently powerful GPU available.\n\n### Download the MPT-7B instruct model and tokenizer to a local directory cache\n\n```python\nfrom huggingface_hub import snapshot_download\n\nsnapshot_location = snapshot_download(\n    repo_id="mosaicml/mpt-7b-instruct", local_dir="mpt-7b"\n)\n```\n\n### Define the PyFunc model that will be used for the completions endpoint\n\n```python\nimport transformers\nimport mlflow\nimport torch\n\n\nclass MPT(mlflow.pyfunc.PythonModel):\n    def load_context(self, context):\n        """\n        This method initializes the tokenizer and language model\n        using the specified model snapshot directory.\n        """\n        # Initialize tokenizer and language model\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n            context.artifacts["snapshot"], padding_side="left"\n        )\n\n        config = transformers.AutoConfig.from_pretrained(\n            context.artifacts["snapshot"], trust_remote_code=True\n        )\n        # Comment out this configuration setting if not running on a GPU or if triton is not installed.\n        # Note that triton dramatically improves the inference speed performance\n        config.attn_config["attn_impl"] = "triton"\n\n        self.model = transformers.AutoModelForCausalLM.from_pretrained(\n            context.artifacts["snapshot"],\n            config=config,\n            torch_dtype=torch.bfloat16,\n            trust_remote_code=True,\n        )\n\n        # NB: If you do not have a CUDA-capable device or have torch installed with CUDA support\n ', '       config.attn_config["attn_impl"] = "triton"\n\n        self.model = transformers.AutoModelForCausalLM.from_pretrained(\n            context.artifacts["snapshot"],\n            config=config,\n            torch_dtype=torch.bfloat16,\n            trust_remote_code=True,\n        )\n\n        # NB: If you do not have a CUDA-capable device or have torch installed with CUDA support\n        # this setting will not function correctly. Setting device to \'cpu\' is valid, but\n        # the performance will be very slow.\n        self.model.to(device="cuda")\n\n        self.model.eval()\n\n    def _build_prompt(self, instruction):\n        """\n        This method generates the prompt for the model.\n        """\n        INSTRUCTION_KEY = "### Instruction:"\n        RESPONSE_KEY = "### Response:"\n        INTRO_BLURB = (\n            "Below is an instruction that describes a task. "\n            "Write a response that appropriately completes the request."\n        )\n\n        return f"""{INTRO_BLURB}\n        {INSTRUCTION_KEY}\n        {instruction}\n        {RESPONSE_KEY}\n        """\n\n    def predict(self, context, model_input, params=None):\n        """\n        This method generates prediction for the given input.\n        """\n    ', ' "Write a response that appropriately completes the request."\n        )\n\n        return f"""{INTRO_BLURB}\n        {INSTRUCTION_KEY}\n        {instruction}\n        {RESPONSE_KEY}\n        """\n\n    def predict(self, context, model_input, params=None):\n        """\n        This method generates prediction for the given input.\n        """\n        prompt = model_input["prompt"][0]\n        temperature = model_input.get("temperature", [1.0])[0]\n        max_tokens = model_input.get("max_tokens", [100])[0]\n\n        # Build the prompt\n        prompt = self._build_prompt(prompt)\n\n        # Encode the input and generate prediction\n        # NB: Sending the tokenized inputs to the GPU here explicitly will not work if your system does not have CUDA support.\n        # If attempting to run this with only CPU support, change \'cuda\' to \'cpu\'\n        encoded_input = self.tokenizer.encode(prompt, return_tensors="pt").to("cuda")\n        output = self.model.generate(\n            encoded_input,\n            do_sample=True,\n            temperature=temperature,\n            max_new_tokens=max_tokens,\n        )\n\n        # Decode the prediction to text\n        generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n\n        # Removing the prompt from the generated text\n        prompt_length = len(self.tokenizer.encode(prompt, return_tensors="pt")[0])\n        generated_response = ', '        do_sample=True,\n            temperature=temperature,\n            max_new_tokens=max_tokens,\n        )\n\n        # Decode the prediction to text\n        generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n\n        # Removing the prompt from the generated text\n        prompt_length = len(self.tokenizer.encode(prompt, return_tensors="pt")[0])\n        generated_response = self.tokenizer.decode(\n            output[0][prompt_length:], skip_special_tokens=True\n        )\n\n        return {"candidates": [generated_response]}\n```\n\n### Specify the model signature, input example, and log the custom model\n\n```python\nimport pandas as pd\nimport mlflow\nfrom mlflow.models.signature import ModelSignature\nfrom mlflow.types import DataType, Schema, ColSpec\n\n# Define input and output schema\ninput_schema = Schema(\n    [\n        ColSpec(DataType.string, "prompt"),\n        ColSpec(DataType.double, "temperature"),\n        ColSpec(DataType.long, "max_tokens"),\n    ]\n)\noutput_schema = Schema([ColSpec(DataType.string, "candidates")])\nsignature = ModelSignature(inputs=input_schema, outputs=output_schema)\n\n\n# Define input example\ninput_example = pd.DataFrame(\n    {"prompt": ["What is machine learning?"], "temperature": [0.5], "max_tokens": [100]}\n)\n\nwith mlflow.start_run():\n    mlflow.pyfunc.log_model(\n        name="mpt-7b-instruct",\n        python_model=MPT(),\n        artifacts={"snapshot": snapshot_location},\n        pip_requirements=[\n            "torch",\n            "transformers",\n            "accelerate",\n            "einops",\n            "sentencepiece",\n        ],\n        input_example=input_example,\n        signature=signature,\n    )\n```\n\n## Starting the model server ', '       pip_requirements=[\n            "torch",\n            "transformers",\n            "accelerate",\n            "einops",\n            "sentencepiece",\n        ],\n        input_example=input_example,\n        signature=signature,\n    )\n```\n\n## Starting the model server for mpt-7B-instruct (Optional)\n\nDue to the size and complexity of the MPT-7B-instruct model, it is highly advised to only attempt to serve this model in an environment that has:\n\n- A powerful GPU that is capable of holding the model weights in GPU memory\n- triton installed\n\nIn order to initialize the MLflow Model Server for a large model such as MPT-7B, a slightly modified cli command must be used. Most notably, the timeout duration must be increased from the\ndefault of 60 seconds and it is highly recommended to utilize only a single Gunicorn worker (since each worker will load its own copy of the model, there is a distinct possibility of crashing the server environment with an out of memory fault).\n\n```commandline\nmlflow models serve -m file:///Users/me/demos/mlruns/0/92d017e23ca04ffa919a935ed54e9334/artifacts/mpt-7b-instruct -h 127.0.0.1 -p 9030 -t 1200 -w 1 --no-conda\n```\n\n## Update the config.yaml to add the MPT-7B-instruct endpoint (Optional)\n\n> **NOTE** If you are adding this endpoint for the example, you will have to manually edit the config.yaml. If the server that is running the MPT-7B-instruct custom PyFunc model\'s inference does not have GPU support,\n> the performance for inference will take a very long time (CPU inference with this model can take tens of minutes for a single query).\n\n```yaml\nendpoints:\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: mlflow-model-serving\n      name: sentence-transformer\n      config:\n        model_server_url: http://127.0.0.1:9020\n  - name: fillmask\n    endpoint_type: llm/v1/completions\n ', 'you will have to manually edit the config.yaml. If the server that is running the MPT-7B-instruct custom PyFunc model\'s inference does not have GPU support,\n> the performance for inference will take a very long time (CPU inference with this model can take tens of minutes for a single query).\n\n```yaml\nendpoints:\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: mlflow-model-serving\n      name: sentence-transformer\n      config:\n        model_server_url: http://127.0.0.1:9020\n  - name: fillmask\n    endpoint_type: llm/v1/completions\n    model:\n      provider: mlflow-model-serving\n      name: fill-mask\n      config:\n        model_server_url: http://127.0.0.1:9010\n  - name: mpt-instruct\n    endpoint_type: llm/v1/completions\n    model:\n      provider: mlflow-model-serving\n      name: mpt-7b-instruct\n      config:\n        model_server_url: http://127.0.0.1:9030\n```\n\n## Start the MLflow AI Gateway\n\nNow that both endpoints (or all 3, if adding in the optional MPT-7B-instruct model endpoint) are defined within the configuration YAML file and the Model Serving servers are ready to receive queries, we can start the MLflow AI Gateway.\n\n```sh\nmlflow gateway start --config-path examples/gateway/mlflow_serving/config.yaml --port 7000\n```\n\nIf adding the mpt-7b-instruct model, start the MLflow AI Gateway by directing the `--config-path` argument to the location of the `config.yaml` file that you\'ve created with the endpoint\'s addition.\n\n## Query the MLflow AI Gateway\n\nSee the [example script](example.py) within this directory to see how to query these two models that are being served.\n\n### Query the mpt-7B-instruct endpoint (Optional)\n\nIn order to query the mpt-7b-instruct model, the example shown in the script can be modified by adding an additional query call, as shown below:\n\n```python\n# Querying the optional mpt-7b-instruct endpoint\nresponse_mpt = query(\n    endpoint="mpt-instruct",\n    data={\n        "prompt": "What is the purpose of an attention mask in a transformers model?",\n        "temperature": 0.1,\n   ', 'endpoint\'s addition.\n\n## Query the MLflow AI Gateway\n\nSee the [example script](example.py) within this directory to see how to query these two models that are being served.\n\n### Query the mpt-7B-instruct endpoint (Optional)\n\nIn order to query the mpt-7b-instruct model, the example shown in the script can be modified by adding an additional query call, as shown below:\n\n```python\n# Querying the optional mpt-7b-instruct endpoint\nresponse_mpt = query(\n    endpoint="mpt-instruct",\n    data={\n        "prompt": "What is the purpose of an attention mask in a transformers model?",\n        "temperature": 0.1,\n        "max_tokens": 200,\n    },\n)\nprint(f"Fluent API response for mpt-instruct: {response_mpt}")\n```\n', "## Example endpoint configuration for MosaicML\n\nTo see an example of specifying both the completions and the embeddings endpoints for MosaicML, see [the configuration](config.yaml) YAML file.\n\nThis configuration file specifies three endpoints: 'completions', 'embeddings', and 'chat', using MosaicML's models 'mpt-7b-instruct', 'instructor-xl', and 'llama2-70b-chat', respectively.\n\n## Setting a MosaicML API Key\n\nThis example requires a [MosaicML API key](https://docs.mosaicml.com/en/latest/getting_started.html):\n\n```sh\nexport MOSAICML_API_KEY=...\n```\n", "## Example endpoint configuration for OpenAI\n\nTo view an example of OpenAI endpoint configurations, see [the configuration example](config.yaml) YAML file for OpenAI.\n\nThis configuration shows all 3 supported endpoint types: chat, completions, and embeddings.\n\n## Setting the OpenAI API Key\n\nAn OpenAI API key is required for the configuration. If you haven't already, obtain an [OpenAI API key](https://platform.openai.com/account/api-keys).\n\nWith the key, export it to your environment variables. Replace the '...' with your actual API key:\n\n```sh\nexport OPENAI_API_KEY=...\n```\n", "## Example endpoint configuration for PaLM\n\nTo see an example of specifying both the completions and the embeddings endpoints for PaLM, see [the configuration](config.yaml) YAML file.\n\nThis configuration file specifies three endpoints: 'completions', 'embeddings', and 'chat', using PaLM's models 'text-bison-001', 'embedding-gecko-001', and 'chat-bison-001', respectively.\n\n## Setting a PaLM API Key\n\nThis example requires a [PaLM API key](https://developers.generativeai.google/tutorials/setup):\n\n```sh\nexport PALM_API_KEY=...\n```\n", "## Example endpoint configuration for plugin provider\n\nTo see an example of specifying the chat endpoint for a plugin provider,\nsee [the configuration](config.yaml) YAML file.\n\nWe implement our plugin provider package `my_llm` under `./my-llm` folder. It implements the chat method.\n\nThis configuration file specifies one endpoint: 'chat', using the model 'my-model-0.1.2'.\n\n## Setting up the server\n\nFirst, install the provider package `my_llm`:\n\n```sh\npip install -e ./my-llm\n```\n\nThen, start the server:\n\n```sh\nMY_LLM_API_KEY=some-api-key mlflow gateway start --config-path config.yaml --port 7000\n```\n\nTo clean up the installed package after the example, run\n\n```sh\npip uninstall my_llm\n```\n", "## Example endpoint configuration for TogetherAI\n\nTo see an example of specifying both the completions and the embeddings endpoints for TogetherAI, see [the configuration](config.yaml) YAML file.\n\nThis configuration file specifies two endpoints: 'completions' and 'embeddings', both using TogetherAI's provided models 'mistralai/Mixtral-8x7B-v0.1' and 'togethercomputer/m2-bert-80M-8k-retrieval', respectively.\n\n## Setting a Mistral API Key\n\nThis example requires a [TogetherAI API key](https://docs.together.ai/docs/):\n\n```sh\nexport TOGETHERAI_API_KEY=...\n```\n", '# Unity Catalog Integration\n\nThis example demonstrates how to use the Unity Catalog (UC) integration with MLflow AI Gateway.\n\n## Pre-requisites\n\n1. Install the required packages:\n\n```bash\npip install mlflow openai databricks-sdk\n```\n\n2. Create the UC function used in `run.py` by running the following command on Databricks notebook:\n\n```\n%sql\n\nCREATE OR REPLACE FUNCTION\nmy.uc_func.add (\n  x INTEGER COMMENT \'The first number to add.\',\n  y INTEGER COMMENT \'The second number to add.\'\n)\nRETURNS INTEGER\nLANGUAGE SQL\nRETURN x + y\n```\n\nTo define your own function, see https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.html#create-function-sql-and-python.\n\n3. Create a SQL warehouse in Databricks by following the instructions at https://docs.databricks.com/en/compute/sql-warehouse/create.html.\n\n## Running the example script\n\nFirst, run the deployments server:\n\n```bash\n# Required to authenticate with Databricks. See https://docs.databricks.com/en/dev-tools/auth/index.html#supported-authentication-types-by-databricks-tool-or-sdk for other authentication methods.\nexport DATABRICKS_HOST="..."   # e.g. https://my.databricks.com\nexport DATABRICKS_TOKEN="..."\n\n# Required to execute UC functions. See https://docs.databricks.com/en/integrations/compute-details.html#get-connection-details-for-a-databricks-compute-resource for how to get the http path of your warehouse.\n# The last part of the http path is the warehouse ID.\n#\n# /sql/1.0/warehouses/1234567890123456\n#                     ^^^^^^^^^^^^^^^^\nexport DATABRICKS_WAREHOUSE_ID="..."\n\n# Enable Unity Catalog integration\nexport MLFLOW_ENABLE_UC_FUNCTIONS=true\n\nmlflow gateway start --config-path examples/gateway/openai/config.yaml --port 7000\n```\n\nOnce the server starts running, run the example script:\n\n```bash\n# Replace `my.uc_func.add` if your UC function has a different name\npython examples/gateway/uc_functions/run.py  --uc-function-name my.uc_func.add\n```\n', '# Examples for LightGBM Autologging\n\nLightGBM autologging functionalities are demonstrated through two examples. The first example in the `lightgbm_native` folder logs a Booster model trained by `lightgbm.train()`. The second example in the `lightgbm_sklearn` folder shows how autologging works for LightGBM scikit-learn models. The autologging for all LightGBM models is enabled via `mlflow.lightgbm.autolog()`.\n', '# LightGBM Example\n\nThis example trains a LightGBM classifier with the iris dataset and logs hyperparameters, metrics, and trained model.\n\n## Running the code\n\n```\npython train.py --colsample-bytree 0.8 --subsample 0.9\n```\n\nYou can try experimenting with different parameter values like:\n\n```\npython train.py --learning-rate 0.4 --colsample-bytree 0.7 --subsample 0.8\n```\n\nThen you can open the MLflow UI to track the experiments and compare your runs via:\n\n```\nmlflow ui\n```\n\n## Running the code as a project\n\n```\nmlflow run . -P learning_rate=0.2 -P colsample_bytree=0.8 -P subsample=0.9\n```\n', '# XGBoost Scikit-learn Model Example\n\nThis example trains an [`LightGBM.LGBMClassifier`](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html) with the diabetes dataset and logs hyperparameters, metrics, and trained model.\n\nLike the other LightGBM example, we enable autologging for LightGBM scikit-learn models via `mlflow.lightgbm.autolog()`. Saving / loading models also supports LightGBM scikit-learn models.\n\nYou can run this example using the following command:\n\n```shell\npython train.py\n```\n', '# MLflow LlamaIndex Workflow Example\n\nThis example demonstrates how to build and optimize a Retrieval-Augmented Generation (RAG) workflow using [LlamaIndex](https://www.llamaindex.ai/) integrated with [MLflow](https://mlflow.org/docs/latest/llms/llama-index/index.html). The example covers various retrieval strategies such as vector search, BM25, and web search, along with logging, model tracking, and performance evaluation in MLflow.\n\n![Hybrid RAG Concept](static/images/llama_index_workflow_hybrid_rag_concept.png)\n\n![Evaluation Result](static/images/llama_index_workflow_result_chart.png)\n\n## Set Up\n\nThis repository contains a complete workflow definition, a hands-on notebook, and a sample dataset for running experiments. To clone it to your working environment, use the following command:\n\n```shell\ngit clone https://github.com/mlflow/mlflow.git\n```\n\nAfter cloning the repository, set up the virtual environment by running:\n\n```\ncd mlflow/examples/llama_index/workflow\nchmod +x install.sh\n./install.sh\n```\n\nOnce the installation is complete, start Jupyter Notebook within the Poetry environment using:\n\n```\npoetry run jupyter notebook\n```\n', "# MLflow examples for LLM use cases\n\nThis directory includes several examples for tracking, evaluating, and scoring models with LLMs.\n\n## Summarization\n\nThe `summarization/summarization.py` script uses prompt engineering to build two summarization models for news articles with LangChain. It leverages the `mlflow.langchain` flavor to package and log the models to MLflow, `mlflow.evaluate()` to evaluate each model's performance on a small example dataset, and `mlflow.pyfunc.load_model()` to load and score the best packaged model on a new example article.\n\nTo run the example as an MLflow Project, simply execute the following command from this directory:\n\n```\n$ cd summarization && mlflow run .\n```\n\nTo run the example as a Python script, simply execute the following command from this directory:\n\n```\n$ cd summarization && python summarization.py\n```\n\nNote that this example requires MLflow 2.4.0 or greater to run. Additionally, you must have [LangChain](https://python.langchain.com/en/latest/index.html) and the [OpenAI Python client](https://pypi.org/project/openai/) installed in order to run the example. We also recommend installing the [Hugging Face Evaluate library](https://huggingface.co/docs/evaluate/index) to compute [ROUGE metrics](<https://en.wikipedia.org/wiki/ROUGE_(metric)>) for summary quality. Finally, you must specify a valid OpenAI API key in the `OPENAI_API_KEY` environment variable.\n\n## Question answering\n\nThe `question_answering/question_answering.py` script uses prompt engineering to build two models that answer questions about MLflow.\n\nIt leverages the `mlflow.openai` flavor to package and log the models to MLflow, `mlflow.evaluate()` to evaluate each model's performance on some example questions, and `mlflow.pyfunc.load_model()` to load and score the best packaged model on a new example question.\n\nTo run the example as an MLflow Project, simply execute the following command from this directory:\n\n```\n$ cd question_answering && mlflow run .\n```\n\nTo run the example as a Python script, simply execute the following command from this directory:\n\n```\n$ cd question_answering && python question_answering.py\n```\n\nNote that this example requires MLflow 2.4.0 or greater to run. Additionally, you must have the [OpenAI Python client](https://pypi.org/project/openai/), [tiktoken](https://pypi.org/project/tiktoken/), and [tenacity](https://pypi.org/project/tenacity/) installed in order to run the example. Finally, you must specify a valid OpenAI API key in the `OPENAI_API_KEY` environment variable.\n", '# MLflow Artifacts Example\n\nThis directory contains a set of files for demonstrating the MLflow Artifacts Service.\n\n## What does the MLflow Artifacts Service do?\n\nThe MLflow Artifacts Service serves as a proxy between the client and artifact storage (e.g. S3)\nand allows the client to upload, download, and list artifacts via REST API without configuring\na set of credentials required to access resources in the artifact storage (e.g. `AWS_ACCESS_KEY_ID`\nand `AWS_SECRET_ACCESS_KEY` for S3).\n\n## Quick start\n\nFirst, launch the tracking server with the artifacts service via `mlflow server`:\n\n```sh\n# Launch a tracking server with the artifacts service\n$ mlflow server \\\n    --backend-store-uri=mlruns \\\n    --artifacts-destination ./mlartifacts \\\n    --default-artifact-root http://localhost:5000/api/2.0/mlflow-artifacts/artifacts/experiments \\\n    --gunicorn-opts "--log-level debug"\n```\n\nNotes:\n\n- `--artifacts-destination` specifies the base artifact location from which to resolve artifact upload/download/list requests. In this examples, we\'re using a local directory `./mlartifacts`, but it can be changed to a s3 bucket or\n- `--default-artifact-root` points to the `experiments` directory of the artifacts service. Therefore, the default artifact location of a newly-created experiment is set to `./mlartifacts/experiments/<experiment_id>`.\n- `--gunicorn-opts "--log-level debug"` is specified to print out request logs but can be omitted if unnecessary.\n- `--artifacts-only` disables all other endpoints for the tracking server apart from those involved in listing, uploading, and downloading artifacts. This makes the MLflow server a single-purpose proxy for artifact handling only.\n\nThen, run `example.py` that performs upload, download, and list operations for artifacts:\n\n```\n$ MLFLOW_TRACKING_URI=http://localhost:5000 python example.py\n```\n\nAfter running the command above, the server should print out request logs for artifact operations:\n\n```diff\n...\n[2021-11-05 19:13:34 +0900] [92800] [DEBUG] POST /api/2.0/mlflow/runs/create\n[2021-11-05 19:13:34 +0900] [92800] [DEBUG] GET /api/2.0/mlflow/runs/get\n[2021-11-05 19:13:34 +0900] [92802] [DEBUG] PUT /api/2.0/mlflow-artifacts/artifacts/0/a1b2c3d4/artifacts/a.txt\n[2021-11-05 19:13:34 +0900] [92802] [DEBUG] PUT /api/2.0/mlflow-artifacts/artifacts/0/a1b2c3d4/artifacts/dir/b.txt\n[2021-11-05 19:13:34 +0900] [92802] [DEBUG] POST /api/2.0/mlflow/runs/update\n[2021-11-05 19:13:34 +0900] [92802] [DEBUG] GET /api/2.0/mlflow-artifacts/artifacts\n...\n```\n\nThe contents of the `mlartifacts` directory should look like this:\n\n```sh\n$ tree mlartifacts\nmlartifacts\nâ””â”€â”€ experiments\n    â””â”€â”€ 0  # experiment ID\n        â””â”€â”€ a1b2c3d4  # run ID\n            â””â”€â”€ artifacts\n                â”œâ”€â”€ a.txt\n      ', '/api/2.0/mlflow/runs/create\n[2021-11-05 19:13:34 +0900] [92800] [DEBUG] GET /api/2.0/mlflow/runs/get\n[2021-11-05 19:13:34 +0900] [92802] [DEBUG] PUT /api/2.0/mlflow-artifacts/artifacts/0/a1b2c3d4/artifacts/a.txt\n[2021-11-05 19:13:34 +0900] [92802] [DEBUG] PUT /api/2.0/mlflow-artifacts/artifacts/0/a1b2c3d4/artifacts/dir/b.txt\n[2021-11-05 19:13:34 +0900] [92802] [DEBUG] POST /api/2.0/mlflow/runs/update\n[2021-11-05 19:13:34 +0900] [92802] [DEBUG] GET /api/2.0/mlflow-artifacts/artifacts\n...\n```\n\nThe contents of the `mlartifacts` directory should look like this:\n\n```sh\n$ tree mlartifacts\nmlartifacts\nâ””â”€â”€ experiments\n    â””â”€â”€ 0  # experiment ID\n        â””â”€â”€ a1b2c3d4  # run ID\n            â””â”€â”€ artifacts\n                â”œâ”€â”€ a.txt\n                â””â”€â”€ dir\n                    â””â”€â”€ b.txt\n\n5 directories, 2 files\n```\n\nTo delete the logged artifacts, run the following command:\n\n```bash\nmlflow gc --backend-store-uri=mlruns --run-ids <run_id>\n```\n\n### Clean up\n\n```sh\n# Remove experiment and run data\n$ rm -rf mlruns\n\n# Remove artifacts\n$ rm -rf mlartifacts\n```\n\n## Advanced example using `docker-compose`\n\n[`docker-compose.yml`](./docker-compose.yml) provides a more advanced setup than the quick-start example above:\n\n- Tracking service uses PostgreSQL as a backend store.\n- Artifact service uses MinIO as a artifact store.\n- Tracking and artifacts services are running on different servers.\n\n```sh\n# Build services\n$ docker-compose build\n\n# Launch tracking and artifacts servers in the background\n$ docker-compose up -d\n\n# Run `example.py` in the client container\n$ docker-compose run -v ${PWD}/example.py:/app/example.py client python example.py\n```\n\nYou can view the logged artifacts on MinIO Console served at http://localhost:9001. The login username and password are `user` and `password`.\n\n### Clean up\n\n```sh\n# Remove containers, networks, volumes, and images\n$ docker-compose down --rmi all --volumes --remove-orphans\n```\n\n### Development\n\n```sh\n# Build services using the dev version of mlflow\n$ ./build.sh\n$ docker-compose run -v ${PWD}/example.py:/app/example.py client python example.py\n```\n', '# OpenAI Autologging Examples\n\n## Using OpenAI client\n\nThe recommended way of using `openai` is to instantiate a client\nusing `openai.OpenAI()`. You can run the following example to use\nautologging using such client.\n\nBefore running these examples, ensure that you have the following additional libraries installed:\n\n```shell\npip install tenacity tiktoken \'openai>=1.17\'\n```\n\nYou can run the example via your command prompt as follows:\n\n```shell\npython examples/openai/autologging/instantiated_client.py --api-key="your-api-key"\n```\n\n## Using module-level client\n\n`openai` exposes a module client instance that can be used to make requests.\nYou can run the following example to use autologging with the module client.\n\n```shell\nexport OPENAI_API_KEY="your-api-key"\npython examples/openai/autologging/module_client.py\n```\n', '# Pyfunc model example\n\nThis example demonstrates the use of a pyfunc model with custom inference logic.\nMore specifically:\n\n- train a simple classification model\n- create a _pyfunc_ model that encapsulates the classification model with an attached module for custom inference logic\n\n## Structure of this example\n\nThis examples contains a `train.py` file that trains a scikit-learn model with iris dataset and uses MLflow Tracking APIs to log the model. The nested **mlflow run** delivers the packaging of `pyfunc` model and `custom_code` module is attached\nto act as a custom inference logic layer in inference time.\n\n```\nâ”œâ”€â”€ train.py\nâ”œâ”€â”€ infer_model_code_path.py\nâ””â”€â”€ custom_code.py\n```\n\n## Running this example\n\n1. Train and log the model\n\n```\n$ python train.py\n```\n\nor train and log the model using inferred code paths\n\n```\n$ python infer_model_code_paths.py\n```\n\n2. Serve the pyfunc model\n\n```bash\n# Replace <pyfunc_run_id> with the run ID obtained in the previous step\n$ mlflow models serve -m "runs:/<pyfunc_run_id>/model" -p 5001\n```\n\n3. Send a request\n\n```\n$ curl http://127.0.0.1:5001/invocations -H \'Content-Type: application/json\' -d \'{\n  "dataframe_records": [[1, 1, 1, 1]]\n}\'\n```\n\nThe response should look like this:\n\n```\n[0]\n```\n', '# PySpark ML Autologging Examples\n\nThis directory contains examples for demonstrating how PySpark ML autologging works.\n\n| File                     | Description                        |\n| :----------------------- | :--------------------------------- |\n| `logistic_regression.py` | Train a `LogisticRegression` model |\n| `one_vs_rest.py`         | Train a `OneVsRest` model          |\n', '# PySpark ML connect Examples\n\nThis directory contains examples for demonstrating how to log PySpark ML connect model.\n\n| File          | Description                                           |\n| :------------ | :---------------------------------------------------- |\n| `pipeline.py` | Use mlflow to Log a PySpark ML connect pipeline model |\n', "## Ax Hyperparameter Optimization Example\n\nIn this example, we train a Pytorch Lightning model to classify Iris flower classification dataset.\nA parent run will be created during the training process,which would dump the baseline model and relevant parameters,metrics and model along with its summary,subsequently followed by a set of nested child runs, which will dump the trial results.\nThe best parameters would be dumped into the parent run once the experiments are completed.\n\n### Running the code\n\nTo run the example via MLflow, navigate to the `mlflow/examples/pytorch/AxHyperOptimizationPTL` directory and run the command\n\n```\nmlflow run .\n```\n\nThis will run `AxHyperOptimizationPTL.py` with the default set of parameters such as `max_epochs=3` and `total_trials=3`. You can see the default value in the `MLproject` file.\n\nIn order to run the file with custom parameters, run the command\n\n```\nmlflow run . -P max_epochs=X -P total_trials=Y\n```\n\nwhere `X` is your desired value for `max_epochs` and `Y` is your desired value for `total_trials`.\n\nIf you have the required modules for the file and would like to skip the creation of a conda environment, add the argument `--env-manager=local`.\n\n```\nmlflow run . --env-manager=local\n```\n\n### Viewing results in the MLflow UI\n\nOnce the code is finished executing, you can view the run's metrics, parameters, and details by running the command\n\n```\nmlflow ui\n```\n\nand navigating to [http://localhost:5000](http://localhost:5000).\n\nFor more details on MLflow tracking, see [the docs](https://www.mlflow.org/docs/latest/tracking.html#mlflow-tracking).\n\n### Passing custom training parameters\n\nThe parameters can be overridden via the command line:\n\n1. max_epochs - Number of epochs to train model. Training can be interrupted early via Ctrl+C\n2. total_trials - Number of experimental trials\n\nFor example:\n\n```\nmlflow run . -P max_epochs=3 -P total_trials=3\n```\n\nOr to run the training script directly with custom parameters:\n\n```\npython ax_hpo_iris.py --max_epochs 3 --total_trials 3\n```\n\nBy running the above mentioned script, the hyperparameters are logged into MLFLow as nested runs.\n\n![Ax HPO Runs](screenshots/ax_hpo.png)\n\nThe child run contains the details of the hyperparameters used during that particular trial.\n\n![Trial Run](screenshots/trial_run.png)\n\nAnd the parent run contains the details of the optimum parameters derived by running n trials.\n\n![Parent Run](screenshots/parent_run.png)\n\n## Logging to a custom tracking server\n\nTo configure MLflow to log to a custom (non-default) tracking location, set the `MLFLOW_TRACKING_URI` environment variable, e.g. via `export MLFLOW_TRACKING_URI=http://localhost:5000`. For more details, see [the docs](https://mlflow.org/docs/latest/tracking.html#where-runs-are-recorded).\n", '## BERT news classification example\n\nIn this example, we train a Pytorch Lightning model to classify news articles into "World", "Sports", "Business" and "Sci/Tech" categories. The code, adapted from this [repository](https://github.com/ricardorei/lightning-text-classification/blob/master/classifier.py), is almost entirely dedicated to model training, with the addition of a single `mlflow.pytorch.autolog()` call to enable automatic logging of params, metrics, and models.\n\n### Running the code\n\nTo run the example via MLflow, navigate to the `mlflow/examples/pytorch/BertNewsClassification` directory and run the command\n\n```bash\nmlflow run .\n```\n\nThis will run `bert_classification.py` with the default set of parameters such as `--max_epochs=5`. You can see the default value in the `MLproject` file.\n\nIn order to run the file with custom parameters, run the command\n\n```bash\nmlflow run . -P max_epochs=X\n```\n\nwhere `X` is your desired value for `max_epochs`.\n\nIf you have the required modules for the file and would like to skip the creation of a conda environment, add the argument `--env-manager=local`.\n\n```bash\nmlflow run . --env-manager=local\n```\n\n### Viewing results in the MLflow UI\n\nOnce the code is finished executing, you can view the run\'s metrics, parameters, and details by running the command\n\n```bash\nmlflow ui\n```\n\nand navigating to [http://localhost:5000](http://localhost:5000).\n\nFor more details on MLflow tracking, see [the docs](https://www.mlflow.org/docs/latest/tracking.html#mlflow-tracking).\n\n### Passing custom training parameters\n\nThe parameters can be overridden via the command line:\n\n1. max_epochs - Number of epochs to train model. Training can be interrupted early via Ctrl+C\n2. devices - Number of GPUs.\n3. strategy - [strategy](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#trainer-class-api) (e.g. "ddp" for the Distributed Data Parallel backend) to use for training. By default, no strategy is used.\n4. accelerator - [accelerator](https://lightning.ai/docs/pytorch/stable/extensions/accelerator.html) (e.g. "gpu" - for running in GPU environment. Set to "cpu" by default)\n5. batch_size - Input batch size for training\n6. num_workers - Number of worker threads to load training data\n7. lr - Learning rate\n\nFor example:\n\n```bash\nmlflow run . -P max_epochs=5 -P devices=1 -P batch_size=32 -P num_workers=2 -P learning_rate=0.01 -P strategy="ddp" -P accelerator=gpu\n```\n\nOr to run the training script directly with custom parameters:\n\n```bash\npython bert_classification.py \\\n    --trainer.max_epochs 5 \\\n    --trainer.devices 1 \\\n    --trainer.strategy "ddp" \\\n    --trainer.accelerator "gpu" \\\n    --data.batch_size 64 \\\n    --data.num_workers 3 \\\n    --data.num_samples 2000 \\\n    --model.lr 0.001 \\\n    --data.dataset "20newsgroups"\n```\n\n## ', 'Input batch size for training\n6. num_workers - Number of worker threads to load training data\n7. lr - Learning rate\n\nFor example:\n\n```bash\nmlflow run . -P max_epochs=5 -P devices=1 -P batch_size=32 -P num_workers=2 -P learning_rate=0.01 -P strategy="ddp" -P accelerator=gpu\n```\n\nOr to run the training script directly with custom parameters:\n\n```bash\npython bert_classification.py \\\n    --trainer.max_epochs 5 \\\n    --trainer.devices 1 \\\n    --trainer.strategy "ddp" \\\n    --trainer.accelerator "gpu" \\\n    --data.batch_size 64 \\\n    --data.num_workers 3 \\\n    --data.num_samples 2000 \\\n    --model.lr 0.001 \\\n    --data.dataset "20newsgroups"\n```\n\n## Logging to a custom tracking server\n\nTo configure MLflow to log to a custom (non-default) tracking location, set the MLFLOW_TRACKING_URI environment variable, e.g. via export MLFLOW_TRACKING_URI=http://localhost:5000/. For more details, see [the docs](https://mlflow.org/docs/latest/tracking.html#where-runs-are-recorded).\n', "## Using Captum and MLflow to interpret Pytorch models\n\nIn this example, we will demonstrate the basic features of the [Captum](https://captum.ai/) interpretability,and logging those features using mlflow library through an example model trained on the Titanic survival data.\nWe will first train a deep neural network on the data using PyTorch and use Captum to understand which of the features were most important and how the network reached its prediction.\n\nyou can get more details about used attributions methods used in this example\n\n1. [Titanic_Basic_Interpret](https://captum.ai/tutorials/Titanic_Basic_Interpret)\n2. [integrated-gradients](https://captum.ai/docs/algorithms#primary-attribution)\n3. [layer-attributions](https://captum.ai/docs/algorithms#layer-attribution)\n\n### Running the code\n\nTo run the example via MLflow, navigate to the `mlflow/examples/pytorch/CaptumExample` directory and run the command\n\n```\nmlflow run .\n```\n\nThis will run `Titanic_Captum_Interpret.py` with default parameter values, e.g. `--max_epochs=100` and `--use_pretrained_model False`. You can see the full set of parameters in the `MLproject` file within this directory.\n\nIn order to run the file with custom parameters, run the command\n\n```\nmlflow run . -P max_epochs=X\n```\n\nwhere `X` is your desired value for `max_epochs`.\n\nIf you have the required modules for the file and would like to skip the creation of a conda environment, add the argument `--env-manager=local`.\n\n```\nmlflow run . --env-manager=local\n```\n\n### Viewing results in the MLflow UI\n\nOnce the code is finished executing, you can view the run's metrics, parameters, and details by running the command\n\n```\nmlflow ui\n```\n\nand navigating to [http://localhost:5000](http://localhost:5000).\n\nFor more details on MLflow tracking, see [the docs](https://www.mlflow.org/docs/latest/tracking.html#mlflow-tracking).\n\n### Passing custom training parameters\n\nThe parameters can be overridden via the command line:\n\n1. max_epochs - Number of epochs to train model. Training can be interrupted early via Ctrl+C\n2. lr - Learning rate\n3. use_pretrained_model - If want to use pretrained model\n\nFor example:\n\n```\nmlflow run . -P max_epochs=5 -P learning_rate=0.01 -P use_pretrained_model=True\n```\n\nOr to run the training script directly with custom parameters:\n\n```sh\npython Titanic_Captum_Interpret.py \\\n    --max_epochs 50 \\\n    --lr 0.1\n```\n\n## Logging to a custom tracking server\n\nTo configure MLflow to log to a custom (non-default) tracking location, set the MLFLOW_TRACKING_URI environment variable, e.g. via export MLFLOW_TRACKING_URI=http://localhost:5000/. For more details, see [the docs](https://mlflow.org/docs/latest/tracking.html#where-runs-are-recorded).\n", '## Iterative Pruning\n\nPruning is the process of compressing a neural network that involves removing weights from a trained model.\nPruning techniques include removing the neurons within a specific layer, or setting the weights of connections that are already near zero to zero. This script applies the latter technique.\nPruning a model reduces its size, at the cost of worsened model accuracy.\n\nFor more information check - [Pytorch Pruning Tutorial](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html)\n\nIn this example, we train a model to classify MNIST handwritten digit recognition dataset, and then apply iterative pruning to compress the model. The initial model ("base model") along with its parameters, metrics and summary are stored in mlflow.\nSubsequently, the base model is pruned iteratively by using the custom\ninputs provided from the cli. Ax is a platform for optimizing any kind of experiment, including machine learning experiments,\nA/B tests, and simulations. [Ax](https://ax.dev/docs/why-ax.html) can optimize discrete configurations using multi-armed bandit optimization,\nand continuous (e.g., integer or floating point)-valued configurations using Bayesian optimization.\n\nThe objective function of the experiment trials is "test_accuracy" based on which the model is evaluated at each trial and the best set of parameters are derived.\nAXClient is used to provide the initial pruning percentage as well as decides the number\nof trails to be run. The summary of the pruned model is captured in a separate file and stored as an artifact in MLflow.\n\n### Running the code to Iteratively Prune the Trained Model\n\nRun the command\n\n`python iterative_prune_mnist.py --max_epochs 10 --total_trials 3`\n\nOnce the code is finished executing, you can view the run\'s metrics, parameters, and details by running the command\n\n```\nmlflow ui\n```\n\nand navigating to [http://localhost:5000](http://localhost:5000).\n\nIn the MLflow UI, the Base Model is stored as the Parent Run and the runs for each iterations of the pruning is logged as nested child runs, as shown in the\nsnippets below:\n\n![prune_ankan](https://user-images.githubusercontent.com/51693147/100785435-a66d6e80-3436-11eb-967a-c96b23625d1c.JPG)\n\nWe can compare the child runs in the UI, as given below:\n\n![prune_capture](https://user-images.githubusercontent.com/51693147/100785071-2515dc00-3436-11eb-8e3a-de2d569287e6.JPG)\n\nFor more information on MLflow tracking, click [here](https://www.mlflow.org/docs/latest/tracking.html#mlflow-tracking) to view documentation.\n', '## MNIST example with MLflow\n\nIn this example, we train a Pytorch Lightning model to predict handwritten digits, leveraging early stopping.\nThe code is almost entirely dedicated to model training, with the addition of a single `mlflow.pytorch.autolog()` call to enable automatic logging of params, metrics, and models,\nincluding the best model from early stopping.\n\n### Running the code\n\nTo run the example via MLflow, navigate to the `mlflow/examples/pytorch/MNIST` directory and run the command\n\n```\nmlflow run .\n```\n\nThis will run `mnist_autolog_example.py` with the default set of parameters such as `max_epochs=5`. You can see the default value in the `MLproject` file.\n\nIn order to run the file with custom parameters, run the command\n\n```\nmlflow run . -P max_epochs=X\n```\n\nwhere `X` is your desired value for `max_epochs`.\n\nIf you have the required modules for the file and would like to skip the creation of a conda environment, add the argument `--env-manager=local`.\n\n```\nmlflow run . --env-manager=local\n```\n\n### Viewing results in the MLflow UI\n\nOnce the code is finished executing, you can view the run\'s metrics, parameters, and details by running the command\n\n```\nmlflow ui\n```\n\nand navigating to [http://localhost:5000](http://localhost:5000).\n\nFor more details on MLflow tracking, see [the docs](https://www.mlflow.org/docs/latest/tracking.html#mlflow-tracking).\n\n### Passing custom training parameters\n\nThe parameters can be overridden via the command line:\n\n1. max_epochs - Number of epochs to train model. Training can be interrupted early via Ctrl+C\n2. devices - Number of GPUs.\n3. strategy - [strategy](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#trainer-class-api) (e.g. "ddp" for the Distributed Data Parallel backend) to use for training. By default, no strategy is used.\n4. accelerator - [accelerator](https://lightning.ai/docs/pytorch/stable/extensions/accelerator.html) (e.g. "gpu" - for running in GPU environment. Set to "cpu" by default)\n5. batch_size - Input batch size for training\n6. num_workers - Number of worker threads to load training data\n7. learning_rate - Learning rate\n\nFor example:\n\n```\nmlflow run . -P max_epochs=5 -P devices=1 -P batch_size=32 -P num_workers=2 -P learning_rate=0.01 -P strategy="ddp"\n```\n\nOr to run the training script directly with custom parameters:\n\n```sh\npython mnist_autolog_example.py \\\n    --trainer.max_epochs 5 \\\n    --trainer.devices 1 \\\n    --trainer.strategy "ddp" \\\n    --trainer.accelerator "gpu" \\\n    --data.batch_size 64 \\\n    --data.num_workers 3 \\\n    --model.learning_rate 0.001\n```\n\n## Logging to a custom tracking server\n\nTo configure MLflow to log to a custom (non-default) tracking location, set ', 'for training\n6. num_workers - Number of worker threads to load training data\n7. learning_rate - Learning rate\n\nFor example:\n\n```\nmlflow run . -P max_epochs=5 -P devices=1 -P batch_size=32 -P num_workers=2 -P learning_rate=0.01 -P strategy="ddp"\n```\n\nOr to run the training script directly with custom parameters:\n\n```sh\npython mnist_autolog_example.py \\\n    --trainer.max_epochs 5 \\\n    --trainer.devices 1 \\\n    --trainer.strategy "ddp" \\\n    --trainer.accelerator "gpu" \\\n    --data.batch_size 64 \\\n    --data.num_workers 3 \\\n    --model.learning_rate 0.001\n```\n\n## Logging to a custom tracking server\n\nTo configure MLflow to log to a custom (non-default) tracking location, set the MLFLOW_TRACKING_URI environment variable, e.g. via export MLFLOW_TRACKING_URI=http://localhost:5000/. For more details, see [the docs](https://mlflow.org/docs/latest/tracking.html#where-runs-are-recorded).\n', "## Iris classification example with MLflow\n\nThis example demonstrates training a classification model on the Iris dataset, scripting the model with TorchScript, logging the\nscripted model to MLflow using\n[`mlflow.pytorch.log_model`](https://mlflow.org/docs/latest/python_api/mlflow.pytorch.html#mlflow.pytorch.log_model), and\nloading it back for inference using\n[`mlflow.pytorch.load_model`](https://mlflow.org/docs/latest/python_api/mlflow.pytorch.html#mlflow.pytorch.load_model)\n\n### Running the code\n\nTo run the example via MLflow, navigate to the `mlflow/examples/pytorch/torchscript/IrisClassification` directory and run the command\n\n```\nmlflow run .\n```\n\nThis will run `iris_classification.py` with the default set of parameters such as `--max_epochs=5`. You can see the default value in the `MLproject` file.\n\nIn order to run the file with custom parameters, run the command\n\n```\nmlflow run . -P epochs=X\n```\n\nwhere `X` is your desired value for `epochs`.\n\nIf you have the required modules for the file and would like to skip the creation of a conda environment, add the argument `--env-manager=local`.\n\n```\nmlflow run . --env-manager=local\n```\n\nOnce the code is finished executing, you can view the run's metrics, parameters, and details by running the command\n\n```\nmlflow ui\n```\n\nand navigating to [http://localhost:5000](http://localhost:5000).\n\n## Running against a custom tracking server\n\nTo configure MLflow to log to a custom (non-default) tracking location, set the `MLFLOW_TRACKING_URI` environment variable, e.g. via `export MLFLOW_TRACKING_URI=http://localhost:5000/`. For more details, see [the docs](https://mlflow.org/docs/latest/tracking.html#where-runs-are-recorded)\n", "## MNIST example with MLflow\n\nThis example demonstrates training of MNIST handwritten recognition model and logging it as torch scripted model.\n`mlflow.pytorch.log_model()` is used to log the scripted model to MLflow and `mlflow.pytorch.load_model()` to load it from MLflow\n\n### Code related to MLflow:\n\nThis will log the TorchScripted model into MLflow and load the logged model.\n\n## Setting Tracking URI\n\nMLflow tracking URI can be set using the environment variable `MLFLOW_TRACKING_URI`\n\nExample: `export MLFLOW_TRACKING_URI=http://localhost:5000/`\n\nFor more details - https://mlflow.org/docs/latest/tracking.html#where-runs-are-recorded\n\n### Running the code\n\nTo run the example via MLflow, navigate to the `mlflow/examples/pytorch/torchscript/MNIST` directory and run the command\n\n```\nmlflow run .\n```\n\nThis will run `mnist_torchscript.py` with the default set of parameters such as `--max_epochs=5`. You can see the default value in the `MLproject` file.\n\nIn order to run the file with custom parameters, run the command\n\n```\nmlflow run . -P epochs=X\n```\n\nwhere `X` is your desired value for `epochs`.\n\nIf you have the required modules for the file and would like to skip the creation of a conda environment, add the argument `--env-manager=local`.\n\n```\nmlflow run . --env-manager=local\n```\n\nOnce the code is finished executing, you can view the run's metrics, parameters, and details by running the command\n\n```\nmlflow ui\n```\n\nand navigating to [http://localhost:5000](http://localhost:5000).\n\nFor more information on MLflow tracking, click [here](https://www.mlflow.org/docs/latest/tracking.html#mlflow-tracking) to view documentation.\n", '### Train and Publish Locally With RAPIDS and MLflow\n\n**[RAPIDS](https://rapids.ai/)** is a suite of open source libraries for GPU-accelerated analytics.\n\n**[RAPIDS cuML](https://github.com/rapidsai/cuml)** matches the scikit-learn API, so it can build on MLflow\'s existing support for scikit-learn-like models to support\npersistence and deployment."\n\nThe example workflows below train RAPIDs regression models to predict airline flight delays, using\nMLflow to log models and deploy them as local REST API endpoints for real-time inference. You can run them:\n\n- On a GPU-enabled instance for free in Colab. If following this approach, we recommend using the "Jupyter notebook workflow" below\n  and following the setup steps in [this Colab notebook](https://colab.research.google.com/drive/1rY7Ln6rEE1pOlfSHCYOVaqt8OvDO35J0#forceEdit=true&offline=true&sandboxMode=true) to configure your\n  environment.\n\n- On your own machine with an NVIDIA GPU and CUDA installed. See the [RAPIDS getting-started guide](https://rapids.ai/start.html)\n  for more details on necessary prerequisites for running the examples on your own machine.\n\n#### Jupyter Notebook Workflow\n\n[Jupyter Notebook](notebooks/rapids_mlflow.ipynb)\n\n#### CLI Based Workflow\n\n1. Create data\n   1. `cd examples/rapids/mlflow_project`\n      ```shell script\n      # Create iris.csv\n      python -c "from sklearn.datasets import load_iris; d = load_iris(as_frame=True); d.frame.to_csv(\'iris.csv\', index=False)"\n      ```\n1. Set MLflow tracking uri\n   1. ```shell script\n       export MLFLOW_TRACKING_URI=sqlite:////tmp/mlflow-db.sqlite\n      ```\n1. Train the model using a single run.\n   1. ```shell script\n      # Launch the job\n      mlflow run . -e simple\\\n               --experiment-name RAPIDS-CLI \\\n               -P max_depth=10 -P max_features=0.75 -P n_estimators=500 \\\n               -P conda-env=$PWD/envs/conda.yaml \\\n               -P fpath=iris.csv\n      ```\n1. Train the model with Hyperopt\n\n   1. ```shell script\n      # Launch the job\n      mlflow run . -e hyperopt \\\n   ', '     --experiment-name RAPIDS-CLI \\\n               -P max_depth=10 -P max_features=0.75 -P n_estimators=500 \\\n               -P conda-env=$PWD/envs/conda.yaml \\\n               -P fpath=iris.csv\n      ```\n1. Train the model with Hyperopt\n\n   1. ```shell script\n      # Launch the job\n      mlflow run . -e hyperopt \\\n               --experiment-name RAPIDS-CLI \\\n               -P conda-env=$PWD/envs/conda.yaml \\\n               -P fpath=iris.csv\n      ```\n   1. In the output, note: "Created version \'[VERSION]\' of model \'rapids_mlflow\'"\n\n1. Deploy your model\n\n   1. Deploy your model\n      1. `$ mlflow models serve --env-manager=local -m models:/rapids_mlflow_cli/[VERSION] -p 55755`\n\n1. Query the deployed model with test data `src/sample_server_query.sh` example script.\n   1. `bash src/sample_server_query.sh`\n', "# MLflow-Ray-Serve deployment plugin\n\nIn this example, we will first train a model to classify the Iris dataset using `sklearn`. Next, we will deploy our model on Ray Serve and then scale it up, all using the MLflow Ray Serve plugin.\n\nThe plugin supports both a command line interface and a Python API. Below we will use the command line interface. For the full API documentation, see https://www.mlflow.org/docs/latest/cli.html#mlflow-deployments and https://www.mlflow.org/docs/latest/python_api/mlflow.deployments.html.\n\n## Plugin Installation\n\nPlease follow the installation instructions for the Ray Serve deployment plugin: https://github.com/ray-project/mlflow-ray-serve\n\n## Instructions\n\nFirst, navigate to the directory for this example, `mlflow/examples/ray_serve/`.\n\nSecond, run `python train_model.py`. This trains and saves our classifier to the MLflow Model Registry and sets up automatic logging to MLflow. It also prints the mean squared error and the target names, which are species of iris:\n\n```\nMSE: 1.04\nTarget names:  ['setosa' 'versicolor' 'virginica']\n```\n\nNext, set the MLflow Tracking URI environment variable to the location where the Model Registry resides:\n\n`export MLFLOW_TRACKING_URI=sqlite:///mlruns.db`\n\nNow start a Ray cluster with the following command:\n\n`ray start --head`\n\nNext, start a long-running Ray Serve instance on your Ray cluster:\n\n`serve start`\n\nRay Serve is now running and ready to deploy MLflow models. The MLflow Ray Serve plugin features both a Python API as well as a command-line interface. For this example, we'll use the command line interface.\n\nFinally, we can deploy our model by creating an instance using the following command:\n\n`mlflow deployments create -t ray-serve -m models:/RayMLflowIntegration/1 --name iris:v1`\n\nThe `-t` parameter here is the deployment target, which in our case is Ray Serve. The `-m` parameter is the Model URI, which consists of the registered model name and version in the Model Registry.\n\nWe can now run a prediction on our deployed model as follows. The file `input.json` contains a sample input containing the sepal length, sepal width, petal length, petal width of a sample flower. Now we can get the prediction using the following command:\n\n`mlflow deployments predict -t ray-serve --name iris:v1 --input-path input.json`\n\nThis will output `[0]`, `[1]`, or `[2]`, corresponding to the species listed above in the target names.\n\nWe can scale our deployed model up to use several replicas, improving throughput:\n\n`mlflow deployments update -t ray-serve --name iris:v1 --config num_replicas=2`\n\nHere we only used 2 ", 'the registered model name and version in the Model Registry.\n\nWe can now run a prediction on our deployed model as follows. The file `input.json` contains a sample input containing the sepal length, sepal width, petal length, petal width of a sample flower. Now we can get the prediction using the following command:\n\n`mlflow deployments predict -t ray-serve --name iris:v1 --input-path input.json`\n\nThis will output `[0]`, `[1]`, or `[2]`, corresponding to the species listed above in the target names.\n\nWe can scale our deployed model up to use several replicas, improving throughput:\n\n`mlflow deployments update -t ray-serve --name iris:v1 --config num_replicas=2`\n\nHere we only used 2 replicas, but you can use as many as you like, depending on how many CPU cores are available in your Ray cluster.\n\nThe deployed model instance can be deleted as follows:\n\n`mlflow deployments delete -t ray-serve --name iris:v1`\n\nTo tear down the Ray cluster, run the following command:\n\n`ray stop`\n', '### MLflow restore model dependencies examples\n\nThe example "restore_model_dependencies_example.ipynb" in this directory illustrates\nhow you can use the `mlflow.pyfunc.get_model_dependencies` API to get the dependencies from a model URI\nand install them, restoring the exact python environment that was used to build the model.\n\n#### Prerequisites\n\n```\npip install scikit-learn\n```\n\n#### How to run the example\n\nUse jupyter to load the notebook "restore_model_dependencies_example.ipynb" and run the notebook.\n', '# SHAP Examples\n\nExamples demonstrating use of the `mlflow.shap` APIs for model explainability.\n\n| File                                                         | Task                      | Description                                                    |\n| :----------------------------------------------------------- | :------------------------ | :------------------------------------------------------------- |\n| [regression.py](regression.py)                               | Regression                | Log explanations for a LinearRegression model                  |\n| [binary_classification.py](binary_classification.py)         | Binary classification     | Log explanations for a binary RandomForestClassifier model     |\n| [multiclass_classification.py](multiclass_classification.py) | Multiclass classification | Log explanations for a multiclass RandomForestClassifier model |\n\n## Prerequisites\n\nRun the following command to install required packages:\n\n```\npip install mlflow scikit-learn shap matplotlib\n```\n\n## How to run the scripts\n\n```bash\npython <script_name>\n```\n\n## How to view the logged explanations:\n\n- Run `mlflow ui` to launch the MLflow UI.\n- Open http://127.0.0.1:5000 on your browser.\n- Click the latest run in the runs table.\n- Scroll down to the artifact viewer.\n- Open a folder named `model_explanations_shap`.\n', '# Examples for scikit-learn Autologging\n\n| File                                           | Description                                         |\n| :--------------------------------------------- | :-------------------------------------------------- |\n| [linear_regression.py](./linear_regression.py) | Train a [LinearRegression][lr] model                |\n| [pipeline.py](./pipeline.py)                   | Train a [Pipeline][pipe] model                      |\n| [grid_search_cv.py](./grid_search_cv.py)       | Perform a parameter search using [GridSearchCV][gs] |\n\n[lr]: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n[pipe]: https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n[gs]: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n', '# Scikit-learn ElasticNet Diabetes Example\n\nThis example trains an ElasticNet regression model for predicting diabetes progression. The example uses [matplotlib](https://matplotlib.org/), which requires different Python dependencies for Linux and OSX. The [linux](linux) and [osx](osx) subdirectories include appropriate MLflow projects for each respective platform.\n', '# Sktime Example\n\nThis example trains a `Sktime` NaiveForecaster model using the Longley dataset for\nforecasting with exogenous variables. It shows a custom model type implementation\nthat logs the training hyper-parameters, evaluation metrics and the trained model\nas an artifact.\n\n## Running the code\n\nRun the `train.py` module to create a new MLflow experiment and to\ncompute interval forecasts loading the trained model in native `sktime`\nflavor and `pyfunc` flavor:\n\n```\npython train.py\n```\n\nTo view the newly created experiment and logged artifacts open the MLflow UI:\n\n```\nmlflow ui\n```\n\n## Model serving\n\nThis section illustrates an example of serving the `pyfunc` flavor to a local REST\nAPI endpoint and subsequently requesting a prediction from the served model. To serve the model run the command below where you substitute the run id printed during execution of the `train.py` module:\n\n```\nmlflow models serve -m runs:/<run_id>/model --env-manager local --host 127.0.0.1\n\n```\n\nOpen a new terminal and run the `score_model.py` module to request a prediction from the served model (for more details read the [MLflow deployment API reference](https://mlflow.org/docs/latest/models.html#deploy-mlflow-models)):\n\n```\npython score_model.py\n```\n\n## Running the code as a project\n\nYou can also run the code as a project as follows:\n\n```\nmlflow run .\n\n```\n\n## Running unit tests\n\nThe `test_sktime_model_export.py` module includes a number of tests that can be\nexecuted as follows:\n\n```\npytest test_sktime_model_export.py\n\n```\n\nWhile these tests will depend on the specifics of each individual flavor and in particular the design of the model wrapper interface (e.g. `_SktimeModelWrapper`), the above module can provide some orientation\nfor the type of tests that can be useful when creating a new custom model flavor.\n', '### MLflow Spark UDF Examples\n\nThe examples in this directory illustrate how you can use the `mlflow.pyfunc.spark_udf` API for batch inference,\nincluding environment reproducibility capabilities with argument `env_manager="conda"`,\nwhich creates a spark UDF for model inference that executes in an environment containing the exact dependency\nversions used during training.\n\n- Example `spark_udf.py` runs a sklearn model inference via spark UDF\n  using a python environment containing the precise versions of dependencies used during model training.\n\n#### Prerequisites\n\n```\npip install scikit-learn\n```\n\n#### How to run the examples\n\nSimple example:\n\n```\npython spark_udf.py\n```\n\nSpark UDF example with input data of datetime type:\n\n```\npython spark_udf_datetime.py\n```\n\nSpark UDF example with input data of struct and array type:\n\n```\npython structs_and_arrays.py\n```\n\nSpark UDF example using prebuilt model environment:\n\n```\npython spark_udf_with_prebuilt_env.py\n```\n', "# Statsmodels Example\n\nThis example trains a Statsmodels OLS (Ordinary Least Squares) model with synthetically generated data\nand logs hyperparameters, metric (MSE), and trained model.\n\n## Running the code\n\n```\npython train.py --inverse-method qr\n```\n\nThe inverse method is the method used to compute the inverse matrix, and can be either qr or pinv (default).\n'pinv' uses the Moore-Penrose pseudoinverse to solve the least squares problem. 'qr' uses the QR factorization.\nYou can try experimenting with both, as well as omitting the --inverse-method argument.\n\nThen you can open the MLflow UI to track the experiments and compare your runs via:\n\n```\nmlflow ui\n```\n\n## Running the code as a project\n\n```\nmlflow run . -P inverse_method=qr\n\n```\n", '## MLflow automatic Logging with SynapseML\n\n[MLflow automatic logging](https://www.mlflow.org/docs/latest/tracking.html#automatic-logging) allows you to log metrics, parameters, and models without the need for explicit log statements.\nSynapseML supports autologging for every model in the library.\n\nInstall SynapseML library following this [guidance](https://microsoft.github.io/SynapseML/docs/getting_started/installation/)\n\nDefault mlflow [log_model_allowlist file](https://github.com/mlflow/mlflow/blob/master/mlflow/pyspark/ml/log_model_allowlist.txt) already includes some SynapseML models. To enable more models, you could use `mlflow.pyspark.ml.autolog(log_model_allowlist=YOUR_SET_OF_MODELS)` function, or follow the below guidance by specifying a link to the file and update spark configuration.\n\nTo enable autologging with your custom log_model_allowlist file:\n\n1. Put your customized log_model_allowlist file at a place that your code has access to. ([SynapseML official log_model_allowlist file](https://mmlspark.blob.core.windows.net/publicwasb/log_model_allowlist.txt))\n   For example:\n\n- In Synapse `wasb://<containername>@<accountname>.blob.core.windows.net/PATH_TO_YOUR/log_model_allowlist.txt`\n- In Databricks `/dbfs/FileStore/PATH_TO_YOUR/log_model_allowlist.txt`.\n\n2. Set spark configuration `spark.mlflow.pysparkml.autolog.logModelAllowlistFile` to the path of your `log_model_allowlist.txt` file.\n3. Call `mlflow.pyspark.ml.autolog()` before your training code to enable autologging for all supported models.\n\nNote:\n\nIf you want to support autologging of PySpark models not present in the log_model_allowlist file, you can add such models to the file.\n\n## Configuration process in Databricks as an example\n\n1. Install latest MLflow via `%pip install mlflow -u`\n2. Upload your customized `log_model_allowlist.txt` file to dbfs by clicking File/Upload Data button on Databricks UI.\n3. Set Cluster Spark configuration following [this documentation](https://docs.microsoft.com/en-us/azure/databricks/clusters/configure#spark-configuration)\n\n```\nspark.mlflow.pysparkml.autolog.logModelAllowlistFile /dbfs/FileStore/PATH_TO_YOUR/log_model_allowlist.txt\n```\n\n4. Run the following line before your training code executes.\n\n```python\nimport mlflow\n\nmlflow.pyspark.ml.autolog()\n```\n\nYou can customize how autologging works by supplying appropriate [parameters](https://www.mlflow.org/docs/latest/python_api/mlflow.pyspark.ml.html#mlflow.pyspark.ml.autolog).\n\n5. To find your experiment\'s results via the `Experiments` tab of the MLflow UI.\n   <img src="https://mmlspark.blob.core.windows.net/graphics/adb_experiments.png" width="1200" />\n\n## Example for ConditionalKNNModel\n\n```python\nfrom pyspark.ml.linalg import Vectors\nfrom synapse.ml.nn import ConditionalKNN\n\ndf = spark.createDataFrame(\n    [\n        (Vectors.dense(2.0, 2.0, 2.0), "foo", 1),\n        (Vectors.dense(2.0, 2.0, 4.0), "foo", 3),\n        (Vectors.dense(2.0, 2.0, 6.0), "foo", 4),\n        (Vectors.dense(2.0, 2.0, 8.0), "foo", 3),\n        (Vectors.dense(2.0, 2.0, 10.0), "foo", 1),\n        (Vectors.dense(2.0, 2.0, 12.0), "foo", 2),\n        (Vectors.dense(2.0, 2.0, 14.0), "foo", 0),\n        (Vectors.dense(2.0, 2.0, 16.0), "foo", 1),\n        (Vectors.dense(2.0, 2.0, 18.0), "foo", 3),\n  ', '2.0), "foo", 1),\n        (Vectors.dense(2.0, 2.0, 4.0), "foo", 3),\n        (Vectors.dense(2.0, 2.0, 6.0), "foo", 4),\n        (Vectors.dense(2.0, 2.0, 8.0), "foo", 3),\n        (Vectors.dense(2.0, 2.0, 10.0), "foo", 1),\n        (Vectors.dense(2.0, 2.0, 12.0), "foo", 2),\n        (Vectors.dense(2.0, 2.0, 14.0), "foo", 0),\n        (Vectors.dense(2.0, 2.0, 16.0), "foo", 1),\n        (Vectors.dense(2.0, 2.0, 18.0), "foo", 3),\n        (Vectors.dense(2.0, 2.0, 20.0), "foo", 0),\n        (Vectors.dense(2.0, 4.0, 2.0), "foo", 2),\n        (Vectors.dense(2.0, 4.0, 4.0), "foo", 4),\n        (Vectors.dense(2.0, 4.0, 6.0), "foo", 2),\n        (Vectors.dense(2.0, 4.0, 8.0), "foo", 2),\n        (Vectors.dense(2.0, 4.0, 10.0), "foo", 4),\n        (Vectors.dense(2.0, 4.0, 12.0), "foo", 3),\n        (Vectors.dense(2.0, 4.0, 14.0), "foo", 2),\n        (Vectors.dense(2.0, 4.0, 16.0), "foo", 1),\n        (Vectors.dense(2.0, 4.0, 18.0), "foo", 4),\n        (Vectors.dense(2.0, 4.0, 20.0), "foo", 4),\n    ],\n    ["features", "values", "labels"],\n)\n\ncnn = ConditionalKNN().setOutputCol("prediction")\ncnnm = cnn.fit(df)\n\ntest_df = spark.createDataFrame(\n    [\n        (Vectors.dense(2.0, 2.0, 2.0), "foo", 1, [0, 1]),\n        (Vectors.dense(2.0, 2.0, 4.0), "foo", 4, [0, 1]),\n        (Vectors.dense(2.0, 2.0, 6.0), "foo", 2, [0, 1]),\n        (Vectors.dense(2.0, 2.0, 8.0), "foo", 4, [0, 1]),\n        (Vectors.dense(2.0, 2.0, 10.0), "foo", 4, [0, 1]),\n    ],\n    ["features", "values", "labels", "conditioner"],\n)\n\ndisplay(cnnm.transform(test_df))\n```\n\nThis code should log one run with a ConditionalKNNModel artifact and its parameters.\n<img src="https://mmlspark.blob.core.windows.net/graphics/autologgingRunSample.png" width="1200" />\n', '# Examples for XGBoost Autologging\n\nTwo examples are provided to demonstrate XGBoost autologging functionalities. The `xgboost_native` folder contains an example that logs a Booster model trained by `xgboost.train()`. The `xgboost_sklearn` includes another example showing how autologging works for XGBoost scikit-learn models. In fact, there is no difference in turning on autologging for all XGBoost models. That is, `mlflow.xgboost.autolog()` works for all XGBoost models.\n', '# XGBoost Example\n\nThis example trains an XGBoost classifier with the iris dataset and logs hyperparameters, metrics, and trained model.\n\n## Running the code\n\n```\npython train.py --learning-rate 0.2 --colsample-bytree 0.8 --subsample 0.9\n```\n\nYou can try experimenting with different parameter values like:\n\n```\npython train.py --learning-rate 0.4 --colsample-bytree 0.7 --subsample 0.8\n```\n\nThen you can open the MLflow UI to track the experiments and compare your runs via:\n\n```\nmlflow ui\n```\n\n## Running the code as a project\n\n```\nmlflow run . -P learning_rate=0.2 -P colsample_bytree=0.8 -P subsample=0.9\n```\n', '# XGBoost Scikit-learn Model Example\n\nThis example trains an [`XGBoost.XGBRegressor`](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor) with the diabetes dataset and logs hyperparameters, metrics, and trained model.\n\nLike the other XGBoost example, we enable autologging for XGBoost scikit-learn models via `mlflow.xgboost.autolog()`. Saving / loading models also supports XGBoost scikit-learn models.\n\nYou can run this example using the following command:\n\n```\npython train.py\n```\n', '# MLflow Skinny\n\n`mlflow-skinny` a lightweight version of MLflow that is designed to be used in environments where you want to minimize the size of the package.\n\n## Core Files\n\n| File               | Description                                                                     |\n| ------------------ | ------------------------------------------------------------------------------- |\n| `mlflow`           | A symlink that points to the `mlflow` directory in the root of the repository.  |\n| `pyproject.toml`   | The package metadata. Autogenerate by [`dev/pyproject.py`](../dev/pyproject.py) |\n| `README_SKINNY.md` | The package description. Autogenerate by [`dev/skinny.py`](../dev/pyproject.py) |\n\n## Installation\n\n```sh\n# If you have a local clone of the repository\npip install ./libs/skinny\n\n# If you want to install the latest version from GitHub\npip install git+https://github.com/mlflow/mlflow.git#subdirectory=libs/skinny\n```\n', '<!--  Autogenerated by dev/pyproject.py. Do not edit manually.  -->\n\nðŸ“£ This is the `mlflow-skinny` package, a lightweight MLflow package without SQL storage, server, UI, or data science dependencies.\nAdditional dependencies can be installed to leverage the full feature set of MLflow. For example:\n\n- To use the `mlflow.sklearn` component of MLflow Models, install `scikit-learn`, `numpy` and `pandas`.\n- To use SQL-based metadata storage, install `sqlalchemy`, `alembic`, and `sqlparse`.\n- To use serving-based features, install `flask` and `pandas`.\n\n---\n\n<br>\n<br>\n\n<h1 align="center" style="border-bottom: none">\n    <a href="https://mlflow.org/">\n        <img alt="MLflow logo" src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/logo.svg" width="200" />\n    </a>\n</h1>\n<h2 align="center" style="border-bottom: none">Open-Source Platform for Productionizing AI</h2>\n\nMLflow is an open-source developer platform to build AI/LLM applications and models with confidence. Enhance your AI applications with end-to-end **experiment tracking**, **observability**, and **evaluations**, all in one integrated platform.\n\n<div align="center">\n\n[![Python SDK](https://img.shields.io/pypi/v/mlflow)](https://pypi.org/project/mlflow/)\n[![PyPI Downloads](https://img.shields.io/pypi/dm/mlflow)](https://pepy.tech/projects/mlflow)\n[![License](https://img.shields.io/github/license/mlflow/mlflow)](https://github.com/mlflow/mlflow/blob/main/LICENSE)\n<a href="https://twitter.com/intent/follow?screen_name=mlflow" target="_blank">\n<img src="https://img.shields.io/twitter/follow/mlflow?logo=X&color=%20%23f5f5f5"\n      alt="follow on X(Twitter)"></a>\n<a href="https://www.linkedin.com/company/mlflow-org/" target="_blank">\n<img src="https://custom-icon-badges.demolab.com/badge/LinkedIn-0A66C2?logo=linkedin-white&logoColor=fff"\n      alt="follow on LinkedIn"></a>\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/mlflow/mlflow)\n\n</div>\n\n<div align="center">\n   <div>\n      <a href="https://mlflow.org/"><strong>Website</strong></a> Â·\n      <a href="https://mlflow.org/docs/latest/index.html"><strong>Docs</strong></a> Â·\n      <a href="https://github.com/mlflow/mlflow/issues/new/choose"><strong>Feature Request</strong></a> Â·\n      <a href="https://mlflow.org/blog"><strong>News</strong></a> Â·\n      <a href="https://www.youtube.com/@mlflowoss"><strong>YouTube</strong></a> Â·\n      <a href="https://lu.ma/mlflow?k=c"><strong>Events</strong></a>\n   </div>\n</div>\n\n<br>\n\n## ðŸš€ Installation\n\nTo install the MLflow Python package, run the following command:\n\n```\npip install mlflow\n```\n\n## ðŸ“¦ Core Components\n\nMLflow is **the only platform that provides a unified solution for all your AI/ML needs**, including LLMs, Agents, Deep Learning, and traditional machine learning.\n\n### ðŸ’¡ For LLM / GenAI Developers\n\n<table>\n  <tr>\n    <td>\n    <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-tracing.png" alt="Tracing" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/llms/tracing/index.html"><strong>ðŸ” Tracing / Observability</strong></a>\n        <br><br>\n        <div>Trace the internal states of your LLM/agentic applications for debugging quality issues and monitoring performance with ease.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/tracing/quickstart/python-openai/">Getting Started â†’</a>\n        <br><br>\n ', 'Learning, and traditional machine learning.\n\n### ðŸ’¡ For LLM / GenAI Developers\n\n<table>\n  <tr>\n    <td>\n    <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-tracing.png" alt="Tracing" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/llms/tracing/index.html"><strong>ðŸ” Tracing / Observability</strong></a>\n        <br><br>\n        <div>Trace the internal states of your LLM/agentic applications for debugging quality issues and monitoring performance with ease.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/tracing/quickstart/python-openai/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n    <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-llm-eval.png" alt="LLM Evaluation" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/genai/eval-monitor/"><strong>ðŸ“Š LLM Evaluation</strong></a>\n        <br><br>\n        <div>A suite of automated model evaluation tools, seamlessly integrated with experiment tracking to compare across multiple versions.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/eval-monitor/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-prompt.png" alt="Prompt Management">\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/prompt-registry/"><strong>ðŸ¤– Prompt Management</strong></a>\n        <br><br>\n        <div>Version, track, and reuse prompts across your organization, helping maintain consistency and improve collaboration in prompt development.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-registry/create-and-edit-prompts/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-logged-model.png" alt="MLflow Hero">\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/version-tracking/"><strong>ðŸ“¦ App Version Tracking</strong></a>\n        <br><br>\n  ', ' <br><br>\n        <div>Version, track, and reuse prompts across your organization, helping maintain consistency and improve collaboration in prompt development.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-registry/create-and-edit-prompts/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-logged-model.png" alt="MLflow Hero">\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/version-tracking/"><strong>ðŸ“¦ App Version Tracking</strong></a>\n        <br><br>\n        <div>MLflow keeps track of many moving parts in your AI applications, such as models, prompts, tools, and code, with end-to-end lineage.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/version-tracking/quickstart/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n</table>\n\n### ðŸŽ“ For Data Scientists\n\n<table>\n  <tr>\n    <td colspan="2" align="center" >\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-experiment.png" alt="Tracking" width=50%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/ml/tracking/"><strong>ðŸ“ Experiment Tracking</strong></a>\n        <br><br>\n        <div>Track your models, parameters, metrics, and evaluation results in ML experiments and compare them using an interactive UI.</div><br>\n        <a href="https://mlflow.org/docs/latest/ml/tracking/quickstart/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-model-registry.png" alt="Model Registry" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/ml/model-registry/"><strong>ðŸ’¾ Model Registry</strong></a>\n        <br><br>\n        <div> A centralized model store designed to collaboratively manage the full lifecycle and deployment of machine learning models.</div><br>\n        <a href="https://mlflow.org/docs/latest/ml/model-registry/tutorial/">Getting Started â†’</a>\n   ', '   <br><br>\n    </div>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-model-registry.png" alt="Model Registry" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/ml/model-registry/"><strong>ðŸ’¾ Model Registry</strong></a>\n        <br><br>\n        <div> A centralized model store designed to collaboratively manage the full lifecycle and deployment of machine learning models.</div><br>\n        <a href="https://mlflow.org/docs/latest/ml/model-registry/tutorial/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-deployment.png" alt="Deployment" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/ml/deployment/"><strong>ðŸš€ Deployment</strong></a>\n        <br><br>\n        <div> Tools for seamless model deployment to batch and real-time scoring on platforms like Docker, Kubernetes, Azure ML, and AWS SageMaker.</div><br>\n        <a href="https://mlflow.org/docs/latest/ml/deployment/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n</table>\n\n## ðŸŒ Hosting MLflow Anywhere\n\n<div align="center" >\n  <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-providers.png" alt="Providers" width=100%>\n</div>\n\nYou can run MLflow in many different environments, including local machines, on-premise servers, and cloud infrastructure.\n\nTrusted by thousands of organizations, MLflow is now offered as a managed service by most major cloud providers:\n\n- [Amazon SageMaker](https://aws.amazon.com/sagemaker-ai/experiments/)\n- [Azure ML](https://learn.microsoft.com/en-us/azure/machine-learning/concept-mlflow?view=azureml-api-2)\n- [Databricks](https://www.databricks.com/product/managed-mlflow)\n- [Nebius](https://nebius.com/services/managed-mlflow)\n\nFor hosting MLflow on your own infrastructure, please refer to [this guidance](https://mlflow.org/docs/latest/ml/tracking/#tracking-setup).\n\n## ðŸ—£ï¸ Supported Programming Languages\n\n- [Python](https://pypi.org/project/mlflow/)\n- [TypeScript / JavaScript](https://www.npmjs.com/package/mlflow-tracing)\n- [Java](https://mvnrepository.com/artifact/org.mlflow/mlflow-client)\n- [R](https://cran.r-project.org/web/packages/mlflow/readme/README.html)\n\n## ðŸ”— Integrations\n\nMLflow is natively integrated with many popular machine learning frameworks and GenAI libraries.\n\n![Integrations](https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-integrations.png)\n\n## Usage Examples\n\n### Experiment Tracking ([Doc](https://mlflow.org/docs/latest/ml/tracking/))\n\nThe following examples trains a simple regression model with scikit-learn, while enabling MLflow\'s [autologging](https://mlflow.org/docs/latest/tracking/autolog.html) feature for experiment tracking.\n\n```python\nimport mlflow\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Enable MLflow\'s automatic experiment tracking for scikit-learn\nmlflow.sklearn.autolog()\n\n# Load the training dataset\ndb = load_diabetes()\nX_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n\nrf ', 'a managed service by most major cloud providers:\n\n- [Amazon SageMaker](https://aws.amazon.com/sagemaker-ai/experiments/)\n- [Azure ML](https://learn.microsoft.com/en-us/azure/machine-learning/concept-mlflow?view=azureml-api-2)\n- [Databricks](https://www.databricks.com/product/managed-mlflow)\n- [Nebius](https://nebius.com/services/managed-mlflow)\n\nFor hosting MLflow on your own infrastructure, please refer to [this guidance](https://mlflow.org/docs/latest/ml/tracking/#tracking-setup).\n\n## ðŸ—£ï¸ Supported Programming Languages\n\n- [Python](https://pypi.org/project/mlflow/)\n- [TypeScript / JavaScript](https://www.npmjs.com/package/mlflow-tracing)\n- [Java](https://mvnrepository.com/artifact/org.mlflow/mlflow-client)\n- [R](https://cran.r-project.org/web/packages/mlflow/readme/README.html)\n\n## ðŸ”— Integrations\n\nMLflow is natively integrated with many popular machine learning frameworks and GenAI libraries.\n\n![Integrations](https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-integrations.png)\n\n## Usage Examples\n\n### Experiment Tracking ([Doc](https://mlflow.org/docs/latest/ml/tracking/))\n\nThe following examples trains a simple regression model with scikit-learn, while enabling MLflow\'s [autologging](https://mlflow.org/docs/latest/tracking/autolog.html) feature for experiment tracking.\n\n```python\nimport mlflow\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Enable MLflow\'s automatic experiment tracking for scikit-learn\nmlflow.sklearn.autolog()\n\n# Load the training dataset\ndb = load_diabetes()\nX_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n\nrf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\n# MLflow triggers logging automatically upon model fitting\nrf.fit(X_train, y_train)\n```\n\nOnce the above code finishes, run the following command in a separate terminal and access the MLflow UI via the printed URL. An MLflow **Run** should be automatically created, which tracks the training dataset, hyper parameters, performance metrics, the trained model, dependencies, and even more.\n\n```\nmlflow ui\n```\n\n### Evaluating Models ([Doc](https://mlflow.org/docs/latest/model-evaluation/index.html))\n\nThe following example runs automatic evaluation for question-answering tasks with several built-in metrics.\n\n```python\nimport mlflow\nimport pandas as pd\n\n# Evaluation set contains (1) input question (2) model outputs (3) ground truth\ndf = pd.DataFrame(\n    {\n        "inputs": ["What is MLflow?", "What is Spark?"],\n        "outputs": [\n            "MLflow is an innovative fully self-driving airship powered by AI.",\n            "Sparks is an American pop and rock duo formed in Los Angeles.",\n        ],\n        "ground_truth": [\n            "MLflow is an open-source platform for productionizing AI.",\n            "Apache Spark is an open-source, distributed computing system.",\n        ],\n    }\n)\neval_dataset = mlflow.data.from_pandas(\n    df, predictions="outputs", targets="ground_truth"\n)\n\n# Start an MLflow Run to record the evaluation results to\nwith mlflow.start_run(run_name="evaluate_qa"):\n    ', 'is an American pop and rock duo formed in Los Angeles.",\n        ],\n        "ground_truth": [\n            "MLflow is an open-source platform for productionizing AI.",\n            "Apache Spark is an open-source, distributed computing system.",\n        ],\n    }\n)\neval_dataset = mlflow.data.from_pandas(\n    df, predictions="outputs", targets="ground_truth"\n)\n\n# Start an MLflow Run to record the evaluation results to\nwith mlflow.start_run(run_name="evaluate_qa"):\n    # Run automatic evaluation with a set of built-in metrics for question-answering models\n    results = mlflow.evaluate(\n        data=eval_dataset,\n        model_type="question-answering",\n    )\n\nprint(results.tables["eval_results_table"])\n```\n\n### Observability ([Doc](https://mlflow.org/docs/latest/llms/tracing/index.html))\n\nMLflow Tracing provides LLM observability for various GenAI libraries such as OpenAI, LangChain, LlamaIndex, DSPy, AutoGen, and more. To enable auto-tracing, call `mlflow.xyz.autolog()` before running your models. Refer to the documentation for customization and manual instrumentation.\n\n```python\nimport mlflow\nfrom openai import OpenAI\n\n# Enable tracing for OpenAI\nmlflow.openai.autolog()\n\n# Query OpenAI LLM normally\nresponse = OpenAI().chat.completions.create(\n    model="gpt-4o-mini",\n    messages=[{"role": "user", "content": "Hi!"}],\n    temperature=0.1,\n)\n```\n\nThen navigate to the "Traces" tab in the MLflow UI to find the trace records OpenAI query.\n\n## ðŸ’­ Support\n\n- For help or questions about MLflow usage (e.g. "how do I do X?") visit the [documentation](https://mlflow.org/docs/latest/index.html).\n- In the documentation, you can ask the question to our AI-powered chat bot. Click on the **"Ask AI"** button at the right bottom.\n- Join the [virtual events](https://lu.ma/mlflow?k=c) like office hours and meetups.\n- To report a bug, file a documentation issue, or submit a feature request, please [open a GitHub issue](https://github.com/mlflow/mlflow/issues/new/choose).\n- For release announcements and other discussions, please subscribe to our mailing list (mlflow-users@googlegroups.com)\n  or join us on [Slack](https://mlflow.org/slack).\n\n## ðŸ¤ Contributing\n\nWe happily welcome contributions to MLflow!\n\n- Submit [bug reports](https://github.com/mlflow/mlflow/issues/new?template=bug_report_template.yaml) and [feature requests](https://github.com/mlflow/mlflow/issues/new?template=feature_request_template.yaml)\n- Contribute for [good-first-issues](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) and [help-wanted](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22)\n- Writing about MLflow and sharing your experience\n\nPlease see our [contribution guide](CONTRIBUTING.md) to learn more about contributing to MLflow.\n\n## â­ï¸ Star History\n\n<a href="https://star-history.com/#mlflow/mlflow&Date">\n <picture>\n  ', 'chat bot. Click on the **"Ask AI"** button at the right bottom.\n- Join the [virtual events](https://lu.ma/mlflow?k=c) like office hours and meetups.\n- To report a bug, file a documentation issue, or submit a feature request, please [open a GitHub issue](https://github.com/mlflow/mlflow/issues/new/choose).\n- For release announcements and other discussions, please subscribe to our mailing list (mlflow-users@googlegroups.com)\n  or join us on [Slack](https://mlflow.org/slack).\n\n## ðŸ¤ Contributing\n\nWe happily welcome contributions to MLflow!\n\n- Submit [bug reports](https://github.com/mlflow/mlflow/issues/new?template=bug_report_template.yaml) and [feature requests](https://github.com/mlflow/mlflow/issues/new?template=feature_request_template.yaml)\n- Contribute for [good-first-issues](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) and [help-wanted](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22)\n- Writing about MLflow and sharing your experience\n\nPlease see our [contribution guide](CONTRIBUTING.md) to learn more about contributing to MLflow.\n\n## â­ï¸ Star History\n\n<a href="https://star-history.com/#mlflow/mlflow&Date">\n <picture>\n   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date&theme=dark" />\n   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date" />\n   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date" />\n </picture>\n</a>\n\n## âœï¸ Citation\n\nIf you use MLflow in your research, please cite it using the "Cite this repository" button at the top of the [GitHub repository page](https://github.com/mlflow/mlflow), which will provide you with citation formats including APA and BibTeX.\n\n## ðŸ‘¥ Core Members\n\nMLflow is currently maintained by the following core members with significant contributions from hundreds of exceptionally talented community members.\n\n- [Ben Wilson](https://github.com/BenWilson2)\n- [Corey Zumar](https://github.com/dbczumar)\n- [Daniel Lok](https://github.com/daniellok-db)\n- [Gabriel Fu](https://github.com/gabrielfu)\n- [Harutaka Kawamura](https://github.com/harupy)\n- [Serena Ruan](https://github.com/serena-ruan)\n- [Tomu Hirata](https://github.com/TomeHirata)\n- [Weichen Xu](https://github.com/WeichenXu123)\n- [Yuki Watanabe](https://github.com/B-Step62)\n', '# MLflow Tracing: An Open-Source SDK for Observability and Monitoring GenAI ApplicationsðŸ”\n\n[![Latest Docs](https://img.shields.io/badge/docs-latest-success.svg?style=for-the-badge)](https://mlflow.org/docs/latest/index.html)\n[![Apache 2 License](https://img.shields.io/badge/license-Apache%202-brightgreen.svg?style=for-the-badge&logo=apache)](https://github.com/mlflow/mlflow/blob/master/LICENSE.txt)\n[![Slack](https://img.shields.io/badge/slack-@mlflow--users-CF0E5B.svg?logo=slack&logoColor=white&labelColor=3F0E40&style=for-the-badge)](https://mlflow.org/community/#slack)\n[![Twitter](https://img.shields.io/twitter/follow/MLflow?style=for-the-badge&labelColor=00ACEE&logo=twitter&logoColor=white)](https://twitter.com/MLflow)\n\nMLflow Tracing (`mlflow-tracing`) is an open-source, lightweight Python package that only includes the minimum set of dependencies and functionality\nto instrument your code/models/agents with [MLflow Tracing Feature](https://mlflow.org/docs/latest/tracing). It is designed to be a perfect fit for production environments where you want:\n\n- **âš¡ï¸ Faster Deployment**: The package size and dependencies are significantly smaller than the full MLflow package, allowing for faster deployment times in dynamic environments such as Docker containers, serverless functions, and cloud-based applications.\n- **ðŸ”§ Simplified Dependency Management**: A smaller set of dependencies means less work keeping up with dependency updates, security patches, and breaking changes from upstream libraries.\n- **ðŸ“¦ Portability**: With the less number of dependencies, MLflow Tracing can be easily deployed across different environments and platforms, without worrying about compatibility issues.\n- **ðŸ”’ Fewer Security Risks**: Each dependency potentially introduces security vulnerabilities. By reducing the number of dependencies, MLflow Tracing minimizes the attack surface and reduces the risk of security breaches.\n\n## âœ¨ Features\n\n- [Automatic Tracing](https://mlflow.org/docs/latest/tracing/integrations/) for AI libraries (OpenAI, LangChain, DSPy, Anthropic, etc...). Follow the link for the full list of supported libraries.\n- [Manual instrumentation APIs](https://mlflow.org/docs/latest/tracing/api/manual-instrumentation) such as `@trace` decorator.\n- [Production Monitoring](https://mlflow.org/docs/latest/tracing/production)\n- Other tracing APIs such as `mlflow.set_trace_tag`, `mlflow.search_traces`, etc.\n\n## ðŸŒ Choose Backend\n\nThe MLflow Trace package is designed to work with a remote hosted MLflow server as a backend. This allows you to log your traces to a central location, making it easier to manage and analyze your traces. There are several different options for hosting your MLflow server, including:\n\n- [Databricks](https://docs.databricks.com/machine-learning/mlflow/managed-mlflow.html) - Databricks offers a FREE, fully managed MLflow server as a part of their platform. This is the easiest way to get started with MLflow tracing, without having to set up any infrastructure.\n- [Amazon SageMaker](https://aws.amazon.com/sagemaker-ai/experiments/) - MLflow on Amazon SageMaker is a fully managed service offered as part of the SageMaker platform by AWS, including tracing and other MLflow features such as model registry.\n- [Nebius](https://nebius.com/) - Nebius, a cutting-edge cloud platform for GenAI explorers, offers a fully managed MLflow server.\n- [Self-hosting](https://mlflow.org/docs/latest/tracking/#tracking_setup) - MLflow is a fully open-source project, allowing you to self-host your own MLflow ', 'your MLflow server, including:\n\n- [Databricks](https://docs.databricks.com/machine-learning/mlflow/managed-mlflow.html) - Databricks offers a FREE, fully managed MLflow server as a part of their platform. This is the easiest way to get started with MLflow tracing, without having to set up any infrastructure.\n- [Amazon SageMaker](https://aws.amazon.com/sagemaker-ai/experiments/) - MLflow on Amazon SageMaker is a fully managed service offered as part of the SageMaker platform by AWS, including tracing and other MLflow features such as model registry.\n- [Nebius](https://nebius.com/) - Nebius, a cutting-edge cloud platform for GenAI explorers, offers a fully managed MLflow server.\n- [Self-hosting](https://mlflow.org/docs/latest/tracking/#tracking_setup) - MLflow is a fully open-source project, allowing you to self-host your own MLflow server and keep your data private. This is a great option if you want to have full control over your data and infrastructure.\n\n## ðŸš€ Getting Started\n\n### Installation\n\nTo install the MLflow Python package, run the following command:\n\n```bash\npip install mlflow-tracing\n```\n\nTo install from the source code, run the following command:\n\n```bash\npip install git+https://github.com/mlflow/mlflow.git#subdirectory=libs/tracing\n```\n\n> **NOTE:** It is **not** recommended to co-install this package with the full MLflow package together, as it may cause version mismatches issues.\n\n### Connect to the MLflow Server\n\nTo connect to your MLflow server to log your traces, set the `MLFLOW_TRACKING_URI` environment variable or use the `mlflow.set_tracking_uri` function:\n\n```python\nimport mlflow\n\nmlflow.set_tracking_uri("databricks")\n# Specify the experiment to log the traces to\nmlflow.set_experiment("/Path/To/Experiment")\n```\n\n### Start Logging Traces\n\n```python\nimport openai\n\nclient = openai.OpenAI(api_key="<your-api-key>")\n\n# Enable auto-tracing for OpenAI\nmlflow.openai.autolog()\n\n# Call the OpenAI API as usual\nresponse = client.chat.completions.create(\n    model="gpt-4.1-mini",\n    messages=[{"role": "user", "content": "Hello, how are you?"}],\n)\n```\n\n## ðŸ“˜ Documentation\n\nOfficial documentation for MLflow Tracing can be found at [here](https://mlflow.org/docs/latest/tracing).\n\n## ðŸ›‘ Features _Not_ Included\n\nThe following MLflow features are not included in this package.\n\n- MLflow tracking server and UI.\n- MLflow\'s other tracking capabilities such as Runs, Model Registry, Projects, etc.\n- Evaluate models/agents and log evaluation results.\n\nTo leverage the full feature set of MLflow, install the full package by running `pip install mlflow`.\n', '<h1 align="center" style="border-bottom: none">\n    <div>\n        <a href="https://mlflow.org/"><picture>\n            <img alt="MLflow Logo" src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/logo.svg" width="200" />\n        </picture></a>\n        <br>\n        MLflow TypeScript SDK\n    </div>\n</h1>\n<h2 align="center" style="border-bottom: none"></h2>\n\n<p align="center">\n  <a href="https://github.com/mlflow/mlflow"><img src="https://img.shields.io/github/stars/mlflow/mlflow?style=social" alt="stars"></a>\n  <a href="https://www.npmjs.com/package/mlflow-tracing"><img src="https://img.shields.io/npm/v/mlflow-tracing.svg" alt="version"></a>\n  <a href="https://www.npmjs.com/package/mlflow-tracing"><img src="https://img.shields.io/npm/dt/mlflow-tracing.svg" alt="downloads"></a>\n  <a href="https://github.com/mlflow/mlflow/blob/main/LICENSE"><img src="https://img.shields.io/github/license/mlflow/mlflow" alt="license"></a>\n</p>\n\nMLflow Typescript SDK is a variant of the [MLflow Python SDK](https://github.com/mlflow/mlflow) that provides a TypeScript API for MLflow.\n\n> [!IMPORTANT]\n> MLflow Typescript SDK is catching up with the Python SDK. Currently only support [Tracing]() and [Feedback Collection]() features. Please raise an issue in Github if you need a feature that is not supported.\n\n## Packages\n\n| Package                                | NPM                                                                                                                                         | Description                                       ', "                                                            | Description                                                |\n| -------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------- |\n| [mlflow-tracing](./core)               | [![npm package](https://img.shields.io/npm/v/mlflow-tracing?style=flat-square)](https://www.npmjs.com/package/mlflow-tracing)               | The core tracing functionality and manual instrumentation. |\n| [mlflow-openai](./integrations/openai) | [![npm package](https://img.shields.io/npm/v/mlflow-tracing-openai?style=flat-square)](https://www.npmjs.com/package/mlflow-tracing-openai) | Auto-instrumentation integration for OpenAI.               |\n\n## Installation\n\n```bash\nnpm install mlflow-tracing\n```\n\n> [!NOTE]\n> MLflow Typescript SDK requires Node.js 20 or higher.\n\n## Quickstart\n\nStart MLflow Tracking Server if you don't have one already:\n\n```bash\npip install mlflow\nmlflow server --backend-store-uri sqlite:///mlruns.db --port 5000\n```\n\nSelf-hosting MLflow server requires Python 3.10 or higher. If you don't have one, you can also use [managed MLflow service](https://mlflow.org/#get-started) for free to get started quickly.\n\nInstantiate MLflow SDK in your application:\n\n```typescript\nimport * as mlflow from 'mlflow-tracing';\n\nmlflow.init({\n  trackingUri: 'http://localhost:5000',\n  experimentId: '<experiment-id>'\n});\n```\n\nCreate a trace:\n\n```typescript\n// Wrap a function with mlflow.trace to generate a span when the function is called.\n// MLflow will automatically record the function name, arguments, return value,\n// latency, and exception information to the span.\nconst getWeather = mlflow.trace(\n  (city: string) => {\n    return `The weather in ${city} is sunny`;\n  },\n  // Pass options to set span name. See https://mlflow.org/docs/latest/genai/tracing/app-instrumentation/typescript-sdk\n  // for the full list of options.\n  { name: 'get-weather' }\n);\ngetWeather('San Francisco');\n\n// Alternatively, start and end span manually\nconst span = mlflow.startSpan({ name: 'my-span' });\nspan.end();\n```\n\nView traces in MLflow UI:\n\n![MLflow Tracing UI](https://github.com/mlflow/mlflow/blob/891fed9a746477f808dd2b82d3abb2382293c564/docs/static/images/llms/tracing/quickstart/openai-tool-calling-trace-detail.png?raw=true)\n\n## Trace Usage\n\nMLflow Tracing empowers ", '\'<experiment-id>\'\n});\n```\n\nCreate a trace:\n\n```typescript\n// Wrap a function with mlflow.trace to generate a span when the function is called.\n// MLflow will automatically record the function name, arguments, return value,\n// latency, and exception information to the span.\nconst getWeather = mlflow.trace(\n  (city: string) => {\n    return `The weather in ${city} is sunny`;\n  },\n  // Pass options to set span name. See https://mlflow.org/docs/latest/genai/tracing/app-instrumentation/typescript-sdk\n  // for the full list of options.\n  { name: \'get-weather\' }\n);\ngetWeather(\'San Francisco\');\n\n// Alternatively, start and end span manually\nconst span = mlflow.startSpan({ name: \'my-span\' });\nspan.end();\n```\n\nView traces in MLflow UI:\n\n![MLflow Tracing UI](https://github.com/mlflow/mlflow/blob/891fed9a746477f808dd2b82d3abb2382293c564/docs/static/images/llms/tracing/quickstart/openai-tool-calling-trace-detail.png?raw=true)\n\n## Trace Usage\n\nMLflow Tracing empowers you throughout the end-to-end lifecycle of your application. Here\'s how it helps you at each step of the workflow, click on each section to learn more:\n\n<details>\n<summary><strong>ðŸ” Build & Debug</strong></summary>\n\n<table>\n<tr>\n<td width="60%">\n\n#### Smooth Debugging Experience\n\nMLflow\'s tracing capabilities provide deep insights into what happens beneath the abstractions of your application, helping you precisely identify where issues occur.\n\n[Learn more â†’](https://mlflow.org/docs/latest/genai/tracing/observe-with-traces)\n\n</td>\n<td width="40%">\n\n![Trace Debug](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-trace-debug.png)\n\n</td>\n</tr>\n</table>\n\n</details>\n\n<details>\n<summary><strong>ðŸ’¬ Human Feedback</strong></summary>\n\n<table>\n<tr>\n<td width="60%">\n\n#### Track Annotation and User Feedback Attached to Traces\n\nCollecting and managing feedback is essential for improving your application. MLflow Tracing allows you to attach user feedback and annotations directly to traces, creating a rich dataset for analysis.\n\nThis feedback data helps you understand user satisfaction, identify areas for improvement, and build better evaluation datasets based on real user interactions.\n\n[Learn more â†’](https://mlflow.org/docs/latest/genai/tracing/collect-user-feedback)\n\n</td>\n<td width="40%">\n\n![Human Feedback](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-human-feedback.png)\n\n</td>\n</tr>\n</table>\n\n</details>\n\n<details>\n<summary><strong>ðŸ“Š Evaluation</strong></summary>\n\n<table>\n<tr>\n<td width="60%">\n\n#### Systematic Quality Assessment Throughout Your Application\n\nEvaluating the performance of your application is crucial, but creating a reliable evaluation process can be challenging. Traces serve as a rich data source, helping you assess quality with precise metrics for all components.\n\nWhen combined with MLflow\'s evaluation capabilities, you get a seamless experience for assessing and improving your application\'s performance.\n\n[Learn more â†’](https://mlflow.org/docs/latest/genai/eval-monitor)\n\n</td>\n<td width="40%">\n\n![Evaluation](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-trace-evaluation.png)\n\n</td>\n</tr>\n</table>\n\n</details>\n\n<details>\n<summary><strong>ðŸš€ Production Monitoring</strong></summary>\n\n<table>\n<tr>\n<td width="60%">\n\n#### Monitor Applications with Your Favorite Observability Stack\n\nMachine learning projects don\'t end with the first launch. Continuous monitoring and incremental improvement are critical to long-term success.\n\nIntegrated with various observability platforms such as Databricks, Datadog, Grafana, and Prometheus, MLflow Tracing provides a comprehensive solution for monitoring your applications in production.\n\n[Learn more â†’](https://mlflow.org/docs/latest/genai/tracing/prod-tracing)\n\n</td>\n<td width="40%">\n\n![Monitoring](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-monitoring.png)\n\n</td>\n</tr>\n</table>\n\n</details>\n\n<details>\n<summary><strong>ðŸ“¦ Dataset Collection</strong></summary>\n\n<table>\n<tr>\n<td width="60%">\n\n#### Create High-Quality Evaluation Datasets from Production Traces\n\nTraces from production are ', 'rich data source, helping you assess quality with precise metrics for all components.\n\nWhen combined with MLflow\'s evaluation capabilities, you get a seamless experience for assessing and improving your application\'s performance.\n\n[Learn more â†’](https://mlflow.org/docs/latest/genai/eval-monitor)\n\n</td>\n<td width="40%">\n\n![Evaluation](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-trace-evaluation.png)\n\n</td>\n</tr>\n</table>\n\n</details>\n\n<details>\n<summary><strong>ðŸš€ Production Monitoring</strong></summary>\n\n<table>\n<tr>\n<td width="60%">\n\n#### Monitor Applications with Your Favorite Observability Stack\n\nMachine learning projects don\'t end with the first launch. Continuous monitoring and incremental improvement are critical to long-term success.\n\nIntegrated with various observability platforms such as Databricks, Datadog, Grafana, and Prometheus, MLflow Tracing provides a comprehensive solution for monitoring your applications in production.\n\n[Learn more â†’](https://mlflow.org/docs/latest/genai/tracing/prod-tracing)\n\n</td>\n<td width="40%">\n\n![Monitoring](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-monitoring.png)\n\n</td>\n</tr>\n</table>\n\n</details>\n\n<details>\n<summary><strong>ðŸ“¦ Dataset Collection</strong></summary>\n\n<table>\n<tr>\n<td width="60%">\n\n#### Create High-Quality Evaluation Datasets from Production Traces\n\nTraces from production are invaluable for building comprehensive evaluation datasets. By capturing real user interactions and their outcomes, you can create test cases that truly represent your application\'s usage patterns.\n\nThis comprehensive data capture enables you to create realistic test scenarios, validate model performance on actual usage patterns, and continuously improve your evaluation datasets.\n\n[Learn more â†’](https://mlflow.org/docs/latest/genai/tracing/search-traces#creating-evaluation-datasets)\n\n</td>\n<td width="40%">\n\n![Dataset Collection](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-trace-dataset.png)\n\n</td>\n</tr>\n</table>\n\n</details>\n\n## Documentation ðŸ“˜\n\nOfficial documentation for MLflow Typescript SDK can be found [here](https://mlflow.org/docs/latest/genai/tracing/app-instrumentation/typescript-sdk).\n\n## License\n\nThis project is licensed under the [Apache License 2.0](https://github.com/mlflow/mlflow/blob/master/LICENSE.txt).\n', "# MLflow Typescript SDK - Core\n\nThis is the core package of the [MLflow Typescript SDK](https://github.com/mlflow/mlflow/tree/main/libs/typescript). It is a skinny package that includes the core tracing functionality and manual instrumentation.\n\n| Package              | NPM                                                                                                                           | Description                                                |\n| -------------------- | ----------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------- |\n| [mlflow-tracing](./) | [![npm package](https://img.shields.io/npm/v/mlflow-tracing?style=flat-square)](https://www.npmjs.com/package/mlflow-tracing) | The core tracing functionality and manual instrumentation. |\n\n## Installation\n\n```bash\nnpm install mlflow-tracing\n```\n\n## Quickstart\n\nStart MLflow Tracking Server if you don't have one already:\n\n```bash\npip install mlflow\nmlflow server --backend-store-uri sqlite:///mlruns.db --port 5000\n```\n\nSelf-hosting MLflow server requires Python 3.10 or higher. If you don't have one, you can also use [managed MLflow service](https://mlflow.org/#get-started) for free to get started quickly.\n\nInstantiate MLflow SDK in your application:\n\n```typescript\nimport * as mlflow from 'mlflow-tracing';\n\nmlflow.init({\n  trackingUri: 'http://localhost:5000',\n  experimentId: '<experiment-id>'\n});\n```\n\nCreate a trace:\n\n```typescript\n// Wrap a function with mlflow.trace to generate a span when the function is called.\n// MLflow will automatically record the function name, arguments, return value,\n// latency, and exception information to the span.\nconst getWeather = mlflow.trace(\n  (city: string) => {\n    return `The weather in ${city} is sunny`;\n  },\n ", "mlflow\nmlflow server --backend-store-uri sqlite:///mlruns.db --port 5000\n```\n\nSelf-hosting MLflow server requires Python 3.10 or higher. If you don't have one, you can also use [managed MLflow service](https://mlflow.org/#get-started) for free to get started quickly.\n\nInstantiate MLflow SDK in your application:\n\n```typescript\nimport * as mlflow from 'mlflow-tracing';\n\nmlflow.init({\n  trackingUri: 'http://localhost:5000',\n  experimentId: '<experiment-id>'\n});\n```\n\nCreate a trace:\n\n```typescript\n// Wrap a function with mlflow.trace to generate a span when the function is called.\n// MLflow will automatically record the function name, arguments, return value,\n// latency, and exception information to the span.\nconst getWeather = mlflow.trace(\n  (city: string) => {\n    return `The weather in ${city} is sunny`;\n  },\n  // Pass options to set span name. See https://mlflow.org/docs/latest/genai/tracing/app-instrumentation/typescript-sdk\n  // for the full list of options.\n  { name: 'get-weather' }\n);\ngetWeather('San Francisco');\n\n// Alternatively, start and end span manually\nconst span = mlflow.startSpan({ name: 'my-span' });\nspan.end();\n```\n\n## Documentation ðŸ“˜\n\nOfficial documentation for MLflow Typescript SDK can be found [here](https://mlflow.org/docs/latest/genai/tracing/app-instrumentation/typescript-sdk).\n\n## License\n\nThis project is licensed under the [Apache License 2.0](https://github.com/mlflow/mlflow/blob/master/LICENSE.txt).\n", "# MLflow Typescript SDK - OpenAI\n\nSeamlessly integrate [MLflow Tracing](https://github.com/mlflow/mlflow/tree/main/libs/typescript) with OpenAI to automatically trace your OpenAI API calls.\n\n| Package             | NPM                                                                                                                                         | Description                                  |\n| ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |\n| [mlflow-openai](./) | [![npm package](https://img.shields.io/npm/v/mlflow-tracing-openai?style=flat-square)](https://www.npmjs.com/package/mlflow-tracing-openai) | Auto-instrumentation integration for OpenAI. |\n\n## Installation\n\n```bash\nnpm install mlflow-openai\n```\n\nThe package includes the [`mlflow-tracing`](https://github.com/mlflow/mlflow/tree/main/libs/typescript) package and `openai` package as peer dependencies. Depending on your package manager, you may need to install these two packages separately.\n\n## Quickstart\n\nStart MLflow Tracking Server if you don't have one already:\n\n```bash\npip install mlflow\nmlflow server --backend-store-uri sqlite:///mlruns.db --port 5000\n```\n\nSelf-hosting MLflow server requires Python 3.10 or higher. If you don't have one, you can also use [managed MLflow service](https://mlflow.org/#get-started) for free to get started quickly.\n\nInstantiate MLflow SDK in your application:\n\n```typescript\nimport * as mlflow from 'mlflow-tracing';\n\nmlflow.init({\n  trackingUri: 'http://localhost:5000',\n  experimentId: '<experiment-id>'\n});\n```\n\nCreate a trace:\n\n```typescript\nimport { OpenAI } from 'openai';\nimport { tracedOpenAI } from 'mlflow-openai';\n\n// Wrap the OpenAI client with the tracedOpenAI function\nconst client = tracedOpenAI(new OpenAI());\n\n// Invoke the client as usual\nconst response = await client.chat.completions.create({\n  model: 'o4-mini',\n  messages: [\n    { ", 'MLflow Tracking Server if you don\'t have one already:\n\n```bash\npip install mlflow\nmlflow server --backend-store-uri sqlite:///mlruns.db --port 5000\n```\n\nSelf-hosting MLflow server requires Python 3.10 or higher. If you don\'t have one, you can also use [managed MLflow service](https://mlflow.org/#get-started) for free to get started quickly.\n\nInstantiate MLflow SDK in your application:\n\n```typescript\nimport * as mlflow from \'mlflow-tracing\';\n\nmlflow.init({\n  trackingUri: \'http://localhost:5000\',\n  experimentId: \'<experiment-id>\'\n});\n```\n\nCreate a trace:\n\n```typescript\nimport { OpenAI } from \'openai\';\nimport { tracedOpenAI } from \'mlflow-openai\';\n\n// Wrap the OpenAI client with the tracedOpenAI function\nconst client = tracedOpenAI(new OpenAI());\n\n// Invoke the client as usual\nconst response = await client.chat.completions.create({\n  model: \'o4-mini\',\n  messages: [\n    { role: \'system\', content: \'You are a helpful weather assistant.\' },\n    { role: \'user\', content: "What\'s the weather like in Seattle?" }\n  ]\n});\n```\n\nView traces in MLflow UI:\n\n![MLflow Tracing UI](https://github.com/mlflow/mlflow/blob/891fed9a746477f808dd2b82d3abb2382293c564/docs/static/images/llms/tracing/quickstart/single-openai-trace-detail.png?raw=true)\n\n## Documentation ðŸ“˜\n\nOfficial documentation for MLflow Typescript SDK can be found [here](https://mlflow.org/docs/latest/genai/tracing/app-instrumentation/typescript-sdk).\n\n## License\n\nThis project is licensed under the [Apache License 2.0](https://github.com/mlflow/mlflow/blob/master/LICENSE.txt).\n', '---\nnamespace: genai\ndescription: Analyzes the traces logged in an MLflow experiment to find operational and quality issues automatically, generating a markdown report.\n---\n\n# Analyze Experiment\n\nAnalyzes traces in an MLflow experiment for quality issues, performance problems, and patterns.\n\n## Step 1: Setup and Configuration\n\n### 1.1 Collect Experiment Information\n\n- **REQUIRED FIRST**: Ask user "How do you want to authenticate to MLflow?"\n\n  **Option 1: Local/Self-hosted MLflow**\n\n  - Ask for tracking URI (one of):\n    - SQLite: `sqlite:////path/to/mlflow.db`\n    - PostgreSQL: `postgresql://user:password@host:port/database`\n    - MySQL: `mysql://user:password@host:port/database`\n    - File Store: `file:///path/to/mlruns` or just `/path/to/mlruns`\n  - Ask user to create an environment file (e.g., `mlflow.env`) containing:\n    ```\n    MLFLOW_TRACKING_URI=<provided_uri>\n    ```\n\n  **Option 2: Databricks**\n\n  - Ask which authentication method:\n    - **PAT Auth**: Request `DATABRICKS_HOST` and `DATABRICKS_TOKEN`\n    - **Profile Auth**: Request `DATABRICKS_CONFIG_PROFILE` name\n  - Ask user to create an environment file (e.g., `mlflow.env`) containing:\n\n    ```\n    # For PAT Auth:\n    MLFLOW_TRACKING_URI=databricks\n    DATABRICKS_HOST=<provided_host>\n    DATABRICKS_TOKEN=<provided_token>\n\n    # OR for Profile Auth:\n    MLFLOW_TRACKING_URI=databricks\n    DATABRICKS_CONFIG_PROFILE=<provided_profile>\n    ```\n\n  **Option 3: Environment Variables Already Set**\n\n  - Ask user "Do you already have MLflow environment variables set in your shell (bashrc/zshrc)?"\n  - If yes, test connection directly: `uv run python -m mlflow experiments search --max-results 10`\n  - If this works, skip env file creation and use commands without `--env-file` flag\n  - If not, fall back to Options 1 or 2\n\n- Ask user for the path to their environment file (if using Options 1-2)\n- Verify connection by listing experiments: `uv run --env-file <env_file_path> python -m mlflow experiments search --max-results 10`\n- **Option to search by name**: If user knows the experiment name, use `--filter-string` parameter:\n  - `uv run --env-file <env_file_path> python -m mlflow experiments search --filter-string "name LIKE \'%experiment_name%\'" --max-results 10`\n- Ask user for experiment ID or let them choose from the list\n- **WAIT for user response** - do not continue ', 'env file creation and use commands without `--env-file` flag\n  - If not, fall back to Options 1 or 2\n\n- Ask user for the path to their environment file (if using Options 1-2)\n- Verify connection by listing experiments: `uv run --env-file <env_file_path> python -m mlflow experiments search --max-results 10`\n- **Option to search by name**: If user knows the experiment name, use `--filter-string` parameter:\n  - `uv run --env-file <env_file_path> python -m mlflow experiments search --filter-string "name LIKE \'%experiment_name%\'" --max-results 10`\n- Ask user for experiment ID or let them choose from the list\n- **WAIT for user response** - do not continue until they provide the experiment ID\n- Add `MLFLOW_EXPERIMENT_ID=<experiment_id>` to their environment file\n- Run `uv run --env-file <env_file_path> python -m mlflow traces --help` to understand the CLI commands and options\n\n### 1.2 Test Trace Retrieval\n\n- Call `uv run --env-file <env_file_path> python -m mlflow traces search --max-results 5` to verify:\n  - Traces exist in the experiment\n  - CLI is working properly (using local MLflow installation)\n  - Database connection is valid\n- Extract sample trace IDs for testing\n- Get one full trace with `uv run --env-file <env_file_path> python -m mlflow traces get --trace-id <id>` to understand the data structure\n\n## Step 2: Analysis Phase\n\n### 2.1 Bulk Trace Collection\n\n- Search for a larger sample using `--max-results` parameter (start with 20-50 traces for initial analysis)\n- **IMPORTANT**: Use `--max-results` to limit results for users with hundreds of thousands of experiments/traces\n- Extract key fields: trace_id, state, execution_duration_ms, request_preview, response_preview\n\n### 2.1.5 Understand Agent Purpose and Capabilities\n\n- Analyze trace inputs/outputs to understand the agent\'s task:\n  - Extract trace inputs/outputs: `--extract-fields info.trace_metadata.\\`mlflow.traceInputs\\`,info.trace_metadata.\\`mlflow.traceOutputs\\``\n  - Examine these fields to understand:\n    - Types of questions users ask\n    - Types of responses the agent provides\n    - Common patterns in user interactions\n  - Identify available tools by examining spans with type "TOOL":\n    - What tools are available to the agent?\n    - What data sources can the agent access?\n    - What capabilities do these tools provide?\n- Generate a 1-paragraph agent description covering:\n  - **What ', 'inputs/outputs to understand the agent\'s task:\n  - Extract trace inputs/outputs: `--extract-fields info.trace_metadata.\\`mlflow.traceInputs\\`,info.trace_metadata.\\`mlflow.traceOutputs\\``\n  - Examine these fields to understand:\n    - Types of questions users ask\n    - Types of responses the agent provides\n    - Common patterns in user interactions\n  - Identify available tools by examining spans with type "TOOL":\n    - What tools are available to the agent?\n    - What data sources can the agent access?\n    - What capabilities do these tools provide?\n- Generate a 1-paragraph agent description covering:\n  - **What the agent\'s job is** (e.g., "a boating agent that answers questions about weather and helps users plan trips")\n  - **What data sources it has access to** (APIs, databases, etc.)\n- **Present this description to the user** and ask for confirmation/corrections\n- **WAIT for user response** - do not proceed until they confirm or provide corrections\n- **Ask if they want to focus the analysis on anything specific** (or do a general report)\n  - If they provide specific focus areas, use these as additional context for hypothesis formation\n  - Don\'t overfit to their focus - still do comprehensive analysis, but prioritize their areas of interest\n  - Their specific concerns should become hypotheses to validate/invalidate during analysis\n- **WAIT for user response** before proceeding to section 2.2\n- Use agent context + any specific focus areas for all subsequent hypothesis testing in sections 2.2+\n\n### 2.2 Operational Issues Analysis (Hypothesis-Driven Approach)\n\n**NOTE: Use MLflow CLI commands for trace exploration - DO NOT use inline Python scripts during this phase**\n\n**Show your thinking as you go**: Always explain your hypothesis development process including:\n\n- Current hypothesis being tested\n- Evidence found: ALWAYS show BOTH trace input (user request) AND trace output (agent response), plus tools called\n- Reasoning for supporting/refuting the hypothesis\n\nProcess traces in batches of 10, building and refining hypotheses with each batch:\n\n1. Form initial hypotheses from first batch\n2. With each new batch: validate, refute, or expand hypotheses\n3. Continue until patterns stabilize\n\n**After confirming ANY hypothesis (operational or quality)**: Track assessments for inclusion in final report:\n\n- **1:1 Correspondence**: Each assessment ', 'commands for trace exploration - DO NOT use inline Python scripts during this phase**\n\n**Show your thinking as you go**: Always explain your hypothesis development process including:\n\n- Current hypothesis being tested\n- Evidence found: ALWAYS show BOTH trace input (user request) AND trace output (agent response), plus tools called\n- Reasoning for supporting/refuting the hypothesis\n\nProcess traces in batches of 10, building and refining hypotheses with each batch:\n\n1. Form initial hypotheses from first batch\n2. With each new batch: validate, refute, or expand hypotheses\n3. Continue until patterns stabilize\n\n**After confirming ANY hypothesis (operational or quality)**: Track assessments for inclusion in final report:\n\n- **1:1 Correspondence**: Each assessment must correspond to ONE specific issue/hypothesis\n- Use snake_case names as assessment keys (e.g., `overly_verbose`, `tool_failure`, `rate_limited`, `slow_response`)\n- Track which traces exhibit each issue with detailed rationales\n- Document specifics like:\n\n  - For quality issues: exact character counts, repetition counts, unnecessary sections\n  - For operational issues: exact durations, error messages, timeout values\n\n- **Error Analysis**\n\n  - Filter for ERROR traces: `uv run --env-file <env_file_path> python -m mlflow traces search --filter "info.state = \'ERROR\'" --max-results 10`\n  - **Adjust --max-results as needed**: Start with 10-20, increase if you need more examples to identify patterns\n  - **Pattern Analysis Focus**: Identify WHY errors occur by examining:\n    - Tool/API failures in spans (look for spans with type "TOOL" that failed)\n    - Rate limiting responses from external APIs\n    - Authentication/permission errors\n    - Timeout patterns (compare execution_duration_ms)\n    - Input validation failures\n    - Resource unavailability (databases, services down)\n  - Example hypotheses to test:\n    - Certain types of queries consistently trigger tool failures\n    - Errors cluster around specific time ranges (service outages)\n    - Fast failures (~2s) indicate input validation vs slower failures (~30s) indicate timeouts\n    - Specific tools/APIs are unreliable and cause cascading failures\n    - Rate limiting from external services causes batch failures\n  - **Note**: You may discover other operational error patterns as you analyze the traces\n\n- **Performance Problems (High Latency Analysis)**\n ', 'failures\n    - Resource unavailability (databases, services down)\n  - Example hypotheses to test:\n    - Certain types of queries consistently trigger tool failures\n    - Errors cluster around specific time ranges (service outages)\n    - Fast failures (~2s) indicate input validation vs slower failures (~30s) indicate timeouts\n    - Specific tools/APIs are unreliable and cause cascading failures\n    - Rate limiting from external services causes batch failures\n  - **Note**: You may discover other operational error patterns as you analyze the traces\n\n- **Performance Problems (High Latency Analysis)**\n  - Filter for OK traces with high latency: `uv run --env-file <env_file_path> python -m mlflow traces search --filter "info.state = \'OK\'" --max-results 10`\n  - **Adjust --max-results as needed**: Start with 10-20, increase if you need more examples to identify patterns\n  - **Pattern Analysis Focus**: Identify WHY traces are slow by examining:\n    - Tool call duration patterns in spans\n    - Number of sequential vs parallel tool calls\n    - Specific slow APIs/tools (database queries, web requests, etc.)\n    - Cold start vs warm execution patterns\n    - Resource contention indicators\n  - Example hypotheses to test:\n    - Complex queries with multiple sequential tool calls have multiplicative latency\n    - Certain tools/APIs are consistent performance bottlenecks (>5s per call)\n    - First queries in sessions are slower due to cold start overhead\n    - Database queries without proper indexing cause delays\n    - Network timeouts or retries inflate execution time\n    - Parallel tool execution is not properly implemented\n  - **Note**: You may discover other performance patterns as you analyze the traces\n\n### 2.3 Quality Issues Analysis (Hypothesis-Driven Approach)\n\n**NOTE: Use MLflow CLI commands for trace exploration - DO NOT use inline Python scripts during this phase**\n\nFocus on response quality, not operational performance:\n\n- **Content Quality Issues**\n  - Sample both OK and ERROR traces\n  - Example hypotheses to test:\n    - Agent ', "start overhead\n    - Database queries without proper indexing cause delays\n    - Network timeouts or retries inflate execution time\n    - Parallel tool execution is not properly implemented\n  - **Note**: You may discover other performance patterns as you analyze the traces\n\n### 2.3 Quality Issues Analysis (Hypothesis-Driven Approach)\n\n**NOTE: Use MLflow CLI commands for trace exploration - DO NOT use inline Python scripts during this phase**\n\nFocus on response quality, not operational performance:\n\n- **Content Quality Issues**\n  - Sample both OK and ERROR traces\n  - Example hypotheses to test:\n    - Agent provides overly verbose responses for simple questions\n    - Some text/information is repeated unnecessarily across responses\n    - Conversation context carries over inappropriately\n    - Agent asks follow-up questions instead of attempting tasks\n    - Responses are inconsistent for similar queries\n    - Agent provides incorrect or outdated information\n    - Response format is inappropriate for the query type\n  - **Note**: You may discover other quality issues as you analyze the traces\n\n### 2.4 Strengths and Successes Analysis (Hypothesis-Driven Approach)\n\n**NOTE: Use MLflow CLI commands for trace exploration - DO NOT use inline Python scripts during this phase**\n\nProcess successful traces to identify what's working well:\n\n- **Successful Interactions**\n\n  - Filter for OK traces with good outcomes\n  - Example hypotheses to test:\n    - Agent provides comprehensive, helpful responses for complex queries\n    - Certain types of questions consistently get high-quality answers\n    - Tool usage is appropriate and effective for specific scenarios\n    - Response format is well-structured for particular use cases\n\n- **Effective Tool Usage**\n\n  - Examine traces where tools are used successfully\n  - Example hypotheses to test:\n    - Agent selects appropriate tools for different query types\n    - Multi-step tool usage produces better outcomes\n    - Certain tool combinations work particularly well together\n\n- **Quality Responses**\n  - Identify traces with good response quality\n  - Example hypotheses to test:\n  ", 'types of questions consistently get high-quality answers\n    - Tool usage is appropriate and effective for specific scenarios\n    - Response format is well-structured for particular use cases\n\n- **Effective Tool Usage**\n\n  - Examine traces where tools are used successfully\n  - Example hypotheses to test:\n    - Agent selects appropriate tools for different query types\n    - Multi-step tool usage produces better outcomes\n    - Certain tool combinations work particularly well together\n\n- **Quality Responses**\n  - Identify traces with good response quality\n  - Example hypotheses to test:\n    - Agent provides right level of detail for complex questions\n    - Safety/important information is appropriately included\n    - Agent successfully handles follow-up questions in context\n\n### 2.5 Generate Final Report\n\n- Ask user where to save the report (markdown file path, e.g., `experiment_analysis.md`)\n- **ONLY NOW use uv inline Python scripts for statistical calculations** - never compute stats manually\n- Inline Python scripts are ONLY for final math/statistics, NOT for trace exploration\n- Use `uv run --env-file <env_file_path> python -c "..."` for any Python calculations that need MLflow access\n- Generate a single comprehensive markdown report with:\n  - **Summary statistics** (computed via `uv run --env-file <env_file_path> python -c "..."` with collected trace data):\n    - Total traces analyzed\n    - Success rate (OK vs ERROR percentage)\n    - Average, median, p95 latency for successful traces\n    - Error rate distribution by duration (fast fails vs timeouts)\n  - **Operational Issues** (errors, latency, performance):\n    - For each confirmed operational hypothesis:\n      - Clear statement of the hypothesis\n      - Example trace IDs that support the hypothesis\n      - BOTH trace input (user request) AND trace output (agent response) excerpts from those traces\n      - Tools called (spans of type "TOOL") and their durations/failures\n      - Root cause analysis: WHY the issue occurs (rate limiting, API failures, timeouts, etc.)\n ', 'by duration (fast fails vs timeouts)\n  - **Operational Issues** (errors, latency, performance):\n    - For each confirmed operational hypothesis:\n      - Clear statement of the hypothesis\n      - Example trace IDs that support the hypothesis\n      - BOTH trace input (user request) AND trace output (agent response) excerpts from those traces\n      - Tools called (spans of type "TOOL") and their durations/failures\n      - Root cause analysis: WHY the issue occurs (rate limiting, API failures, timeouts, etc.)\n      - **Trace assessments**: List specific trace IDs that exhibit this issue with detailed rationales explaining why each trace demonstrates the pattern\n      - Quantitative evidence (frequency, timing patterns, etc.) - computed via Python\n  - **Quality Issues** (content problems, user experience):\n    - For each confirmed quality hypothesis:\n      - Clear statement of the hypothesis\n      - Example trace IDs that support the hypothesis\n      - BOTH trace input (user request) AND trace output (agent response) excerpts from those traces\n      - **Trace assessments**: List specific trace IDs that exhibit this issue with detailed rationales explaining why each trace demonstrates the pattern\n      - Quantitative evidence (frequency, assessment patterns, etc.) - computed via Python\n  - **Refuted Hypotheses** (briefly noted)\n  - Recommendations for improvement based on confirmed issues\n', '# MLflow Claude Code Integration\n\nThis module provides automatic tracing integration between Claude Code and MLflow.\n\n## Module Structure\n\n- **`config.py`** - Configuration management (settings files, environment variables)\n- **`hooks.py`** - Claude Code hook setup and management\n- **`cli.py`** - MLflow CLI commands (`mlflow autolog claude`)\n- **`tracing.py`** - Core tracing logic and processors\n- **`hooks/`** - Hook implementation handlers\n\n## Installation\n\n```bash\npip install mlflow\n```\n\n## Usage\n\nSet up Claude Code tracing in any project directory:\n\n```bash\n# Set up tracing in current directory\nmlflow autolog claude\n\n# Set up tracing in specific directory\nmlflow autolog claude ~/my-project\n\n# Set up with custom tracking URI\nmlflow autolog claude -u file://./custom-mlruns\nmlflow autolog claude -u sqlite:///mlflow.db\n\n# Set up with Databricks\nmlflow autolog claude -u databricks -e 123456789\n\n# Check status\nmlflow autolog claude --status\n\n# Disable tracing\nmlflow autolog claude --disable\n```\n\n## How it Works\n\n1. **Setup**: The `mlflow autolog claude` command configures Claude Code hooks in a `.claude/settings.json` file\n2. **Automatic Tracing**: When you use the `claude` command in the configured directory, your conversations are automatically traced to MLflow\n3. **View Traces**: Use `mlflow ui` to view your conversation traces\n\n## Configuration\n\nThe setup creates two types of configuration:\n\n### Claude Code Hooks\n\n- **PostToolUse**: Captures tool usage during conversations\n- **Stop**: Processes complete conversations into MLflow traces\n\n### Environment Variables\n\n- `MLFLOW_CLAUDE_TRACING_ENABLED=true`: Enables tracing\n- `MLFLOW_TRACKING_URI`: Where to store traces (defaults to local `.claude/mlflow/runs`)\n- `MLFLOW_EXPERIMENT_ID` or `MLFLOW_EXPERIMENT_NAME`: Which experiment to use\n\n## Examples\n\n### Basic Local Setup\n\n```bash\nmlflow autolog claude\ncd .\nclaude "help me write a function"\nmlflow ui --backend-store-uri sqlite:///mlflow.db\n```\n\n### Databricks Integration\n\n```bash\nmlflow autolog claude -u databricks -e 123456789\nclaude "analyze this data"\n# View traces in Databricks\n```\n\n### Custom Project Setup\n\n```bash\nmlflow autolog claude ~/my-ai-project -u sqlite:///mlflow.db -n "My AI Project"\ncd ~/my-ai-project\nclaude "refactor this code"\nmlflow ui --backend-store-uri sqlite:///mlflow.db\n```\n\n## Troubleshooting\n\n### Check Status\n\n```bash\nmlflow autolog claude --status\n```\n\n### Disable Tracing\n\n```bash\nmlflow autolog claude --disable\n```\n\n### View Raw Configuration\n\nThe configuration is stored in `.claude/settings.json`:\n\n```bash\ncat .claude/settings.json\n```\n\n## Requirements\n\n- Python 3.10+ (required by MLflow)\n- MLflow installed (`pip install mlflow`)\n- Claude Code CLI installed\n', '# MLflow Java Client\n\nJava client for [MLflow](https://mlflow.org) REST API.\nSee also the MLflow [Python API](https://mlflow.org/docs/latest/python_api/index.html)\nand [REST API](https://mlflow.org/docs/latest/rest-api.html).\n\n## Requirements\n\n- Java 1.8\n- Maven\n- Run the [MLflow Tracking Server 0.4.2](https://mlflow.org/docs/latest/tracking.html#running-a-tracking-server)\n\n## Build\n\n### Build with tests\n\nThe MLflow Java client tests require that MLflow is on the PATH (to start a local server),\nso it is recommended to run them from within a development conda environment.\n\nTo build a deployable JAR and run tests:\n\n```\nmvn package\n```\n\n## Run\n\nTo run a simple sample.\n\n```\njava -cp target/mlflow-java-client-0.4.2.jar \\\n  com.databricks.mlflow.client.samples.QuickStartDriver http://localhost:5001\n```\n\n## JSON Serialization\n\nMLflow Java client uses [Protobuf](https://developers.google.com/protocol-buffers/) 3.6.0 to serialize the JSON payload.\n\n- [service.proto](../mlflow/protos/service.proto) - Protobuf definition of data objects.\n- [com.databricks.api.proto.mlflow.Service.java](src/main/java/com/databricks/api/proto/mlflow/Service.java) - Generated Java classes of all data objects.\n- [generate_protos.py](generate_protos.py) - One time script to generate Service.java. If service.proto changes you will need to re-run this script.\n- Javadoc can be generated by running `mvn javadoc:javadoc`. The output will be in [target/site/apidocs/index.html](target/site/apidocs/index.html).\n  Here is the javadoc for [Service.java](target/site/apidocs/com/databricks/api/proto/mlflow/Service.html).\n\n## Java Client API\n\nSee [ApiClient.java](src/main/java/org/mlflow/client/ApiClient.java)\nand [Service.java domain objects](src/main/java/org/mlflow/api/proto/mlflow/Service.java).\n\n```\nRun getRun(String runId)\nRunInfo createRun()\nRunInfo createRun(String experimentId)\nRunInfo createRun(String experimentId, String appName)\nRunInfo createRun(CreateRun request)\nList<RunInfo> listRunInfos(String experimentId)\n\n\nList<Experiment> searchExperiments()\nGetExperiment.Response getExperiment(String experimentId)\nOptional<Experiment> getExperimentByName(String experimentName)\nlong createExperiment(String experimentName)\n\nvoid logParam(String runId, String key, String value)\nvoid logMetric(String runId, String key, float value)\nvoid setTerminated(String runId)\nvoid setTerminated(String runId, RunStatus status)\nvoid setTerminated(String runId, RunStatus status, long endTime)\nListArtifacts.Response listArtifacts(String runId, String path)\n```\n\n## Usage\n\n### Java Usage\n\nFor a simple example see [QuickStartDriver.java](src/main/java/org/mlflow/tracking/samples/QuickStartDriver.java).\nFor full examples of API coverage see the [tests](src/test/java/org/mlflow/tracking) such as [MlflowClientTest.java](src/test/java/org/mlflow/tracking/MlflowClientTest.java).\n\n```\npackage org.mlflow.tracking.samples;\n\nimport java.util.List;\nimport java.util.Optional;\n\nimport org.apache.log4j.Level;\nimport org.apache.log4j.LogManager;\n\nimport org.mlflow.api.proto.Service.*;\nimport org.mlflow.tracking.MlflowClient;\n\n/**\n * This is an example application which uses the MLflow Tracking API to create and manage\n * experiments and runs.\n */\npublic class QuickStartDriver {\n  public static void main(String[] args) throws Exception {\n    (new QuickStartDriver()).process(args);\n  }\n\n  void process(String[] args) throws Exception {\n    MlflowClient client;\n    if (args.length < 1) {\n      client = new MlflowClient();\n    } else {\n      client = new MlflowClient(args[0]);\n    }\n\n    boolean verbose = args.length >= 2 && "true".equals(args[1]);\n    if (verbose) {\n      LogManager.getLogger("org.mlflow.client").setLevel(Level.DEBUG);\n    }\n\n    System.out.println("====== createExperiment");\n    String expName ', 'public static void main(String[] args) throws Exception {\n    (new QuickStartDriver()).process(args);\n  }\n\n  void process(String[] args) throws Exception {\n    MlflowClient client;\n    if (args.length < 1) {\n      client = new MlflowClient();\n    } else {\n      client = new MlflowClient(args[0]);\n    }\n\n    boolean verbose = args.length >= 2 && "true".equals(args[1]);\n    if (verbose) {\n      LogManager.getLogger("org.mlflow.client").setLevel(Level.DEBUG);\n    }\n\n    System.out.println("====== createExperiment");\n    String expName = "Exp_" + System.currentTimeMillis();\n    String expId = client.createExperiment(expName);\n    System.out.println("createExperiment: expId=" + expId);\n\n    System.out.println("====== getExperiment");\n    GetExperiment.Response exp = client.getExperiment(expId);\n    System.out.println("getExperiment: " + exp);\n\n    System.out.println("====== searchExperiments");\n    List<Experiment> exps = client.searchExperiments();\n    System.out.println("#experiments: " + exps.size());\n    exps.forEach(e -> System.out.println("  Exp: " + e));\n\n    createRun(client, expId);\n\n    System.out.println("====== getExperiment again");\n    GetExperiment.Response exp2 = client.getExperiment(expId);\n    System.out.println("getExperiment: " + exp2);\n\n    System.out.println("====== getExperiment by name");\n    Optional<Experiment> exp3 = client.getExperimentByName(expName);\n    System.out.println("getExperimentByName: " + exp3);\n  }\n\n  void createRun(MlflowClient client, String expId) {\n    System.out.println("====== createRun");\n\n    // Create run\n    String sourceFile = "MyFile.java";\n    RunInfo runCreated = client.createRun(expId, sourceFile);\n    System.out.println("CreateRun: " + runCreated);\n    String runId = runCreated.getRunUuid();\n\n    // Log parameters\n    client.logParam(runId, "min_samples_leaf", "2");\n    client.logParam(runId, "max_depth", "3");\n\n    // Log metrics\n    client.logMetric(runId, "auc", 2.12F);\n    client.logMetric(runId, "accuracy_score", 3.12F);\n    client.logMetric(runId, "zero_one_loss", 4.12F);\n\n    // Update finished run\n    client.setTerminated(runId, RunStatus.FINISHED);\n\n    // Get run details\n    Run run = client.getRun(runId);\n    System.out.println("GetRun: " + run);\n    client.close();\n  }\n}\n```\n', '# mlflow: R interface for MLflow\n\n[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/mlflow)](https://cran.r-project.org/package=mlflow)\n\n- Install [MLflow](https://mlflow.org/) from R to track experiments\n  locally.\n- Connect to MLflow servers to share experiments with others.\n- Use MLflow to export models that can be served locally and remotely.\n\n## Prerequisites\n\nTo use the MLflow R API, you must install [the MLflow Python package](https://pypi.org/project/mlflow/).\n\n```bash\npip install mlflow\n```\n\nOptionally, you can set the `MLFLOW_PYTHON_BIN` and `MLFLOW_BIN` environment variables to specify\nthe Python and MLflow binaries to use. By default, the R client automatically finds them using\n`Sys.which("python")` and `Sys.which("mlflow")`.\n\n```bash\nexport MLFLOW_PYTHON_BIN=/path/to/bin/python\nexport MLFLOW_BIN=/path/to/bin/mlflow\n```\n\n## Installation\n\nInstall `mlflow` as follows:\n\n```r\ndevtools::install_github("mlflow/mlflow", subdir = "mlflow/R/mlflow")\n```\n\n## Development\n\nInstall the `mlflow` package as follows:\n\n```r\ndevtools::install_github("mlflow/mlflow", subdir = "mlflow/R/mlflow")\n```\n\nThen install the latest released `mlflow` runtime.\n\nHowever, currently, the development runtime of `mlflow` is also\nrequired; which means you also need to download or clone the `mlflow`\nGitHub repo:\n\n```bash\ngit clone https://github.com/mlflow/mlflow\n```\n\nAnd upgrade the runtime to the development version as follows:\n\n```bash\n# Upgrade to the latest development version\npip install -e <local github repo>\n```\n\n## Tracking\n\nMLflow Tracking allows you to logging parameters, code versions,\nmetrics, and output files when running R code and for later visualizing\nthe results.\n\nMLflow allows you to group runs under experiments, which can be useful\nfor comparing runs intended to tackle a particular task. You can create\nand activate a new experiment locally using `mlflow` as follows:\n\n```r\nlibrary(mlflow)\nmlflow_set_experiment("Test")\n```\n\nThen you can list view your experiments from MLflows user interface by\nrunning:\n\n```r\nmlflow_ui()\n```\n\n<img src="tools/readme/mlflow-user-interface.png" class="screenshot" width=520 />\n\nYou can also use a MLflow server to track and share experiments, see\n[running a tracking\nserver](https://www.mlflow.org/docs/latest/tracking.html#running-a-tracking-server),\nand then make use of this server by running:\n\n```r\nmlflow_set_tracking_uri("http://tracking-server:5000")\n```\n\nOnce the tracking url is defined, the experiments will be stored and\ntracked in the specified server which others will also be able to\naccess.\n\n## Projects\n\nAn MLflow Project is a format for packaging data science code in a\nreusable and reproducible way.\n\nMLflow projects can be [explicitly\ncreated](https://www.mlflow.org/docs/latest/projects.html#specifying-projects)\nor implicitly used by running `R` with `mlflow` from the terminal as\nfollows:\n\n```bash\nmlflow run examples/r_wine --entry-point train.R\n```\n\nNotice that is equivalent to running from `examples/r_wine`,\n\n```bash\nRscript -e "mlflow::mlflow_source(\'train.R\')"\n```\n\nand `train.R` performing training and logging as follows:\n\n```r\nlibrary(mlflow)\n\n# read parameters\ncolumn <- mlflow_log_param("column", 1)\n\n# log total rows\nmlflow_log_metric("rows", nrow(iris))\n\n# train model\nmodel <- lm(\n  Sepal.Width ~ x,\n  data.frame(Sepal.Width = iris$Sepal.Width, x = iris[,column])\n)\n\n# log models intercept\nmlflow_log_metric("intercept", model$coefficients[["(Intercept)"]])\n```\n\n### Parameters\n\nYou will often want to parameterize your scripts to support running and\ntracking multiple experiments. You ', 'others will also be able to\naccess.\n\n## Projects\n\nAn MLflow Project is a format for packaging data science code in a\nreusable and reproducible way.\n\nMLflow projects can be [explicitly\ncreated](https://www.mlflow.org/docs/latest/projects.html#specifying-projects)\nor implicitly used by running `R` with `mlflow` from the terminal as\nfollows:\n\n```bash\nmlflow run examples/r_wine --entry-point train.R\n```\n\nNotice that is equivalent to running from `examples/r_wine`,\n\n```bash\nRscript -e "mlflow::mlflow_source(\'train.R\')"\n```\n\nand `train.R` performing training and logging as follows:\n\n```r\nlibrary(mlflow)\n\n# read parameters\ncolumn <- mlflow_log_param("column", 1)\n\n# log total rows\nmlflow_log_metric("rows", nrow(iris))\n\n# train model\nmodel <- lm(\n  Sepal.Width ~ x,\n  data.frame(Sepal.Width = iris$Sepal.Width, x = iris[,column])\n)\n\n# log models intercept\nmlflow_log_metric("intercept", model$coefficients[["(Intercept)"]])\n```\n\n### Parameters\n\nYou will often want to parameterize your scripts to support running and\ntracking multiple experiments. You can define parameters with type under\na `params_example.R` example as follows:\n\n```r\nlibrary(mlflow)\n\n# define parameters\nmy_int <- mlflow_param("my_int", 1, "integer")\nmy_num <- mlflow_param("my_num", 1.0, "numeric")\n\n# log parameters\nmlflow_log_param("param_int", my_int)\nmlflow_log_param("param_num", my_num)\n```\n\nThen run `mlflow run` with custom parameters as\nfollows\n\n    mlflow run tests/testthat/examples/ --entry-point params_example.R -P my_int=10 -P my_num=20.0 -P my_str=XYZ\n\n    === Created directory /var/folders/ks/wm_bx4cn70s6h0r5vgqpsldm0000gn/T/tmpi6d2_wzf for downloading remote URIs passed to arguments of type \'path\' ===\n    === Running command \'source /miniconda2/bin/activate mlflow-da39a3ee5e6b4b0d3255bfef95601890afd80709 && Rscript -e "mlflow::mlflow_source(\'params_example.R\')" --args --my_int 10 --my_num 20.0 --my_str XYZ\' in run with ID \'191b489b2355450a8c3cc9bf96cb1aa3\' ===\n    === Run (ID \'191b489b2355450a8c3cc9bf96cb1aa3\') succeeded ===\n\nRun results that we can view with `mlflow_ui()`.\n\n## Models\n\nAn MLflow Model is a standard format for packaging machine learning\nmodels that can be used in a variety of downstream toolsâ€”for example,\nreal-time serving through a REST API or batch inference on Apache Spark.\nThey provide a convention to save a model in different "flavors" that\ncan be understood by different downstream tools.\n\nTo save a model use `mlflow_save_model()`. For instance, you can add the\nfollowing lines to the previous `train.R` script:\n\n```r\n# train model (...)\n\n# save model\nmlflow_save_model(\n  crate(~ stats::predict(model, .x), model)\n)\n```\n\nAnd trigger a run with that will also save your model as follows:\n\n```bash\nmlflow run train.R\n```\n\nEach MLflow Model is simply a directory containing arbitrary files,\ntogether with an MLmodel file in the root of the directory that can\ndefine multiple flavors that the model can be viewed in.\n\nThe directory containing the model looks as follows:\n\n```r\ndir("model")\n```\n\n    ## [1] "crate.bin" "MLmodel"\n\nand the model definition `model/MLmodel` like:\n\n```r\ncat(paste(readLines("model/MLmodel"), collapse = "\\n"))\n```\n\n   ', 'understood by different downstream tools.\n\nTo save a model use `mlflow_save_model()`. For instance, you can add the\nfollowing lines to the previous `train.R` script:\n\n```r\n# train model (...)\n\n# save model\nmlflow_save_model(\n  crate(~ stats::predict(model, .x), model)\n)\n```\n\nAnd trigger a run with that will also save your model as follows:\n\n```bash\nmlflow run train.R\n```\n\nEach MLflow Model is simply a directory containing arbitrary files,\ntogether with an MLmodel file in the root of the directory that can\ndefine multiple flavors that the model can be viewed in.\n\nThe directory containing the model looks as follows:\n\n```r\ndir("model")\n```\n\n    ## [1] "crate.bin" "MLmodel"\n\nand the model definition `model/MLmodel` like:\n\n```r\ncat(paste(readLines("model/MLmodel"), collapse = "\\n"))\n```\n\n    ## flavors:\n    ##   crate:\n    ##     version: 0.1.0\n    ##     model: crate.bin\n    ## time_created: 18-10-03T22:18:25.25.55\n    ## run_id: 4286a3d27974487b95b19e01b7b3caab\n\nLater on, the R model can be deployed which will perform predictions\nusing\n`mlflow_rfunc_predict()`:\n\n```r\nmlflow_rfunc_predict("model", data = data.frame(x = c(0.3, 0.2)))\n```\n\n    ## Warning in mlflow_snapshot_warning(): Running without restoring the\n    ## packages snapshot may not reload the model correctly. Consider running\n    ## \'mlflow_restore_snapshot()\' or setting the \'restore\' parameter to \'TRUE\'.\n\n    ## 3.400381396714573.40656987651099\n\n    ##        1        2\n    ## 3.400381 3.406570\n\n## Deployment\n\nMLflow provides tools for deployment on a local machine and several\nproduction environments. You can use these tools to easily apply your\nmodels in a production environment.\n\nYou can serve a model by running,\n\n```bash\nmlflow rfunc serve model\n```\n\nwhich is equivalent to\nrunning,\n\n```bash\nRscript -e "mlflow_rfunc_serve(\'model\')"\n```\n\n<img src="tools/readme/mlflow-serve-rfunc.png" class="screenshot" width=520 />\n\nYou can also run:\n\n```bash\nmlflow rfunc predict model data.json\n```\n\nwhich is equivalent to running,\n\n```bash\nRscript -e "mlflow_rfunc_predict(\'model\', \'data.json\')"\n```\n\n## Dependencies\n\nWhen running a project, `mlflow_snapshot()` is automatically called to\ngenerate a `r-dependencies.txt` file which contains a list of required\npackages and versions.\n\nHowever, restoring dependencies is not automatic since it\'s usually an\nexpensive operation. To restore dependencies run:\n\n```r\nmlflow_restore_snapshot()\n```\n\nNotice that the `MLFLOW_SNAPSHOT_CACHE` environment variable can be set\nto a cache directory to improve the time required to restore\ndependencies.\n\n## RStudio\n\nTo enable fast iteration while tracking with MLflow improvements over a\nmodel, [RStudio 1.2.897](https://dailies.rstudio.com/) an ', 'production environment.\n\nYou can serve a model by running,\n\n```bash\nmlflow rfunc serve model\n```\n\nwhich is equivalent to\nrunning,\n\n```bash\nRscript -e "mlflow_rfunc_serve(\'model\')"\n```\n\n<img src="tools/readme/mlflow-serve-rfunc.png" class="screenshot" width=520 />\n\nYou can also run:\n\n```bash\nmlflow rfunc predict model data.json\n```\n\nwhich is equivalent to running,\n\n```bash\nRscript -e "mlflow_rfunc_predict(\'model\', \'data.json\')"\n```\n\n## Dependencies\n\nWhen running a project, `mlflow_snapshot()` is automatically called to\ngenerate a `r-dependencies.txt` file which contains a list of required\npackages and versions.\n\nHowever, restoring dependencies is not automatic since it\'s usually an\nexpensive operation. To restore dependencies run:\n\n```r\nmlflow_restore_snapshot()\n```\n\nNotice that the `MLFLOW_SNAPSHOT_CACHE` environment variable can be set\nto a cache directory to improve the time required to restore\ndependencies.\n\n## RStudio\n\nTo enable fast iteration while tracking with MLflow improvements over a\nmodel, [RStudio 1.2.897](https://dailies.rstudio.com/) an be configured\nto automatically trigger `mlflow_run()` when sourced. This is enabled by\nincluding a `# !source mlflow::mlflow_run` comment at the top of the R\nscript as\nfollows:\n\n<img src="tools/readme/mlflow-source-rstudio.png" class="screenshot" width=520 />\n\n## Contributing\n\nSee the [MLflow contribution guidelines](https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md).\n', '# CLAUDE.md - MLflow Frontend Development\n\nThis file provides guidance to Claude Code when working with the MLflow frontend code in this directory.\n\n**For contribution guidelines, code standards, and additional development information not covered here, please refer to [CONTRIBUTING.md](../../../CONTRIBUTING.md).**\n\n## Consistency is Critical\n\n**IMPORTANT**: Always be consistent with the rest of the repository. This is extremely important!\n\nBefore implementing any feature:\n1. Read through similar files to understand their structure and patterns\n2. Do NOT invent new components if they already exist\n3. Use existing patterns and conventions found in the codebase\n4. Check for similar functionality that already exists\n\n## Development Server\n\n**IMPORTANT**: Always start the development server from the repository root for the best development experience with hot reload:\n\n```bash\n# MUST be run from the repository root\nnohup uv run bash dev/run-dev-server.sh > /tmp/mlflow-dev-server.log 2>&1 &\n\n# Monitor the logs\ntail -f /tmp/mlflow-dev-server.log\n\n# Servers will be available at:\n# - MLflow backend: http://localhost:5000\n# - React frontend: http://localhost:3000 (with hot reload)\n```\n\nThis provides fast edit-refresh for UI development - changes to React components will automatically reload in the browser.\n\n## Available Yarn Scripts\n\nWhen running from the repository root, use this pattern:\n\n```bash\n# Example: Run any yarn command from root\npushd mlflow/server/js && yarn <command>; popd\n```\n\nAvailable scripts:\n\n```bash\n# Development\nyarn start              # Start dev server (port 3000) with hot reload\nyarn build              # Build production bundle\n\n# Testing\nyarn test               # Run Jest tests\nyarn test:watch         # Run tests in watch mode\nyarn test:ci            # Run tests with coverage for CI\n\n# Code Quality\nyarn lint               # Run ESLint\nyarn lint:fix           # Run ESLint with auto-fix\nyarn prettier:check     # Check Prettier formatting\nyarn prettier:fix       # Fix Prettier formatting\nyarn type-check         # Run ', "        # Run tests in watch mode\nyarn test:ci            # Run tests with coverage for CI\n\n# Code Quality\nyarn lint               # Run ESLint\nyarn lint:fix           # Run ESLint with auto-fix\nyarn prettier:check     # Check Prettier formatting\nyarn prettier:fix       # Fix Prettier formatting\nyarn type-check         # Run TypeScript type checking\n\n# Combined Checks\nyarn check-all          # Run all checks (lint, prettier, i18n, type-check)\n\n# Other Commands\nyarn storybook          # Start Storybook for component development\nyarn build-storybook    # Build static Storybook\nyarn i18n:check         # Check i18n translations\n```\n\n### Before Committing\n\n**IMPORTANT**: Always run these checks and fix any remaining issues before committing:\n\n```bash\n# From repository root\npushd mlflow/server/js && yarn check-all; popd\n\n# Fix any issues that are reported\n```\n\n## UI Components and Design System\n\n### Use Databricks Design System Components\n\n**Always use components from `@databricks/design-system` when available.** Do not create custom components if they already exist in the design system.\n\nCommon components include:\n\n- `Button`, `IconButton` - for actions\n- `Input`, `Textarea`, `Select` - for form inputs  \n- `Modal`, `Drawer` - for overlays\n- `Table`, `TableRow`, `TableCell` - for data tables\n- `Tabs`, `TabPane` - for tabbed interfaces\n- `Alert`, `Notification` - for feedback\n- `Spinner`, `Skeleton` - for loading states\n- `Tooltip`, `Popover` - for additional information\n- `Card` - for content containers\n- `Typography` - for text styling\n\nExample import:\n\n```typescript\nimport { Button, Modal, Input } from '@databricks/design-system';\n```\n\n### Theme Usage\n\nUse the design system theme for consistent styling:\n\n```typescript\nimport { useDesignSystemTheme } from '@databricks/design-system';\n\nconst Component = () => {\n  const { theme } = useDesignSystemTheme();\n  \n  return (\n    <div style={{ \n      color: theme.colors.textPrimary,\n      padding: theme.spacing.md,\n      fontSize: theme.typography.fontSizeBase\n    }}>\n  ", 'tabbed interfaces\n- `Alert`, `Notification` - for feedback\n- `Spinner`, `Skeleton` - for loading states\n- `Tooltip`, `Popover` - for additional information\n- `Card` - for content containers\n- `Typography` - for text styling\n\nExample import:\n\n```typescript\nimport { Button, Modal, Input } from \'@databricks/design-system\';\n```\n\n### Theme Usage\n\nUse the design system theme for consistent styling:\n\n```typescript\nimport { useDesignSystemTheme } from \'@databricks/design-system\';\n\nconst Component = () => {\n  const { theme } = useDesignSystemTheme();\n  \n  return (\n    <div style={{ \n      color: theme.colors.textPrimary,\n      padding: theme.spacing.md,\n      fontSize: theme.typography.fontSizeBase\n    }}>\n      Content\n    </div>\n  );\n};\n```\n\n### Spacing Guidelines\n\n**ALWAYS use `theme.spacing` values instead of hard-coded pixel widths.** This ensures consistency and maintainability across the application.\n\n```typescript\n// âœ… GOOD - Use theme spacing\n<div style={{ \n  padding: theme.spacing.md,\n  marginBottom: theme.spacing.lg,\n  gap: theme.spacing.sm \n}} />\n\n// âŒ BAD - Avoid hard-coded pixels\n<div style={{ \n  padding: \'16px\',\n  marginBottom: \'24px\',\n  gap: \'8px\'\n}} />\n```\n\nCommon spacing values:\n- `theme.spacing.xs` - Extra small spacing (4px)\n- `theme.spacing.sm` - Small spacing (8px)\n- `theme.spacing.md` - Medium spacing (16px)\n- `theme.spacing.lg` - Large spacing (24px)\n- `theme.spacing.xl` - Extra large spacing (32px)\n\nFor custom spacing needs, use the spacing function:\n```typescript\n// When you need a specific multiple of the base unit\npadding: theme.spacing(2.5) // 20px (2.5 * 8px base unit)\n```\n\n### Finding the Right Component\n\nWhen looking for a component:\n\n1. First check `@databricks/design-system` imports in existing code\n2. Component names may not be exact (e.g., "dropdown" could be `Select`, `DialogCombobox`, or `DropdownMenu`)\n3. Look at similar UI patterns in the codebase for examples\n4. If multiple matches exist, choose based on the use case\n\n### Discovering Available Components Dynamically\n\nTo see ALL components available in the design system:\n\n```bash\n# From mlflow/server/js directory, check what\'s exported\ncat node_modules/@databricks/design-system/dist/design-system/index.d.ts\n\n# This file lists every component as: export * from \'./ComponentName\';\n# Each line represents a component you can import\n```\n\nThis is the definitive source for available components - more reliable than checking folders since it shows only what\'s publicly exported.\n\n### Viewing Component Documentation in Storybook\n\nYou can use Playwright to view the component documentation and examples in Storybook:\n\n```\nhttps://ui-infra.dev.databricks.com/storybook/js/packages/du-bois/index.html?path=/docs/primitives-<component-name>--docs\n```\n\nFor example:\n- Alert: `https://ui-infra.dev.databricks.com/storybook/js/packages/du-bois/index.html?path=/docs/primitives-alert--docs`\n- Button: `https://ui-infra.dev.databricks.com/storybook/js/packages/du-bois/index.html?path=/docs/primitives-button--docs`\n- Modal: `https://ui-infra.dev.databricks.com/storybook/js/packages/du-bois/index.html?path=/docs/primitives-modal--docs`\n\nUse ', "at similar UI patterns in the codebase for examples\n4. If multiple matches exist, choose based on the use case\n\n### Discovering Available Components Dynamically\n\nTo see ALL components available in the design system:\n\n```bash\n# From mlflow/server/js directory, check what's exported\ncat node_modules/@databricks/design-system/dist/design-system/index.d.ts\n\n# This file lists every component as: export * from './ComponentName';\n# Each line represents a component you can import\n```\n\nThis is the definitive source for available components - more reliable than checking folders since it shows only what's publicly exported.\n\n### Viewing Component Documentation in Storybook\n\nYou can use Playwright to view the component documentation and examples in Storybook:\n\n```\nhttps://ui-infra.dev.databricks.com/storybook/js/packages/du-bois/index.html?path=/docs/primitives-<component-name>--docs\n```\n\nFor example:\n- Alert: `https://ui-infra.dev.databricks.com/storybook/js/packages/du-bois/index.html?path=/docs/primitives-alert--docs`\n- Button: `https://ui-infra.dev.databricks.com/storybook/js/packages/du-bois/index.html?path=/docs/primitives-button--docs`\n- Modal: `https://ui-infra.dev.databricks.com/storybook/js/packages/du-bois/index.html?path=/docs/primitives-modal--docs`\n\nUse Playwright MCP to navigate to these URLs and see live examples, props documentation, and usage patterns.\n\n## Browser Testing with Playwright\n\nFor testing UI changes in a real browser, Claude Code can use the Playwright MCP (Model Context Protocol) integration.\n\n### Checking Playwright MCP Status\n\nTo check if Playwright MCP is available:\n\n- Look for browser testing tools in available MCP functions\n- Try using browser navigation or screenshot capabilities\n\n### Installing Playwright MCP\n\nIf Playwright MCP is not available and you need to test UI changes, you can install it:\n\n```bash\nclaude mcp add playwright npx '@playwright/mcp@latest'\n```\n\n**Note**: After installation, you must restart Claude Code for the integration to be available.\n\n### Using Playwright MCP\n\nOnce installed, you can:\n\n- Navigate to the development server\n- Take screenshots of UI components\n- Interact with forms and buttons\n- Verify UI changes are working correctly\n\nExample workflow:\n\n1. Make changes to React components\n2. Wait for hot reload (automatic)\n3. Use Playwright to navigate to `http://localhost:3000`\n4. Take screenshots or interact with the updated UI\n5. Verify the changes work as expected\n\n## Project Structure\n\n```text\nmlflow/server/js/\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ experiment-tracking/    # Experiment tracking UI\nâ”‚   â”œâ”€â”€ model-registry/         # Model registry UI  \nâ”‚   â”œâ”€â”€ common/                 # Shared components\nâ”‚   â”œâ”€â”€ shared/                 # Shared utilities\nâ”‚   â””â”€â”€ app.tsx          ", "the updated UI\n5. Verify the changes work as expected\n\n## Project Structure\n\n```text\nmlflow/server/js/\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ experiment-tracking/    # Experiment tracking UI\nâ”‚   â”œâ”€â”€ model-registry/         # Model registry UI  \nâ”‚   â”œâ”€â”€ common/                 # Shared components\nâ”‚   â”œâ”€â”€ shared/                 # Shared utilities\nâ”‚   â””â”€â”€ app.tsx                # Main app entry point\nâ”œâ”€â”€ vendor/                     # Third-party dependencies\nâ”œâ”€â”€ package.json               # Dependencies and scripts\nâ”œâ”€â”€ tsconfig.json              # TypeScript configuration\nâ”œâ”€â”€ webpack.config.js          # Webpack bundler config\nâ””â”€â”€ jest.config.js             # Jest test configuration\n```\n\n## Key Technologies\n\n- **React 18**: UI framework\n- **TypeScript**: Type safety\n- **Redux**: State management\n- **Apollo Client**: GraphQL client\n- **Ant Design (antd)**: UI component library\n- **AG-Grid**: Data table component\n- **Jest**: Testing framework\n- **React Testing Library**: Component testing\n- **Webpack**: Module bundler\n\n## Common Tasks\n\n### Adding a New Component\n\n1. Create component file in appropriate directory\n2. Add TypeScript types/interfaces\n3. Write component with hooks (functional components preferred)\n4. Add unit tests in same directory with `.test.tsx` extension\n5. Add to Storybook if it's a reusable component\n\n### Updating GraphQL Queries\n\n1. Modify query in relevant `.graphql` file\n2. Run codegen to update TypeScript types (if configured)\n3. Update components using the query\n\n### Testing Components\n\n```bash\n# Run tests for a specific component\nyarn test ComponentName\n\n# Run tests in watch mode for development\nyarn test --watch\n\n# Update snapshots if needed\nyarn test -u\n```\n\n### Debugging\n\n1. Use React Developer Tools browser extension\n2. Redux DevTools for state debugging\n3. Browser console for network requests\n4. Source maps are enabled in development mode\n\n## Code ", "directory\n2. Add TypeScript types/interfaces\n3. Write component with hooks (functional components preferred)\n4. Add unit tests in same directory with `.test.tsx` extension\n5. Add to Storybook if it's a reusable component\n\n### Updating GraphQL Queries\n\n1. Modify query in relevant `.graphql` file\n2. Run codegen to update TypeScript types (if configured)\n3. Update components using the query\n\n### Testing Components\n\n```bash\n# Run tests for a specific component\nyarn test ComponentName\n\n# Run tests in watch mode for development\nyarn test --watch\n\n# Update snapshots if needed\nyarn test -u\n```\n\n### Debugging\n\n1. Use React Developer Tools browser extension\n2. Redux DevTools for state debugging\n3. Browser console for network requests\n4. Source maps are enabled in development mode\n\n## Code Style\n\n- Use functional components with hooks\n- Prefer TypeScript strict mode\n- Follow existing patterns in the codebase\n- Use meaningful component and variable names\n- Add JSDoc comments for complex logic\n- Keep components small and focused\n\n## Best Practices\n\n### Data Fetching\n\n**Use React Query** for all API calls and data fetching:\n\n```typescript\n// Good: Using React Query\nconst { data, isLoading, error } = useQuery({\n  queryKey: ['experiments', experimentId],\n  queryFn: () => fetchExperiment(experimentId),\n});\n\n// Avoid: Manual fetch in useEffect\n// useEffect(() => { fetch(...) }, [])\n```\n\n### State Management\n\n**Avoid useEffect** when possible. Prefer deriving state with `useMemo`:\n\n```typescript\n// Good: Derive state with useMemo\nconst filteredRuns = useMemo(() => {\n  return runs.filter(run => run.status === 'active');\n}, [runs]);\n\n// Avoid: useEffect to update state\n// useEffect(() => {\n//   setFilteredRuns(runs.filter(run => run.status === 'active'));\n// }, [runs]);\n```\n\nUse `useEffect` only for:\n\n- Side effects (DOM manipulation, subscriptions)\n- Synchronizing with external systems\n- Cleanup operations\n\n## Performance Considerations\n\n- Use React.memo for expensive components\n- Implement virtualization for large lists (AG-Grid handles this)\n- Lazy load routes and heavy components\n", "# Jupter Notebook Trace UI Renderer\n\nThis directory contains a standalone notebook renderer that is built as a separate entry point from the main MLflow application.\n\n## Architecture\n\nThe notebook renderer is configured as a separate webpack entry point that generates its own HTML file and JavaScript bundle, completely independent of the main MLflow application.\n\n### Build Configuration\n\nThe webpack configuration in `craco.config.js` handles the dual-entry setup:\n\n1. **Entry Points**:\n\n   - `main`: The main MLflow application (`src/index.tsx`)\n   - `ml-model-trace-renderer`: The notebook renderer (`src/shared/web-shared/model-trace-explorer/oss-notebook-renderer/index.ts`)\n\n2. **Output Structure**:\n\n   ```\n   build/\n   â”œâ”€â”€ index.html                           # Main app HTML (excludes notebook renderer)\n   â”œâ”€â”€ static/js/main.[hash].js             # Main app bundle\n   â”œâ”€â”€ static/css/main.[hash].css           # Main app styles\n   â””â”€â”€ lib/notebook-trace-renderer/\n       â”œâ”€â”€ index.html                       # Notebook renderer HTML\n       â””â”€â”€ js/ml-model-trace-renderer.[hash].js  # Notebook renderer bundle\n   ```\n\n3. **Path Resolution**:\n   - Main app uses relative paths: `static-files/static/js/...`\n   - Notebook renderer uses absolute paths: `/static-files/lib/notebook-trace-renderer/js/...`\n   - Dynamic chunks use absolute paths: `/static-files/static/...` (via `__webpack_public_path__`)\n\n### Key Configuration Details\n\n#### Separate Entry Configuration\n\n```javascript\nwebpackConfig.entry = {\n  main: webpackConfig.entry, // Preserve original entry as 'main'\n  'ml-model-trace-renderer': path.resolve(\n    __dirname,\n    'src/shared/web-shared/model-trace-explorer/oss-notebook-renderer/index.ts',\n  ),\n};\n```\n\n#### Output Path Functions\n\n```javascript\nwebpackConfig.output = {\n  filename: (pathData) => {\n    return pathData.chunk.name === 'ml-model-trace-renderer'\n      ? 'lib/notebook-trace-renderer/js/[name].[contenthash].js'\n      : 'static/js/[name].[contenthash:8].js';\n  },\n  // ... similar for chunkFilename\n};\n```\n\n#### HTML Plugin Configuration\n\n- **Main app**: Excludes notebook renderer chunks via `excludeChunks: ['ml-model-trace-renderer']`\n- **Notebook renderer**: Includes only its own chunks via `chunks: ['ml-model-trace-renderer']`\n\n#### Runtime Path Override\n\nThe notebook renderer sets `__webpack_public_path__ = '/static-files/'` at ", "Configuration Details\n\n#### Separate Entry Configuration\n\n```javascript\nwebpackConfig.entry = {\n  main: webpackConfig.entry, // Preserve original entry as 'main'\n  'ml-model-trace-renderer': path.resolve(\n    __dirname,\n    'src/shared/web-shared/model-trace-explorer/oss-notebook-renderer/index.ts',\n  ),\n};\n```\n\n#### Output Path Functions\n\n```javascript\nwebpackConfig.output = {\n  filename: (pathData) => {\n    return pathData.chunk.name === 'ml-model-trace-renderer'\n      ? 'lib/notebook-trace-renderer/js/[name].[contenthash].js'\n      : 'static/js/[name].[contenthash:8].js';\n  },\n  // ... similar for chunkFilename\n};\n```\n\n#### HTML Plugin Configuration\n\n- **Main app**: Excludes notebook renderer chunks via `excludeChunks: ['ml-model-trace-renderer']`\n- **Notebook renderer**: Includes only its own chunks via `chunks: ['ml-model-trace-renderer']`\n\n#### Runtime Path Override\n\nThe notebook renderer sets `__webpack_public_path__ = '/static-files/'` at runtime to ensure dynamically loaded chunks use the correct absolute paths.\n\n## Files\n\n- `index.ts`: Entry point that sets webpack public path and bootstraps the renderer\n- `bootstrap.tsx`: Main renderer component\n- `index.html`: HTML template for the standalone renderer\n- `index.css`: Styles for the renderer\n\n## Usage\n\nThe notebook renderer is built automatically as part of the main build process:\n\n```bash\nyarn build\n```\n\nThis generates both the main application and the standalone notebook renderer, accessible at:\n\n- Main app: `/static-files/index.html`\n- Notebook renderer: `/static-files/lib/notebook-trace-renderer/index.html`\n\n## Development Notes\n\n- The renderer is completely independent of the main app - no shared runtime dependencies\n- Uses absolute paths to avoid complex relative path calculations\n- Webpack code splitting works correctly for both entry points\n- CSS extraction is configured separately for each entry point\n", "# MLflow Tracking database migrations\n\nThis directory contains configuration scripts and database migration logic for MLflow tracking\ndatabases, using the Alembic migration library (https://alembic.sqlalchemy.org). To run database\nmigrations, use the `mlflow db upgrade` CLI command. To add and modify database migration logic,\nsee the contributor guide at https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md.\n\nIf you encounter failures while executing migrations, please file a GitHub issue at\nhttps://github.com/mlflow/mlflow/issues.\n\n## Migration descriptions\n\n### 89d4b8295536_create_latest_metrics_table\n\nThis migration creates a `latest_metrics` table and populates it with the latest metric entry for\neach unique `(run_id, metric_key)` tuple. Latest metric entries are computed based on `step`,\n`timestamp`, and `value`.\n\nThis migration may take a long time for databases containing a large number of metric entries. You\ncan determine the total number of metric entries using the following query:\n\n```sql\nSELECT count(*) FROM metrics GROUP BY metrics.key, run_uuid;\n```\n\nAdditionally, query join latency during the migration increases with the number of unique\n`(run_id, metric_key)` tuples. You can determine the total number of unique tuples using\nthe following query:\n\n```sql\nSELECT count(*) FROM (\n   SELECT metrics.key, run_uuid FROM metrics GROUP BY run_uuid, metrics.key\n) unique_metrics;\n```\n\nFor reference, migrating a Tracking database with the following attributes takes roughly\n**three seconds** on MySQL 5.7:\n\n- `3702` unique metrics\n- `466860` total metric entries\n- `186` runs\n- An average of `125` entries per unique metric\n\n#### Recovering from a failed migration\n\nIf the **create_latest_metrics_table** migration fails, simply delete the `latest_metrics`\ntable from your Tracking database as follows:\n\n```sql\nDROP TABLE latest_metrics;\n```\n\nAlembic does not stamp the database with an updated version unless the corresponding migration\ncompletes successfully. Therefore, when this migration fails, the database remains on the\nprevious version, and deleting the `latest_metrics` table is sufficient to restore the database\nto its prior state.\n\nIf the migration fails to complete due to excessive latency, please try executing the\n`mlflow db upgrade` command on the same host machine where the database is running. This will\nreduce the overhead of the migration's queries and batch insert operation.\n", '# Instructions\n\nThis directory contains files to test MLflow tracking operations using the following databases:\n\n- PostgreSQL\n- MySQL\n- Microsoft SQL Server\n- SQLite\n\n## Prerequisites\n\n- Docker\n- Docker Compose V2\n\n## Build Services\n\n```bash\n# Build a service\nservice=mlflow-sqlite\n./tests/db/compose.sh build --build-arg DEPENDENCIES="$(python dev/extract_deps.py)" $service\n\n# Build all services\n./tests/db/compose.sh build --build-arg DEPENDENCIES="$(python dev/extract_deps.py)"\n```\n\n## Run Services\n\n```bash\n# Run a service (`pytest tests/db` is executed by default)\n./tests/db/compose.sh run --rm $service\n\n# Run all services\nfor service in $(./tests/db/compose.sh config --services | grep \'^mlflow-\')\ndo\n  ./tests/db/compose.sh run --rm "$service"\ndone\n\n# Run tests\n./tests/db/compose.sh run --rm $service pytest /path/to/directory/or/script\n\n# Run a python script\n./tests/db/compose.sh run --rm $service python /path/to/script\n```\n\n## Clean Up Services\n\n```bash\n# Clean up containers, networks, and volumes\n./tests/db/compose.sh down --volumes --remove-orphans\n\n# Clean up containers, networks, volumes, and images\n./tests/db/compose.sh down --volumes --remove-orphans --rmi all\n```\n\n## Other Useful Commands\n\n```bash\n# View database logs\n./tests/db/compose.sh logs --follow <database service>\n```\n', '# Adding `examples` unit tests to `pytest` test suite\n\nTwo types of test runs for code in `examples` directory are supported:\n\n- Examples run by `mlflow run`\n- Examples run by another command, such as the `python` interpreter\n\nEach of these types of runs are implemented using `@pytest.mark.parametrize` decorator. Adding a new\nexample to test involves updating the decorator list as described below.\n\nFor purpose of discussion, `new_example_dir` designates the\ndirectory the example code is found, i.e., it is located in `examples/new_example_dir`.\n\n## Examples that utilize `mlflow run` construct\n\nThe `@pytest.mark.mark.parametrize` decorator for `def test_mlflow_run_example(directory, params):`\nis updated.\n\nIf the example is executed by `cd examples/new_example_dir && mlflow run . -P param1=99 -P param2=3`, then\nthis `tuple` is added to the decorator list\n\n```\n("new_example_dir", ["-P", "param1=123", "-P", "param2=99"])\n```\n\nas shown below\n\n```\n@pytest.mark.parametrize(("directory", "params"), [\n    ("sklearn_elasticnet_wine", ["-P", "alpha=0.5"]),\n    (os.path.join("sklearn_elasticnet_diabetes", "linux"), []),\n    ("new_example_dir", ["-P", "param1=123", "-P", "param2=99"]),\n])\ndef test_mlflow_run_example(directory, params):\n```\n\nThe `tuple` for an example requiring no parameters is simply:\n\n```\n("new_example_dir", []),\n```\n\n## Examples that are executed with another command\n\nFor an example that is not run by `mlflow run`, the list in\n`@pytest.mark.parametrize` decorator for `test_command_example(tmpdir, directory, command):` is updated.\n\nExamples invoked by `cd examples/new_example_dir && python train.py` require this tuple added\nto the decorator\'s list\n\n```\n("new_example_dir", ["python", "train.py"]),\n```\n\nas shown below\n\n```\n@pytest.mark.parametrize(("directory", "command"), [\n    (\'sklearn_logistic_regression\', [\'python\', \'train.py\']),\n    (\'h2o\', [\'python\', \'random_forest.py\']),\n    (\'quickstart\', [\'python\', \'mlflow_tracking.py\']),\n    ("new_example_dir", ["python", "train.py"]),\n])\ndef test_command_example(tmpdir, directory, command):\n```\n\nIf the example requires arguments to run, i.e., `python train.py arg1 arg2`, then the\ntuple would look like this\n\n```\n(\'new_example_dir\', [\'python\', \'train.py\', \'arg1\', \'arg2\'])\n```\n', '# Historical Pyfunc Models\n\nThese serialized model files are used in backwards compatibility tests, so we can ensure that models logged with old versions of MLflow are still able to be loaded in newer versions.\n\nThese files were created by running the following:\n\n1. First, install the desired MLflow version with `$ pip install mlflow=={version_number}`\n2. Next, run the following script from MLflow root:\n\n```python\nimport mlflow\n\n\nclass MyModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input):\n        return model_input\n\n\nmodel = MyModel()\n\nmlflow.pyfunc.save_model(\n    python_model=model,\n    path=f"tests/resources/pyfunc_models/{mlflow.__version__}",\n)\n```\n', 'Copyright 2018 Databricks, Inc.  All rights reserved.\n\n\t\t\t\tApache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      "License" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      "Licensor" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      "Legal Entity" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      "control" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      "You" (or "Your") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      "Source" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      "Object" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, ', '  "You" (or "Your") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      "Source" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      "Object" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      "Work" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      "Derivative Works" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      "Contribution" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or ', 'include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      "Contribution" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, "submitted"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as "Not a Contribution."\n\n      "Contributor" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such ', '     on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n  ', '     cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a "NOTICE" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n     ', 'those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a "NOTICE" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n    ', '        notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an "AS IS" ', '6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an "AS IS" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of ', 'to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets "[]"\n      replaced with your own identifying information. (Don\'t include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class ', 'END OF TERMS AND CONDITIONS\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets "[]"\n      replaced with your own identifying information. (Don\'t include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same "printed page" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the "License");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an "AS IS" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n', '# Dev script dependencies\nclick\nruamel.yaml.clib!=0.2.7\nruamel.yaml\nrequests\npackaging\npydantic\npyyaml\ntoml\n', 'https://mlflow.org/docs/latest/auth/index.html\nhttps://mlflow.org/docs/latest/auth/python-api.html\nhttps://mlflow.org/docs/latest/cli.html\nhttps://mlflow.org/docs/latest/deep-learning/keras/quickstart/quickstart_keras.html\nhttps://mlflow.org/docs/latest/deep-learning/pytorch/guide/index.html\nhttps://mlflow.org/docs/latest/deep-learning/tensorflow/guide/index.html\nhttps://mlflow.org/docs/latest/deployment/index.html\nhttps://mlflow.org/docs/latest/getting-started/intro-quickstart/index.html\nhttps://mlflow.org/docs/latest/index.html\nhttps://mlflow.org/docs/latest/introduction/index.html\nhttps://mlflow.org/docs/latest/llms/custom-pyfunc-for-llms/index.html\nhttps://mlflow.org/docs/latest/llms/custom-pyfunc-for-llms/notebooks/custom-pyfunc-advanced-llm.html\nhttps://mlflow.org/docs/latest/llms/custom-pyfunc-for-llms/notebooks/index.html\nhttps://mlflow.org/docs/latest/llms/deployments/guides/index.html\nhttps://mlflow.org/docs/latest/llms/deployments/guides/step1-create-deployments.html\nhttps://mlflow.org/docs/latest/llms/deployments/guides/step2-query-deployments.html\nhttps://mlflow.org/docs/latest/llms/deployments/index.html\nhttps://mlflow.org/docs/latest/llms/deployments/uc_integration.html\nhttps://mlflow.org/docs/latest/llms/index.html\nhttps://mlflow.org/docs/latest/llms/langchain/autologging.html\nhttps://mlflow.org/docs/latest/llms/langchain/guide/index.html\nhttps://mlflow.org/docs/latest/llms/langchain/index.html\nhttps://mlflow.org/docs/latest/llms/langchain/notebooks/langchain-quickstart.html\nhttps://mlflow.org/docs/latest/llms/llama-index/index.html\nhttps://mlflow.org/docs/latest/llms/llm-evaluate/index.html\nhttps://mlflow.org/docs/latest/llms/openai/guide/index.html\nhttps://mlflow.org/docs/latest/llms/openai/index.html\nhttps://mlflow.org/docs/latest/llms/sentence-transformers/guide/index.html\nhttps://mlflow.org/docs/latest/llms/sentence-transformers/index.html\nhttps://mlflow.org/docs/latest/llms/tracing/index.html\nhttps://mlflow.org/docs/latest/llms/tracing/overview.html\nhttps://mlflow.org/docs/latest/llms/transformers/index.html\nhttps://mlflow.org/docs/latest/model-evaluation/index.html\nhttps://mlflow.org/docs/latest/model-registry.html\nhttps://mlflow.org/docs/latest/model/dependencies.html\nhttps://mlflow.org/docs/latest/model/notebooks/signature_examples.html\nhttps://mlflow.org/docs/latest/model/signatures.html\nhttps://mlflow.org/docs/latest/models.html\nhttps://mlflow.org/docs/latest/python_api/index.html\nhttps://mlflow.org/docs/latest/rest-api.html\nhttps://mlflow.org/docs/latest/system-metrics/index.html\nhttps://mlflow.org/docs/latest/tracking.html\nhttps://mlflow.org/docs/latest/tracking/artifacts-stores.html\nhttps://mlflow.org/docs/latest/tracking/autolog.html\nhttps://mlflow.org/docs/latest/tracking/backend-stores.html\nhttps://mlflow.org/docs/latest/tracking/data-api.html\nhttps://mlflow.org/docs/latest/tracking/server.html\nhttps://mlflow.org/docs/latest/tracking/tracking-api.html\nhttps://mlflow.org/docs/latest/tracking/tutorials/local-database.html\nhttps://mlflow.org/docs/latest/tracking/tutorials/remote-server.html', 'promptflow\njinja2\n', '../../LICENSE.txt', '../../LICENSE.txt', '# classification\npyspark.ml.classification.LinearSVCModel\npyspark.ml.classification.DecisionTreeClassificationModel\npyspark.ml.classification.GBTClassificationModel\npyspark.ml.classification.LogisticRegressionModel\npyspark.ml.classification.RandomForestClassificationModel\npyspark.ml.classification.NaiveBayesModel\n\n# clustering\npyspark.ml.clustering.BisectingKMeansModel\npyspark.ml.clustering.KMeansModel\npyspark.ml.clustering.GaussianMixtureModel\n\n# Regression\npyspark.ml.regression.AFTSurvivalRegressionModel\npyspark.ml.regression.DecisionTreeRegressionModel\npyspark.ml.regression.GBTRegressionModel\npyspark.ml.regression.GeneralizedLinearRegressionModel\npyspark.ml.regression.LinearRegressionModel\npyspark.ml.regression.RandomForestRegressionModel\n\n# Featurizer model\npyspark.ml.feature.BucketedRandomProjectionLSHModel\npyspark.ml.feature.ChiSqSelectorModel\npyspark.ml.feature.CountVectorizerModel\npyspark.ml.feature.IDFModel\npyspark.ml.feature.ImputerModel\npyspark.ml.feature.MaxAbsScalerModel\npyspark.ml.feature.MinHashLSHModel\npyspark.ml.feature.MinMaxScalerModel\npyspark.ml.feature.OneHotEncoderModel\npyspark.ml.feature.RobustScalerModel\npyspark.ml.feature.RFormulaModel\npyspark.ml.feature.StandardScalerModel\npyspark.ml.feature.StringIndexerModel\npyspark.ml.feature.VarianceThresholdSelectorModel\npyspark.ml.feature.VectorIndexerModel\npyspark.ml.feature.UnivariateFeatureSelectorModel\n\n# composite model\npyspark.ml.classification.OneVsRestModel\n\n# pipeline model\npyspark.ml.pipeline.PipelineModel\n\n# Hyper-parameter tuning\npyspark.ml.tuning.CrossValidatorModel\npyspark.ml.tuning.TrainValidationSplitModel\n\n# SynapeML models\nsynapse.ml.cognitive.*\nsynapse.ml.exploratory.*\nsynapse.ml.featurize.*\nsynapse.ml.geospatial.*\nsynapse.ml.image.*\nsynapse.ml.io.*\nsynapse.ml.isolationforest.*\nsynapse.ml.lightgbm.*\nsynapse.ml.nn.*\nsynapse.ml.opencv.*\nsynapse.ml.stages.*\nsynapse.ml.vw.*\n', 'pytest==8.4.0\n# transformers 4.51.0 has this issue:\n# https://github.com/huggingface/transformers/issues/37326\ntransformers!=4.51.0\n# https://github.com/BerriAI/litellm/issues/10373\nlitellm!=1.67.4\n# https://github.com/run-llama/llama_index/issues/18587\nllama-index-core!=0.12.34\n# https://github.com/mangiucugna/json_repair/issues/124\njson_repair!=0.45.0\n# https://github.com/huggingface/transformers/issues/38269\ntransformers!=4.52.2\ntransformers!=4.52.1\n# TODO(https://github.com/mlflow/mlflow/issues/15847): Remove this constraint when MLflow is ready for pyspark 4.0.0. Pyspark 3.5.6 has the same issue.\npyspark<3.5.6\n', '-r extra-ml-requirements.txt\n-r test-requirements.txt\n-r lint-requirements.txt\n-r doc-requirements.txt\n', '# Minimum version that works with Python 3.10\nsphinx==4.2.0\njinja2==3.0.3\n# to be compatible with jinja2==3.0.3\nflask<=2.2.5\nsphinx-autobuild\nsphinx-click\n# to be compatible with docutils==0.16\nsphinx-tabs==3.2.0\n# redirect handling\nsphinx-reredirects==0.1.3\n# Pin sphinxcontrib packages. Their newer versions are incompatible with sphinx==4.2.0.\nsphinxcontrib-applehelp<1.0.8\nsphinxcontrib-devhelp<1.0.6\nsphinxcontrib-htmlhelp<2.0.4\nsphinxcontrib-serializinghtml<1.1.10\nsphinxcontrib-qthelp<1.0.7\n', '-r doc-min-requirements.txt\ntensorflow-cpu<=2.12.0; platform_system!="Darwin" or platform_machine!="arm64"\ntensorflow-macos<=2.12.0; platform_system=="Darwin" and platform_machine=="arm64"\npyspark\ndatasets\n# nbsphinx and ipython are required for jupyter notebook rendering\nnbsphinx==0.8.8\n# ipython 8.7.0 is an incompatible release\nipython!=8.7.0\nkeras\ntorch>=1.11.0\ntorchvision>=0.12.0\nlightning>=1.8.1\nscrapy\nipywidgets>=8.1.1\n# incremental==24.7.0 requires setuptools>=61.0, which causes https://github.com/mlflow/mlflow/issues/8635\nincremental<24.7.0\n# this is an extra dependency for the auth app which\n# is not included in the core mlflow requirements\nFlask-WTF<2\n# required for testing polars dataset integration\npolars>=1\n# required for the genai evaluation example\nopenai\n', '## This file describes extra ML library dependencies that you, as an end user,\n## must install in order to use various MLflow Python modules.\n# Required by mlflow.spacy\n# TODO: Remove `<3.8` once we bump the minimim supported python version of MLflow to 3.9.\nspacy>=3.3.0,<3.8\n# Required by mlflow.tensorflow\ntensorflow>=2.10.0; platform_system!="Darwin" or platform_machine!="arm64"\ntensorflow-macos>=2.10.0; platform_system=="Darwin" and platform_machine=="arm64"\n# Required by mlflow.pytorch\ntorch>=1.11.0\ntorchvision>=0.12.0\nlightning>=1.8.1\n# Required by mlflow.xgboost\nxgboost>=0.82\n# Required by mlflow.lightgbm\nlightgbm\n# Required by mlflow.catboost\ncatboost\n# Required by mlflow.statsmodels\nstatsmodels\n# Required by mlflow.h2o\nh2o\n# Required by mlflow.onnx\nonnx>=1.11.0\nonnxruntime\ntf2onnx\n# Required by mlflow.spark and using Delta with MLflow Tracking datasets\npyspark\n# Required by mlflow.paddle\npaddlepaddle\n# Required by mlflow.prophet\n# NOTE: Prophet\'s whl build process will fail with dependencies not being present.\n#   Installation will default to setup.py in order to install correctly.\n#   To install in dev environment, ensure that gcc>=8 is installed to allow pystan\n#   to compile the model binaries. See: https://gcc.gnu.org/install/\n# Avoid 0.25 due to https://github.com/dr-prodigy/python-holidays/issues/1200\nholidays!=0.25\nprophet\n# Required by mlflow.shap\n# and shap evaluation functionality\nshap>=0.42.1\n# Required by mlflow.pmdarima\npmdarima\n# Required by mlflow.diviner\ndiviner\n# Required for using Hugging Face datasets with MLflow Tracking\n# Avoid datasets < 2.19.1 due to an incompatibility issue https://github.com/huggingface/datasets/issues/6737\ndatasets>=2.19.1\n# Required by mlflow.transformers\ntransformers\nsentencepiece\nsetfit\nlibrosa\nffmpeg\naccelerate\n# Required by mlflow.openai\nopenai\ntiktoken\ntenacity\n# Required by mlflow.llama_index\nllama_index\n# Required for an agent example of mlflow.llama_index\nllama-index-agent-openai\n# Required by mlflow.langchain\nlangchain\n# Required by mlflow.promptflow\npromptflow\n# Required by mlflow.sentence_transformers\nsentence-transformers\n# Required by mlflow.anthropic\nanthropic\n# Required by mlflow.ag2\nag2\n# Required by mlflow.dspy\n# In dspy 2.6.9, `dspy.__name__` is not \'dspy\', but \'dspy.__metadata__\',\n# which causes auto-logging tests to fail.\ndspy!=2.6.9\n# Required by mlflow.litellm\nlitellm\n# Required by mlflow.gemini\ngoogle-genai\n# Required by mlflow.groq\ngroq\n# Required by mlflow.mistral\nmistralai\n# Required by mlflow.autogen\nautogen-agentchat\n# Required by mlflow.semantic_kernel\nsemantic-kernel\n# Required by mlflow.agno\nagno\n# Required by mlflow.strands\nstrands-agents\n', 'ruff==0.12.10\nblack[jupyter]==23.7.0\nblacken-docs==1.18.0\npre-commit==4.0.1\ntoml==0.10.2\nmypy==1.17.1\npytest==8.4.0\npydantic==2.11.7\n-e ./dev/clint\n', '## Test-only dependencies\npytest\npytest-cov\n', '## Dependencies required to run tests\n# Required for testing utilities for parsing pip requirements\npip>=20.1\n## Test-only dependencies\npytest\npytest-asyncio\npytest-repeat\npytest-cov\npytest-timeout\npytest-localserver==0.5.0\nmoto>=4.2.0,<5,!=4.2.5\nazure-storage-blob>=12.0.0\nazure-storage-file-datalake>=12.9.1\nazure-identity>=1.6.1\npillow\nplotly\nkaleido\n# Required by tuning tests\nhyperopt\n# Required by evaluator tests\nshap\n# Required to evaluate language models in `mlflow.evaluate`\nevaluate\nnltk\nrouge_score\ntextstat\ntiktoken\n# Required by progress bar tests\nipywidgets\ntqdm\n# Required for LLM eval in `mlflow.evaluate`\nopenai\n# Required for showing pytest stats\npsutil\n# SQLAlchemy == 2.0.25 requires typing_extensions >= 4.6.0\ntyping_extensions>=4.6.0\n# Required for importing boto3 ClientError directly for testing\nbotocore>=1.34\npyspark\n# Required for testing the opentelemetry exporter of tracing\nopentelemetry-exporter-otlp-proto-grpc\nopentelemetry-exporter-otlp-proto-http\n# Required for testing mlflow.server.auth\nFlask-WTF<2\n# required for testing polars dataset integration\npolars>=1\n# required for testing mlflow.genai.optimize_prompt\ndspy\n', 'promptflow[azure]\npromptflow-tools\npython-dotenv\n', 'mlflow\ncloudpickle==2.2.1\nscikit-learn==1.5.2\n', 'bcrypt==3.2.0\ncloudpickle==2.0.0\nconfigparser==5.2.0\ncryptography==39.0.1\ndatabricks-feature-engineering==0.2.1\ndatabricks-rag-studio==0.2.0.dev0\nentrypoints==0.4\ngoogle-cloud-storage==2.11.0\ngrpcio-status==1.48.1\nlangchain==0.1.20\nmlflow[gateway]==2.12.2\nnumpy==1.23.5\npackaging==23.2\npandas==1.5.3\nprotobuf==4.24.0\npsutil==5.9.0\npyarrow==8.0.0\npydantic==1.10.6\npyyaml==6.0\nrequests==2.28.1\ntornado==6.1\n', 'bcrypt==3.2.0\ncloudpickle==2.0.0\nconfigparser==5.2.0\ncryptography==39.0.1\ndatabricks-feature-engineering==0.2.1\nentrypoints==0.4\ngoogle-cloud-storage==2.11.0\ngrpcio-status==1.48.1\nlangchain==0.1.20\nmlflow[gateway]==2.12.2\nnumpy==1.23.5\npackaging==23.2\npandas==1.5.3\nprotobuf==4.24.0\npsutil==5.9.0\npyarrow==8.0.0\npydantic==1.10.6\npyyaml==6.0\nrequests==2.28.1\ntornado==6.1\n', 'mlflow\nscikit-learn==1.4.2\n', 'mlflow\nscikit-learn==1.4.2\n', 'mlflow==2.7.1\ncloudpickle==2.2.1\n', 'mlflow==2.8.1\ncloudpickle==2.2.1\n', "MLflow Contributor Covenant Code of Conduct\n===========================================\n\n.. contents:: **Table of Contents**\n  :local:\n  :depth: 4\n\nOur Pledge\n##########\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\nOur Standards\n#############\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a professional setting\n\nOur Responsibilities\n####################\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\nScope\n#####\n\nThis Code of Conduct applies both within project spaces and in public spaces\nwhen an individual is representing the project or its community. Examples of\nrepresenting a project or community include using an official project e-mail\naddress, posting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event. Representation of a project may be\nfurther defined and clarified by project maintainers.\n\nEnforcement\n###########\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the Technical Steering Committee defined `here <https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#governance>`_.\nAll complaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter ", "individual is representing the project or its community. Examples of\nrepresenting a project or community include using an official project e-mail\naddress, posting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event. Representation of a project may be\nfurther defined and clarified by project maintainers.\n\nEnforcement\n###########\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the Technical Steering Committee defined `here <https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#governance>`_.\nAll complaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\nAttribution\n###########\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq\n", '=========================\nExtra MLflow Dependencies\n=========================\n\nWhen you `install the MLflow Python package <https://mlflow.org/docs/latest/quickstart.html#installing-mlflow>`_,\na set of core dependencies needed to use most MLflow functionality (tracking, projects, models APIs)\nis also installed.\n\nHowever, in order to use certain framework-specific MLflow APIs or configuration options,\nyou need to install additional, "extra" dependencies. For example, the model persistence APIs under\nthe ``mlflow.sklearn`` module require scikit-learn to be installed. Some of the most common MLflow\nextra dependencies can be installed via ``pip install mlflow[extras]``.\n\nThe full set of extra dependencies are documented, along with the modules that depend on them,\nin the following files:\n\n* extra-ml-requirements.txt: ML libraries needed to use model persistence and inference APIs\n* test-requirements.txt: Libraries required to use non-default artifact-logging and tracking server configurations\n', '\nThis document is a hands-on manual for doing issue and pull request triage for `MLflow issues\non GitHub <https://github.com/mlflow/mlflow/issues>`_ .\nThe purpose of triage is to speed up issue management and get community members faster responses.\n\nIssue and pull request triage has three steps:\n\n- assign one or more process labels (e.g. ``needs design`` or ``help wanted``),\n- mark a priority, and\n- label one or more relevant areas, languages, or integrations to help route issues to appropriate contributors or reviewers.\n\nThe remainder of the document describes the labels used in each of these steps and how to apply them.\n\nAssign appropriate process labels\n#######\nAssign at least one process label to every issue you triage.\n\n- ``needs author feedback``: We need input from the author of the issue or PR to proceed.\n- | ``needs design``: This feature is large or tricky enough that we think it warrants a design doc\n  | and review before someone begins implementation.\n- | ``needs committer feedback``: The issue has a design that is ready for committer review, or there is\n  | an issue or pull request that needs feedback from a committer about the approach or appropriateness\n  | of the contribution.\n- | ``needs review``: Use this label for issues that need a more detailed design review or pull\n  | requests ready for review (all questions answered, PR updated if requests have been addressed,\n  | tests passing).\n- ``help wanted``: We would like community help for this issue.\n- ``good first issue``: This would make a good first issue.\n\n\nAssign priority\n#######\n\nYou should assign a priority to each issue you triage. We use `kubernetes-style <https://github.com/\nkubernetes/community/blob/master/contributors/guide/issue-triage.md#define-priority>`_ priority\nlabels.\n\n- | ``priority/critical-urgent``: This is the highest priority and should be worked on by\n  | somebody right now. This should typically be reserved for things like security bugs,\n  | regressions, release blockers.\n- | ``priority/important-soon``: The issue is worked on by the community currently or will\n  | be very soon, ideally in time for the next release.\n- | ``priority/important-longterm``: Important over the long term, but may not be staffed or\n  | may need multiple releases to complete. Also used for things we know are on a\n  ', "priority to each issue you triage. We use `kubernetes-style <https://github.com/\nkubernetes/community/blob/master/contributors/guide/issue-triage.md#define-priority>`_ priority\nlabels.\n\n- | ``priority/critical-urgent``: This is the highest priority and should be worked on by\n  | somebody right now. This should typically be reserved for things like security bugs,\n  | regressions, release blockers.\n- | ``priority/important-soon``: The issue is worked on by the community currently or will\n  | be very soon, ideally in time for the next release.\n- | ``priority/important-longterm``: Important over the long term, but may not be staffed or\n  | may need multiple releases to complete. Also used for things we know are on a\n  | contributor's roadmap in the next few months. We can use this in conjunction with\n  | ``help wanted`` to mark issues we would like to get help with. If someone begins actively\n  | working on an issue with this label and we think it may be merged by the next release, change\n  | the priority to ``priority/important-soon``.\n- | ``priority/backlog``: We believe it is useful but don't see it being prioritized in the\n  | next few months. Use this for issues that are lower priority than ``priority/important-longterm``.\n  | We welcome community members to pick up a ``priority/backlog`` issue, but there may be some\n  | delay in getting support through design review or pull request feedback.\n- | ``priority/awaiting-more-evidence``: Lowest priority. Possibly useful, but not yet enough\n  | support to actually get it done. This is a good place to put issues that could be useful but\n  | require more evidence to demonstrate broad value. Don't use it as a way to say no.\n  | If we think it doesn't fit in MLflow, we should just say that and why.\n\nLabel relevant areas\n#######\n\nAssign one more labels for relevant component or interface surface areas, languages, or\nintegrations. As a principle, we aim to have the minimal set of labels needed to help route issues\nand PRs to appropriate contributors. For example, a ``language/python`` label would not be\nparticularly helpful for routing issues to committers, since most PRs involve Python code.\n``language/java`` and ``language/r`` make sense to have, as the clients ", 'but\n  | require more evidence to demonstrate broad value. Don\'t use it as a way to say no.\n  | If we think it doesn\'t fit in MLflow, we should just say that and why.\n\nLabel relevant areas\n#######\n\nAssign one more labels for relevant component or interface surface areas, languages, or\nintegrations. As a principle, we aim to have the minimal set of labels needed to help route issues\nand PRs to appropriate contributors. For example, a ``language/python`` label would not be\nparticularly helpful for routing issues to committers, since most PRs involve Python code.\n``language/java`` and ``language/r`` make sense to have, as the clients in these languages differ from the Python client and aren\'t maintained by many people. As with process labels, we\ntake inspiration from Kubernetes on naming conventions.\n\nComponents\n""""""""\n- ``area/artifacts``: Artifact stores and artifact logging\n- ``area/build``: Build and test infrastructure for MLflow\n- ``area/docs``: MLflow documentation pages\n- ``area/evaluation``: MLflow model evaluation features, evaluation metrics, and evaluation workflows\n- ``area/examples``: Example code\n- ``area/gateway``: AI Gateway service, Gateway client APIs, third-party Gateway integrations\n- ``area/model-registry``: Model Registry service, APIs, and the fluent client calls for Model Registry\n- ``area/models``: MLmodel format, model serialization/deserialization, flavors\n- ``area/projects``: MLproject format, project execution backends\n- ``area/prompt``: MLflow prompt engineering features, prompt templates, and prompt management\n- ``area/scoring``: MLflow Model server, model deployment tools, Spark UDFs\n- ``area/server-infra``: MLflow Tracking server backend\n- ``area/tracing``: MLflow Tracing features, tracing APIs, and LLM tracing functionality\n- ``area/tracking``: Tracking Service, tracking client APIs, autologging\n\nInterface Surface\n""""""""\n- ``area/uiux``: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- ``area/docker``: Docker use across MLflow\'s components, such as MLflow Projects and MLflow Models\n- ``area/sqlalchemy``: Use of SQLAlchemy in the Tracking Service or Model Registry\n- ``area/windows``: Windows support\n\nLanguage Surface\n""""""""\n- ``language/r``: R APIs and clients\n- ``language/java``: Java APIs and clients\n- ``language/new``: Proposals for new client languages\n\nIntegrations\n""""""""\n- ``integrations/azure``: Azure and Azure ML integrations\n- ``integrations/sagemaker``: SageMaker integrations\n- ``integrations/databricks``: Databricks integrations\n', "Dockerized Model Training with MLflow\n-------------------------------------\nThis directory contains an MLflow project that trains a linear regression model on the UC Irvine\nWine Quality Dataset. The project uses a Docker image to capture the dependencies needed to run\ntraining code. Running a project in a Docker environment (as opposed to Conda) allows for capturing\nnon-Python dependencies, e.g. Java libraries. In the future, we also hope to add tools to MLflow\nfor running Dockerized projects e.g. on a Kubernetes cluster for scale out.\n\nStructure of this MLflow Project\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThis MLflow project contains a ``train.py`` file that trains a scikit-learn model and uses\nMLflow Tracking APIs to log the model and its metadata (e.g., hyperparameters and metrics)\nfor later use and reference. ``train.py`` operates on the Wine Quality Dataset, which is included\nin ``wine-quality.csv``.\n\nMost importantly, the project also includes an ``MLproject`` file, which specifies the Docker\ncontainer environment in which to run the project using the ``docker_env`` field:\n\n.. code-block:: yaml\n\n  docker_env:\n    image:  mlflow-docker-example\n\nHere, ``image`` can be any valid argument to ``docker run``, such as the tag, ID or URL of a Docker\nimage (see `Docker docs <https://docs.docker.com/engine/reference/run/#general-form>`_). The above\nexample references a locally-stored image (``mlflow-docker-example``) by tag.\n\nFinally, the project includes a ``Dockerfile`` that is used to build the image referenced by the\n``MLproject`` file. The ``Dockerfile`` specifies library dependencies required by the project, such\nas ``mlflow`` and ``scikit-learn``.\n\nRunning this Example\n^^^^^^^^^^^^^^^^^^^^\n\nFirst, install MLflow (via ``pip install mlflow``) and install\n`Docker <https://www.docker.com/get-started>`_.\n\nThen, build the image for the project's Docker container environment. You must use the same image\nname that is given by the ``docker_env.image`` field of the MLproject file. In this example, the\nimage name is ``mlflow-docker-example``. Issue the following command to build an image with this\nname:\n\n.. code-block:: bash\n\n  docker build -t mlflow-docker-example -f Dockerfile .\n\nNote that the name if the image used in the ``docker build`` command, ``mlflow-docker-example``,\nmatches the name of the image referenced in the ``MLproject`` file.\n\nFinally, run the example project using ``mlflow run examples/docker -P alpha=0.5``.\n\n.. note::\n    If running this example on a Mac with Apple silicon, ensure that Docker Desktop is running and\n    that you are logged in to the Docker Desktop service.\n    If ", "MLproject file. In this example, the\nimage name is ``mlflow-docker-example``. Issue the following command to build an image with this\nname:\n\n.. code-block:: bash\n\n  docker build -t mlflow-docker-example -f Dockerfile .\n\nNote that the name if the image used in the ``docker build`` command, ``mlflow-docker-example``,\nmatches the name of the image referenced in the ``MLproject`` file.\n\nFinally, run the example project using ``mlflow run examples/docker -P alpha=0.5``.\n\n.. note::\n    If running this example on a Mac with Apple silicon, ensure that Docker Desktop is running and\n    that you are logged in to the Docker Desktop service.\n    If you are modifying the example ``DockerFile`` to specify older versions of ``scikit-learn``,\n    you should enable `Rosetta compatibility <https://docs.docker.com/desktop/settings/mac/#features-in-development>`_\n    in the Docker Desktop configuration settings to ensure that the appropriate ``cython`` compiler is used.\n\nWhat happens when the project is run?\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nRunning ``mlflow run examples/docker`` builds a new Docker image based on ``mlflow-docker-example``\nthat also contains our project code. The resulting image is tagged as\n``mlflow-docker-example-<git-version>`` where ``<git-version>`` is the git commit ID. After the image is\nbuilt, MLflow executes the default (main) project entry point within the container using ``docker run``.\n\nEnvironment variables, such as ``MLFLOW_TRACKING_URI``, are propagated inside the container during\nproject execution. When running against a local tracking URI, MLflow mounts the host system's\ntracking directory (e.g., a local ``mlruns`` directory) inside the container so that metrics and\nparams logged during project execution are accessible afterwards.\n", 'How To Train and Deploy Image Classifier with MLflow and Keras\n--------------------------------------------------------------\n\nIn this example we demonstrate how to train and deploy image classification models with MLflow.\nWe train a VGG16 deep learning model to classify flower species from photos using a `dataset\n<http://download.tensorflow.org/example_images/flower_photos.tgz>`_ available from `tensorflow.org\n<http://www.tensorflow.org>`_. Note that although we use Keras to train the model in this case,\na similar approach can be applied to other deep learning frameworks such as ``PyTorch``.\n\nThe MLflow model produced by running this example can be deployed to any MLflow supported endpoints.\nAll the necessary image preprocessing is packaged with the model. The model can therefore be applied\nto image data directly. All that is required in order to pass new data to the model is to encode the\nimage binary data as base64 encoded string in pandas DataFrame (standard interface for MLflow python\nfunction models). The included Python scripts demonstrate how the model can be deployed to a REST\nAPI endpoint for realtime evaluation or to Spark for batch scoring..\n\nIn order to include custom image pre-processing logic with the model, we define the model as a\ncustom python function model wrapping around the underlying Keras model. The wrapper provides\nnecessary preprocessing to convert input data into multidimensional arrays expected by the\nKeras model. The preprocessing logic is stored with the model as a code dependency. Here is an\nexample of the output model directory layout:\n\n.. code-block:: bash\n\n   tree model\n\n::\n\n   model\n   â”œâ”€â”€ MLmodel\n   â”œâ”€â”€ code\n   â”‚\xa0\xa0 â””â”€â”€ image_pyfunc.py\n   â”œâ”€â”€ data\n   â”‚\xa0\xa0 â””â”€â”€ image_model\n   â”‚\xa0\xa0     â”œâ”€â”€ conf.yaml\n   â”‚\xa0\xa0     â””â”€â”€ keras_model\n   â”‚\xa0\xa0         â”œâ”€â”€ MLmodel\n   â”‚\xa0\xa0         â”œâ”€â”€ conda.yaml\n   â”‚\xa0\xa0         â””â”€â”€ model.h5\n   â””â”€â”€ mlflow_env.yml\n\n\n\nThe example contains the following files:\n\n * MLproject\n   Contains definition of this project. Contains only one entry point to train the model.\n\n * conda.yaml\n   Defines project dependencies. NOTE: You might want to change tensorflow package to ', '  â”‚\xa0\xa0     â”œâ”€â”€ conf.yaml\n   â”‚\xa0\xa0     â””â”€â”€ keras_model\n   â”‚\xa0\xa0         â”œâ”€â”€ MLmodel\n   â”‚\xa0\xa0         â”œâ”€â”€ conda.yaml\n   â”‚\xa0\xa0         â””â”€â”€ model.h5\n   â””â”€â”€ mlflow_env.yml\n\n\n\nThe example contains the following files:\n\n * MLproject\n   Contains definition of this project. Contains only one entry point to train the model.\n\n * conda.yaml\n   Defines project dependencies. NOTE: You might want to change tensorflow package to tensorflow-gpu\n   if you have gpu(s) available.\n\n * train.py\n   Main entry point of the projects. Handles command line arguments and possibly downloads the\n   dataset.\n\n * image_pyfunc.py\n   The implementation of the model train and also of the outputed custom python flavor model. Note\n   that the same preprocessing code that is used during model training is packaged with the output\n   model and is used during scoring.\n\n * score_images_rest.py\n   Score an image or a directory of images using a model deployed to a REST endpoint.\n\n * score_images_spark.py\n   Score an image or a directory of images using model deployed to Spark.\n\n\n\nRunning this Example\n^^^^^^^^^^^^^^^^^^^^\n\nTo train the model, run the example as a standard MLflow project:\n\n\n.. code-block:: bash\n\n    mlflow run examples/flower_classifier\n\nThis will download the training dataset from ``tensorflow.org``, train a classifier using Keras and\nlog results with MLflow.\n\nTo test your model, run the included scoring scripts. For example, say your model was trained with\nrun_id ``101``.\n\n- To test REST api scoring do the following two steps:\n\n  1. Deploy the model as a local REST endpoint by running ``mlflow models serve``:\n\n    .. code-block:: bash\n\n        # deploy the model to local REST api endpoint\n        mlflow models serve --model-uri runs:/101/model --port 54321\n\n  1. Apply the model to new data using the provided score_images_rest.py script:\n\n    .. code-block:: bash\n\n     ', 'your model, run the included scoring scripts. For example, say your model was trained with\nrun_id ``101``.\n\n- To test REST api scoring do the following two steps:\n\n  1. Deploy the model as a local REST endpoint by running ``mlflow models serve``:\n\n    .. code-block:: bash\n\n        # deploy the model to local REST api endpoint\n        mlflow models serve --model-uri runs:/101/model --port 54321\n\n  1. Apply the model to new data using the provided score_images_rest.py script:\n\n    .. code-block:: bash\n\n        # score the deployed model\n        python score_images_rest.py --host http://127.0.0.1 --port 54321 /path/to/images/for/scoring\n\n\n- To test batch scoring in Spark, run score_images_spark.py to score the model in Spark like this:\n\n  .. code-block:: bash\n\n    python score_images_spark.py --model-uri runs:/101/model /path/to/images/for/scoring\n', 'Hyperparameter Tuning Example\n------------------------------\n\nExample of how to do hyperparameter tuning with MLflow and some popular optimization libraries.\n\nThis example tries to optimize the RMSE metric of a Keras deep learning model on a wine quality\ndataset. The Keras model is fitted by the ``train`` entry point and has two hyperparameters that we\ntry to optimize: ``learning-rate`` and ``momentum``. The input dataset is split into three parts: training,\nvalidation, and test. The training dataset is used to fit the model and the validation dataset is used to\nselect the best hyperparameter values, and the test set is used to evaluate expected performance and\nto verify that we did not overfit on the particular training and validation combination. All three\nmetrics are logged with MLflow and you can use the MLflow UI to inspect how they vary between different\nhyperparameter values.\n\nexamples/hyperparam/MLproject has 4 targets:\n  * train:\n    train a simple deep learning model on the wine-quality dataset from our tutorial.\n    It has 2 tunable hyperparameters: ``learning-rate`` and ``momentum``.\n    Contains examples of how Keras callbacks can be used for MLflow integration.\n  * random:\n    perform simple random search over the parameter space.\n  * hyperopt:\n    use `Hyperopt <https://github.com/hyperopt/hyperopt>`_ to optimize hyperparameters.\n\n\nRunning this Example\n^^^^^^^^^^^^^^^^^^^^\n\nYou can run any of the targets as a standard MLflow run.\n\n.. code-block:: bash\n\n    mlflow experiments create -n individual_runs\n\nCreates experiment for individual runs and return its experiment ID.\n\n.. code-block:: bash\n\n    mlflow experiments create -n hyper_param_runs\n\nCreates an experiment for hyperparam runs and return its experiment ID.\n\n.. code-block:: bash\n\n    mlflow run -e train --experiment-id <individual_runs_experiment_id> examples/hyperparam\n\nRuns the Keras deep learning training with default parameters and log it in experiment 1.\n\n.. code-block:: bash\n\n    mlflow run -e random --experiment-id <hyperparam_experiment_id> examples/hyperparam\n\n.. code-block:: bash\n\n    mlflow run -e hyperopt --experiment-id <hyperparam_experiment_id> examples/hyperparam\n\nRuns the hyperparameter tuning with either random search or Hyperopt and log the\nresults under ``hyperparam_experiment_id``.\n\nYou can compare these results by using ``mlflow ui``.\n', "Multistep Workflow Example\n--------------------------\nThis MLproject aims to be a fully self-contained example of how to\nchain together multiple different MLflow runs which each encapsulate\na transformation or training step, allowing a clear definition of the\ninterface between the steps, as well as allowing for caching and reuse\nof the intermediate results.\n\nAt a high level, our goal is to predict users' ratings of movie given\na history of their ratings for other movies. This example is based\non `this webinar <https://databricks.com/blog/2018/07/13/scalable-end-to-end-deep-learning-using-tensorflow-and-databricks-on-demand-webinar-and-faq-now-available.html>`_\nby @brookewenig and @smurching.\n\n.. image:: ../../docs/source/_static/images/tutorial-multistep-workflow.png?raw=true\n\nThere are four steps to this workflow:\n\n- **load_raw_data.py**: Downloads the MovieLens dataset\n  (a set of triples of user id, movie id, and rating) as a CSV and puts\n  it into the artifact store.\n\n- **etl_data.py**: Converts the MovieLens CSV from the\n  previous step into Parquet, dropping unnecessary columns along the way.\n  This reduces the input size from 500 MB to 49 MB, and allows columnar\n  access of the data.\n\n- **als.py**: Runs Alternating Least Squares for collaborative\n  filtering on the Parquet version of MovieLens to estimate the\n  movieFactors and userFactors. This produces a relatively accurate estimator.\n\n- **train_keras.py**: Trains a neural network on the\n  original data, supplemented by the ALS movie/userFactors -- we hope\n  this can improve upon the ALS estimations.\n\nWhile we can run each of these steps manually, here we have a driver\nrun, defined as **main** (main.py). This run will run\nthe steps in order, passing the results of one to the next.\nAdditionally, this run will attempt to determine if a sub-run has\nalready been executed successfully with the same parameters and, if so,\nreuse the cached results.\n\nRunning this Example\n^^^^^^^^^^^^^^^^^^^^\nIn order for the multistep workflow to find the other steps, you must\nexecute ``mlflow run`` from this directory. So, in order to find out if\nthe Keras model does in fact improve upon the ALS model, you can simply\nrun:\n\n.. code-block:: bash\n\n    cd examples/multistep_workflow\n    mlflow run .\n\n\nThis downloads and transforms the MovieLens dataset, trains an ALS\nmodel, and then trains a Keras model -- you can compare the results by\nusing ``mlflow ui``.\n\nYou can also try changing the number of ALS iterations or Keras hidden\nunits:\n\n.. code-block:: bash\n\n  ", 'the same parameters and, if so,\nreuse the cached results.\n\nRunning this Example\n^^^^^^^^^^^^^^^^^^^^\nIn order for the multistep workflow to find the other steps, you must\nexecute ``mlflow run`` from this directory. So, in order to find out if\nthe Keras model does in fact improve upon the ALS model, you can simply\nrun:\n\n.. code-block:: bash\n\n    cd examples/multistep_workflow\n    mlflow run .\n\n\nThis downloads and transforms the MovieLens dataset, trains an ALS\nmodel, and then trains a Keras model -- you can compare the results by\nusing ``mlflow ui``.\n\nYou can also try changing the number of ALS iterations or Keras hidden\nunits:\n\n.. code-block:: bash\n\n    mlflow run . -P als_max_iter=20 -P keras_hidden_units=50\n', 'mlflow REST API Example\n-----------------------\nThis simple example shows how you could use MLflow REST API to create new\nruns inside an experiment to log parameters/metrics.\n\nTo run this example code do the following:\n\nOpen a terminal and navigate to the ``/tmp`` directory and start the mlflow tracking server::\n\n  mlflow server\n\nIn another terminal window navigate to the ``mlflow/examples/rest_api`` directory.  Run the example code\nwith this command::\n\n  python mlflow_tracking_rest_api.py\n\nProgram options::\n\n  usage: mlflow_tracking_rest_api.py [-h] [--hostname HOSTNAME] [--port PORT]\n                                   [--experiment-id EXPERIMENT_ID]\n\n  MLflow REST API Example\n\n  optional arguments:\n    -h, --help            show this help message and exit\n    --hostname HOSTNAME   MLflow server hostname/ip (default: localhost)\n    --port PORT           MLflow server port number (default: 5000)\n    --experiment-id EXPERIMENT_ID\n                            Experiment ID (default: 0)\n', 'Python Package Anti-Tampering with MLflow\n-----------------------------------------\nThis directory contains an MLflow project showing how to harden the ML supply chain, and in particular\nhow to protect against Python package tampering by enforcing\n`hash checks <https://pip.pypa.io/en/latest/cli/pip_install/#hash-checking-mode>`_ on packages.\n\nRunning this Example\n^^^^^^^^^^^^^^^^^^^^\n\nFirst, install MLflow (via ``pip install mlflow``).\n\nThe model is trained locally by running:\n\n.. code-block:: bash\n\n  mlflow run .\n\nAt the end of the training, note the run ID (say ``e651fcd4dab140a2bd4d3745a32370ac``).\n\nThe model is served locally by running:\n\n.. code-block:: bash\n\n  mlflow models serve -m runs:/e651fcd4dab140a2bd4d3745a32370ac/model\n\nInference is performed by sending JSON POST requests to http://localhost:5000/invocations:\n\n.. code-block:: bash\n\n  curl -X POST -d "{\\"dataframe_split\\": {\\"data\\":[[0.0199132142,0.0506801187,0.1048086895,0.0700725447,-0.0359677813,-0.0266789028,-0.0249926566,-0.002592262,0.0037117382,0.0403433716]]}}" -H "Content-Type: application/json" http://localhost:5000/invocations\n\nWhich returns ``[235.11371081266924]``.\n\nStructure of this MLflow Project\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. code-block:: yaml\n\n  name: mlflow-supply-chain-security\n  channels:\n  - nodefaults\n  dependencies:\n  - python=3.9\n  - pip\n  - pip:\n    - --require-hashes\n    - -r requirements.txt\n\nThis ensures that all the package requirements referenced in ``requirements.txt`` have been pinned through both version and hash:\n\n.. code-block:: text\n\n  mlflow==1.20.2 \\\n      --hash=sha256:963c22532e82a93450674ab97d62f9e528ed0906b580fadb7c003e696197557c \\\n      --hash=sha256:b15ff0c7e5e64f864a0b40c99b9a582227315eca2065d9f831db9aeb8f24637b\n  numpy==1.21.4 \\\n      --hash=sha256:0b78ecfa070460104934e2caf51694ccd00f37d5e5dbe76f021b1b0b0d221823 \\\n  ...\n\nThat same conda environment is referenced when logging the model in ``train.py`` so the environment matches during inference:\n\n.. code-block:: python\n\n  mlflow.sklearn.log_model(\n      model,\n      name="model",\n      signature=mlflow.models.infer_signature(X_train[:10], y_train[:10]),\n      input_example=X_train[:10],\n      conda_env="conda.yaml",\n  )\n\nThe package requirements are managed in ``requirements.in``:\n\n.. code-block:: text\n\n  pandas==1.3.2\n  scikit-learn==0.24.2\n  mlflow==1.20.2\n\nThey are compiled using ``pip-tools`` to resolve all the package dependencies, their versions, and their hashes:\n\n.. code-block:: bash\n\n  pip install pip-tools\n  pip-compile --generate-hashes --output-file=requirements.txt requirements.in\n', '{\n  "kube-context": "docker-for-desktop",\n  "kube-job-template-path": "examples/docker/kubernetes_job_template.yaml",\n  "repository-uri": "username/mlflow-kubernetes-example"\n}\n', '[[4.6, 3.1, 1.5, 0.2]]\n', '{\n  "name": "mlflow-typescript",\n  "private": true,\n  "description": "TypeScript implementation of MLflow Tracing SDK. This is the root workspace package that includes all the public packages as sub-directories.",\n  "workspaces": [\n    "core",\n    "integrations/*"\n  ],\n  "scripts": {\n    "build": "npm run build:subpackages",\n    "build:subpackages": "npm run build:core && npm run build:integrations",\n    "build:core": "cd core && npm run build",\n    "build:integrations": "cd integrations/openai && npm run build",\n    "test": "npm run test:subpackages",\n    "test:subpackages": "npm run test:core && npm run test:integrations",\n    "test:core": "cd core && npm run test",\n    "test:integrations": "cd integrations/openai && npm run test",\n    "lint": "eslint . --ext .ts",\n    "lint:fix": "eslint . --ext .ts --fix",\n    "format": "prettier --write .",\n    "format:check": "prettier --check .",\n    "prepare": "npm run build"\n  },\n  "devDependencies": {\n    "typedoc": "^0.28.0"\n  }\n}\n', '{\n  "name": "mlflow-tracing",\n  "version": "0.1.0",\n  "description": "TypeScript implementation of MLflow Tracing SDK for LLM observability",\n  "repository": {\n    "type": "git",\n    "url": "https://github.com/mlflow/mlflow.git"\n  },\n  "homepage": "https://mlflow.org/",\n  "author": {\n    "name": "MLflow",\n    "url": "https://mlflow.org/"\n  },\n  "bugs": {\n    "url": "https://github.com/mlflow/mlflow/issues"\n  },\n  "license": "Apache-2.0",\n  "keywords": [\n    "mlflow",\n    "tracing",\n    "observability",\n    "opentelemetry",\n    "llm",\n    "javascript",\n    "typescript"\n  ],\n  "main": "dist/index.js",\n  "types": "dist/index.d.ts",\n  "scripts": {\n    "build": "tsc",\n    "test": "jest",\n    "lint": "eslint . --ext .ts",\n    "lint:fix": "eslint . --ext .ts --fix",\n    "format": "prettier --write .",\n    "format:check": "prettier --check ."\n  },\n  "dependencies": {\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/sdk-node": "^0.201.1",\n    "@types/json-bigint": "^1.0.4",\n    "fast-safe-stringify": "^2.1.1",\n    "ini": "^5.0.0",\n    "json-bigint": "^1.0.0"\n  },\n  "devDependencies": {\n    "@types/ini": "^4.1.1",\n    "@types/jest": "^29.5.3",\n    "@types/node": "^20.4.5",\n    "@typescript-eslint/eslint-plugin": "^6.21.0",\n    "@typescript-eslint/parser": "^6.21.0",\n    "openai": "^4.0.0",\n    "eslint": "^8.57.1",\n    "jest": "^29.6.2",\n    "msw": "^2.10.3",\n    "prettier": "^3.5.3",\n    "ts-jest": "^29.1.1",\n    "tsx": "^4.7.0",\n    "typescript": "^5.8.3",\n    "whatwg-fetch": "^3.6.20"\n  },\n  "engines": {\n    "node": ">=18"\n  },\n  "files": [\n    "dist/"\n  ]\n}\n', '{\n  "name": "mlflow-openai",\n  "version": "0.1.0",\n  "description": "OpenAI integration package for MLflow Tracing",\n  "repository": {\n    "type": "git",\n    "url": "https://github.com/mlflow/mlflow.git"\n  },\n  "homepage": "https://mlflow.org/",\n  "author": {\n    "name": "MLflow",\n    "url": "https://mlflow.org/"\n  },\n  "bugs": {\n    "url": "https://github.com/mlflow/mlflow/issues"\n  },\n  "license": "Apache-2.0",\n  "keywords": [\n    "mlflow",\n    "tracing",\n    "observability",\n    "opentelemetry",\n    "llm",\n    "openai",\n    "javascript",\n    "typescript"\n  ],\n  "main": "dist/index.js",\n  "types": "dist/index.d.ts",\n  "scripts": {\n    "build": "tsc",\n    "test": "jest",\n    "lint": "eslint . --ext .ts",\n    "lint:fix": "eslint . --ext .ts --fix",\n    "format": "prettier --write .",\n    "format:check": "prettier --check ."\n  },\n  "peerDependencies": {\n    "mlflow-tracing": "^0.1.0-rc.0",\n    "openai": "^4.0.0"\n  },\n  "devDependencies": {\n    "jest": "^29.6.2",\n    "typescript": "^5.8.3"\n  },\n  "engines": {\n    "node": ">=18"\n  },\n  "files": [\n    "dist/"\n  ]\n}\n', '{\n  "name": "@mlflow/mlflow",\n  "version": "0.1.0",\n  "scripts": {\n    "start": "craco start",\n    "storybook": "start-storybook -p 6006 -s public",\n    "build-storybook": "build-storybook -s public",\n    "test": "craco --max_old_space_size=8192 test --env=jsdom --colors --watchAll=false",\n    "test:watch": "yarn test --watch",\n    "test:ci": "CI=true craco test --env=jsdom --colors --forceExit --ci --coverage",\n    "lint": "eslint --ext js,jsx,ts,tsx src",\n    "lint:fix": "eslint --ext js,jsx,ts,tsx src --fix",\n    "type-check": "tsc --noEmit",\n    "prettier": "prettier",\n    "prettier:fix": "prettier . --write",\n    "prettier:check": "prettier . --check",\n    "i18n:check": "yarn i18n --lint",\n    "i18n": "node scripts/extract-i18n.js",\n    "check-all": "yarn lint && yarn prettier:check && yarn i18n:check && yarn type-check",\n    "knip": "knip --reporter markdown --preprocessor ./knip-preprocessor.ts",\n    "build": "craco --max_old_space_size=8192 build",\n    "graphql-codegen": "python ../../../dev/proto_to_graphql/code_generator.py && yarn graphql-codegen:clean && yarn graphql-codegen:base",\n    "graphql-codegen:base": "graphql-codegen --config ./src/graphql/graphql-codegen.ts",\n    "graphql-codegen:clean": "find . -path \'**/__generated__/*.ts\' | xargs rm"\n  },\n  "dependencies": {\n    "@ag-grid-community/client-side-row-model": "^27.2.1",\n    "@ag-grid-community/core": "^27.2.1",\n    "@ag-grid-community/react": "^27.2.1",\n    "@apollo/client": "^3.6.9",\n    "@craco/craco": "7.0.0-alpha.0",\n    "@databricks/design-system": "^1.12.20",\n    "@emotion/cache": "^11.11.0",\n    "@emotion/react": "^11.11.3",\n    "@tanstack/react-query": "^4.29.17",\n    "@tanstack/react-table": "^8.8.2",\n    "@tanstack/react-virtual": "^3.8.1",\n    "@types/react-virtualized": "^9.21.9",\n    "babel-jest": "^27.5.1",\n    "buffer": "^6.0.3",\n    "bytes": "3.0.0",\n    "classnames": "^2.2.6",\n    "cookie": "0.3.1",\n    "cronstrue": "^1.94.0",\n    "d3-array": "^3.2.4",\n    "d3-scale": "^2.1.0",\n    "dateformat": "3.0.3",\n    "diff": "5.1.0",\n    "file-saver": "^2.0.5",\n    "font-awesome": "4.7.0",\n    "graphql": "^15.5.0",\n    "http-proxy-middleware": "^1.0.3",\n    "immutable": "3.8.1",\n    "invariant": "^2.2.4",\n    "js-yaml": "^3.14.0",\n    "json-bigint": "databricks/json-bigint#a1defaf9cd8dd749f0fd4d5f83a22cd846789658",\n    "leaflet": "^1.5.1",\n    "lodash": "^4.17.21",\n    "moment": "^2.29.4",\n    ', '"buffer": "^6.0.3",\n    "bytes": "3.0.0",\n    "classnames": "^2.2.6",\n    "cookie": "0.3.1",\n    "cronstrue": "^1.94.0",\n    "d3-array": "^3.2.4",\n    "d3-scale": "^2.1.0",\n    "dateformat": "3.0.3",\n    "diff": "5.1.0",\n    "file-saver": "^2.0.5",\n    "font-awesome": "4.7.0",\n    "graphql": "^15.5.0",\n    "http-proxy-middleware": "^1.0.3",\n    "immutable": "3.8.1",\n    "invariant": "^2.2.4",\n    "js-yaml": "^3.14.0",\n    "json-bigint": "databricks/json-bigint#a1defaf9cd8dd749f0fd4d5f83a22cd846789658",\n    "leaflet": "^1.5.1",\n    "lodash": "^4.17.21",\n    "moment": "^2.29.4",\n    "pako": "0.2.7",\n    "papaparse": "^5.3.2",\n    "parcoord-es": "^2.2.10",\n    "pdfjs-dist": "^5.3.31",\n    "plotly.js": "2.5.1",\n    "prop-types": "^15.8.1",\n    "qs": "6.10.5",\n    "rc-image": "~5.2.4",\n    "react": "^18.2.0",\n    "react-dnd": "^15.1.1",\n    "react-dnd-html5-backend": "^15.1.2",\n    "react-dom": "^18.2.0",\n    "react-draggable": "^4.4.6",\n    "react-error-boundary": "^4.0.2",\n    "react-hook-form": "^7.36.0",\n    "react-iframe": "1.8.0",\n    "react-intl": "^6.0.4",\n    "react-markdown-10": "npm:react-markdown@10",\n    "react-mde": "^11.0.0",\n    "react-pdf": "^10.0.1",\n    "react-plotly.js": "^2.5.1",\n    "react-redux": "^7.2.5",\n    "react-resizable": "^3.0.4",\n    "react-router": "^6.4.0",\n    "react-router-dom": "^6.4.0",\n    "react-syntax-highlighter": "^15.4.5",\n    "react-transition-group": "^4.4.1",\n    "react-treebeard": "2.1.0",\n    "react-vega": "^7.6.0",\n    "react-virtual": "^2.10.4",\n    "react-virtualized": "^9.21.2",\n    "redux": "^4.1.1",\n    "redux-promise-middleware": "^5.1.1",\n    "redux-thunk": "^2.3.0",\n    "remark-gfm-4": "npm:remark-gfm@4",\n    "sanitize-html": "^1.18.5",\n    "showdown": "^1.8.6",\n    "stream-browserify": "^3.0.0",\n    "stylis": "^4.0.10",\n    "url": "^0.11.0",\n    "use-clipboard-copy": "^0.2.0",\n    "use-debounce": "^10.0.4",\n    "use-sync-external-store": "^1.2.0",\n    "wavesurfer.js": "^7.8.8",\n    "yup": "^1.6.1"\n  },\n  "devDependencies": {\n    "@babel/core": "^7.27.3",\n    "@babel/eslint-parser": "^7.22.15",\n    "@babel/preset-env": "^7.27.2",\n    "@babel/preset-react": "^7.27.1",\n    ', '"react-virtualized": "^9.21.2",\n    "redux": "^4.1.1",\n    "redux-promise-middleware": "^5.1.1",\n    "redux-thunk": "^2.3.0",\n    "remark-gfm-4": "npm:remark-gfm@4",\n    "sanitize-html": "^1.18.5",\n    "showdown": "^1.8.6",\n    "stream-browserify": "^3.0.0",\n    "stylis": "^4.0.10",\n    "url": "^0.11.0",\n    "use-clipboard-copy": "^0.2.0",\n    "use-debounce": "^10.0.4",\n    "use-sync-external-store": "^1.2.0",\n    "wavesurfer.js": "^7.8.8",\n    "yup": "^1.6.1"\n  },\n  "devDependencies": {\n    "@babel/core": "^7.27.3",\n    "@babel/eslint-parser": "^7.22.15",\n    "@babel/preset-env": "^7.27.2",\n    "@babel/preset-react": "^7.27.1",\n    "@babel/preset-typescript": "^7.27.1",\n    "@emotion/babel-plugin": "^11.11.0",\n    "@emotion/babel-preset-css-prop": "^11.11.0",\n    "@emotion/eslint-plugin": "^11.7.0",\n    "@formatjs/cli": "^4.2.15",\n    "@graphql-codegen/cli": "^5.0.0",\n    "@graphql-codegen/typescript": "^4.0.1",\n    "@graphql-codegen/typescript-operations": "^4.0.1",\n    "@jest/globals": "^30.0.2",\n    "@storybook/addon-actions": "^6.5.5",\n    "@storybook/addon-docs": "^6.5.5",\n    "@storybook/addon-essentials": "^6.5.5",\n    "@storybook/addon-links": "^6.5.5",\n    "@storybook/builder-webpack5": "6.5.5",\n    "@storybook/manager-webpack5": "6.5.5",\n    "@storybook/node-logger": "^6.5.5",\n    "@storybook/preset-create-react-app": "^4.1.0",\n    "@storybook/react": "^6.5.5",\n    "@testing-library/dom": "^10.4.0",\n    "@testing-library/jest-dom": "^6.4.2",\n    "@testing-library/react": "^16.1.0",\n    "@testing-library/user-event": "^14.5.2",\n    "@types/d3-array": "^3.2.1",\n    "@types/d3-scale": "^2.1.0",\n    "@types/d3-selection": "^1.3.0",\n    "@types/diff": "^5.1.0",\n    "@types/file-saver": "^2.0.3",\n    "@types/invariant": "^2.2.35",\n    "@types/jest": "^29.5.14",\n    "@types/pako": "^2.0.0",\n    "@types/plotly.js": "^1.54.21",\n    "@types/react": "^17.0.50",\n    "@types/react-dom": "^17.0.17",\n    "@types/react-plotly.js": "^2.5.0",\n    "@types/react-resizable": "^3.0.3",\n    "@types/react-router": "^5.1.20",\n    "@types/react-router-dom": "^5.3.3",\n    "@types/react-transition-group": "^4.4.4",\n    "@types/stylis": "^4.0.1",\n    "@types/use-sync-external-store": "^0.0.3",\n    "@typescript-eslint/eslint-plugin": "^5.28.0",\n    "@typescript-eslint/parser": "^5.28.0",\n    "@wojtekmaj/enzyme-adapter-react-17": "^0.6.3",\n    "argparse": "^2.0.1",\n    "babel-plugin-formatjs": "^10.2.14",\n    "babel-plugin-react-require": "^3.1.3",\n    "confusing-browser-globals": "^1.0.11",\n    "enzyme": "^3.11.0",\n    "eslint": "^8.25.0",\n    "eslint-config-prettier": "^8.5.0",\n    ', '"@types/plotly.js": "^1.54.21",\n    "@types/react": "^17.0.50",\n    "@types/react-dom": "^17.0.17",\n    "@types/react-plotly.js": "^2.5.0",\n    "@types/react-resizable": "^3.0.3",\n    "@types/react-router": "^5.1.20",\n    "@types/react-router-dom": "^5.3.3",\n    "@types/react-transition-group": "^4.4.4",\n    "@types/stylis": "^4.0.1",\n    "@types/use-sync-external-store": "^0.0.3",\n    "@typescript-eslint/eslint-plugin": "^5.28.0",\n    "@typescript-eslint/parser": "^5.28.0",\n    "@wojtekmaj/enzyme-adapter-react-17": "^0.6.3",\n    "argparse": "^2.0.1",\n    "babel-plugin-formatjs": "^10.2.14",\n    "babel-plugin-react-require": "^3.1.3",\n    "confusing-browser-globals": "^1.0.11",\n    "enzyme": "^3.11.0",\n    "eslint": "^8.25.0",\n    "eslint-config-prettier": "^8.5.0",\n    "eslint-config-standard": "10.2.1",\n    "eslint-import-resolver-webpack": "0.8.4",\n    "eslint-loader": "2.1.1",\n    "eslint-plugin-chai-expect": "1.1.1",\n    "eslint-plugin-chai-friendly": "^0.7.2",\n    "eslint-plugin-cypress": "^2.12.1",\n    "eslint-plugin-flowtype": "^8.0.3",\n    "eslint-plugin-formatjs": "^3.1.5",\n    "eslint-plugin-import": "^2.26.0",\n    "eslint-plugin-jest": "^26.5.3",\n    "eslint-plugin-jsx-a11y": "^6.7.1",\n    "eslint-plugin-no-lookahead-lookbehind-regexp": "^0.1.0",\n    "eslint-plugin-no-only-tests": "^2.6.0",\n    "eslint-plugin-node": "5.2.1",\n    "eslint-plugin-prettier": "^4.0.0",\n    "eslint-plugin-promise": "3.6.0",\n    "eslint-plugin-react": "^7.30.0",\n    "eslint-plugin-react-hooks": "^4.6.0",\n    "eslint-plugin-standard": "3.0.1",\n    "eslint-plugin-testing-library": "^6.1.0",\n    "fast-glob": "^3.2.11",\n    "graphql-codegen-typescript-operation-types": "^2.0.1",\n    "jest-canvas-mock": "^2.2.0",\n    "jest-localstorage-mock": "^2.3.0",\n    "knip": "^5.30.2",\n    "msw": "^1.2.3",\n    "prettier": "^2.8.0",\n    "react-17": "npm:react@^17.0.2",\n    "react-dom-17": "npm:react-dom@^17.0.2",\n    "react-scripts": "5.0.0",\n    "react-test-renderer-17": "npm:react-test-renderer@^17.0.2",\n    "redux-mock-store": "^1.5.3",\n    "resolve": "^1.22.1",\n    "stream-browserify": "^3.0.0",\n    "tsconfig-paths-webpack-plugin": "^4.0.1",\n    "typescript": "^5.8.3",\n    "webpack": "^5.69.0",\n    "whatwg-fetch": "^3.6.17"\n  },\n  "private": true,\n  "engines": {\n    "node": "^22.16.0"\n  },\n  "resolutions": {\n    "@floating-ui/dom@^0.5.3": "patch:@floating-ui/dom@npm%3A0.5.4#yarn/patches/@floating-ui-dom-0.5.4.diff",\n    "@types/react": "^17.0.50",\n    "@emotion/react": "11.11.0",\n    "@types/react-plotly.js/@types/plotly.js": "^1.54.6",\n    "d3-transition": "3.0.1",\n    "react-dev-utils/fork-ts-checker-webpack-plugin": "6.5.3",\n    "postcss-preset-env/autoprefixer": "10.4.5",\n    "rc-virtual-list@^3.2.0": "patch:rc-virtual-list@npm%3A3.2.0#yarn/patches/rc-virtual-list-npm-3.2.0-5efaefc12e.patch",\n    "rc-virtual-list@^3.0.3": "patch:rc-virtual-list@npm%3A3.2.0#yarn/patches/rc-virtual-list-npm-3.2.0-5efaefc12e.patch",\n ', '"react-test-renderer-17": "npm:react-test-renderer@^17.0.2",\n    "redux-mock-store": "^1.5.3",\n    "resolve": "^1.22.1",\n    "stream-browserify": "^3.0.0",\n    "tsconfig-paths-webpack-plugin": "^4.0.1",\n    "typescript": "^5.8.3",\n    "webpack": "^5.69.0",\n    "whatwg-fetch": "^3.6.17"\n  },\n  "private": true,\n  "engines": {\n    "node": "^22.16.0"\n  },\n  "resolutions": {\n    "@floating-ui/dom@^0.5.3": "patch:@floating-ui/dom@npm%3A0.5.4#yarn/patches/@floating-ui-dom-0.5.4.diff",\n    "@types/react": "^17.0.50",\n    "@emotion/react": "11.11.0",\n    "@types/react-plotly.js/@types/plotly.js": "^1.54.6",\n    "d3-transition": "3.0.1",\n    "react-dev-utils/fork-ts-checker-webpack-plugin": "6.5.3",\n    "postcss-preset-env/autoprefixer": "10.4.5",\n    "rc-virtual-list@^3.2.0": "patch:rc-virtual-list@npm%3A3.2.0#yarn/patches/rc-virtual-list-npm-3.2.0-5efaefc12e.patch",\n    "rc-virtual-list@^3.0.3": "patch:rc-virtual-list@npm%3A3.2.0#yarn/patches/rc-virtual-list-npm-3.2.0-5efaefc12e.patch",\n    "rc-virtual-list@^3.0.1": "patch:rc-virtual-list@npm%3A3.2.0#yarn/patches/rc-virtual-list-npm-3.2.0-5efaefc12e.patch"\n  },\n  "//": "homepage is hard to configure without resorting to env variables and doesn\'t play nicely with other webpack settings. This field should be removed.",\n  "homepage": "static-files",\n  "browserslist": [\n    "defaults"\n  ],\n  "babel": {\n    "env": {\n      "test": {\n        "plugins": [\n          [\n            "babel-plugin-formatjs",\n            {\n              "idInterpolationPattern": "[sha512:contenthash:base64:6]",\n              "removeDefaultMessage": false\n            }\n          ]\n        ]\n      }\n    }\n  }\n}\n', '{\n  "short_name": "React App",\n  "name": "Create React App Sample",\n  "icons": [\n    {\n      "src": "favicon.ico",\n      "sizes": "64x64 32x32 24x24 16x16",\n      "type": "image/x-icon"\n    }\n  ],\n  "start_url": "./index.html",\n  "display": "standalone",\n  "theme_color": "#000000",\n  "background_color": "#ffffff"\n}\n', '{\n  "columns": [\n    "company_name",\n    "company_goal",\n    "prompt",\n    "output",\n    "MLFLOW_model",\n    "MLFLOW_route_type",\n    "MLFLOW_latency"\n  BROKEN', '{\n  "columns": [\n    "company_name",\n    "company_goal",\n    "prompt",\n    "output",\n    "MLFLOW_model",\n    "MLFLOW_route_type",\n    "MLFLOW_latency"\n  ],\n  "data": [\n    [\n      "Abc",\n      "bottom line revenue",\n      "You are a marketing consultant for a technology company. Develop a marketing strategy report for Abc aiming to bottom line revenue",\n      " Here is an outline for a marketing strategy report aimed at increasing bottom line revenue for the technology company Abc:\\n\\nIntroduction \\n- Brief background on Abc - products/services, target markets, competitive landscape\\n- Objective of report: Provide recommendations to increase bottom line revenue through marketing strategies\\n\\nMarket Analysis\\n- Trends in Abc\'s industry and target markets\\n- Customer analysis - demographics, psychographics, buying behavior \\n- Competitor analysis - positioning, pricing,",\n      "",\n      "",\n      "7788.29999999702"\n    ],\n    [\n      "XYZ Company",\n      "Increase top-line revenue",\n      "You are a marketing consultant for a technology company. Develop a marketing strategy report for XYZ Company aiming to Increase top-line revenue",\n      " Here is an outline for a marketing strategy report aimed at increasing top-line revenue for XYZ Company:\\n\\nXYZ Company \\nMarketing Strategy Report\\n\\nExecutive Summary\\n- Brief overview of key recommendations to increase revenue \\n\\nCurrent Situation Analysis\\n- Background on XYZ Company\'s products/services, target customers, competitive landscape\\n- Analysis of current marketing efforts and sales performance \\n\\nOpportunities for Growth \\n- New customer segments to target\\n- Additional products/services to meet customer needs",\n      "claude-2",\n      "llm/v1/completions",\n      "11563.60000000149"\n    ]\n  ]\n}\n', '{\n  "columns": [0, 1],\n  "data": [\n    ["a", "b"],\n    [1, 2]\n  ]\n}\n', '{\n  "current": {\n    "path": "/static/lib/ml-model-trace-renderer/index.html"\n  },\n  "2": {\n    "path": "/static/lib/ml-model-trace-renderer/2/index.html",\n    "commit": "93d4afc7a2e876f74cbba56c2db3d05edc91e872"\n  },\n  "oss": {\n    "path": "/static/lib/ml-model-trace-renderer/oss/index.html",\n    "commit": "b5595f5c6263c1c8e3614d85eb1d233d28789bb9"\n  },\n  "3": {\n    "path": "/static/lib/ml-model-trace-renderer/3/index.html",\n    "commit": "93d4afc7a2e876f74cbba56c2db3d05edc91e872"\n  }\n}\n', '{\n  "bd263e2b04b04460a40c1acae72a18ae": {\n    "metric_1": -2.5797830282214,\n    "metric_0": -2.434975966906267,\n    "metric_3": -0.9688077263066934,\n    "metric_2": -1.4438003481072212\n  },\n  "55461e2180fb40338072c04ff86fd0f9": {\n    "metric_1": -2.2857451912150792,\n    "metric_0": 3.4173603047073176,\n    "metric_3": -0.24019895935855473,\n    "metric_2": -0.7097425052930393\n  },\n  "123810810b234e9b8b97fb1e00abd9aa": {\n    "metric_1": 0.7308706035999548,\n    "metric_0": 3.0994891921059544,\n    "metric_3": 1.9819820891007573,\n    "metric_2": 0.48569560278784785\n  },\n  "66fbc3c813944c1a80d2336849c6e72f": {\n    "metric_1": -2.672718621393841,\n    "metric_0": -1.7902590711838267,\n    "metric_3": 2.477982822663786,\n    "metric_2": 1.273023064731822\n  },\n  "83698fafa8714bd3929b9c38bf6cdee8": {\n    "metric_1": -2.292131339453693,\n    "metric_0": 3.3064205155126096,\n    "metric_3": -0.46183104891365634,\n    "metric_2": 0.8465206209214458\n  },\n  "79461e9d7aa24e18a626f61b047315c8": {\n    "metric_1": -0.006443567706641673,\n    "metric_0": 1.8136489475666746,\n    "metric_3": 2.6700440103809013,\n    "metric_2": 0.1556304999295106\n  },\n  "6870761df41f4350adbd37f2a18eb641": {\n    "metric_1": -0.24614682477958416,\n    "metric_0": -1.5486858485543848,\n    "metric_3": -2.742733532695466,\n    "metric_2": 3.0344898094132358\n  },\n  "eb124e3ef9c04109a65372aab4222307": {\n    "metric_1": -1.9754772100389224,\n    "metric_0": -0.6234240819980461,\n    "metric_3": -2.4020972270978844,\n    "metric_2": 2.7962318576455436\n  },\n  "39c152b25fb04c08b3cf4a4c8ebccb7c": {\n    "metric_1": -2.149649101035413,\n    "metric_0": 0.7310583410679579,\n    "metric_3": 1.1244662209560552,\n    "metric_2": -2.0257045260054656\n  },\n  "1b92583de96c4fd88ff5b04e866aff8c": {\n    "metric_1": -0.02959618784025775,\n    "metric_0": -0.2931365711894838,\n    "metric_3": 3.1567281048311546,\n    "metric_2": 2.9203148639651495\n  },\n  "3c28149e5ab44efb8d7f4b3c11ab0cb1": {\n    "metric_1": -2.0944626407865288,\n    "metric_0": 3.779234079600906,\n    "metric_3": -2.9467929849482024,\n    "metric_2": -1.4704105222912913\n  },\n  "1afafa10e69a4083bb88a96b23547b7b": {\n    "metric_1": -0.10343236992763893,\n    "metric_0": 2.28372065230554,\n    "metric_3": 2.0481770225881615,\n    "metric_2": -1.1601122975618\n  },\n  "5f9c5e48ff844c0498199b390ca9c1a4": {\n    "metric_1": 2.4422641299267553,\n    "metric_0": -1.8107502356154743,\n    "metric_3": 3.6679213677343423,\n    "metric_2": -1.8061767363300147\n  },\n  "0ffbde81192e43a48482434219cc4458": {\n    "metric_1": -1.5559004670240322,\n    "metric_0": 0.8721310864910716,\n    "metric_3": 2.5822778193072846,\n    "metric_2": 0.6969033758109711\n  ', '},\n  "3c28149e5ab44efb8d7f4b3c11ab0cb1": {\n    "metric_1": -2.0944626407865288,\n    "metric_0": 3.779234079600906,\n    "metric_3": -2.9467929849482024,\n    "metric_2": -1.4704105222912913\n  },\n  "1afafa10e69a4083bb88a96b23547b7b": {\n    "metric_1": -0.10343236992763893,\n    "metric_0": 2.28372065230554,\n    "metric_3": 2.0481770225881615,\n    "metric_2": -1.1601122975618\n  },\n  "5f9c5e48ff844c0498199b390ca9c1a4": {\n    "metric_1": 2.4422641299267553,\n    "metric_0": -1.8107502356154743,\n    "metric_3": 3.6679213677343423,\n    "metric_2": -1.8061767363300147\n  },\n  "0ffbde81192e43a48482434219cc4458": {\n    "metric_1": -1.5559004670240322,\n    "metric_0": 0.8721310864910716,\n    "metric_3": 2.5822778193072846,\n    "metric_2": 0.6969033758109711\n  },\n  "f9b1cb0a470e4f9b98bd4d36a51e4d31": {\n    "metric_1": 2.9364188950633814,\n    "metric_0": -0.2344822065779013,\n    "metric_3": 3.8849138907360397,\n    "metric_2": -2.524561921321326\n  },\n  "99d7cdc4d77f4103937bbb9a70c5d4c8": {\n    "metric_1": 0.8548586864241012,\n    "metric_0": 0.9891059238008051,\n    "metric_3": 1.7255056515257134,\n    "metric_2": -0.958042549612474\n  },\n  "f662d9fdd072422899f1a91dca132e45": {\n    "metric_eq_ts_step": 4.7\n  },\n  "8c27fb8d3d734fc988eafc7e130af2c1": {\n    "metric_1": 0.2568792149591985,\n    "metric_0": -2.9251778063945633,\n    "metric_3": 3.5795601350695536,\n    "metric_2": 2.2358743640336654\n  },\n  "46830b05f4914ce980ee960921474308": {\n    "metric_1": -2.4512019495254855,\n    "metric_0": -1.7363712773941686,\n    "metric_3": -0.6764695293332332,\n    "metric_2": 0.902757467400038\n  },\n  "2bf6a4001dda47a89bd0dd1b638900c8": {\n    "metric_1": 1.4881757006157033,\n    "metric_0": -2.184521948675489,\n    "metric_3": 1.5024111371215891,\n    "metric_2": -0.9992038125180369\n  },\n  "bf3f29f1c16741e8b2de46b8af7f26db": {\n    "metric_1": -2.856136683397856,\n    "metric_0": 1.7351206329209488,\n    "metric_3": 3.3574106252540243,\n    "metric_2": 1.898369877601997\n  },\n  "8a56867ed3af4ead84ac5c215c43d2f2": {\n    "metric_1": -1.6845465954407057,\n    "metric_0": 2.799113454790449,\n    "metric_3": -1.9554854183785264,\n    "metric_2": 1.2596708758008042\n  },\n  "0e77b745f78c4985825c66aafb15f45e": {\n    "metric_1": -0.5966921249245285,\n    "metric_0": 0.38303481722942134,\n    "metric_3": 1.4751754578912797,\n    "metric_2": -2.935488516752223\n  },\n  "f8a6361f177a40b291751d743ec0cd52": {\n    "metric_1": -2.52066603748033,\n    "metric_0": -0.7281695813441598,\n    "metric_3": 0.3529736105779344,\n    "metric_2": -1.603741841379303\n  },\n  "8c6d7c99dc014b15b5ccec784110b83f": {\n    "metric_1": -1.2231009149094496,\n    "metric_0": 1.4079626574511428,\n  ', '  "metric_3": 3.3574106252540243,\n    "metric_2": 1.898369877601997\n  },\n  "8a56867ed3af4ead84ac5c215c43d2f2": {\n    "metric_1": -1.6845465954407057,\n    "metric_0": 2.799113454790449,\n    "metric_3": -1.9554854183785264,\n    "metric_2": 1.2596708758008042\n  },\n  "0e77b745f78c4985825c66aafb15f45e": {\n    "metric_1": -0.5966921249245285,\n    "metric_0": 0.38303481722942134,\n    "metric_3": 1.4751754578912797,\n    "metric_2": -2.935488516752223\n  },\n  "f8a6361f177a40b291751d743ec0cd52": {\n    "metric_1": -2.52066603748033,\n    "metric_0": -0.7281695813441598,\n    "metric_3": 0.3529736105779344,\n    "metric_2": -1.603741841379303\n  },\n  "8c6d7c99dc014b15b5ccec784110b83f": {\n    "metric_1": -1.2231009149094496,\n    "metric_0": 1.4079626574511428,\n    "metric_3": 2.5892028641452907,\n    "metric_2": 3.643033981543657\n  },\n  "9af4b84d78524c4ab08161e7b5f7f2dc": {\n    "metric_1": 0.2702968551842675,\n    "metric_0": 0.11248586282952555,\n    "metric_3": -1.533809108651962,\n    "metric_2": 1.678493181317803\n  }\n}\n', '{\n  "kube-context": "docker-for-desktop",\n  "kube-job-template-path": "examples/docker/kubernetes_job_template.yaml",\n  "repository-uri": "username/mlflow-kubernetes-example"\n}\n', '{ "messages": [{ "role": "user", "content": "What is Retrieval-augmented Generation?" }] }\n', 'build_dependencies:\n  - pip\ndependencies:\n  - diviner\n  - mlflow>=1.24.1\n', 'apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: "{replaced with MLflow Project name}"\n  namespace: mlflow\nspec:\n  ttlSecondsAfterFinished: 100\n  backoffLimit: 0\n  template:\n    spec:\n      containers:\n        - name: "{replaced with MLflow Project name}"\n          image: "{replaced with URI of Docker image created during Project execution}"\n          command: ["{replaced with MLflow Project entry point command}"]\n          resources:\n            limits:\n              memory: 512Mi\n            requests:\n              memory: 256Mi\n      restartPolicy: Never\n', 'build_dependencies:\n  - pip==22.2.2\ndependencies:\n  - mlflow>=1.6\n  - pandas==1.5.0\n  - scikit-learn==1.1.3\n  - tensorflow==2.10.0\n  - pillow==9.2.0\n', 'endpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: ai21labs\n      name: j2-mid\n      config:\n        ai21labs_api_key: $AI21LABS_API_KEY\n', 'endpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: anthropic\n      name: claude-1.3-100k\n      config:\n        anthropic_api_key: $ANTHROPIC_API_KEY\n', 'endpoints:\n  - name: chat\n    endpoint_type: llm/v1/chat\n    model:\n      provider: openai\n      name: gpt-4o-mini\n      config:\n        openai_api_type: "azure"\n        openai_api_key: $OPENAI_API_KEY\n        openai_deployment_name: "{your_deployment_name}"\n        openai_api_base: "https://{your_resource_name}-azureopenai.openai.azure.com/"\n        openai_api_version: "2023-05-15"\n\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: openai\n      name: gpt-4o-mini\n      config:\n        openai_api_type: "azuread"\n        openai_api_key: $AZURE_AAD_TOKEN\n        openai_deployment_name: "{your_deployment_name}"\n        openai_api_base: "https://{your_resource_name}-azureopenai.openai.azure.com/"\n        openai_api_version: "2023-05-15"\n\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: openai\n      name: text-embedding-ada-002\n      config:\n        openai_api_type: "azure"\n        openai_api_key: $OPENAI_API_KEY\n        openai_deployment_name: "{your_deployment_name}"\n        openai_api_base: "https://{your_resource_name}-azureopenai.openai.azure.com/"\n        openai_api_version: "2023-05-15"\n', 'endpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: amazon-bedrock\n      name: amazon.titan-tg1-large\n      config:\n        aws_config:\n          aws_region: us-east-1\n          aws_access_key_id: $AWS_ACCESS_KEY_ID\n          aws_secret_access_key: $AWS_SECRET_ACCESS_KEY\n', 'endpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: cohere\n      name: command\n      config:\n        cohere_api_key: $COHERE_API_KEY\n\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: cohere\n      name: embed-english-light-v2.0\n      config:\n        cohere_api_key: $COHERE_API_KEY\n', 'endpoints:\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: gemini\n      name: gemini-embedding-exp-03-07\n      config:\n        gemini_api_key: $GEMINI_API_KEY\n\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: gemini\n      name: gemini-2.0-flash\n      config:\n        gemini_api_key: $GEMINI_API_KEY\n\n  - name: chat\n    endpoint_type: llm/v1/chat\n    model:\n      provider: gemini\n      name: gemini-2.0-flash\n      config:\n        gemini_api_key: $GEMINI_API_KEY\n', 'endpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: "huggingface-text-generation-inference"\n      name: falcon-7b-instruct\n      config:\n        hf_server_url: http://127.0.0.1:8080\n', 'endpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: mistral\n      name: mistral-tiny\n      config:\n        mistral_api_key: $MISTRAL_API_KEY\n\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: mistral\n      name: mistral-embed\n      config:\n        mistral_api_key: $MISTRAL_API_KEY\n', 'endpoints:\n  - name: fillmask\n    endpoint_type: llm/v1/completions\n    model:\n      provider: mlflow-model-serving\n      name: mask-fill\n      config:\n        model_server_url: http://127.0.0.1:9010\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: mlflow-model-serving\n      name: sentence-transformer\n      config:\n        model_server_url: http://127.0.0.1:9020\n', 'endpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: mosaicml\n      name: mpt-7b-instruct\n      config:\n        mosaicml_api_key: $MOSAICML_API_KEY\n\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: mosaicml\n      name: instructor-xl\n      config:\n        mosaicml_api_key: $MOSAICML_API_KEY\n\n  - name: chat\n    endpoint_type: llm/v1/chat\n    model:\n      provider: mosaicml\n      name: llama2-70b-chat\n      config:\n        mosaicml_api_key: $MOSAICML_API_KEY\n', 'endpoints:\n  - name: chat\n    endpoint_type: llm/v1/chat\n    model:\n      provider: openai\n      name: gpt-4o-mini\n      config:\n        openai_api_key: $OPENAI_API_KEY\n    limit:\n      renewal_period: minute\n      calls: 10\n\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: openai\n      name: gpt-4o-mini\n      config:\n        openai_api_key: $OPENAI_API_KEY\n\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: openai\n      name: text-embedding-ada-002\n      config:\n        openai_api_key: $OPENAI_API_KEY\n', 'endpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: palm\n      name: text-bison-001\n      config:\n        palm_api_key: $PALM_API_KEY\n\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: palm\n      name: embedding-gecko-001\n      config:\n        palm_api_key: $PALM_API_KEY\n\n  - name: chat\n    endpoint_type: llm/v1/chat\n    model:\n      provider: palm\n      name: chat-bison-001\n      config:\n        palm_api_key: $PALM_API_KEY\n', 'endpoints:\n  - name: chat\n    endpoint_type: llm/v1/chat\n    model:\n      provider: my_llm\n      name: my-model-0.1.2\n      config:\n        my_llm_api_key: $MY_LLM_API_KEY\n', 'endpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: togetherai\n      name: mistralai/Mixtral-8x7B-v0.1\n      config:\n        togetherai_api_key: $TOGETHERAI_API_KEY\n\n  - name: chat\n    endpoint_type: llm/v1/chat\n    model:\n      provider: togetherai\n      name: mistralai/Mixtral-8x7B-Instruct-v0.1\n      config:\n        togetherai_api_key: $TOGETHERAI_API_KEY\n\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: togetherai\n      name: togethercomputer/m2-bert-80M-8k-retrieval\n      config:\n        togetherai_api_key: $TOGETHERAI_API_KEY\n', 'build_dependencies:\n  - pip\ndependencies:\n  - h2o\n  - mlflow>=1.0\n  - numpy\n  - pandas\n', 'build_dependencies:\n  - pip\ndependencies:\n  - numpy\n  - click\n  - pandas\n  - scipy\n  - scikit-learn\n  - tensorflow==2.10.0\n  - matplotlib\n  - mlflow>=1.6\n  - hyperopt\n  - protobuf<4.0.0\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow>=1.6.0\n  - matplotlib\n  - lightgbm\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow>=1.6.0\n  - matplotlib\n  - lightgbm\n  - cloudpickle>=2.0.0\n', 'python: "3.10"\nbuild_dependencies:\n  - pip\ndependencies:\n  - openai>=0.27.2\n  - tiktoken>=0.4.0\n  - tenacity>=8.2.2\n  - mlflow>=2.4.0\n', 'python: "3.10"\nbuild_dependencies:\n  - pip\ndependencies:\n  - langchain>=0.0.244\n  - openai>=0.27.2\n  - evaluate>=0.4.0\n  - mlflow>=2.4.0\n  - tiktoken>=0.4.0\n', 'build_dependencies:\n  - pip\ndependencies:\n  - tensorflow==1.15.2\n  - keras==2.2.4\n  - mlflow>=1.0\n  - pyspark\n  - requests\n  - click\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - paddlepaddle==2.1.0\n  - cloudpickle==1.6.0\n', 'build_dependencies:\n  - pip\ndependencies:\n  - pmdarima\n  - mlflow>=1.23.1\n', '$schema: https://azuremlschemas.azureedge.net/promptflow/latest/Flow.schema.json\n\ninputs:\n  text:\n    type: string\n    default: Hello World!\n\noutputs:\n  output:\n    type: string\n    reference: ${llm.output}\n\nnodes:\n  - name: hello_prompt\n    type: python\n    source:\n      type: code\n      path: render_template.py\n    inputs:\n      text: ${inputs.text}\n      template: |\n        system:\n        Your task is to generate what I ask.\n        user:\n        Write a simple {{text}} program that displays the greeting message.\n  - name: llm\n    type: python\n    source:\n      type: code\n      path: hello.py\n    inputs:\n      prompt: ${hello_prompt.output}\n      deployment_name: gpt-4o-mini\n      max_tokens: "120"\nenvironment:\n  image: mcr.microsoft.com/azureml/promptflow/promptflow-runtime:latest\n  python_requirements_txt: requirements.txt\n', 'build_dependencies:\n  - pip\ndependencies:\n  - prophet>=1.0.1\n', 'build_dependencies:\n  - pip\ndependencies:\n  - torch\n  - torchvision\n  - mlflow\n  - tensorboardX\n', 'channels:\n  - conda-forge\ndependencies:\n  - python=3.8.2\n  - pip\n  - pip:\n      - mlflow\n      - lightning==2.0.0\n      - jsonargparse[signatures]>=4.17.0\n      # typeguard is used for type validation in the ax-platform code base. 3.0.0 release has\n      # breaking changes that need to be resolved in ax. Remove this pin when\n      # https://github.com/facebook/Ax/issues/1509 is addressed\n      - typeguard<3.0.0\n      - ax-platform\n      - torchvision>=0.15.1\n      - torch>=2.0\n      # gyptorch 1.9.x is incompatible with the versions of botorch\n      # required by many versions of pytorch\n      - gpytorch<1.9.0\n      - protobuf<4.0.0\n      # Pinning pandas version less than 1.4.4 due to https://github.com/facebook/Ax/issues/1153\n      - pandas<=1.4.4\n      # Numpy>=2 is not compatible with pandas<=1.4.4\n      - numpy<2\n      # TODO: Remove this requirement once ax-platform achieves compatibility with SQLAlchemy 2.x\n      - sqlalchemy<2\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - lightning==2.0.0\n  - jsonargparse[signatures]>=4.17.0\n  # typeguard is used for type validation in the ax-platform code base. 3.0.0 release has\n  # breaking changes that need to be resolved in ax. Remove this pin when\n  # https://github.com/facebook/Ax/issues/1509 is addressed\n  - typeguard<3.0.0\n  - ax-platform\n  - torchvision>=0.15.1\n  - torch>=2.0\n  # gyptorch 1.9.x is incompatible with the versions of botorch\n  # required by many versions of pytorch\n  - gpytorch<1.9.0\n  - protobuf<4.0.0\n  # Pinning pandas version less than 1.4.4 due to https://github.com/facebook/Ax/issues/1153\n  - pandas<=1.4.4\n  # Numpy>=2 is not compatible with pandas<=1.4.4\n  - numpy<2\n  # TODO: Remove this requirement once ax-platform achieves compatibility with SQLAlchemy 2.x\n  - sqlalchemy<2\n', 'channels:\n  - conda-forge\ndependencies:\n  - python=3.8.2\n  - pip\n  - pip:\n      - mlflow\n      - scikit-learn\n      - cloudpickle==1.6.0\n      - boto3\n      - transformers>=4.0.0\n      - pandas\n      - numpy<2.0\n      - torch>=2.0.0\n      - torchdata>=0.6.0\n      - torchtext==0.15.1\n      - lightning==2.0.0\n      - jsonargparse[signatures]>=4.17.0\n      - protobuf<4.0.0\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - scikit-learn\n  - cloudpickle==1.6.0\n  - boto3\n  - transformers>=4.0.0\n  - pandas\n  - numpy<2.0\n  - torch>=2.0\n  - torchdata\n  - torchtext==0.16.2\n  - lightning\n  - jsonargparse[signatures]>=4.17.0\n  - protobuf<4.0.0\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - pandas\n  - scipy\n  - captum\n  - boto3\n  - scikit-learn\n  - prettytable\n  - ipython\n  - torch\n', 'channels:\n  - conda-forge\ndependencies:\n  - python=3.8.2\n  - pip\n  - pip:\n      - mlflow\n      - torchvision>=0.15.1\n      - cloudpickle==1.6.0\n      - lightning==2.0.0\n      - jsonargparse[signatures]>=4.17.0\n      # typeguard is used for type validation in the ax-platform code base. 3.0.0 release has\n      # breaking changes that need to be resolved in ax. Remove this pin when\n      # https://github.com/facebook/Ax/issues/1509 is addressed\n      - typeguard<3.0.0\n      - ax-platform\n      - prettytable\n      - torch>=2.0\n      - protobuf<4.0.0\n      # gyptorch 1.9.x is incompatible with the versions of botorch\n      # required by many versions of pytorch\n      - gpytorch<1.9.0\n      # Pinning pandas version less than 1.4.4 due to https://github.com/facebook/Ax/issues/1153\n      - pandas<=1.4.4\n      # Numpy>=2 is not compatible with pandas<=1.4.4\n      - numpy<2\n      # ax-platform 0.2.x is not yet compatible with SQLAlchemy 2.x\n      # TODO: Remove this requirement once ax-platform achieves compatibility with SQLAlchemy 2.x\n      - sqlalchemy<2\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - torchvision>=0.15.1\n  - cloudpickle==1.6.0\n  - lightning==2.0.0\n  - jsonargparse[signatures]>=4.17.0\n  # typeguard is used for type validation in the ax-platform code base. 3.0.0 release has\n  # breaking changes that need to be resolved in ax. Remove this pin when\n  # https://github.com/facebook/Ax/issues/1509 is addressed\n  - typeguard<3.0.0\n  - ax-platform\n  - prettytable\n  - torch>=2.0\n  # gyptorch 1.9.x is incompatible with the versions of botorch\n  # required by many versions of pytorch\n  - gpytorch<1.9.0\n  - protobuf<4.0.0\n  # Pinning pandas version less than 1.4.4 due to https://github.com/facebook/Ax/issues/1153\n  - pandas<=1.4.4\n  # Numpy>=2 is not compatible with pandas<=1.4.4\n  - numpy<2\n  # ax-platform 0.2.x is not yet compatible with SQLAlchemy 2.x\n  # TODO: Remove this requirement once ax-platform achieves compatibility with SQLAlchemy 2.x\n  - sqlalchemy<2\n', 'channels:\n  - conda-forge\ndependencies:\n  - python=3.8.2\n  - pip\n  - pip:\n      - mlflow\n      - torchvision>=0.15.1\n      - torch>=2.0\n      - lightning==2.0.0\n      - jsonargparse[signatures]>=4.17.0\n      - protobuf<4.0.0\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - torchvision>=0.15.1\n  - torch>=2.0\n  - lightning==2.0.0\n  - jsonargparse[signatures]>=4.17.0\n  - protobuf<4.0.0\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - torch==1.8.0\n  - torchvision==0.9.1\n  - pytorch-lightning==1.0.2\n', 'build_dependencies:\n  - pip\ndependencies:\n  - scikit-learn\n  - cloudpickle==1.6.0\n  - boto3\n  - torchvision>=0.9.1\n  - torch>=1.9.0\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - cloudpickle==1.6.0\n  - boto3\n  - torchvision>=0.9.1\n  - torch>=1.9.0\n', 'name: mlflow\nchannels:\n  - rapidsai\n  - nvidia\n  - conda-forge\n  - defaults\ndependencies:\n  - _libgcc_mutex=0.1=conda_forge\n  - _openmp_mutex=4.5=1_llvm\n  - arrow-cpp=0.15.0=py37h090bef1_2\n  - bokeh=2.1.0=py37hc8dfbb8_0\n  - boost-cpp=1.70.0=h8e57a91_2\n  - brotli=1.0.7=he1b5a44_1002\n  - bzip2=1.0.8=h516909a_2\n  - c-ares=1.15.0=h516909a_1001\n  - ca-certificates=2020.4.5.2=hecda079_0\n  - certifi=2020.4.5.2=py37hc8dfbb8_0\n  - click=7.1.2=pyh9f0ad1d_0\n  - cloudpickle=1.4.1=py_0\n  - cudatoolkit=10.2.89=h6bb024c_0\n  - cudf=0.14.0=py37_0\n  - cudnn=7.6.5=cuda10.2_0\n  - cuml=0.14.0=cuda10.2_py37_0\n  - cupy=7.5.0=py37h940342b_0\n  - cytoolz=0.10.1=py37h516909a_0\n  - dask=2.18.1=py_0\n  - dask-core=2.18.1=py_0\n  - dask-cudf=0.14.0=py37_0\n  - distributed=2.18.0=py37hc8dfbb8_0\n  - dlpack=0.2=he1b5a44_1\n  - double-conversion=3.1.5=he1b5a44_2\n  - fastavro=0.23.4=py37h8f50634_0\n  - fastrlock=0.5=py37h3340039_0\n  - freetype=2.10.2=he06d7ca_0\n  - fsspec=0.7.4=py_0\n  - gflags=2.2.2=he1b5a44_1002\n  - glog=0.4.0=h49b9bf7_3\n  - grpc-cpp=1.23.0=h18db393_0\n  - heapdict=1.0.1=py_0\n  - icu=64.2=he1b5a44_1\n  - jinja2=2.11.2=pyh9f0ad1d_0\n  - joblib=0.15.1=py_0\n  - jpeg=9d=h516909a_0\n  - ld_impl_linux-64=2.33.1=h53a641e_7\n  - libblas=3.8.0=16_openblas\n  - libcblas=3.8.0=16_openblas\n  - libcudf=0.14.0=cuda10.2_0\n  - libcuml=0.14.0=cuda10.2_0\n  - libcumlprims=0.14.1=cuda10.2_0\n  - libedit=3.1.20181209=hc058e9b_0\n  - libevent=2.1.10=h72c5cf5_0\n  - libffi=3.3=he6710b0_1\n  - libgcc-ng=9.2.0=h24d8f2e_2\n  - libgfortran-ng=7.5.0=hdf63c60_6\n  - libhwloc=2.1.0=h3c4fd83_0\n  - libiconv=1.15=h516909a_1006\n  - liblapack=3.8.0=16_openblas\n  - libllvm8=8.0.1=hc9558a2_0\n  - libnvstrings=0.14.0=cuda10.2_0\n  - libopenblas=0.3.9=h5ec1e0e_0\n  - libpng=1.6.37=hed695b0_1\n  - libprotobuf=3.8.0=h8b12597_0\n  - librmm=0.14.0=cuda10.2_0\n  - libstdcxx-ng=9.1.0=hdf63c60_0\n  - libtiff=4.1.0=hfc65ed5_0\n  - libxml2=2.9.10=hee79883_0\n  - llvm-openmp=10.0.0=hc9558a2_0\n  - llvmlite=0.32.1=py37h5202443_0\n  - locket=0.2.0=py_2\n  - lz4-c=1.8.3=he1b5a44_1001\n  - markupsafe=1.1.1=py37h8f50634_1\n  - msgpack-python=1.0.0=py37h99015e2_1\n  - nccl=2.6.4.1=hc6a2c23_0\n  - ncurses=6.2=he6710b0_1\n  - numba=0.49.1=py37h0da4684_0\n  - numpy=1.17.5=py37h95a1406_0\n  - nvstrings=0.14.0=py37_0\n  - olefile=0.46=py_0\n  - openssl=1.1.1g=h516909a_0\n  - packaging=20.4=pyh9f0ad1d_0\n  - pandas=0.25.3=py37hb3f55d8_0\n  - parquet-cpp=1.5.1=2\n  - partd=1.1.0=py_0\n  - pillow=5.3.0=py37h00a061d_1000\n  - pip=20.1.1=py37_1\n  - psutil=5.7.0=py37h8f50634_1\n  - pyarrow=0.15.0=py37h8b68381_1\n  - pyparsing=2.4.7=pyh9f0ad1d_0\n  - python=3.8.13=h12debd9_0\n  - python-dateutil=2.8.1=py_0\n  - python_abi=3.8=2_cp38\n  - pytz=2020.1=pyh9f0ad1d_0\n  - pyyaml=5.3.1=py37h8f50634_0\n  - re2=2020.04.01=he1b5a44_0\n  - readline=8.0=h7b6447c_0\n  - rmm=0.14.0=py37_0\n  - setuptools=47.3.0=py37_0\n  - six=1.15.0=pyh9f0ad1d_0\n  - snappy=1.1.8=he1b5a44_2\n  - sortedcontainers=2.2.2=pyh9f0ad1d_0\n  - spdlog=1.6.1=hc9558a2_0\n  - sqlite=3.31.1=h62c20be_1\n  - tblib=1.6.0=py_0\n  - thrift-cpp=0.12.0=hf3afdfd_1004\n  - tk=8.6.8=hbc83047_0\n  - toolz=0.10.0=py_0\n  - tornado=6.0.4=py37h8f50634_1\n  - typing_extensions=3.7.4.2=py_0\n  - ucx=1.8.0+gf6ec8d4=cuda10.2_20\n  - ucx-py=0.14.0+gf6ec8d4=py37_0\n  - uriparser=0.9.3=he1b5a44_1\n  - wheel=0.34.2=py37_0\n  - xz=5.2.5=h7b6447c_0\n  - yaml=0.2.5=h516909a_0\n  - zict=2.0.0=py_0\n  - zlib=1.2.11=h7b6447c_3\n  - zstd=1.4.3=h3b9ef0a_0\n  - pip:\n    ', 'psutil=5.7.0=py37h8f50634_1\n  - pyarrow=0.15.0=py37h8b68381_1\n  - pyparsing=2.4.7=pyh9f0ad1d_0\n  - python=3.8.13=h12debd9_0\n  - python-dateutil=2.8.1=py_0\n  - python_abi=3.8=2_cp38\n  - pytz=2020.1=pyh9f0ad1d_0\n  - pyyaml=5.3.1=py37h8f50634_0\n  - re2=2020.04.01=he1b5a44_0\n  - readline=8.0=h7b6447c_0\n  - rmm=0.14.0=py37_0\n  - setuptools=47.3.0=py37_0\n  - six=1.15.0=pyh9f0ad1d_0\n  - snappy=1.1.8=he1b5a44_2\n  - sortedcontainers=2.2.2=pyh9f0ad1d_0\n  - spdlog=1.6.1=hc9558a2_0\n  - sqlite=3.31.1=h62c20be_1\n  - tblib=1.6.0=py_0\n  - thrift-cpp=0.12.0=hf3afdfd_1004\n  - tk=8.6.8=hbc83047_0\n  - toolz=0.10.0=py_0\n  - tornado=6.0.4=py37h8f50634_1\n  - typing_extensions=3.7.4.2=py_0\n  - ucx=1.8.0+gf6ec8d4=cuda10.2_20\n  - ucx-py=0.14.0+gf6ec8d4=py37_0\n  - uriparser=0.9.3=he1b5a44_1\n  - wheel=0.34.2=py37_0\n  - xz=5.2.5=h7b6447c_0\n  - yaml=0.2.5=h516909a_0\n  - zict=2.0.0=py_0\n  - zlib=1.2.11=h7b6447c_3\n  - zstd=1.4.3=h3b9ef0a_0\n  - pip:\n      - alembic==1.4.2\n      - attrs==19.3.0\n      - backcall==0.2.0\n      - bleach==3.1.5\n      - chardet==3.0.4\n      - cycler==0.10.0\n      - databricks-cli==0.11.0\n      - decorator==4.4.2\n      - defusedxml==0.6.0\n      - docker==4.2.1\n      - entrypoints==0.3\n      - flask==1.1.2\n      - future==0.18.2\n      - gitdb==4.0.5\n      - gitpython==3.1.3\n      - gorilla==0.3.0\n      - gunicorn==20.0.4\n      - hyperopt==0.2.4\n      - idna==2.9\n      - importlib-metadata==1.6.1\n      - ipykernel==5.3.0\n      - ipython==7.15.0\n      - ipython-genutils==0.2.0\n      - ipywidgets==7.5.1\n      - itsdangerous==1.1.0\n      - jedi==0.17.0\n      - json5==0.9.5\n      - jsonschema==3.2.0\n      - jupyter==1.0.0\n      - jupyter-client==6.1.3\n      - jupyter-console==6.1.0\n      - jupyter-core==4.6.3\n      - jupyterlab==2.1.4\n      - jupyterlab-server==1.1.5\n      - kiwisolver==1.2.0\n      - lab==6.0\n  ', 'ipython==7.15.0\n      - ipython-genutils==0.2.0\n      - ipywidgets==7.5.1\n      - itsdangerous==1.1.0\n      - jedi==0.17.0\n      - json5==0.9.5\n      - jsonschema==3.2.0\n      - jupyter==1.0.0\n      - jupyter-client==6.1.3\n      - jupyter-console==6.1.0\n      - jupyter-core==4.6.3\n      - jupyterlab==2.1.4\n      - jupyterlab-server==1.1.5\n      - kiwisolver==1.2.0\n      - lab==6.0\n      - mako==1.1.3\n      - matplotlib==3.2.2\n      - mistune==0.8.4\n      - mlflow==1.8.0\n      - nbconvert==5.6.1\n      - nbformat==5.0.7\n      - networkx==2.4\n      - notebook==6.0.3\n      - pandocfilters==1.4.2\n      - parso==0.7.0\n      - pexpect==4.8.0\n      - pickleshare==0.7.5\n      - prometheus-client==0.8.0\n      - prometheus-flask-exporter==0.14.1\n      - prompt-toolkit==3.0.5\n      - protobuf==3.12.2\n      - ptyprocess==0.6.0\n      - pygments==2.6.1\n      - pyrsistent==0.16.0\n      - python-editor==1.0.4\n      - pyzmq==19.0.1\n      - qtconsole==4.7.4\n      - qtpy==1.9.0\n      - querystring-parser==1.2.4\n      - requests==2.24.0\n      - scikit-learn==0.23.1\n      - scipy==1.4.1\n      - send2trash==1.5.0\n      - simplejson==3.17.0\n      - sklearn==0.0\n      - smmap==3.0.4\n      - sqlalchemy==1.3.13\n      - sqlparse==0.4.2\n      - tabulate==0.8.7\n      - terminado==0.8.3\n      - ', ' - qtconsole==4.7.4\n      - qtpy==1.9.0\n      - querystring-parser==1.2.4\n      - requests==2.24.0\n      - scikit-learn==0.23.1\n      - scipy==1.4.1\n      - send2trash==1.5.0\n      - simplejson==3.17.0\n      - sklearn==0.0\n      - smmap==3.0.4\n      - sqlalchemy==1.3.13\n      - sqlparse==0.4.2\n      - tabulate==0.8.7\n      - terminado==0.8.3\n      - testpath==0.4.4\n      - threadpoolctl==2.1.0\n      - tqdm==4.46.1\n      - traitlets==4.3.3\n      - txt2tags==3.7\n      - urllib3==1.25.9\n      - wcwidth==0.2.4\n      - webencodings==0.5.1\n      - websocket-client==0.57.0\n      - werkzeug==1.0.1\n      - widgetsnbextension==3.5.1\n      - zipp==3.1.0\n', 'name: mlflow\nchannels:\n  - rapidsai\n  - nvidia\n  - conda-forge\n  - defaults\ndependencies:\n  - _libgcc_mutex=0.1=conda_forge\n  - _openmp_mutex=4.5=1_llvm\n  - arrow-cpp=0.15.0=py37h090bef1_2\n  - bokeh=2.1.0=py37hc8dfbb8_0\n  - boost-cpp=1.70.0=h8e57a91_2\n  - brotli=1.0.7=he1b5a44_1002\n  - bzip2=1.0.8=h516909a_2\n  - c-ares=1.15.0=h516909a_1001\n  - ca-certificates=2020.4.5.2=hecda079_0\n  - certifi=2020.4.5.2=py37hc8dfbb8_0\n  - click=7.1.2=pyh9f0ad1d_0\n  - cloudpickle=1.4.1=py_0\n  - cudatoolkit=10.2.89=h6bb024c_0\n  - cudf=0.14.0=py37_0\n  - cudnn=7.6.5=cuda10.2_0\n  - cuml=0.14.0=cuda10.2_py37_0\n  - cupy=7.5.0=py37h940342b_0\n  - cytoolz=0.10.1=py37h516909a_0\n  - dask=2.18.1=py_0\n  - dask-core=2.18.1=py_0\n  - dask-cudf=0.14.0=py37_0\n  - distributed=2.18.0=py37hc8dfbb8_0\n  - dlpack=0.2=he1b5a44_1\n  - double-conversion=3.1.5=he1b5a44_2\n  - fastavro=0.23.4=py37h8f50634_0\n  - fastrlock=0.5=py37h3340039_0\n  - freetype=2.10.2=he06d7ca_0\n  - fsspec=0.7.4=py_0\n  - gflags=2.2.2=he1b5a44_1002\n  - glog=0.4.0=h49b9bf7_3\n  - grpc-cpp=1.23.0=h18db393_0\n  - heapdict=1.0.1=py_0\n  - icu=64.2=he1b5a44_1\n  - jinja2=2.11.2=pyh9f0ad1d_0\n  - joblib=0.15.1=py_0\n  - jpeg=9d=h516909a_0\n  - ld_impl_linux-64=2.33.1=h53a641e_7\n  - libblas=3.8.0=16_openblas\n  - libcblas=3.8.0=16_openblas\n  - libcudf=0.14.0=cuda10.2_0\n  - libcuml=0.14.0=cuda10.2_0\n  - libcumlprims=0.14.1=cuda10.2_0\n  - libedit=3.1.20181209=hc058e9b_0\n  - libevent=2.1.10=h72c5cf5_0\n  - libffi=3.3=he6710b0_1\n  - libgcc-ng=9.2.0=h24d8f2e_2\n  - libgfortran-ng=7.5.0=hdf63c60_6\n  - libhwloc=2.1.0=h3c4fd83_0\n  - libiconv=1.15=h516909a_1006\n  - liblapack=3.8.0=16_openblas\n  - libllvm8=8.0.1=hc9558a2_0\n  - libnvstrings=0.14.0=cuda10.2_0\n  - libopenblas=0.3.9=h5ec1e0e_0\n  - libpng=1.6.37=hed695b0_1\n  - libprotobuf=3.8.0=h8b12597_0\n  - librmm=0.14.0=cuda10.2_0\n  - libstdcxx-ng=9.1.0=hdf63c60_0\n  - libtiff=4.1.0=hfc65ed5_0\n  - libxml2=2.9.10=hee79883_0\n  - llvm-openmp=10.0.0=hc9558a2_0\n  - llvmlite=0.32.1=py37h5202443_0\n  - locket=0.2.0=py_2\n  - lz4-c=1.8.3=he1b5a44_1001\n  - markupsafe=1.1.1=py37h8f50634_1\n  - msgpack-python=1.0.0=py37h99015e2_1\n  - nccl=2.6.4.1=hc6a2c23_0\n  - ncurses=6.2=he6710b0_1\n  - numba=0.49.1=py37h0da4684_0\n  - numpy=1.17.5=py37h95a1406_0\n  - nvstrings=0.14.0=py37_0\n  - olefile=0.46=py_0\n  - openssl=1.1.1g=h516909a_0\n  - packaging=20.4=pyh9f0ad1d_0\n  - pandas=0.25.3=py37hb3f55d8_0\n  - parquet-cpp=1.5.1=2\n  - partd=1.1.0=py_0\n  - pillow=5.3.0=py37h00a061d_1000\n  - pip=20.1.1=py37_1\n  - psutil=5.7.0=py37h8f50634_1\n  - pyarrow=0.15.0=py37h8b68381_1\n  - pyparsing=2.4.7=pyh9f0ad1d_0\n  - python=3.8.13=h12debd9_0\n  - python-dateutil=2.8.1=py_0\n  - python_abi=3.8=2_cp38\n  - pytz=2020.1=pyh9f0ad1d_0\n  - pyyaml=5.3.1=py37h8f50634_0\n  - re2=2020.04.01=he1b5a44_0\n  - readline=8.0=h7b6447c_0\n  - rmm=0.14.0=py37_0\n  - setuptools=47.3.0=py37_0\n  - six=1.15.0=pyh9f0ad1d_0\n  - snappy=1.1.8=he1b5a44_2\n  - sortedcontainers=2.2.2=pyh9f0ad1d_0\n  - spdlog=1.6.1=hc9558a2_0\n  - sqlite=3.31.1=h62c20be_1\n  - tblib=1.6.0=py_0\n  - thrift-cpp=0.12.0=hf3afdfd_1004\n  - tk=8.6.8=hbc83047_0\n  - toolz=0.10.0=py_0\n  - tornado=6.0.4=py37h8f50634_1\n  - typing_extensions=3.7.4.2=py_0\n  - ucx=1.8.0+gf6ec8d4=cuda10.2_20\n  - ucx-py=0.14.0+gf6ec8d4=py37_0\n  - uriparser=0.9.3=he1b5a44_1\n  - wheel=0.34.2=py37_0\n  - xz=5.2.5=h7b6447c_0\n  - yaml=0.2.5=h516909a_0\n  - zict=2.0.0=py_0\n  - zlib=1.2.11=h7b6447c_3\n  - zstd=1.4.3=h3b9ef0a_0\n  - pip:\n    ', 'psutil=5.7.0=py37h8f50634_1\n  - pyarrow=0.15.0=py37h8b68381_1\n  - pyparsing=2.4.7=pyh9f0ad1d_0\n  - python=3.8.13=h12debd9_0\n  - python-dateutil=2.8.1=py_0\n  - python_abi=3.8=2_cp38\n  - pytz=2020.1=pyh9f0ad1d_0\n  - pyyaml=5.3.1=py37h8f50634_0\n  - re2=2020.04.01=he1b5a44_0\n  - readline=8.0=h7b6447c_0\n  - rmm=0.14.0=py37_0\n  - setuptools=47.3.0=py37_0\n  - six=1.15.0=pyh9f0ad1d_0\n  - snappy=1.1.8=he1b5a44_2\n  - sortedcontainers=2.2.2=pyh9f0ad1d_0\n  - spdlog=1.6.1=hc9558a2_0\n  - sqlite=3.31.1=h62c20be_1\n  - tblib=1.6.0=py_0\n  - thrift-cpp=0.12.0=hf3afdfd_1004\n  - tk=8.6.8=hbc83047_0\n  - toolz=0.10.0=py_0\n  - tornado=6.0.4=py37h8f50634_1\n  - typing_extensions=3.7.4.2=py_0\n  - ucx=1.8.0+gf6ec8d4=cuda10.2_20\n  - ucx-py=0.14.0+gf6ec8d4=py37_0\n  - uriparser=0.9.3=he1b5a44_1\n  - wheel=0.34.2=py37_0\n  - xz=5.2.5=h7b6447c_0\n  - yaml=0.2.5=h516909a_0\n  - zict=2.0.0=py_0\n  - zlib=1.2.11=h7b6447c_3\n  - zstd=1.4.3=h3b9ef0a_0\n  - pip:\n      - alembic==1.4.2\n      - attrs==19.3.0\n      - backcall==0.2.0\n      - bleach==3.1.5\n      - chardet==3.0.4\n      - cycler==0.10.0\n      - databricks-cli==0.11.0\n      - decorator==4.4.2\n      - defusedxml==0.6.0\n      - docker==4.2.1\n      - entrypoints==0.3\n      - flask==1.1.2\n      - future==0.18.2\n      - gitdb==4.0.5\n      - gitpython==3.1.3\n      - gorilla==0.3.0\n      - gunicorn==20.0.4\n      - hyperopt==0.2.4\n      - idna==2.9\n      - importlib-metadata==1.6.1\n      - ipykernel==5.3.0\n      - ipython==7.15.0\n      - ipython-genutils==0.2.0\n      - ipywidgets==7.5.1\n      - itsdangerous==1.1.0\n      - jedi==0.17.0\n      - json5==0.9.5\n      - jsonschema==3.2.0\n      - jupyter==1.0.0\n      - jupyter-client==6.1.3\n      - jupyter-console==6.1.0\n      - jupyter-core==4.6.3\n      - jupyterlab==2.1.4\n      - jupyterlab-server==1.1.5\n      - kiwisolver==1.2.0\n      - lab==6.0\n  ', 'ipython==7.15.0\n      - ipython-genutils==0.2.0\n      - ipywidgets==7.5.1\n      - itsdangerous==1.1.0\n      - jedi==0.17.0\n      - json5==0.9.5\n      - jsonschema==3.2.0\n      - jupyter==1.0.0\n      - jupyter-client==6.1.3\n      - jupyter-console==6.1.0\n      - jupyter-core==4.6.3\n      - jupyterlab==2.1.4\n      - jupyterlab-server==1.1.5\n      - kiwisolver==1.2.0\n      - lab==6.0\n      - mako==1.1.3\n      - matplotlib==3.2.2\n      - mistune==0.8.4\n      - mlflow==1.8.0\n      - nbconvert==5.6.1\n      - nbformat==5.0.7\n      - networkx==2.4\n      - notebook==6.0.3\n      - pandocfilters==1.4.2\n      - parso==0.7.0\n      - pexpect==4.8.0\n      - pickleshare==0.7.5\n      - prometheus-client==0.8.0\n      - prometheus-flask-exporter==0.14.1\n      - prompt-toolkit==3.0.5\n      - protobuf==3.12.2\n      - ptyprocess==0.6.0\n      - pygments==2.6.1\n      - pyrsistent==0.16.0\n      - python-editor==1.0.4\n      - pyzmq==19.0.1\n      - qtconsole==4.7.4\n      - qtpy==1.9.0\n      - querystring-parser==1.2.4\n      - requests==2.24.0\n      - scikit-learn==0.23.1\n      - scipy==1.4.1\n      - send2trash==1.5.0\n      - simplejson==3.17.0\n      - sklearn==0.0\n      - smmap==3.0.4\n      - sqlalchemy==1.3.13\n      - sqlparse==0.4.2\n      - tabulate==0.8.7\n      - terminado==0.8.3\n      - ', ' - qtconsole==4.7.4\n      - qtpy==1.9.0\n      - querystring-parser==1.2.4\n      - requests==2.24.0\n      - scikit-learn==0.23.1\n      - scipy==1.4.1\n      - send2trash==1.5.0\n      - simplejson==3.17.0\n      - sklearn==0.0\n      - smmap==3.0.4\n      - sqlalchemy==1.3.13\n      - sqlparse==0.4.2\n      - tabulate==0.8.7\n      - terminado==0.8.3\n      - testpath==0.4.4\n      - threadpoolctl==2.1.0\n      - tqdm==4.46.1\n      - traitlets==4.3.3\n      - txt2tags==3.7\n      - urllib3==1.25.9\n      - wcwidth==0.2.4\n      - webencodings==0.5.1\n      - websocket-client==0.57.0\n      - werkzeug==1.0.1\n      - widgetsnbextension==3.5.1\n      - zipp==3.1.0\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - numpy\n  - matplotlib\n  - pandas\n  - scipy\n  - scikit-learn\n  - cloudpickle\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow>=1.0\n  - cloudpickle\n  - numpy\n  - matplotlib\n  - pandas\n  - scikit-learn\n', 'name: tutorial\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.9\n  - pip\n  - pip:\n      - scikit-learn==1.4.2\n      - mlflow>=1.0\n      - pandas\n', 'build_dependencies:\n  - pip\ndependencies:\n  - scikit-learn==1.4.2\n  - mlflow>=1.0\n  - pandas\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow>=1.0\n  - scipy\n  - scikit-learn\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow<3,>=2.1\n  - sktime==0.16.0\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow>=1.0\n  - spacy==3.8.2\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - statsmodels\n  - scikit-learn\n', 'python: "3.10"\nbuild_dependencies:\n  - pip\ndependencies:\n  - numpy\n  - pandas\n  - scipy\n  - scikit-learn\n  - mlflow\n', 'name: tutorial\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.8\n  - pip\n  - pip:\n      - mlflow>=2.0\n      - tensorflow>=2.8\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow>=2.0\n  - tensorflow>=2.8\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - scikit-learn\n  - matplotlib\n  - xgboost\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - pandas\n  - scikit-learn\n  - xgboost\n', '# These are the core requirements for the complete MLflow platform, which augments\n# the skinny client functionality with support for running the MLflow Tracking\n# Server & UI. It also adds project backends such as Docker and Kubernetes among\n# other capabilities. When we release a new major/minor version, this file is\n# automatically updated as a part of the release process.\n\nalembic:\n  pip_release: alembic\n  max_major_version: 1\n  # alembic 1.10.0 contains a regression: https://github.com/sqlalchemy/alembic/issues/1195\n  unsupported: ["1.10.0"]\n\ndocker:\n  pip_release: docker\n  minimum: "4.0.0"\n  max_major_version: 7\n\nflask:\n  pip_release: Flask\n  max_major_version: 3\n\nnumpy:\n  pip_release: numpy\n  max_major_version: 2\n\nscipy:\n  pip_release: scipy\n  max_major_version: 1\n\npandas:\n  pip_release: pandas\n  max_major_version: 2\n\nsqlalchemy:\n  pip_release: sqlalchemy\n  minimum: "1.4.0"\n  max_major_version: 2\n\ncryptography:\n  pip_release: cryptography\n  minimum: "43.0.0"\n  max_major_version: 45\n\ngunicorn:\n  pip_release: gunicorn\n  max_major_version: 23\n  markers: "platform_system != \'Windows\'"\n\nwaitress:\n  pip_release: waitress\n  max_major_version: 3\n  markers: "platform_system == \'Windows\'"\n\nscikit-learn:\n  pip_release: scikit-learn\n  max_major_version: 1\n\npyarrow:\n  pip_release: pyarrow\n  minimum: "4.0.0"\n  max_major_version: 21\n\nmatplotlib:\n  pip_release: matplotlib\n  max_major_version: 3\n\ngraphene:\n  pip_release: graphene\n  max_major_version: 3\n\nfastmcp:\n  pip_release: fastmcp\n  minimum: "2.0.0"\n  max_major_version: 2\n', '# These are the extra requirements for MLflow Gateway, which can be installed\n# on top of the core requirements using `pip install mlflow[gateway]`.\n# When we release a new major/minor version, this file is automatically updated\n# as a part of the release process.\n\nfastapi:\n  pip_release: fastapi\n  max_major_version: 0\n\nuvicorn:\n  pip_release: uvicorn\n  extras:\n    - standard\n  max_major_version: 0\n\nwatchfiles:\n  pip_release: watchfiles\n  max_major_version: 1\n\naiohttp:\n  pip_release: aiohttp\n  max_major_version: 3\n\nboto3:\n  pip_release: boto3\n  minimum: "1.28.56"\n  max_major_version: 1\n\ntiktoken:\n  pip_release: tiktoken\n  max_major_version: 0\n\nslowapi:\n  pip_release: slowapi\n  max_major_version: 0\n  minimum: "0.1.9"\n', '# Minimal requirements for the skinny MLflow client which provides a limited\n# subset of functionality such as: RESTful client functionality for Tracking and\n# Model Registry, as well as support for Project execution against local backends\n# and Databricks. When we release a new major/minor version, this file is automatically\n# updated as a part of the release process.\n\nclick:\n  pip_release: click\n  minimum: "7.0"\n  max_major_version: 8\n\ncloudpickle:\n  pip_release: cloudpickle\n  max_major_version: 3\n\npython-dotenv:\n  pip_release: python-dotenv\n  minimum: "0.19.0"\n  max_major_version: 1\n\ngitpython:\n  pip_release: gitpython\n  minimum: "3.1.9"\n  max_major_version: 3\n\npyyaml:\n  pip_release: pyyaml\n  minimum: "5.1"\n  max_major_version: 6\n\nprotobuf:\n  pip_release: protobuf\n  minimum: "3.12.0"\n  max_major_version: 6\n\nrequests:\n  pip_release: requests\n  minimum: "2.17.3"\n  max_major_version: 2\n\npackaging:\n  pip_release: packaging\n  max_major_version: 25\n\nimportlib_metadata:\n  pip_release: importlib_metadata\n  # Automated dependency detection in MLflow Models relies on\n  # `importlib_metadata.packages_distributions` to resolve a module name to its package name\n  # (e.g. \'sklearn\' -> \'scikit-learn\'). importlib_metadata 3.7.0 or newer supports this function:\n  # https://github.com/python/importlib_metadata/blob/main/CHANGES.rst#v370\n  minimum: "3.7.0"\n  max_major_version: 8\n  unsupported: ["4.7.0"]\n\nsqlparse:\n  pip_release: sqlparse\n  # Lower bound sqlparse for: https://github.com/andialbrecht/sqlparse/pull/567\n  minimum: "0.4.0"\n  max_major_version: 0\n\n# Required for tracing\ncachetools:\n  pip_release: cachetools\n  minimum: "5.0.0"\n  max_major_version: 6\n\n# 1.9.0 is the minimum supported version as NoOpTracer was introduced in 1.9.0\nopentelemetry-api:\n  pip_release: opentelemetry-api\n  minimum: "1.9.0"\n  max_major_version: 2\n\nopentelemetry-sdk:\n  pip_release: opentelemetry-sdk\n  minimum: "1.9.0"\n  max_major_version: 2\n\nopentelemetry-proto:\n  pip_release: opentelemetry-proto\n  minimum: "1.9.0"\n  max_major_version: 2\n\ndatabricks-sdk:\n  pip_release: databricks-sdk\n  minimum: "0.20.0"\n  max_major_version: 0\n\npydantic:\n  pip_release: pydantic\n  minimum: "1.10.8"\n  max_major_version: 2\n\ntyping-extensions:\n  pip_release: typing-extensions\n  minimum: "4.0.0"\n  max_major_version: 4\n\nfastapi:\n  pip_release: fastapi\n  max_major_version: 0\n\nuvicorn:\n  pip_release: uvicorn\n  max_major_version: 0\n', '# Minimal requirements for the MLflow Tracing package. It is a lightweight\n# package that only includes the minimum set of dependencies and functionality\n# to instrument code/models/agents with MLflow Tracing.\n# When we release a new major/minor version, this file is automatically\n# updated as a part of the release process.\n\nprotobuf:\n  pip_release: protobuf\n  minimum: "3.12.0"\n  max_major_version: 6\n\npackaging:\n  pip_release: packaging\n  max_major_version: 25\n\n# Required for tracing\ncachetools:\n  pip_release: cachetools\n  minimum: "5.0.0"\n  max_major_version: 6\n\n# 1.9.0 is the minimum supported version as NoOpTracer was introduced in 1.9.0\nopentelemetry-api:\n  pip_release: opentelemetry-api\n  minimum: "1.9.0"\n  max_major_version: 2\n\nopentelemetry-sdk:\n  pip_release: opentelemetry-sdk\n  minimum: "1.9.0"\n  max_major_version: 2\n\nopentelemetry-proto:\n  pip_release: opentelemetry-proto\n  minimum: "1.9.0"\n  max_major_version: 2\n\ndatabricks-sdk:\n  pip_release: databricks-sdk\n  minimum: "0.20.0"\n  max_major_version: 0\n\npydantic:\n  pip_release: pydantic\n  minimum: "1.10.8"\n  max_major_version: 2\n', 'llm:\n  model_name: "gpt-4o-mini"\n  temperature: 0.7\n', 'embedding_model_query_instructions: "Represent this sentence for searching relevant passages:"\nllm_model: "databricks-dbrx-instruct"\nllm_prompt_template: "You are a trustful assistant."\nretriever_config:\n  k: 5\n  use_mmr: false\nllm_parameters:\n  temperature: 0.01\n  max_tokens: 500\nllm_prompt_template_variables:\n  - "chat_history"\n  - "context"\n  - "question"\n', 'embedding_model_query_instructions: "Represent this sentence for searching relevant passages:"\nllm_model: "databricks-dbrx-instruct"\nllm_prompt_template: "You are a trustful assistant. Answer concisely and clearly."\nretriever_config:\n  k: 5\n  use_mmr: false\nllm_parameters:\n  temperature: 0.01\n  max_tokens: 200\nllm_prompt_template_variables:\n  - "chat_history"\n  - "context"\n  - "question"\n', '$schema: https://azuremlschemas.azureedge.net/promptflow/latest/Flow.schema.json\n\ninputs:\n  text:\n    type: string\n    default: Hello World!\n\noutputs:\n  output:\n    type: string\n    reference: ${llm.output}\n\nnodes:\n  - name: hello_prompt\n    type: python\n    source:\n      type: code\n      path: render_template.py\n    inputs:\n      text: ${inputs.text}\n      template: |\n        system:\n        Your task is to generate what I ask.\n        user:\n        Write a simple {{text}} program that displays the greeting message.\n  - name: llm\n    type: python\n    source:\n      type: code\n      path: echo.py\n    inputs:\n      prompt: ${hello_prompt.output}\nenvironment:\n  image: mcr.microsoft.com/azureml/promptflow/promptflow-runtime:latest\n  python_requirements_txt: requirements.txt\nadditional_includes:\n  - ../additional_file\n', 'channels:\n  - conda-forge\ndependencies:\n  - python=3.10.16\n  - pip<=23.0.1\n  - pip:\n      - mlflow\n      - cloudpickle==2.2.1\n      - scikit-learn==1.5.2\nname: mlflow-env\n', 'python: 3.10.16\nbuild_dependencies:\n  - pip==23.0.1\n  - setuptools==58.1.0\n  - wheel==0.42.0\ndependencies:\n  - -r requirements.txt\n', 'channels:\n  - conda-forge\ndependencies:\n  - python=3.10.12\n  - pip<=22.3.1\n  - pip:\n      - bcrypt==3.2.0\n      - cloudpickle==2.0.0\n      - configparser==5.2.0\n      - cryptography==39.0.1\n      - databricks-feature-engineering==0.2.1\n      - entrypoints==0.4\n      - google-cloud-storage==2.11.0\n      - grpcio-status==1.48.1\n      - langchain==0.1.20\n      - mlflow[gateway]==2.12.2\n      - numpy==1.23.5\n      - packaging==23.2\n      - pandas==1.5.3\n      - protobuf==4.24.0\n      - psutil==5.9.0\n      - pyarrow==8.0.0\n      - pydantic==1.10.6\n      - pyyaml==6.0\n      - requests==2.28.1\n      - tornado==6.1\nname: mlflow-env\n', 'python: 3.10.12\nbuild_dependencies:\n  - pip==22.3.1\n  - setuptools==65.6.3\n  - wheel==0.38.4\ndependencies:\n  - -r requirements.txt\n', 'channels:\n  - conda-forge\ndependencies:\n  - python=3.10.12\n  - pip<=22.3.1\n  - pip:\n      - bcrypt==3.2.0\n      - cloudpickle==2.0.0\n      - configparser==5.2.0\n      - cryptography==39.0.1\n      - databricks-feature-engineering==0.2.1\n      - entrypoints==0.4\n      - google-cloud-storage==2.11.0\n      - grpcio-status==1.48.1\n      - langchain==0.1.20\n      - mlflow[gateway]==2.12.2\n      - numpy==1.23.5\n      - packaging==23.2\n      - pandas==1.5.3\n      - protobuf==4.24.0\n      - psutil==5.9.0\n      - pyarrow==8.0.0\n      - pydantic==1.10.6\n      - pyyaml==6.0\n      - requests==2.28.1\n      - tornado==6.1\nname: mlflow-env\n', 'python: 3.10.12\nbuild_dependencies:\n  - pip==22.3.1\n  - setuptools==65.6.3\n  - wheel==0.38.4\ndependencies:\n  - -r requirements.txt\n', '# Adding a comment here to distinguish the hash of this conda.yaml from the hashes of existing\n# conda.yaml files in the test environment.\nname: tutorial\nchannels:\n  - defaults\ndependencies:\n  - python=3.10\n  - pip:\n      - -e ../../../\n      - psutil\n', 'name: virtualenv-conda\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.10.16\n  - numpy\n  - pip\n  - pip:\n      - mlflow\n      - scikit-learn==1.4.2\n', 'python: "3.10.16"\nbuild_dependencies:\n  - pip\ndependencies:\n  - -r requirements.txt\n', 'python: "3.10.16"\nbuild_dependencies:\n  - pip\ndependencies:\n  - -r requirements.txt\n', 'channels:\n  - conda-forge\ndependencies:\n  - python=3.9.13\n  - pip<=23.3\n  - pip:\n      - mlflow==2.7.1\n      - cloudpickle==2.2.1\nname: mlflow-env\n', 'python: 3.9.13\nbuild_dependencies:\n  - pip==23.3\n  - setuptools\n  - wheel==0.41.2\ndependencies:\n  - -r requirements.txt\n', 'channels:\n  - conda-forge\ndependencies:\n  - python=3.9.13\n  - pip<=23.3\n  - pip:\n      - mlflow==2.8.1\n      - cloudpickle==2.2.1\nname: mlflow-env\n', 'python: 3.9.13\nbuild_dependencies:\n  - pip==23.3\n  - setuptools\n  - wheel==0.41.2\ndependencies:\n  - -r requirements.txt\n', 'volumes:\n  pgdata:\n  minio-data:\n\nservices:\n  postgres:\n    image: postgres:15\n    container_name: mlflow-postgres\n    environment:\n      POSTGRES_USER: ${POSTGRES_USER}\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n      POSTGRES_DB: ${POSTGRES_DB}\n    volumes:\n      - pgdata:/var/lib/postgresql/data\n    ports:\n      - "5432:5432"\n    healthcheck:\n      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]\n      interval: 5s\n      timeout: 3s\n      retries: 10\n\n  minio:\n    image: minio/minio:latest\n    container_name: mlflow-minio\n    environment:\n      MINIO_ROOT_USER: ${MINIO_ROOT_USER}\n      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}\n    volumes:\n      - minio-data:/data\n    command: server /data --console-address ":9001"\n    ports:\n      - "9000:9000"\n      - "9001:9001"\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${MINIO_PORT}/minio/health/live"]\n      interval: 5s\n      timeout: 3s\n      retries: 20\n\n  create-bucket:\n    image: minio/mc:latest\n    container_name: mlflow-create-bucket\n    depends_on:\n      minio:\n        condition: service_healthy\n    entrypoint: >\n      /bin/sh -c \'\n        mc alias set myminio http://${MINIO_HOST}:${MINIO_PORT} \\\n          ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD} &&\n        mc mb --ignore-existing myminio/${MINIO_BUCKET:-mlflow}\n      \'\n    restart: "no"\n\n  mlflow:\n    image: ghcr.io/mlflow/mlflow:${MLFLOW_VERSION}\n    container_name: mlflow-server\n    depends_on:\n      postgres:\n        condition: service_healthy\n      minio:\n        condition: service_healthy\n      create-bucket:\n       ', '  mc alias set myminio http://${MINIO_HOST}:${MINIO_PORT} \\\n          ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD} &&\n        mc mb --ignore-existing myminio/${MINIO_BUCKET:-mlflow}\n      \'\n    restart: "no"\n\n  mlflow:\n    image: ghcr.io/mlflow/mlflow:${MLFLOW_VERSION}\n    container_name: mlflow-server\n    depends_on:\n      postgres:\n        condition: service_healthy\n      minio:\n        condition: service_healthy\n      create-bucket:\n        condition: service_completed_successfully\n    environment:\n      # Backend store URI built from vars\n      MLFLOW_BACKEND_STORE_URI: ${MLFLOW_BACKEND_STORE_URI}\n\n      # S3/MinIO settings\n      MLFLOW_S3_ENDPOINT_URL: ${MLFLOW_S3_ENDPOINT_URL}\n      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}\n      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}\n      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION}\n      MLFLOW_S3_IGNORE_TLS: "true"\n\n      # Server host/port\n      MLFLOW_HOST: ${MLFLOW_HOST}\n      MLFLOW_PORT: ${MLFLOW_PORT}\n\n    command: >\n      /bin/bash -c "\n        pip install --no-cache-dir psycopg2-binary boto3 &&\n        mlflow server \\\n          --backend-store-uri ${MLFLOW_BACKEND_STORE_URI} \\\n          --default-artifact-root ${MLFLOW_DEFAULT_ARTIFACT_ROOT} \\\n          --host ${MLFLOW_HOST} \\\n          --port ${MLFLOW_PORT}\n      "\n    ports:\n      - "${MLFLOW_PORT}:${MLFLOW_PORT}"\n    healthcheck:\n      test:\n        [\n          "CMD",\n          "python",\n          "-c",\n          "import urllib.request; ', '\\\n          --host ${MLFLOW_HOST} \\\n          --port ${MLFLOW_PORT}\n      "\n    ports:\n      - "${MLFLOW_PORT}:${MLFLOW_PORT}"\n    healthcheck:\n      test:\n        [\n          "CMD",\n          "python",\n          "-c",\n          "import urllib.request; urllib.request.urlopen(\'http://localhost:${MLFLOW_PORT}/health\')",\n        ]\n      interval: 10s\n      timeout: 5s\n      retries: 30\n\nnetworks:\n  default:\n    name: mlflow-network\n', 'version: "3"\nservices:\n  minio:\n    image: minio/minio\n    expose:\n      - "9000"\n    ports:\n      - "9000:9000"\n      # MinIO Console is available at http://localhost:9001\n      - "9001:9001"\n    environment:\n      MINIO_ROOT_USER: "user"\n      MINIO_ROOT_PASSWORD: "password"\n    healthcheck:\n      test: timeout 5s bash -c \':> /dev/tcp/127.0.0.1/9000\' || exit 1\n      interval: 1s\n      timeout: 10s\n      retries: 5\n    # Note there is no bucket by default\n    command: server /data --console-address ":9001"\n\n  minio-create-bucket:\n    image: minio/mc\n    depends_on:\n      minio:\n        condition: service_healthy\n    entrypoint: >\n      bash -c "\n      mc alias set minio http://minio:9000 user password &&\n      if ! mc ls minio/bucket; then\n        mc mb minio/bucket\n      else\n        echo \'bucket already exists\'\n      fi\n      "\n\n  artifacts-server:\n    build:\n      context: .\n      dockerfile: "${DOCKERFILE:-Dockerfile}"\n    depends_on:\n      - minio-create-bucket\n    expose:\n      - "5500"\n    ports:\n      - "5500:5500"\n    environment:\n      MLFLOW_S3_ENDPOINT_URL: http://minio:9000\n      AWS_ACCESS_KEY_ID: "user"\n      AWS_SECRET_ACCESS_KEY: "password"\n    command: >\n      mlflow server\n      --host 0.0.0.0\n      --port 5500\n      --artifacts-destination s3://bucket\n      --gunicorn-opts "--log-level debug"\n   ', '  depends_on:\n      - minio-create-bucket\n    expose:\n      - "5500"\n    ports:\n      - "5500:5500"\n    environment:\n      MLFLOW_S3_ENDPOINT_URL: http://minio:9000\n      AWS_ACCESS_KEY_ID: "user"\n      AWS_SECRET_ACCESS_KEY: "password"\n    command: >\n      mlflow server\n      --host 0.0.0.0\n      --port 5500\n      --artifacts-destination s3://bucket\n      --gunicorn-opts "--log-level debug"\n      --artifacts-only\n\n  postgres:\n    image: postgres\n    restart: always\n    environment:\n      POSTGRES_DB: db\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n\n  tracking-server:\n    build:\n      context: .\n      dockerfile: "${DOCKERFILE:-Dockerfile}"\n    depends_on:\n      - postgres\n      - artifacts-server\n    expose:\n      - "5000"\n    ports:\n      # MLflow UI is available at http://localhost:5000\n      - "5000:5000"\n    command: >\n      mlflow server\n      --host 0.0.0.0\n      --port 5000\n      --backend-store-uri postgresql://user:password@postgres:5432/db\n      --default-artifact-root http://artifacts-server:5500/api/2.0/mlflow-artifacts/artifacts/experiments\n      --gunicorn-opts "--log-level debug"\n\n  client:\n    build:\n      context: .\n      dockerfile: "${DOCKERFILE:-Dockerfile}"\n    depends_on:\n      - tracking-server\n    environment:\n      MLFLOW_TRACKING_URI: http://tracking-server:5000\n', 'services:\n  base:\n    image: mlflow-base\n    build:\n      context: .\n    volumes:\n      - ${PWD}:/mlflow/home\n    working_dir: /mlflow/home\n    entrypoint: /mlflow/home/tests/db/entrypoint.sh\n    command: pytest tests/db\n    environment:\n      DISABLE_RESET_MLFLOW_URI_FIXTURE: "true"\n\n  postgresql:\n    image: postgres\n    restart: always\n    environment:\n      POSTGRES_DB: mlflowdb\n      POSTGRES_USER: mlflowuser\n      POSTGRES_PASSWORD: mlflowpassword\n\n  mlflow-postgresql:\n    depends_on:\n      - postgresql\n    extends:\n      service: base\n    environment:\n      MLFLOW_TRACKING_URI: postgresql://mlflowuser:mlflowpassword@postgresql:5432/mlflowdb\n      INSTALL_MLFLOW_FROM_REPO: true\n\n  migration-postgresql:\n    depends_on:\n      - postgresql\n    extends:\n      service: base\n    environment:\n      MLFLOW_TRACKING_URI: postgresql://mlflowuser:mlflowpassword@postgresql:5432/mlflowdb\n    command: tests/db/check_migration.sh\n\n  mysql:\n    image: mysql\n    restart: always\n    environment:\n      MYSQL_ROOT_PASSWORD: root-password\n      MYSQL_DATABASE: mlflowdb\n      MYSQL_USER: mlflowuser\n      MYSQL_PASSWORD: mlflowpassword\n\n  mlflow-mysql:\n    extends:\n      service: base\n    depends_on:\n      - mysql\n    environment:\n      MLFLOW_TRACKING_URI: mysql://mlflowuser:mlflowpassword@mysql:3306/mlflowdb?charset=utf8mb4\n      INSTALL_MLFLOW_FROM_REPO: true\n\n  migration-mysql:\n    extends:\n      service: base\n    depends_on:\n      - mysql\n    environment:\n      MLFLOW_TRACKING_URI: mysql://mlflowuser:mlflowpassword@mysql:3306/mlflowdb?charset=utf8mb4\n    command: tests/db/check_migration.sh\n\n  mssql:\n    image: mcr.microsoft.com/mssql/server\n    restart: always\n    environment:\n      ACCEPT_EULA: Y\n      SA_PASSWORD: "1Secure*Password1"\n\n  mlflow-mssql:\n    depends_on:\n      - mssql\n    extends:\n  ', '   MLFLOW_TRACKING_URI: mysql://mlflowuser:mlflowpassword@mysql:3306/mlflowdb?charset=utf8mb4\n      INSTALL_MLFLOW_FROM_REPO: true\n\n  migration-mysql:\n    extends:\n      service: base\n    depends_on:\n      - mysql\n    environment:\n      MLFLOW_TRACKING_URI: mysql://mlflowuser:mlflowpassword@mysql:3306/mlflowdb?charset=utf8mb4\n    command: tests/db/check_migration.sh\n\n  mssql:\n    image: mcr.microsoft.com/mssql/server\n    restart: always\n    environment:\n      ACCEPT_EULA: Y\n      SA_PASSWORD: "1Secure*Password1"\n\n  mlflow-mssql:\n    depends_on:\n      - mssql\n    extends:\n      service: base\n    platform: linux/amd64\n    image: mlflow-mssql\n    build:\n      context: .\n      dockerfile: Dockerfile.mssql\n    environment:\n      MLFLOW_TRACKING_URI: mssql+pyodbc://mlflowuser:Mlfl*wpassword1@mssql/mlflowdb?driver=ODBC+Driver+17+for+SQL+Server\n      INSTALL_MLFLOW_FROM_REPO: true\n\n  migration-mssql:\n    depends_on:\n      - mssql\n    extends:\n      service: base\n    platform: linux/amd64\n    image: mlflow-mssql\n    build:\n      context: .\n      dockerfile: Dockerfile.mssql\n    environment:\n      # We could try using ODBC Driver 18 and append `LongAsMax=Yes` to fix error for sqlalchemy<2.0:\n      # [ODBC Driver 17 for SQL Server][SQL Server]The data types varchar and ntext are incompatible in the equal to operator\n      # https://docs.sqlalchemy.org/en/20/dialects/mssql.html#avoiding-sending-large-string-parameters-as-text-ntext\n      MLFLOW_TRACKING_URI: mssql+pyodbc://mlflowuser:Mlfl*wpassword1@mssql/mlflowdb?driver=ODBC+Driver+17+for+SQL+Server\n    command: tests/db/check_migration.sh\n\n  mlflow-sqlite:\n    extends:\n      service: base\n    environment:\n      MLFLOW_TRACKING_URI: "sqlite:////tmp/mlflowdb"\n      INSTALL_MLFLOW_FROM_REPO: true\n\n  migration-sqlite:\n    extends:\n      service: base\n    environment:\n      MLFLOW_TRACKING_URI: "sqlite:////tmp/mlflowdb"\n    command: tests/db/check_migration.sh\n', 'prompt_with_history_str: "Here is a history between you and a human: {chat_history}\\nNow, please answer this question: {question}"\n', 'llm_prompt_template: "Answer the following question based on the context: {context}\\nQuestion: {question}"\nembedding_size: 5\nresponse: "Databricks"\nnot_used_array:\n  - 1\n  - 2\n  - 3\n', 'use_gpu: True\ntemperature: 0.9\ntimeout: 300\n', 'llm_prompt_template: "Answer the following question based on the context: {context}\\nQuestion: {question}"\nembedding_size: 5\nresponse: "Databricks"\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-3-small', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-3-small', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}}
2025-09-05 20:09:09,666 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict OpenAIClient, {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': None, '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://api.openai.com/v1', 'chat_completion_parser': <function get_first_message_content at 0x000001AF3C58AAC0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-3-small', 'dimensions': 256, 'encoding_format': 'float', 'input': ['import { render } from \'@testing-library/react\';\nimport { IntlProvider } from \'react-intl\';\n\nimport { TagAssignmentRow } from \'./TagAssignmentRow\';\n\ndescribe(\'TagAssignmentRow\', () => {\n  it(\'should throw an error if more than 3 children are passed\', () => {\n    const children = Array(4)\n      .fill(null)\n      .map((_, i) => <div key={i} />);\n\n    const renderComponent = () =>\n      render(\n        <IntlProvider locale="en">\n          <TagAssignmentRow>{children}</TagAssignmentRow>\n        </IntlProvider>,\n      );\n\n    expect(renderComponent).toThrow(\'TagAssignmentRow must have 3 children or less\');\n  });\n\n  it(\'should render children\', () => {\n    const children = Array(3)\n      .fill(null)\n      .map((_, i) => <div key={i} />);\n\n    const { container } = render(\n      <IntlProvider locale="en">\n        <TagAssignmentRow>{children}</TagAssignmentRow>\n      </IntlProvider>,\n    );\n\n    expect(container).toMatchSnapshot();\n  });\n});\n', "import invariant from 'invariant';\nimport React from 'react';\n\nimport { useDesignSystemTheme } from '@databricks/design-system';\n\nexport function TagAssignmentRow({ children }: { children: React.ReactNode }) {\n  const { theme } = useDesignSystemTheme();\n\n  const stableChildren = React.Children.toArray(children);\n  invariant(stableChildren.length <= 3, 'TagAssignmentRow must have 3 children or less');\n\n  const parsedChildren = Array(3)\n    .fill(null)\n    .map((_, i) => stableChildren[i] ?? <span key={i} style={{ width: theme.general.heightSm }} />); // Sync width with only icon button width\n\n  return (\n    <div css={{ display: 'grid', gridTemplateColumns: '1fr 1fr min-content', gap: theme.spacing.sm }}>\n      {parsedChildren}\n    </div>\n  );\n}\n", '// Do not modify this file\n\nimport type { ControllerProps, FieldValues, Path } from \'react-hook-form\';\nimport { Controller } from \'react-hook-form\';\n\nimport { TagAssignmentInput } from \'./TagAssignmentField/TagAssignmentInput\';\nimport { useTagAssignmentContext } from \'../context/TagAssignmentContextProvider\';\n\ninterface TagAssignmentValueProps<T extends FieldValues> {\n  rules?: ControllerProps<T>[\'rules\'];\n  index: number;\n  render?: ControllerProps<T>[\'render\'];\n}\n\nexport function TagAssignmentValue<T extends FieldValues>({ rules, index, render }: TagAssignmentValueProps<T>) {\n  const { name, valueProperty } = useTagAssignmentContext<T>();\n\n  return (\n    <Controller\n      rules={rules}\n      name={`${name}.${index}.${valueProperty}` as Path<T>}\n      render={({ field, fieldState, formState }) => {\n        if (render) {\n          return render({ field, fieldState, formState });\n        }\n\n        return (\n          <TagAssignmentInput\n            componentId="TagAssignmentValue.Default.Input"\n            errorMessage={fieldState.error?.message}\n            {...field}\n          />\n        );\n      }}\n    />\n  );\n}\n', 'import { forwardRef } from \'react\';\n\nimport type { InputProps, InputRef } from \'@databricks/design-system\';\nimport { FormUI, Input } from \'@databricks/design-system\';\n\ninterface TagAssignmentInputProps extends InputProps {\n  errorMessage?: string;\n}\n\nexport const TagAssignmentInput: React.ForwardRefExoticComponent<\n  TagAssignmentInputProps & React.RefAttributes<InputRef>\n> = forwardRef<InputRef, TagAssignmentInputProps>(({ errorMessage, ...otherProps }: TagAssignmentInputProps, ref) => {\n  return (\n    <div css={{ flex: 1 }}>\n      <Input validationState={errorMessage ? \'error\' : \'info\'} {...otherProps} ref={ref} />\n      {errorMessage && <FormUI.Message message={errorMessage} type="error" />}\n    </div>\n  );\n});\n', 'import { render, screen } from \'@testing-library/react\';\nimport userEvent from \'@testing-library/user-event\';\n\nimport { TagAssignmentRemoveButtonUI } from \'./TagAssignmentRemoveButtonUI\';\n\ndescribe(\'TagAssignmentRemoveButtonUI\', () => {\n  it(\'should render a button\', async () => {\n    const handleClick = jest.fn();\n    render(<TagAssignmentRemoveButtonUI componentId="test" onClick={handleClick} />);\n\n    const button = screen.getByRole(\'button\');\n    await userEvent.click(button);\n\n    expect(handleClick).toHaveBeenCalledTimes(1);\n  });\n});\n', "import type { ButtonProps } from '@databricks/design-system';\nimport { Button, TrashIcon } from '@databricks/design-system';\n\nexport function TagAssignmentRemoveButtonUI(props: Omit<ButtonProps, 'icon'>) {\n  return <Button icon={<TrashIcon />} {...props} />;\n}\n", "import { render, screen } from '@testing-library/react';\n\nimport { TagAssignmentRowContainer } from './TagAssignmentRowContainer';\n\ndescribe('TagAssignmentRowContainer', () => {\n  it('should render children', () => {\n    render(\n      <TagAssignmentRowContainer>\n        <div>child</div>\n      </TagAssignmentRowContainer>,\n    );\n\n    expect(screen.getByText('child')).toBeInTheDocument();\n  });\n});\n", "import { useDesignSystemTheme } from '@databricks/design-system';\n\nexport function TagAssignmentRowContainer({ children }: { children: React.ReactNode }) {\n  const { theme } = useDesignSystemTheme();\n  return <div css={{ display: 'flex', flexDirection: 'column', gap: theme.spacing.sm }}>{children}</div>;\n}\n", "import invariant from 'invariant';\nimport { createContext, useContext } from 'react';\nimport type { FieldValues, ArrayPath, FieldArray } from 'react-hook-form';\n\nimport type { UseTagAssignmentFormReturn } from '../hooks/useTagAssignmentForm';\n\nexport const TagAssignmentContext = createContext<UseTagAssignmentFormReturn | null>(null);\n\nexport function TagAssignmentContextProvider<\n  T extends FieldValues = FieldValues,\n  K extends ArrayPath<T> = ArrayPath<T>,\n  V extends FieldArray<T, K> = FieldArray<T, K>,\n>({ children, ...props }: { children: React.ReactNode } & UseTagAssignmentFormReturn<T, K, V>) {\n  return <TagAssignmentContext.Provider value={props as any}>{children}</TagAssignmentContext.Provider>;\n}\n\nexport function useTagAssignmentContext<\n  T extends FieldValues = FieldValues,\n  K extends ArrayPath<T> = ArrayPath<T>,\n  V extends FieldArray<T, K> = FieldArray<T, K>,\n>() {\n  const context = useContext(TagAssignmentContext as React.Context<UseTagAssignmentFormReturn<T, K, V> | null>);\n  invariant(context, 'useTagAssignmentContext must be used within a TagAssignmentRoot');\n  return context;\n}\n", 'import { renderHook } from \'@testing-library/react\';\nimport { useForm, FormProvider } from \'react-hook-form\';\nimport { IntlProvider } from \'react-intl\';\n\nimport { useTagAssignmentFieldArray } from \'./useTagAssignmentFieldArray\';\n\nconst DefaultWrapper = ({ children }: { children: React.ReactNode }) => {\n  return <IntlProvider locale="en">{children}</IntlProvider>;\n};\n\ndescribe(\'useTagAssignmentFieldArray\', () => {\n  it(\'should use passed form as prop\', () => {\n    const { result: formResult } = renderHook(() =>\n      useForm<{ input: string; tags: { key: string; value: string }[] }>({\n        defaultValues: { input: \'test_input\', tags: [{ key: \'key1\', value: \'value1\' }] },\n      }),\n    );\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentFieldArray({\n          name: \'tags\',\n          emptyValue: { key: \'\', value: \'\' },\n          form: formResult.current,\n          keyProperty: \'key\',\n        }),\n      { wrapper: DefaultWrapper },\n    );\n    result.current.appendIfPossible({ key: \'foo\', value: \'bar\' }, {});\n\n    const values = result.current.form.getValues();\n    expect(values.tags).toStrictEqual([\n      { key: \'key1\', value: \'value1\' },\n      { key: \'foo\', value: \'bar\' },\n    ]);\n    expect(values.input).toBe(\'test_input\');\n  });\n\n  it(\'should use context form if no form prop is passed\', () => {\n    const Wrapper = ({ children }: { children: React.ReactNode }) => {\n      const methods = useForm<{ input: string; tags: { key: string; value: string }[] }>({\n        defaultValues: { input: \'test_input\', tags: [{ key: \'key1\', value: \'value1\' }] },\n      });\n      return (\n        <IntlProvider locale="en">\n         ', 'expect(values.input).toBe(\'test_input\');\n  });\n\n  it(\'should use context form if no form prop is passed\', () => {\n    const Wrapper = ({ children }: { children: React.ReactNode }) => {\n      const methods = useForm<{ input: string; tags: { key: string; value: string }[] }>({\n        defaultValues: { input: \'test_input\', tags: [{ key: \'key1\', value: \'value1\' }] },\n      });\n      return (\n        <IntlProvider locale="en">\n          c<FormProvider {...methods}>{children}</FormProvider>\n        </IntlProvider>\n      );\n    };\n\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentFieldArray({\n          name: \'tags\',\n          emptyValue: { key: \'\', value: \'\' },\n          keyProperty: \'key\',\n        }),\n      { wrapper: Wrapper },\n    );\n    result.current.appendIfPossible({ key: \'foo\', value: \'bar\' }, {});\n\n    const values = result.current.form.getValues();\n    expect(values[\'tags\']).toStrictEqual([\n      { key: \'key1\', value: \'value1\' },\n      { key: \'foo\', value: \'bar\' },\n    ]);\n    expect(values[\'input\']).toBe(\'test_input\');\n  });\n\n  it(\'should throw an error if no form is passed and not in a form context\', () => {\n    expect(() =>\n      renderHook(\n        () =>\n          useTagAssignmentFieldArray({\n            name: \'tags\',\n            emptyValue: { key: \'\', value: undefined },\n            keyProperty: \'key\',\n ', "]);\n    expect(values['input']).toBe('test_input');\n  });\n\n  it('should throw an error if no form is passed and not in a form context', () => {\n    expect(() =>\n      renderHook(\n        () =>\n          useTagAssignmentFieldArray({\n            name: 'tags',\n            emptyValue: { key: '', value: undefined },\n            keyProperty: 'key',\n          }),\n        { wrapper: DefaultWrapper },\n      ),\n    ).toThrow('Nest your component on a FormProvider or pass a form prop');\n  });\n\n  it('should not add the empty value to the form via appendIfPossible if maxLength is reached', () => {\n    const { result: formResult } = renderHook(() =>\n      useForm<{ tags: { key: string; value: string }[] }>({\n        defaultValues: {\n          tags: [\n            { key: 'key1', value: 'value1' },\n            { key: 'key2', value: 'value2' },\n          ],\n        },\n      }),\n    );\n\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentFieldArray<{ tags: { key: string; value: string }[] }>({\n          name: 'tags',\n          emptyValue: { key: '', value: '' },\n          maxLength: 2,\n          form: formResult.current,\n ", "],\n        },\n      }),\n    );\n\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentFieldArray<{ tags: { key: string; value: string }[] }>({\n          name: 'tags',\n          emptyValue: { key: '', value: '' },\n          maxLength: 2,\n          form: formResult.current,\n          keyProperty: 'key',\n        }),\n      { wrapper: DefaultWrapper },\n    );\n\n    result.current.appendIfPossible({ key: 'not-added', value: 'not-added' }, {});\n    expect(result.current.getTagsValues()).toStrictEqual([\n      { key: 'key1', value: 'value1' },\n      { key: 'key2', value: 'value2' },\n    ]);\n  });\n\n  it('should remove tag when removeOrUpdate is called for tag not at the end of the array', () => {\n    const { result: formResult } = renderHook(() =>\n      useForm<{ tags: { key: string; value: string }[] }>({\n        defaultValues: {\n          tags: [\n            { key: 'key1', value: 'value1' },\n            { key: 'key2', value: 'value2' },\n            { key: '', value: '' },\n          ],\n        },\n      }),\n    );\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentFieldArray({\n         ", "{ key: 'key1', value: 'value1' },\n            { key: 'key2', value: 'value2' },\n            { key: '', value: '' },\n          ],\n        },\n      }),\n    );\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentFieldArray({\n          name: 'tags',\n          emptyValue: { key: '', value: '' },\n          maxLength: 5,\n          form: formResult.current,\n          keyProperty: 'key',\n        }),\n      { wrapper: DefaultWrapper },\n    );\n\n    result.current.removeOrUpdate(0);\n\n    expect(result.current.getTagsValues()).toStrictEqual([\n      { key: 'key2', value: 'value2' },\n      { key: '', value: '' },\n    ]);\n  });\n\n  it('should set last tag to the empty value when removeOrUpdate is called for last tag', () => {\n    const { result: formResult } = renderHook(() =>\n      useForm<{ tags: { key: string; value: string }[] }>({\n        defaultValues: {\n          tags: [\n            { key: 'key1', value: 'value1' },\n            { key: 'key2', value: 'value2' },\n          ],\n        },\n      }),\n    );\n    const { result } = renderHook(\n      () ", "string }[] }>({\n        defaultValues: {\n          tags: [\n            { key: 'key1', value: 'value1' },\n            { key: 'key2', value: 'value2' },\n          ],\n        },\n      }),\n    );\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentFieldArray({\n          name: 'tags',\n          emptyValue: { key: '', value: '' },\n          maxLength: 2,\n          form: formResult.current,\n          keyProperty: 'key',\n        }),\n      { wrapper: DefaultWrapper },\n    );\n\n    result.current.removeOrUpdate(1);\n\n    expect(result.current.getTagsValues()).toStrictEqual([\n      { key: 'key1', value: 'value1' },\n      { key: '', value: '' },\n    ]);\n  });\n\n  it('should add an empty tag to the end of the array when removeOrUpdate is called when the max number of tags are present', () => {\n    const { result: formResult } = renderHook(() =>\n      useForm<{ tags: { key: string; value: string }[] }>({\n        defaultValues: {\n          tags: [\n            { key: 'key1', value: 'value1' },\n            { key: 'key2', value: 'value2' },\n          ],\n        },\n ", "() => {\n    const { result: formResult } = renderHook(() =>\n      useForm<{ tags: { key: string; value: string }[] }>({\n        defaultValues: {\n          tags: [\n            { key: 'key1', value: 'value1' },\n            { key: 'key2', value: 'value2' },\n          ],\n        },\n      }),\n    );\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentFieldArray({\n          name: 'tags',\n          emptyValue: { key: '', value: '' },\n          maxLength: 2,\n          form: formResult.current,\n          keyProperty: 'key',\n        }),\n      { wrapper: DefaultWrapper },\n    );\n\n    result.current.removeOrUpdate(0);\n\n    expect(result.current.getTagsValues()).toStrictEqual([\n      { key: 'key2', value: 'value2' },\n      { key: '', value: '' },\n    ]);\n  });\n});\n", 'import { renderHook } from \'@testing-library/react\';\nimport { useForm, FormProvider } from \'react-hook-form\';\nimport { IntlProvider } from \'react-intl\';\n\nimport { useTagAssignmentForm } from \'./useTagAssignmentForm\';\n\nconst DefaultWrapper = ({ children }: { children: React.ReactNode }) => {\n  return <IntlProvider locale="en">{children}</IntlProvider>;\n};\n\ndescribe(\'useTagAssignmentForm\', () => {\n  it(\'should use passed form as prop\', () => {\n    const { result: formResult } = renderHook(() =>\n      useForm<{ input: string; tags: { key: string; value: undefined }[] }>({ defaultValues: { input: \'test_input\' } }),\n    );\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm({\n          name: \'tags\',\n          emptyValue: { key: \'\', value: undefined },\n          form: formResult.current,\n          keyProperty: \'key\',\n          valueProperty: \'value\',\n        }),\n      { wrapper: DefaultWrapper },\n    );\n    const values = result.current.form.getValues();\n    expect(values.tags).toStrictEqual([{ key: \'\', value: undefined }]);\n    expect(values.input).toBe(\'test_input\');\n  });\n\n  it(\'should use context form if no form prop is passed\', () => {\n    const Wrapper = ({ children }: { children: React.ReactNode }) => {\n      const methods = useForm<{ input: string; tags: { key: string; value: undefined }[] }>({\n        defaultValues: { input: \'test_input\' },\n      });\n      return (\n        <FormProvider {...methods}>\n          <IntlProvider locale="en">{children}</IntlProvider>\n        </FormProvider>\n      );\n    };\n\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm({\n  ', '= useForm<{ input: string; tags: { key: string; value: undefined }[] }>({\n        defaultValues: { input: \'test_input\' },\n      });\n      return (\n        <FormProvider {...methods}>\n          <IntlProvider locale="en">{children}</IntlProvider>\n        </FormProvider>\n      );\n    };\n\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm({\n          name: \'tags\',\n          emptyValue: { key: \'\', value: undefined },\n          keyProperty: \'key\',\n          valueProperty: \'value\',\n        }),\n      { wrapper: Wrapper },\n    );\n\n    const values = result.current.form.getValues();\n    expect(values[\'tags\']).toStrictEqual([{ key: \'\', value: undefined }]);\n    expect(values[\'input\']).toBe(\'test_input\');\n  });\n\n  it(\'should add an empty value on default values provided by form context\', () => {\n    const Wrapper = ({ children }: { children: React.ReactNode }) => {\n      const methods = useForm<{ input: string; tags: { key: string; value: string | undefined }[] }>({\n        defaultValues: { input: \'test_input\', tags: [{ key: \'defaultKey\', value: \'defaultValue\' }] },\n      });\n      return (\n        <FormProvider {...methods}>\n          <IntlProvider locale="en">{children}</IntlProvider>\n        </FormProvider>\n      );\n    };\n\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm({\n          name: \'tags\',\n   ', 'defaultValues: { input: \'test_input\', tags: [{ key: \'defaultKey\', value: \'defaultValue\' }] },\n      });\n      return (\n        <FormProvider {...methods}>\n          <IntlProvider locale="en">{children}</IntlProvider>\n        </FormProvider>\n      );\n    };\n\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm({\n          name: \'tags\',\n          emptyValue: { key: \'\', value: undefined },\n          keyProperty: \'key\',\n          valueProperty: \'value\',\n        }),\n      { wrapper: Wrapper },\n    );\n\n    const values = result.current.form.getValues();\n    expect(values[\'tags\']).toStrictEqual([\n      { key: \'defaultKey\', value: \'defaultValue\' },\n      { key: \'\', value: undefined },\n    ]);\n    expect(values[\'input\']).toBe(\'test_input\');\n  });\n\n  it(\'should throw an error if no form is passed and not in a form context\', () => {\n    expect(() =>\n      renderHook(\n        () =>\n          useTagAssignmentForm({\n            name: \'tags\',\n            emptyValue: { key: \'\', value: undefined },\n            keyProperty: \'key\',\n            valueProperty: \'value\',\n          }),\n        { wrapper: DefaultWrapper },\n      ),\n    ).toThrow(\'Nest your component on a FormProvider or pass a form prop\');\n  });\n\n  ', '          name: \'tags\',\n            emptyValue: { key: \'\', value: undefined },\n            keyProperty: \'key\',\n            valueProperty: \'value\',\n          }),\n        { wrapper: DefaultWrapper },\n      ),\n    ).toThrow(\'Nest your component on a FormProvider or pass a form prop\');\n  });\n\n  it(\'should throw an error if default values are passed and in a form context\', () => {\n    const Wrapper = ({ children }: { children: React.ReactNode }) => {\n      const methods = useForm<{ input: string; tags: { key: string; value: string | undefined }[] }>({\n        defaultValues: { input: \'test_input\' },\n      });\n      return (\n        <FormProvider {...methods}>\n          <IntlProvider locale="en">{children}</IntlProvider>\n        </FormProvider>\n      );\n    };\n\n    expect(() =>\n      renderHook(\n        () =>\n          useTagAssignmentForm({\n            name: \'tags\',\n            emptyValue: { key: \'\', value: undefined },\n            keyProperty: \'key\',\n            valueProperty: \'value\',\n            defaultValues: [{ key: \'defaultKey\', value: \'defaultValue\' }],\n          }),\n        { wrapper: Wrapper },\n      ),\n   ', "       name: 'tags',\n            emptyValue: { key: '', value: undefined },\n            keyProperty: 'key',\n            valueProperty: 'value',\n            defaultValues: [{ key: 'defaultKey', value: 'defaultValue' }],\n          }),\n        { wrapper: Wrapper },\n      ),\n    ).toThrow('Define defaultValues at form context level');\n  });\n\n  it('should use empty value if no default values are passed', () => {\n    const { result: formResult } = renderHook(() => useForm());\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm({\n          name: 'tags',\n          emptyValue: { key: '', value: undefined },\n          keyProperty: 'key',\n          valueProperty: 'value',\n          form: formResult.current,\n        }),\n      { wrapper: DefaultWrapper },\n    );\n\n    const values = result.current.form.getValues();\n    expect(values['tags']).toStrictEqual([{ key: '', value: undefined }]);\n  });\n\n  it('should use default values + empty value if default values are passed', () => {\n    const { result: formResult } = renderHook(() => useForm<{ tags: { key: string; value: string | undefined }[] }>());\n    const defaultValues = [{ key: 'defaultKey', value: 'defaultValue' }];\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm<{ tags: { key: string; value: string | undefined }[] }>({\n         ", "expect(values['tags']).toStrictEqual([{ key: '', value: undefined }]);\n  });\n\n  it('should use default values + empty value if default values are passed', () => {\n    const { result: formResult } = renderHook(() => useForm<{ tags: { key: string; value: string | undefined }[] }>());\n    const defaultValues = [{ key: 'defaultKey', value: 'defaultValue' }];\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm<{ tags: { key: string; value: string | undefined }[] }>({\n          name: 'tags',\n          emptyValue: { key: '', value: undefined },\n          defaultValues,\n          form: formResult.current,\n          keyProperty: 'key',\n          valueProperty: 'value',\n        }),\n      { wrapper: DefaultWrapper },\n    );\n\n    const values = result.current.form.getValues();\n    expect(values.tags).toStrictEqual([\n      { key: 'defaultKey', value: 'defaultValue' },\n      { key: '', value: undefined },\n    ]);\n  });\n\n  it('should not add the empty value to the form if maxLength is reached', () => {\n    const { result: formResult } = renderHook(() => useForm<{ tags: { key: string; value: string | undefined }[] }>());\n    const defaultValues = [\n      { key: 'key1', value: 'value1' },\n      { key: 'key2', value: 'value2' },\n    ];\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm<{ tags: { key: string; value: string | undefined }[] }>({\n          name: 'tags',\n          emptyValue: { ", "useForm<{ tags: { key: string; value: string | undefined }[] }>());\n    const defaultValues = [\n      { key: 'key1', value: 'value1' },\n      { key: 'key2', value: 'value2' },\n    ];\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm<{ tags: { key: string; value: string | undefined }[] }>({\n          name: 'tags',\n          emptyValue: { key: '', value: undefined },\n          defaultValues,\n          maxLength: 2,\n          form: formResult.current,\n          keyProperty: 'key',\n          valueProperty: 'value',\n        }),\n      { wrapper: DefaultWrapper },\n    );\n\n    const values = result.current.form.getValues();\n    expect(values.tags).toStrictEqual([\n      { key: 'key1', value: 'value1' },\n      { key: 'key2', value: 'value2' },\n    ]);\n  });\n\n  it('should wait for loading before resetting', () => {\n    const { result: formResult } = renderHook(() => useForm());\n    const { result, rerender } = renderHook(\n      ({ loading }) =>\n        useTagAssignmentForm({\n          name: 'tags',\n          emptyValue: { key: '', value: undefined },\n          loading,\n          form: formResult.current,\n          keyProperty: 'key',\n          valueProperty: 'value',\n        }),\n     ", "renderHook(\n      ({ loading }) =>\n        useTagAssignmentForm({\n          name: 'tags',\n          emptyValue: { key: '', value: undefined },\n          loading,\n          form: formResult.current,\n          keyProperty: 'key',\n          valueProperty: 'value',\n        }),\n      { initialProps: { loading: true }, wrapper: DefaultWrapper },\n    );\n\n    const initialValues = result.current.form.getValues();\n    expect(initialValues['tags']).toStrictEqual([]);\n\n    rerender({ loading: false });\n\n    const values = result.current.form.getValues();\n    expect(values['tags']).toStrictEqual([{ key: '', value: undefined }]);\n  });\n\n  it('should not override the other filled values when setting default values', () => {\n    const { result: formResult } = renderHook(() =>\n      useForm<{ tags: { key: string; value: string | undefined }[]; input1: string; input2: string }>({\n        defaultValues: {\n          input1: 'test_input1',\n          input2: 'test_input2',\n        },\n      }),\n    );\n    const { result } = renderHook(\n      () =>\n        useTagAssignmentForm({\n          name: 'tags',\n          emptyValue: { key: '', value: undefined },\n          form: formResult.current,\n          keyProperty: 'key',\n          valueProperty: 'value',\n        }),\n      { wrapper: DefaultWrapper },\n    );\n\n ", "result } = renderHook(\n      () =>\n        useTagAssignmentForm({\n          name: 'tags',\n          emptyValue: { key: '', value: undefined },\n          form: formResult.current,\n          keyProperty: 'key',\n          valueProperty: 'value',\n        }),\n      { wrapper: DefaultWrapper },\n    );\n\n    const values = result.current.form.getValues();\n    expect(values.tags).toStrictEqual([{ key: '', value: undefined }]);\n    expect(values.input1).toBe('test_input1');\n    expect(values.input2).toBe('test_input2');\n  });\n});\n", "import invariant from 'invariant';\nimport { useEffect, useState } from 'react';\nimport type { ArrayPath, FieldArray, FieldValues, Path, PathValue, UseFormReturn } from 'react-hook-form';\nimport { useFormContext } from 'react-hook-form';\n\nimport { useTagAssignmentFieldArray } from './useTagAssignmentFieldArray';\n\nexport interface UseTagAssignmentProps<\n  T extends FieldValues,\n  K extends ArrayPath<T> = ArrayPath<T>,\n  V extends FieldArray<T, K> = FieldArray<T, K>,\n> {\n  name: K;\n  maxLength?: number;\n  emptyValue: V;\n  loading?: boolean;\n  defaultValues?: V[];\n  form?: UseFormReturn<T>;\n  keyProperty: keyof V extends string ? keyof V : never;\n  valueProperty: keyof V extends string ? keyof V : never;\n}\n\nexport function useTagAssignmentForm<\n  T extends FieldValues,\n  K extends ArrayPath<T> = ArrayPath<T>,\n  V extends FieldArray<T, K> = FieldArray<T, K>,\n>({\n  name,\n  maxLength,\n  emptyValue,\n  defaultValues,\n  loading,\n  form,\n  keyProperty,\n  valueProperty,\n}: UseTagAssignmentProps<T, K, V>) {\n  const [_emptyValue] = useState(emptyValue);\n\n  const formCtx = useFormContext<T>();\n  const shouldUseFormContext = Boolean(formCtx) && !form;\n  const internalForm = shouldUseFormContext ? formCtx : form;\n\n  invariant(internalForm, 'Nest your component on a FormProvider or pass a form prop');\n  invariant(!(defaultValues && shouldUseFormContext), 'Define defaultValues at form context level');\n\n  const { setValue } = internalForm;\n\n  const fieldArrayMethods = useTagAssignmentFieldArray({\n    name,\n    maxLength,\n    emptyValue,\n    form: internalForm,\n    keyProperty,\n  });\n  const getTagsValues = fieldArrayMethods.getTagsValues;\n\n  useEffect(() => {\n    if (loading) return;\n    if (defaultValues) {\n      const newValues = [...defaultValues];\n      if (!maxLength || (maxLength && newValues.length < maxLength)) {\n        newValues.push(_emptyValue);\n      }\n      setValue(name as Path<T>, newValues as PathValue<T, Path<T>>);\n      return;\n    }\n\n    if (shouldUseFormContext) {\n      const existentValues = getTagsValues() ?? [];\n      if (!maxLength || (maxLength && existentValues.length < maxLength)) {\n        existentValues.push(_emptyValue);\n      }\n      setValue(name as Path<T>, existentValues ", ' if (!maxLength || (maxLength && newValues.length < maxLength)) {\n        newValues.push(_emptyValue);\n      }\n      setValue(name as Path<T>, newValues as PathValue<T, Path<T>>);\n      return;\n    }\n\n    if (shouldUseFormContext) {\n      const existentValues = getTagsValues() ?? [];\n      if (!maxLength || (maxLength && existentValues.length < maxLength)) {\n        existentValues.push(_emptyValue);\n      }\n      setValue(name as Path<T>, existentValues as PathValue<T, Path<T>>);\n      return;\n    }\n\n    setValue(name as Path<T>, [_emptyValue] as PathValue<T, Path<T>>);\n  }, [defaultValues, setValue, loading, maxLength, name, _emptyValue, shouldUseFormContext, getTagsValues]);\n\n  return {\n    ...fieldArrayMethods,\n    form: internalForm,\n    maxLength,\n    emptyValue,\n    name,\n    keyProperty,\n    valueProperty,\n  };\n}\n\nexport type UseTagAssignmentFormReturn<\n  T extends FieldValues = FieldValues,\n  K extends ArrayPath<T> = ArrayPath<T>,\n  V extends FieldArray<T, K> = FieldArray<T, K>,\n> = ReturnType<typeof useTagAssignmentForm<T, K, V>>;\n', "import { FormProvider, useForm } from 'react-hook-form';\n\nimport { TagAssignmentContext } from '../context/TagAssignmentContextProvider';\nimport { useTagAssignmentForm } from '../hooks/useTagAssignmentForm';\n\ninterface TestFormI {\n  tags: {\n    key: string;\n    value: string;\n  }[];\n}\n\ninterface TestTagAssignmentContextProviderProps extends Partial<ReturnType<typeof useTagAssignmentForm<TestFormI>>> {\n  children: React.ReactNode;\n}\n\nexport function TestTagAssignmentContextProvider({ children, ...props }: TestTagAssignmentContextProviderProps) {\n  const form = useForm<TestFormI>();\n  const tagForm = useTagAssignmentForm<TestFormI, 'tags', { key: string; value: string }>({\n    form,\n    name: 'tags',\n    emptyValue: { key: '', value: '' },\n    keyProperty: 'key',\n    valueProperty: 'value',\n  });\n  return (\n    <FormProvider {...form}>\n      <TagAssignmentContext.Provider\n        value={{\n          ...(tagForm as any),\n          ...props,\n        }}\n      >\n        {children}\n      </TagAssignmentContext.Provider>\n    </FormProvider>\n  );\n}\n", 'import { GenericSkeleton, ParagraphSkeleton, Typography, useDesignSystemTheme } from \'@databricks/design-system\';\nimport type { ReactNode } from \'react\';\nimport { useRef } from \'react\';\nimport { FormattedMessage } from \'react-intl\';\nimport useResponsiveContainer from \'./useResponsiveContainer\';\n\nexport interface AsideSectionProps {\n  id: string;\n  title?: ReactNode;\n  content: ReactNode;\n  isTitleLoading?: boolean;\n}\n\nexport type MaybeAsideSection = AsideSectionProps | null;\nexport type AsideSections = Array<MaybeAsideSection>;\n\nconst SIDEBAR_WIDTHS = {\n  sm: 316,\n  lg: 480,\n} as const;\nconst VERTICAL_MARGIN_PX = 16;\nconst DEFAULT_MAX_WIDTH = 450;\n\nexport const OverviewLayout = ({\n  isLoading,\n  asideSections,\n  children,\n  isTabLayout = true,\n  sidebarSize = \'sm\',\n  verticalStackOrder,\n}: {\n  isLoading?: boolean;\n  asideSections: AsideSections;\n  children: ReactNode;\n  isTabLayout?: boolean;\n  sidebarSize?: \'sm\' | \'lg\';\n  verticalStackOrder?: \'main-first\' | \'aside-first\';\n}) => {\n  const { theme } = useDesignSystemTheme();\n  const containerRef = useRef<HTMLDivElement>(null);\n\n  const stackVertically = useResponsiveContainer(containerRef, { small: theme.responsive.breakpoints.lg }) === \'small\';\n\n  // Determine vertical stack order, i.e. should the main content be on top or bottom\n  const verticalDisplayPrimaryContentOnTop = verticalStackOrder === \'main-first\';\n\n  const totalSidebarWidth = SIDEBAR_WIDTHS[sidebarSize];\n  const innerSidebarWidth = totalSidebarWidth - VERTICAL_MARGIN_PX;\n\n  const secondaryStackedStyles = stackVertically\n    ? verticalDisplayPrimaryContentOnTop\n      ? { width: \'100%\' }\n      : { borderBottom: `1px solid ${theme.colors.border}`, width: \'100%\' }\n    : verticalDisplayPrimaryContentOnTop\n    ? {\n        width: innerSidebarWidth,\n      }\n    : {\n        paddingBottom: theme.spacing.sm,\n        width: innerSidebarWidth,\n      };\n\n  return (\n    <div\n      data-testid="entity-overview-container"\n      ref={containerRef}\n      css={{\n        display: \'flex\',\n        flexDirection: stackVertically ? (verticalDisplayPrimaryContentOnTop ? \'column\' : \'column-reverse\') : \'row\',\n        gap: theme.spacing.lg,\n      }}\n    >\n      <div\n        css={{\n      ', '   width: innerSidebarWidth,\n      };\n\n  return (\n    <div\n      data-testid="entity-overview-container"\n      ref={containerRef}\n      css={{\n        display: \'flex\',\n        flexDirection: stackVertically ? (verticalDisplayPrimaryContentOnTop ? \'column\' : \'column-reverse\') : \'row\',\n        gap: theme.spacing.lg,\n      }}\n    >\n      <div\n        css={{\n          display: \'flex\',\n          flexGrow: 1,\n          flexDirection: \'column\',\n          gap: theme.spacing.md,\n          width: stackVertically ? \'100%\' : `calc(100% - ${totalSidebarWidth}px)`,\n        }}\n      >\n        {isLoading ? <GenericSkeleton /> : children}\n      </div>\n      <div\n        style={{\n          display: \'flex\',\n          ...(isTabLayout && { marginTop: -theme.spacing.md }), // remove the gap between tab list and sidebar content\n        }}\n      >\n        <div\n          css={{\n            display: \'flex\',\n            flexDirection: \'column\',\n            gap: theme.spacing.lg,\n            ...secondaryStackedStyles,\n          }}\n        >\n          {isLoading ', "  >\n        <div\n          css={{\n            display: 'flex',\n            flexDirection: 'column',\n            gap: theme.spacing.lg,\n            ...secondaryStackedStyles,\n          }}\n        >\n          {isLoading && <GenericSkeleton />}\n          {!isLoading && <SidebarWrapper secondarySections={asideSections} />}\n        </div>\n      </div>\n    </div>\n  );\n};\n\nconst SidebarWrapper = ({ secondarySections }: { secondarySections: AsideSections }) => {\n  return (\n    <div>\n      {secondarySections\n        .filter((section) => section !== null)\n        .filter((section) => section?.content !== null)\n        .map(({ title, isTitleLoading, content, id }, index) => (\n          <AsideSection title={title} isTitleLoading={isTitleLoading} content={content} key={id} index={index} />\n        ))}\n    </div>\n  );\n};\n\nexport const AsideSectionTitle = ({ children }: { children: ReactNode }) => {\n  const { theme } = useDesignSystemTheme();\n  return (\n    <Typography.Title\n      level={4}\n      style={{\n        whiteSpace: 'nowrap',\n        marginRight: theme.spacing.lg,\n        marginTop: 0,\n      }}\n    >\n      {children}\n    </Typography.Title>\n  );\n};\n\nconst AsideSection = ({\n  title,\n  content,\n  index,\n  isTitleLoading = false,\n}: Omit<AsideSectionProps, 'id'> & {\n  index: number;\n}) => {\n  const { theme } = useDesignSystemTheme();\n\n  const titleComponent ", ' return (\n    <Typography.Title\n      level={4}\n      style={{\n        whiteSpace: \'nowrap\',\n        marginRight: theme.spacing.lg,\n        marginTop: 0,\n      }}\n    >\n      {children}\n    </Typography.Title>\n  );\n};\n\nconst AsideSection = ({\n  title,\n  content,\n  index,\n  isTitleLoading = false,\n}: Omit<AsideSectionProps, \'id\'> & {\n  index: number;\n}) => {\n  const { theme } = useDesignSystemTheme();\n\n  const titleComponent = isTitleLoading ? (\n    <ParagraphSkeleton\n      label={\n        <FormattedMessage\n          defaultMessage="Section title loading"\n          description="Loading skeleton label for overview page section title in Catalog Explorer"\n        />\n      }\n    />\n  ) : title ? (\n    <AsideSectionTitle>{title}</AsideSectionTitle>\n  ) : null;\n\n  const compactStyles = { padding: `${theme.spacing.md}px 0 ${theme.spacing.md}px 0` };\n\n  return (\n    <div\n      css={{\n        ...compactStyles,\n        ...(index === 0 ? {} : { borderTop: `1px solid ${theme.colors.border}` }),\n      }}\n    >\n      {titleComponent}\n      {content}\n    </div>\n  );\n};\n\nexport const KeyValueProperty = ({\n  keyValue,\n  value,\n  maxWidth,\n}: {\n  keyValue: string;\n  value: React.ReactNode;\n  maxWidth?: number | string;\n}) => {\n  const { theme } = useDesignSystemTheme();\n  return (\n    <div\n      css={{\n        display: \'flex\',\n        alignItems: \'center\',\n        \'&:has(+ div)\': {\n          marginBottom: theme.spacing.xs,\n   ', '  {titleComponent}\n      {content}\n    </div>\n  );\n};\n\nexport const KeyValueProperty = ({\n  keyValue,\n  value,\n  maxWidth,\n}: {\n  keyValue: string;\n  value: React.ReactNode;\n  maxWidth?: number | string;\n}) => {\n  const { theme } = useDesignSystemTheme();\n  return (\n    <div\n      css={{\n        display: \'flex\',\n        alignItems: \'center\',\n        \'&:has(+ div)\': {\n          marginBottom: theme.spacing.xs,\n        },\n        maxWidth: maxWidth ?? DEFAULT_MAX_WIDTH,\n        wordBreak: \'break-word\',\n        lineHeight: theme.typography.lineHeightLg,\n      }}\n    >\n      <div\n        css={{\n          color: theme.colors.textSecondary,\n          flex: 0.5,\n          alignSelf: \'start\',\n        }}\n      >\n        {keyValue}\n      </div>\n      <div\n        css={{\n          flex: 1,\n          alignSelf: \'start\',\n          overflow: \'hidden\',\n        }}\n      >\n        {value}\n      </div>\n    </div>\n  );\n};\n\nexport const NoneCell = () => {\n  return (\n    <Typography.Text color="secondary">\n      <FormattedMessage defaultMessage="None" description="Cell value when there\'s no content" />\n    </Typography.Text>\n  );\n};\n', '<head>\n  <link\n    rel="stylesheet"\n    href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/xcode.min.css"\n  />\n  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>\n  <script>\n    hljs.highlightAll();\n  </script>\n  <style>\n    body {\n      margin: 0;\n      font-family: -apple-system, BlinkMacSystemFont, Segoe UI, Roboto,\n        Helvetica Neue, Arial, Noto Sans, sans-serif, Apple Color Emoji,\n        Segoe UI Emoji, Segoe UI Symbol, Noto Color Emoji;\n      -webkit-tap-highlight-color: rgba(0, 0, 0, 0);\n      margin: 0;\n      font-weight: 400;\n      font-size: 13px;\n      line-height: 18px;\n      color: rgb(17, 23, 28);\n    }\n    code {\n      line-height: 18px;\n      font-size: 11px;\n      background: rgb(250, 250, 250) !important;\n    }\n    pre {\n      background: rgb(250, 250, 250);\n      margin: 0;\n      display: none;\n    }\n    pre.active {\n      display: unset;\n    }\n    button {\n      white-space: nowrap;\n      text-align: center;\n      position: relative;\n      cursor: pointer;\n      background: rgba(34, 114, 180, 0) !important;\n      color: rgb(34, 114, 180) !important;\n      border-color: rgba(34, 114, 180, 0) !important;\n      padding: 4px 6px !important;\n      text-decoration: none !important;\n      line-height: 20px !important;\n      box-shadow: none !important;\n      height: 32px !important;\n      display: inline-flex !important;\n      -webkit-box-align: center !important;\n      align-items: center !important;\n      -webkit-box-pack: center ', ' background: rgba(34, 114, 180, 0) !important;\n      color: rgb(34, 114, 180) !important;\n      border-color: rgba(34, 114, 180, 0) !important;\n      padding: 4px 6px !important;\n      text-decoration: none !important;\n      line-height: 20px !important;\n      box-shadow: none !important;\n      height: 32px !important;\n      display: inline-flex !important;\n      -webkit-box-align: center !important;\n      align-items: center !important;\n      -webkit-box-pack: center !important;\n      justify-content: center !important;\n      vertical-align: middle !important;\n    }\n    p {\n      margin: 0;\n      padding: 0;\n    }\n    button:hover {\n      background: rgba(34, 114, 180, 0.08) !important;\n      color: rgb(14, 83, 139) !important;\n    }\n    button:active {\n      background: rgba(34, 114, 180, 0.16) !important;\n      color: rgb(4, 53, 93) !important;\n    }\n    h1 {\n      margin-top: 4px;\n      font-size: 22px;\n    }\n    .info {\n      font-size: 12px;\n      font-weight: 500;\n      line-height: 16px;\n      color: rgb(95, 114, 129);\n    }\n    .tabs {\n      margin-top: 10px;\n      border-bottom: 1px solid rgb(209, 217, 225) !important;\n      display: flex;\n      line-height: 24px;\n    }\n    .tab {\n      font-size: 13px;\n      font-weight: 600 !important;\n      cursor: pointer;\n      margin: 0 24px 0 2px;\n      ', ' line-height: 16px;\n      color: rgb(95, 114, 129);\n    }\n    .tabs {\n      margin-top: 10px;\n      border-bottom: 1px solid rgb(209, 217, 225) !important;\n      display: flex;\n      line-height: 24px;\n    }\n    .tab {\n      font-size: 13px;\n      font-weight: 600 !important;\n      cursor: pointer;\n      margin: 0 24px 0 2px;\n      padding-left: 2px;\n    }\n    .tab:hover {\n      color: rgb(14, 83, 139) !important;\n    }\n    .tab.active {\n      border-bottom: 3px solid rgb(34, 114, 180) !important;\n    }\n    .link {\n      margin-left: 12px;\n      display: inline-block;\n      text-decoration: none;\n      color: rgb(34, 114, 180) !important;\n      font-size: 13px;\n      font-weight: 400;\n    }\n    .link:hover {\n      color: rgb(14, 83, 139) !important;\n    }\n    .link-content {\n      display: flex;\n      gap: 6px;\n      align-items: center;\n    }\n    .caret-up {\n      transform: rotate(180deg);\n    }\n  </style>\n</head>\n<body>\n  <div style="display: flex; align-items: center">\n    The logged model is compatible with the Mosaic AI Agent Framework.\n    <button onclick="toggleCode()">\n      See how to evaluate the model&nbsp;\n      <span\n        role="img"\n        id="caret"\n        aria-hidden="true"\n        class="anticon css-6xix1i"\n        style="font-size: ', '  .caret-up {\n      transform: rotate(180deg);\n    }\n  </style>\n</head>\n<body>\n  <div style="display: flex; align-items: center">\n    The logged model is compatible with the Mosaic AI Agent Framework.\n    <button onclick="toggleCode()">\n      See how to evaluate the model&nbsp;\n      <span\n        role="img"\n        id="caret"\n        aria-hidden="true"\n        class="anticon css-6xix1i"\n        style="font-size: 14px"\n        ><svg\n          xmlns="http://www.w3.org/2000/svg"\n          width="1em"\n          height="1em"\n          fill="none"\n          viewBox="0 0 16 16"\n          aria-hidden="true"\n          focusable="false"\n          class=""\n        >\n          <path\n            fill="currentColor"\n            fill-rule="evenodd"\n            d="M8 8.917 10.947 6 12 7.042 8 11 4 7.042 5.053 6z"\n            clip-rule="evenodd"\n          ></path>\n        </svg>\n      </span>\n    </button>\n  </div>\n  <div id="code" style="display: none">\n    <h1>\n      Agent evaluation\n      <a\n        class="link"\n        href="https://docs.databricks.com/en/generative-ai/agent-evaluation/synthesize-evaluation-set.html?utm_source=mlflow.log_model&utm_medium=notebook"\n        target="_blank"\n      ', '7.042 8 11 4 7.042 5.053 6z"\n            clip-rule="evenodd"\n          ></path>\n        </svg>\n      </span>\n    </button>\n  </div>\n  <div id="code" style="display: none">\n    <h1>\n      Agent evaluation\n      <a\n        class="link"\n        href="https://docs.databricks.com/en/generative-ai/agent-evaluation/synthesize-evaluation-set.html?utm_source=mlflow.log_model&utm_medium=notebook"\n        target="_blank"\n      >\n        <span class="link-content">\n          Learn more\n          <span role="img" aria-hidden="true" class="anticon css-6xix1i"\n            ><svg\n              xmlns="http://www.w3.org/2000/svg"\n              width="1em"\n              height="1em"\n              fill="none"\n              viewBox="0 0 16 16"\n              aria-hidden="true"\n              focusable="false"\n              class=""\n            >\n              <path\n                fill="currentColor"\n                d="M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z"\n              ></path>\n             ', '           class=""\n            >\n              <path\n                fill="currentColor"\n                d="M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z"\n              ></path>\n              <path\n                fill="currentColor"\n                d="M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z"\n              ></path></svg></span></span\n      ></a>\n    </h1>\n    <p class="info">\n      Copy the following code snippet in a notebook cell (right click â†’ copy)\n    </p>\n    <div class="tabs">\n      <div class="tab active" onclick="tabClicked(0)">Using synthetic data</div>\n      <div class="tab" onclick="tabClicked(1)">Using your own dataset</div>\n    </div>\n    <div style="height: 472px">\n      <pre\n        class="active"\n      ><code class="language-python">{{eval_with_synthetic_code}}</code></pre>\n\n      <pre><code class="language-python">{{eval_with_dataset_code}}</code></pre>\n    </div>\n  </div>\n  <script>\n    var codeShown = false;\n    function clip(el) {\n      var range = document.createRange();\n      range.selectNodeContents(el);\n      var sel = window.getSelection();\n      sel.removeAllRanges();\n      sel.addRange(range);\n    }\n\n    function toggleCode() {\n      if (codeShown) {\n        document.getElementById("code").style.display = "none";\n    ', '   ><code class="language-python">{{eval_with_synthetic_code}}</code></pre>\n\n      <pre><code class="language-python">{{eval_with_dataset_code}}</code></pre>\n    </div>\n  </div>\n  <script>\n    var codeShown = false;\n    function clip(el) {\n      var range = document.createRange();\n      range.selectNodeContents(el);\n      var sel = window.getSelection();\n      sel.removeAllRanges();\n      sel.addRange(range);\n    }\n\n    function toggleCode() {\n      if (codeShown) {\n        document.getElementById("code").style.display = "none";\n        codeShown = false;\n      } else {\n        document.getElementById("code").style.display = "block";\n        clip(document.querySelector("pre.active"));\n        codeShown = true;\n      }\n      document.getElementById("caret").classList.toggle("caret-up");\n    }\n\n    function tabClicked(tabIndex) {\n      document.querySelectorAll(".tab").forEach((tab, index) => {\n        if (index === tabIndex) {\n          tab.classList.add("active");\n        } else {\n          tab.classList.remove("active");\n        }\n      });\n      document.querySelectorAll("pre").forEach((pre, index) => {\n        if (index === tabIndex) {\n          pre.classList.add("active");\n        } else {\n          pre.classList.remove("active");\n        }\n      });\n      clip(document.querySelector("pre.active"));\n    }\n  </script>\n</body>\n', '<!DOCTYPE html>\n<html lang="en">\n  <head>\n    <meta charset="utf-8" />\n    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />\n    <link rel="shortcut icon" href="./static-files/favicon.ico" />\n    <meta name="theme-color" content="#000000" />\n    <!--\n      manifest.json provides metadata used when your web app is added to the\n      homescreen on Android. See https://developers.google.com/web/fundamentals/engage-and-retain/web-app-manifest/\n    -->\n    <link rel="manifest" href="./static-files/manifest.json" crossorigin="use-credentials" />\n    <title>MLflow</title>\n  </head>\n\n  <body>\n    <noscript> You need to enable JavaScript to run this app. </noscript>\n    <div id="root" class="mlflow-ui-container"></div>\n    <div id="modal" class="mlflow-ui-container"></div>\n  </body>\n</html>\n', '<!DOCTYPE html>\n<html>\n  <head>\n    <title>Test HTML</title>\n  </head>\n  <body>\n    <h1>Test HTML</h1>\n    <p>This is a test HTML file.</p>\n  </body>\n</html>', '<html>\n  <head></head>\n  <body>\n    <div id="root"></div>\n  </body>\n</html>\n', '<head>\n  <link\n    rel="stylesheet"\n    href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/xcode.min.css"\n  />\n  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>\n  <script>\n    hljs.highlightAll();\n  </script>\n  <style>\n    body {\n      margin: 0;\n      font-family: -apple-system, BlinkMacSystemFont, Segoe UI, Roboto,\n        Helvetica Neue, Arial, Noto Sans, sans-serif, Apple Color Emoji,\n        Segoe UI Emoji, Segoe UI Symbol, Noto Color Emoji;\n      -webkit-tap-highlight-color: rgba(0, 0, 0, 0);\n      margin: 0;\n      font-weight: 400;\n      font-size: 13px;\n      line-height: 18px;\n      color: rgb(17, 23, 28);\n    }\n    code {\n      line-height: 18px;\n      font-size: 11px;\n      background: rgb(250, 250, 250) !important;\n    }\n    pre {\n      background: rgb(250, 250, 250);\n      margin: 0;\n      display: none;\n    }\n    pre.active {\n      display: unset;\n    }\n    button {\n      white-space: nowrap;\n      text-align: center;\n      position: relative;\n      cursor: pointer;\n      background: rgba(34, 114, 180, 0) !important;\n      color: rgb(34, 114, 180) !important;\n      border-color: rgba(34, 114, 180, 0) !important;\n      padding: 4px 6px !important;\n      text-decoration: none !important;\n      line-height: 20px !important;\n      box-shadow: none !important;\n      height: 32px !important;\n      display: inline-flex !important;\n      -webkit-box-align: center !important;\n      align-items: center !important;\n      -webkit-box-pack: center ', ' background: rgba(34, 114, 180, 0) !important;\n      color: rgb(34, 114, 180) !important;\n      border-color: rgba(34, 114, 180, 0) !important;\n      padding: 4px 6px !important;\n      text-decoration: none !important;\n      line-height: 20px !important;\n      box-shadow: none !important;\n      height: 32px !important;\n      display: inline-flex !important;\n      -webkit-box-align: center !important;\n      align-items: center !important;\n      -webkit-box-pack: center !important;\n      justify-content: center !important;\n      vertical-align: middle !important;\n    }\n    p {\n      margin: 0;\n      padding: 0;\n    }\n    button:hover {\n      background: rgba(34, 114, 180, 0.08) !important;\n      color: rgb(14, 83, 139) !important;\n    }\n    button:active {\n      background: rgba(34, 114, 180, 0.16) !important;\n      color: rgb(4, 53, 93) !important;\n    }\n    h1 {\n      margin-top: 4px;\n      font-size: 22px;\n    }\n    .info {\n      font-size: 12px;\n      font-weight: 500;\n      line-height: 16px;\n      color: rgb(95, 114, 129);\n    }\n    .tabs {\n      margin-top: 10px;\n      border-bottom: 1px solid rgb(209, 217, 225) !important;\n      display: flex;\n      line-height: 24px;\n    }\n    .tab {\n      font-size: 13px;\n      font-weight: 600 !important;\n      cursor: pointer;\n      margin: 0 24px 0 2px;\n      ', ' line-height: 16px;\n      color: rgb(95, 114, 129);\n    }\n    .tabs {\n      margin-top: 10px;\n      border-bottom: 1px solid rgb(209, 217, 225) !important;\n      display: flex;\n      line-height: 24px;\n    }\n    .tab {\n      font-size: 13px;\n      font-weight: 600 !important;\n      cursor: pointer;\n      margin: 0 24px 0 2px;\n      padding-left: 2px;\n    }\n    .tab:hover {\n      color: rgb(14, 83, 139) !important;\n    }\n    .tab.active {\n      border-bottom: 3px solid rgb(34, 114, 180) !important;\n    }\n    .link {\n      margin-left: 12px;\n      display: inline-block;\n      text-decoration: none;\n      color: rgb(34, 114, 180) !important;\n      font-size: 13px;\n      font-weight: 400;\n    }\n    .link:hover {\n      color: rgb(14, 83, 139) !important;\n    }\n    .link-content {\n      display: flex;\n      gap: 6px;\n      align-items: center;\n    }\n    .caret-up {\n      transform: rotate(180deg);\n    }\n  </style>\n</head>\n<body>\n  <div style="display: flex; align-items: center">\n    The logged model is compatible with the Mosaic AI Agent Framework.\n    <button onclick="toggleCode()">\n      See how to evaluate the model&nbsp;\n      <span\n        role="img"\n        id="caret"\n        aria-hidden="true"\n        class="anticon css-6xix1i"\n        style="font-size: ', '  .caret-up {\n      transform: rotate(180deg);\n    }\n  </style>\n</head>\n<body>\n  <div style="display: flex; align-items: center">\n    The logged model is compatible with the Mosaic AI Agent Framework.\n    <button onclick="toggleCode()">\n      See how to evaluate the model&nbsp;\n      <span\n        role="img"\n        id="caret"\n        aria-hidden="true"\n        class="anticon css-6xix1i"\n        style="font-size: 14px"\n        ><svg\n          xmlns="http://www.w3.org/2000/svg"\n          width="1em"\n          height="1em"\n          fill="none"\n          viewBox="0 0 16 16"\n          aria-hidden="true"\n          focusable="false"\n          class=""\n        >\n          <path\n            fill="currentColor"\n            fill-rule="evenodd"\n            d="M8 8.917 10.947 6 12 7.042 8 11 4 7.042 5.053 6z"\n            clip-rule="evenodd"\n          ></path>\n        </svg>\n      </span>\n    </button>\n  </div>\n  <div id="code" style="display: none">\n    <h1>\n      Agent evaluation\n      <a\n        class="link"\n        href="https://docs.databricks.com/en/generative-ai/agent-evaluation/synthesize-evaluation-set.html?utm_source=mlflow.log_model&utm_medium=notebook"\n        target="_blank"\n      ', '7.042 8 11 4 7.042 5.053 6z"\n            clip-rule="evenodd"\n          ></path>\n        </svg>\n      </span>\n    </button>\n  </div>\n  <div id="code" style="display: none">\n    <h1>\n      Agent evaluation\n      <a\n        class="link"\n        href="https://docs.databricks.com/en/generative-ai/agent-evaluation/synthesize-evaluation-set.html?utm_source=mlflow.log_model&utm_medium=notebook"\n        target="_blank"\n      >\n        <span class="link-content">\n          Learn more\n          <span role="img" aria-hidden="true" class="anticon css-6xix1i"\n            ><svg\n              xmlns="http://www.w3.org/2000/svg"\n              width="1em"\n              height="1em"\n              fill="none"\n              viewBox="0 0 16 16"\n              aria-hidden="true"\n              focusable="false"\n              class=""\n            >\n              <path\n                fill="currentColor"\n                d="M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z"\n              ></path>\n             ', '           class=""\n            >\n              <path\n                fill="currentColor"\n                d="M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z"\n              ></path>\n              <path\n                fill="currentColor"\n                d="M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z"\n              ></path></svg></span></span\n      ></a>\n    </h1>\n    <p class="info">\n      Copy the following code snippet in a notebook cell (right click â†’ copy)\n    </p>\n    <div class="tabs">\n      <div class="tab active" onclick="tabClicked(0)">Using synthetic data</div>\n      <div class="tab" onclick="tabClicked(1)">Using your own dataset</div>\n    </div>\n    <div style="height: 472px">\n      <pre\n        class="active"\n      ><code class="language-python">%pip install -U databricks-agents\ndbutils.library.restartPython()\n## Run the above in a separate cell ##\n\nfrom databricks.agents.evals import generate_evals_df\nimport mlflow\n\nagent_description = &quot;A chatbot that answers questions about Databricks.&quot;\nquestion_guidelines = &quot;&quot;&quot;\n# User personas\n- A developer new to the Databricks platform\n# Example questions\n- What API lets me parallelize operations over rows of a delta table?\n&quot;&quot;&quot;\n# TODO: Spark/Pandas DataFrame with &quot;content&quot; and &quot;doc_uri&quot; columns.\ndocs = spark.table(&quot;catalog.schema.my_table_of_docs&quot;)\nevals = generate_evals_df(\n    docs=docs,\n    num_evals=25,\n    agent_description=agent_description,\n    question_guidelines=question_guidelines,\n)\neval_result = mlflow.evaluate(data=evals, model=&quot;runs:/1/model&quot;, model_type=&quot;databricks-agent&quot;)\n</code></pre>\n\n      <pre><code class="language-python">%pip install -U databricks-agents\ndbutils.library.restartPython()\n## Run the above in a ', '   ><code class="language-python">%pip install -U databricks-agents\ndbutils.library.restartPython()\n## Run the above in a separate cell ##\n\nfrom databricks.agents.evals import generate_evals_df\nimport mlflow\n\nagent_description = &quot;A chatbot that answers questions about Databricks.&quot;\nquestion_guidelines = &quot;&quot;&quot;\n# User personas\n- A developer new to the Databricks platform\n# Example questions\n- What API lets me parallelize operations over rows of a delta table?\n&quot;&quot;&quot;\n# TODO: Spark/Pandas DataFrame with &quot;content&quot; and &quot;doc_uri&quot; columns.\ndocs = spark.table(&quot;catalog.schema.my_table_of_docs&quot;)\nevals = generate_evals_df(\n    docs=docs,\n    num_evals=25,\n    agent_description=agent_description,\n    question_guidelines=question_guidelines,\n)\neval_result = mlflow.evaluate(data=evals, model=&quot;runs:/1/model&quot;, model_type=&quot;databricks-agent&quot;)\n</code></pre>\n\n      <pre><code class="language-python">%pip install -U databricks-agents\ndbutils.library.restartPython()\n## Run the above in a separate cell ##\n\nimport pandas as pd\nimport mlflow\n\nevals = [\n    {\n        &quot;request&quot;: {\n            &quot;messages&quot;: [\n                {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;How do I convert a Spark DataFrame to Pandas?&quot;}\n            ],\n        },\n        # Optional, needed for judging correctness.\n        &quot;expected_facts&quot;: [\n            &quot;To convert a Spark DataFrame to Pandas, you can use the toPandas() method.&quot;\n        ],\n    }\n]\neval_result = mlflow.evaluate(\n    data=pd.DataFrame.from_records(evals), model=&quot;runs:/1/model&quot;, model_type=&quot;databricks-agent&quot;\n)\n</code></pre>\n    </div>\n  </div>\n  <script>\n    var codeShown = false;\n    function clip(el) {\n      var range = document.createRange();\n      range.selectNodeContents(el);\n      var sel = window.getSelection();\n      sel.removeAllRanges();\n      sel.addRange(range);\n    }\n\n    function toggleCode() {\n      if (codeShown) {\n        document.getElementById("code").style.display = "none";\n        codeShown = false;\n      } else ', ' </div>\n  </div>\n  <script>\n    var codeShown = false;\n    function clip(el) {\n      var range = document.createRange();\n      range.selectNodeContents(el);\n      var sel = window.getSelection();\n      sel.removeAllRanges();\n      sel.addRange(range);\n    }\n\n    function toggleCode() {\n      if (codeShown) {\n        document.getElementById("code").style.display = "none";\n        codeShown = false;\n      } else {\n        document.getElementById("code").style.display = "block";\n        clip(document.querySelector("pre.active"));\n        codeShown = true;\n      }\n      document.getElementById("caret").classList.toggle("caret-up");\n    }\n\n    function tabClicked(tabIndex) {\n      document.querySelectorAll(".tab").forEach((tab, index) => {\n        if (index === tabIndex) {\n          tab.classList.add("active");\n        } else {\n          tab.classList.remove("active");\n        }\n      });\n      document.querySelectorAll("pre").forEach((pre, index) => {\n        if (index === tabIndex) {\n          pre.classList.add("active");\n        } else {\n          pre.classList.remove("active");\n        }\n      });\n      clip(document.querySelector("pre.active"));\n    }\n  </script>\n</body>\n', "@import 'reset.css';\n@import 'common/components/EditableNote.css';\n@import 'model-registry/index.css';\n\na {\n  color: #2374bb;\n}\na:hover,\na:focus {\n  color: #005580;\n}\n\nbody {\n  margin: 0;\n  padding: 0;\n}\n\n#root {\n  height: 100%;\n  display: flex;\n  flex-direction: column;\n}\n", "[class^=ant-]::-ms-clear,\n[class*= ant-]::-ms-clear,\n[class^=ant-] input::-ms-clear,\n[class*= ant-] input::-ms-clear,\n[class^=ant-] input::-ms-reveal,\n[class*= ant-] input::-ms-reveal {\n  display: none;\n}\nhtml,\nbody {\n  width: 100%;\n  height: 100%;\n}\ninput::-ms-clear,\ninput::-ms-reveal {\n  display: none;\n}\n*,\n*::before,\n*::after {\n  box-sizing: border-box;\n}\nhtml {\n  font-family: sans-serif;\n  line-height: 1.15;\n  -webkit-text-size-adjust: 100%;\n  -ms-text-size-adjust: 100%;\n  -ms-overflow-style: scrollbar;\n  -webkit-tap-highlight-color: rgba(0, 0, 0, 0);\n}\n@-ms-viewport {\n  width: device-width;\n}\nbody {\n  margin: 0;\n  color: rgba(0, 0, 0, 0.85);\n  font-size: 14px;\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, 'Noto Sans', sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n  font-variant: tabular-nums;\n  line-height: 1.5715;\n  background-color: #fff;\n  font-feature-settings: 'tnum';\n}\n[tabindex='-1']:focus {\n  outline: none !important;\n}\nhr {\n  box-sizing: content-box;\n  height: 0;\n  overflow: visible;\n}\nh1,\nh2,\nh3,\nh4,\nh5,\nh6 {\n  margin-top: 0;\n  margin-bottom: 0.5em;\n  font-weight: 500;\n}\np {\n  margin-top: 0;\n  margin-bottom: 1em;\n}\nabbr[title],\nabbr[data-original-title] {\n  text-decoration: underline;\n  -webkit-text-decoration: underline dotted;\n          text-decoration: underline dotted;\n  border-bottom: 0;\n  cursor: help;\n}\naddress {\n  margin-bottom: 1em;\n  font-style: normal;\n  line-height: inherit;\n}\ninput[type='text'],\ninput[type='password'],\ninput[type='number'],\ntextarea {\n  -webkit-appearance: none;\n}\nol,\nul,\ndl {\n  margin-top: 0;\n  margin-bottom: 1em;\n}\nol ol,\nul ul,\nol ul,\nul ol {\n  margin-bottom: 0;\n}\ndt {\n  font-weight: 500;\n}\ndd {\n  margin-bottom: 0.5em;\n  margin-left: 0;\n}\nblockquote {\n  margin: 0 0 1em;\n}\ndfn {\n  font-style: italic;\n}\nb,\nstrong {\n  font-weight: bolder;\n}\nsmall {\n  font-size: 80%;\n}\nsub,\nsup {\n  position: relative;\n  font-size: 75%;\n  line-height: 0;\n  vertical-align: baseline;\n}\nsub {\n  bottom: -0.25em;\n}\nsup {\n  top: -0.5em;\n}\na {\n  color: #1890ff;\n  text-decoration: none;\n  background-color: transparent;\n  outline: none;\n  cursor: pointer;\n  transition: color 0.3s;\n  -webkit-text-decoration-skip: objects;\n}\na:hover {\n  color: #40a9ff;\n}\na:active {\n  color: #096dd9;\n}\na:active,\na:hover {\n  text-decoration: none;\n  outline: 0;\n}\na:focus {\n  text-decoration: none;\n  outline: 0;\n}\na[disabled] {\n  color: rgba(0, 0, 0, 0.25);\n  cursor: not-allowed;\n}\npre,\ncode,\nkbd,\nsamp {\n  font-size: 1em;\n  font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;\n}\npre {\n  margin-top: 0;\n  margin-bottom: 1em;\n  overflow: auto;\n}\nfigure {\n  margin: 0 0 1em;\n}\nimg {\n  vertical-align: middle;\n  border-style: none;\n}\nsvg:not(:root) {\n  overflow: hidden;\n}\na,\narea,\nbutton,\n[role='button'],\ninput:not([type='range']),\nlabel,\nselect,\nsummary,\ntextarea {\n  touch-action: manipulation;\n}\ntable {\n  border-collapse: collapse;\n}\ncaption {\n  padding-top: 0.75em;\n  padding-bottom: 0.3em;\n ", ' outline: none;\n  cursor: pointer;\n  transition: color 0.3s;\n  -webkit-text-decoration-skip: objects;\n}\na:hover {\n  color: #40a9ff;\n}\na:active {\n  color: #096dd9;\n}\na:active,\na:hover {\n  text-decoration: none;\n  outline: 0;\n}\na:focus {\n  text-decoration: none;\n  outline: 0;\n}\na[disabled] {\n  color: rgba(0, 0, 0, 0.25);\n  cursor: not-allowed;\n}\npre,\ncode,\nkbd,\nsamp {\n  font-size: 1em;\n  font-family: \'SFMono-Regular\', Consolas, \'Liberation Mono\', Menlo, Courier, monospace;\n}\npre {\n  margin-top: 0;\n  margin-bottom: 1em;\n  overflow: auto;\n}\nfigure {\n  margin: 0 0 1em;\n}\nimg {\n  vertical-align: middle;\n  border-style: none;\n}\nsvg:not(:root) {\n  overflow: hidden;\n}\na,\narea,\nbutton,\n[role=\'button\'],\ninput:not([type=\'range\']),\nlabel,\nselect,\nsummary,\ntextarea {\n  touch-action: manipulation;\n}\ntable {\n  border-collapse: collapse;\n}\ncaption {\n  padding-top: 0.75em;\n  padding-bottom: 0.3em;\n  color: rgba(0, 0, 0, 0.45);\n  text-align: left;\n  caption-side: bottom;\n}\ninput,\nbutton,\nselect,\noptgroup,\ntextarea {\n  margin: 0;\n  color: inherit;\n  font-size: inherit;\n  font-family: inherit;\n  line-height: inherit;\n}\nbutton,\ninput {\n  overflow: visible;\n}\nbutton,\nselect {\n  text-transform: none;\n}\nbutton,\nhtml [type="button"],\n[type="reset"],\n[type="submit"] {\n  -webkit-appearance: button;\n}\nbutton::-moz-focus-inner,\n[type=\'button\']::-moz-focus-inner,\n[type=\'reset\']::-moz-focus-inner,\n[type=\'submit\']::-moz-focus-inner {\n  padding: 0;\n  border-style: none;\n}\ninput[type=\'radio\'],\ninput[type=\'checkbox\'] {\n  box-sizing: border-box;\n  padding: 0;\n}\ninput[type=\'date\'],\ninput[type=\'time\'],\ninput[type=\'datetime-local\'],\ninput[type=\'month\'] {\n  -webkit-appearance: listbox;\n}\ntextarea {\n  overflow: auto;\n  resize: vertical;\n}\nfieldset {\n  min-width: 0;\n  margin: 0;\n  padding: 0;\n  border: 0;\n}\nlegend {\n  display: block;\n  width: 100%;\n  max-width: 100%;\n  margin-bottom: 0.5em;\n  padding: 0;\n  color: inherit;\n  font-size: 1.5em;\n  line-height: inherit;\n  white-space: normal;\n}\nprogress {\n  vertical-align: baseline;\n}\n[type=\'number\']::-webkit-inner-spin-button,\n[type=\'number\']::-webkit-outer-spin-button {\n  height: auto;\n}\n[type=\'search\'] {\n  outline-offset: -2px;\n  -webkit-appearance: none;\n}\n[type=\'search\']::-webkit-search-cancel-button,\n[type=\'search\']::-webkit-search-decoration {\n  -webkit-appearance: none;\n}\n::-webkit-file-upload-button {\n  font: inherit;\n  -webkit-appearance: button;\n}\noutput {\n  display: inline-block;\n}\nsummary {\n  display: list-item;\n}\ntemplate {\n  display: none;\n}\n[hidden] {\n  display: none !important;\n}\nmark {\n  padding: 0.2em;\n  background-color: #feffe6;\n}\n::-moz-selection {\n  color: #fff;\n  background: #1890ff;\n}\n::selection {\n  color: #fff;\n  background: #1890ff;\n}\n.clearfix::before {\n  display: table;\n  content: \'\';\n}\n.clearfix::after {\n  display: table;\n  clear: both;\n  content: \'\';\n}\n', '.mlflow-editable-note-actions {\n  margin-top: 16px;\n}\n\n.mlflow-editable-note-actions button + button {\n  margin-left: 16px;\n}\n\n.mde-header {\n  background: none;\n}\n', '.mlflow-center {\n  text-align: center;\n}\n\n.mlflow-error-image {\n  margin: 12% auto 60px;\n  display: block;\n}\n', '/* Styles for antd `copyable` code snippets */\n\n.mlflow-ui-container .code-keyword {\n  color: rgb(204, 120, 50);\n}\n.mlflow-ui-container .code {\n  color: rgb(100, 110, 120);\n}\n.mlflow-ui-container .code-comment {\n  color: rgb(140, 140, 140);\n}\n.mlflow-ui-container .code-string {\n  color: rgb(106, 165, 89);\n}\n.mlflow-ui-container .code-number {\n  color: rgb(104, 151, 187);\n}\n', 'div.mlflow-artifact-view {\n  display: flex;\n  overflow: hidden;\n}\n\n.mlflow-artifact-left {\n  min-width: 200px;\n  max-width: 400px;\n  flex: 1;\n}\n\n.mlflow-artifact-left li {\n  white-space: nowrap;\n}\n\n.mlflow-artifact-right {\n  flex: 3;\n  min-width: 400px;\n  max-width: calc(100% - 200px); /* 200px is the min-width of .mlflow-artifact-left */\n\n  overflow: hidden;\n  display: flex;\n  flex-direction: column;\n  height: 100%;\n}\n\n.mlflow-artifact-info-left {\n  flex: 1;\n  max-width: 75%;\n}\n.mlflow-artifact-info-right {\n  margin-left: auto;\n}\n\n.mlflow-artifact-info-path {\n  display: flex;\n  align-items: center;\n}\n\n.mlflow-artifact-info-text {\n  min-width: 0;\n}\n\n.mlflow-artifact-info-size {\n  overflow: hidden;\n  text-overflow: ellipsis;\n}\n\n.mlflow-loading-spinner {\n  height: 20px;\n  opacity: 0;\n  -webkit-animation: spin 3s linear infinite;\n  -moz-animation: spin 3s linear infinite;\n  animation: spin 3s linear infinite;\n}\n\n.mlflow-artifact-info-right .model-version-link {\n  display: flex;\n  align-items: baseline;\n  max-width: 140px;\n  padding-top: 1px;\n  padding-left: 4px;\n}\n\n.mlflow-artifact-info-right .model-version-link .model-name {\n  overflow: hidden;\n  text-overflow: ellipsis;\n}\n\n.mlflow-artifact-info-right .model-version-info {\n  font-size: 12px;\n}\n\n.mlflow-artifact-info-right .model-version-info .model-version-link-section {\n  display: flex;\n  align-items: center;\n}\n\n.mlflow-artifact-info-right .model-version-info .model-version-status-text {\n  overflow: hidden;\n  max-width: 160px;\n  text-overflow: ellipsis;\n}\n', '.mlflow-sticky-header {\n  position: sticky;\n  position: -webkit-sticky;\n  left: 0;\n}\n\n.mlflow-compare-run-table {\n  display: block;\n  overflow: auto;\n  width: 100%;\n}\n\n.mlflow-compare-table th.inter-title {\n  padding: 20px 0 0;\n  background: transparent;\n}\n\n.mlflow-compare-table .head-value {\n  overflow: hidden;\n  overflow-wrap: break-word;\n}\n\n.mlflow-compare-table td.data-value,\n.mlflow-compare-table th.data-value {\n  overflow: hidden;\n  max-width: 120px;\n  text-overflow: ellipsis;\n}\n\n.mlflow-responsive-table-container {\n  width: 100%;\n  overflow-x: auto;\n}\n\n.mlflow-compare-table .diff-row .data-value {\n  background-color: rgba(249, 237, 190, 0.5);\n  color: #555;\n}\n\n.mlflow-compare-table .diff-row .head-value {\n  background-color: rgba(249, 237, 190, 1);\n  color: #555;\n}\n\n.mlflow-compare-table .diff-row:hover {\n  background-color: rgba(249, 237, 190, 1);\n  color: #555;\n}\n\n/* Overrides to make it look more like antd */\n.mlflow-compare-table {\n  width: 100%;\n  max-width: 100%;\n  margin-bottom: 20px;\n}\n.mlflow-compare-table th,\n.mlflow-compare-table td {\n  padding: 12px 8px;\n  border-bottom: 1px solid #e8e8e8;\n}\n.mlflow-compare-table th {\n  color: rgba(0, 0, 0, 0.85);\n  font-weight: 500;\n  background-color: rgb(250, 250, 250);\n  text-align: left;\n}\n.mlflow-compare-table > tbody > tr:hover:not(.diff-row) > td:not(.highlight-data) {\n  background-color: rgb(250, 250, 250);\n}\n', '/* Overriding the table styles since antd tables take the full screen by default.\nWe would like to change it to auto to automatically grow based on the columns */\n\n.mlflow-html-table-view table {\n  width: auto;\n  min-width: 400px;\n}\n\n.mlflow-html-table-view th {\n  width: auto;\n  min-width: 200px;\n  margin-right: 80px;\n  font-size: 13px;\n  color: #888;\n}\n', '.mlflow-metrics-plot-container {\n  display: flex;\n  width: 100%;\n  align-items: flex-start;\n}\n\n.mlflow-metrics-plot-container .plot-controls {\n  display: flex;\n  flex-direction: column;\n  min-height: 500px;\n}\n\n.mlflow-metrics-plot-container .plot-controls .inline-control {\n  margin-top: 25px;\n  display: flex;\n  align-items: center;\n}\n\n.mlflow-metrics-plot-container .plot-controls .inline-control .control-label {\n  margin-right: 10px;\n}\n\n.mlflow-metrics-plot-container .plot-controls .block-control {\n  margin-top: 25px;\n}\n\n.mlflow-metrics-plot-container .metrics-plot-data {\n  flex: 1;\n  display: flex;\n  flex-direction: column;\n}\n\n.mlflow-metrics-plot-container .metrics-plot-view-container {\n  min-height: 500px;\n  flex: 1;\n}\n\n.mlflow-metrics-plot-container .metrics-summary {\n  margin: 20px 20px 20px 60px;\n}\n\n.mlflow-metrics-plot-container .metrics-summary .mlflow-html-table-view {\n  margin-bottom: 25px;\n  /* Shrink to fit, so that scroll bars are aligned with the edge of the table */\n  display: inline-block;\n}\n\n/* Reset min-width which is overridden in HtmlTableView as this breaks the\n   table layout when scrolling is enabled and widths are specified */\n.mlflow-metrics-plot-container .metrics-summary .mlflow-html-table-view th {\n  min-width: auto;\n}\n', '.mlflow-html-iframe {\n  border: none;\n}\n\n.mlflow-artifact-html-view {\n  width: 100%;\n  height: 100%;\n  overflow: auto;\n}\n', '.mlflow-show-artifact-logged-model-view {\n  width: 100%;\n  height: 100%;\n  overflow: auto;\n}\n', '.mlflow-ui-container .map-container {\n  height: 100%;\n  width: 100%;\n}\n\n.mlflow-ui-container .leaflet-container {\n  height: 100%;\n  width: 100%;\n}\n', '.mlflow-pdf-outer-container {\n  display: flex;\n  justify-content: center;\n  align-items: center;\n  height: 100%;\n  width: 100%;\n  padding-left: 16px;\n  overflow: hidden;\n}\n.mlflow-pdf-viewer {\n  display: flex;\n  flex-direction: column;\n  overflow-y: scroll;\n  height: 100%;\n}\n\n.mlflow-paginator {\n  display: flex;\n  justify-content: center;\n  align-items: center;\n  position: sticky;\n  z-index: 1001;\n  top: 0;\n  padding-bottom: 15px;\n  background-color: rgba(250, 250, 250, 0.6);\n  padding-top: 10px;\n}\n\n.mlflow-document {\n  display: flex;\n  justify-content: center;\n  align-items: center;\n}\n', '.mlflow-ShowArtifactPage .text-area {\n  box-sizing: border-box;\n  width: 100%;\n  height: 100%;\n  font-family: Menlo, Consolas, monospace;\n}\n\n.mlflow-ShowArtifactPage,\n.mlflow-ShowArtifactPage .text-area-border-box {\n  width: 100%;\n  height: 100%;\n  overflow: hidden;\n}\n', '.mlflow-ui-container .parcoords > svg,\n.mlflow-ui-container .parcoords > canvas {\n  overflow: visible;\n}\n\n.mlflow-ui-container .parcoords svg text.label {\n  cursor: pointer;\n}\n\n.mlflow-ui-container .parcoords svg g.axis-label-tooltip rect {\n  outline: 1px solid black;\n}\n\n.mlflow-ui-container .parcoords svg g.axis-label-tooltip {\n  visibility: hidden;\n  pointer-events: none;\n}\n.mlflow-ui-container .parcoords svg text.label:hover:not(:active) + g.axis-label-tooltip {\n  visibility: visible;\n}\n\n.mlflow-ui-container .parcoords svg g.tick-label-tooltip rect {\n  outline: 1px solid black;\n}\n\n.mlflow-ui-container .parcoords svg g.tick-label-tooltip {\n  visibility: hidden;\n  pointer-events: none;\n}\n.mlflow-ui-container .parcoords svg text:hover:not(:active) + g.tick-label-tooltip {\n  visibility: visible;\n}\n', "@import 'components/ModelVersionTable.css';\n@import 'components/ModelVersionView.css';\n@import 'components/ModelStageTransitionDropdown.css';\n\n/** TODO(Zangr) migrate globally common components and styles into src/common folder */\n.mlflow-metadata-container {\n  display: flex;\n  flex-wrap: wrap;\n  align-items: center;\n}\n\n.mlflow-metadata-entry {\n  margin-bottom: 16px;\n}\n\n.mlflow-icon-fail {\n  color: red;\n}\n", '.sticky-header {\n  position: sticky;\n  left: 0;\n}\n\n.compare-model-table {\n  display: block;\n  overflow: auto;\n  width: 100%;\n}\n\n.compare-table-row {\n  display: inline-flex;\n}\n\n.compare-table .head-value {\n  overflow: hidden;\n  overflow-wrap: break-word;\n  z-index: 10;\n}\n\n.compare-table .diff-row .data-value {\n  background-color: rgba(249, 237, 190, 0.5) ;\n  color: #555;\n}\n\n.compare-table .diff-row:hover,\n.compare-table .diff-row .head-value,\n.compare-table .diff-row .head-value > span {\n  background-color: rgba(249, 237, 190, 1.0) ;\n  color: #555;\n}\n', '.mlflow-pagination-section {\n  padding-bottom: 30px;\n}\n\n.mlflow-ui-container .ant-alert-info .ant-alert-icon {\n  color: #00b379;\n}\n\n.mlflow-search-input-tooltip .du-bois-light-popover-inner .du-bois-light-popover-inner-content {\n  background-color: rgba(0, 0, 0, 0.75);\n  color: white;\n  border-radius: 4px;\n}\n\n.mlflow-search-input-tooltip .du-bois-light-popover-arrow-content {\n  background-color: rgba(0, 0, 0, 0.75);\n}\n', '.mlflow-stage-transition-dropdown .ant-tag {\n  cursor: pointer;\n  border-radius: 4px;\n}\n', '.mlflow-table-endpoint-text {\n  white-space: nowrap;\n  text-overflow: ellipsis;\n  display: block;\n  overflow: hidden;\n}\n', '/* >>> Extract to our own Alert wrapper component */\n.mlflow-status-alert {\n  margin-bottom: 16px;\n  border-radius: 2px;\n}\n\n.mlflow-status-alert .model-version-status-icon {\n  margin-left: -3px;\n}\n\n.mlflow-status-alert.mlflow-status-alert-info {\n  border-left: 2px solid #3895d3;\n}\n\n.mlflow-status-alert.mlflow-status-alert-error {\n  border-left: 2px solid red;\n}\n\n.mlflow-version-follow-icon {\n  margin-left: auto;\n}\n\n.ant-popover-content {\n  max-width: 500px;\n}\n/* <<< Extract to our own Alert wrapper component */\n', '.mlflow-model-select-dropdown .ant-select-dropdown-menu-item-group-title {\n  color: #666;\n  font-weight: bold;\n}\n\n.mlflow-model-select-dropdown .mlflow-create-new-model-option {\n  border-top: 1px solid #ccc;\n}\n\n.mlflow-register-model-form .modal-explanatory-text {\n  color: rgba(0, 0, 0, 0.52);\n  font-size: 13px;\n}\n', "/* Replaceing AntD Image */\n.mlflow-ui-container .rc-image-preview {\n  height: 100%;\n  pointer-events: none;\n  text-align: center;\n}\n\n.mlflow-ui-container .rc-image-preview-mask {\n  position: fixed;\n  top: 0;\n  left: 0;\n  right: 0;\n  bottom: 0;\n  height: 100%;\n  background-color: rgba(0, 0, 0, 0.45);\n  z-index: 1000;\n}\n\n.mlflow-ui-container .rc-image-preview-mask img {\n  max-width: 100%;\n  max-height: 100%;\n  position: absolute;\n  top: 50%;\n  left: 50%;\n  transform: translate(-50%, -50%);\n}\n\n.mlflow-ui-container .rc-image-preview-mask {\n  background-color: rgba(0, 0, 0, 0.45);\n  bottom: 0;\n  height: 100%;\n  left: 0;\n  position: fixed;\n  right: 0;\n  top: 0;\n  z-index: 1000;\n}\n\n.mlflow-ui-container .rc-image-preview-mask-hidden {\n  display: none;\n}\n\n.mlflow-ui-container .rc-image-preview-wrap {\n  -webkit-overflow-scrolling: touch;\n  bottom: 0;\n  left: 0;\n  outline: 0;\n  overflow: auto;\n  position: fixed;\n  right: 0;\n  top: 0px;\n}\n\n.mlflow-ui-container .rc-image-preview-body {\n  bottom: 0;\n  left: 0;\n  overflow: hidden;\n  position: absolute;\n  right: 0;\n  top: 0;\n}\n\n.mlflow-ui-container .rc-image-preview-img {\n  cursor: grab;\n  max-height: 100%;\n  max-width: 100%;\n  pointer-events: auto;\n  transform: scaleX(1);\n  -webkit-user-select: none;\n  -moz-user-select: none;\n  -ms-user-select: none;\n  user-select: none;\n  vertical-align: middle;\n}\n\n.mlflow-ui-container .rc-image-preview-img,\n.mlflow-ui-container .rc-image-preview-img-wrapper {\n  transition: transform 0.3s cubic-bezier(0.215, 0.61, 0.355, 1) 0s;\n}\n\n.mlflow-ui-container .rc-image-preview-img-wrapper {\n  bottom: 0;\n  left: 0;\n  position: absolute;\n  right: 0;\n  top: 0;\n}\n\n.mlflow-ui-container .rc-image-preview-img-wrapper:before {\n  content: '';\n  display: inline-block;\n  height: 50%;\n  margin-right: -1px;\n  width: 1px;\n}\n\n.mlflow-ui-container .rc-image-preview-moving .mlflow-ui-container .rc-image-preview-img {\n  cursor: grabbing;\n}\n\n.mlflow-ui-container .rc-image-preview-moving .mlflow-ui-container .rc-image-preview-img-wrapper {\n  transition-duration: 0s;\n}\n\n.mlflow-ui-container .rc-image-preview-wrap {\n  z-index: 1080;\n}\n\n.mlflow-ui-container .rc-image-preview-operations {\n  font-feature-settings: 'tnum', 'tnum';\n  align-items: center;\n  background: rgba(0, 0, 0, 0.1);\n  box-sizing: border-box;\n  color: rgba(0, 0, 0, 0.85);\n  color: hsla(0, 0%, 100%, 0.85);\n  display: flex;\n  flex-direction: row-reverse;\n  font-size: 14px;\n  font-variant: tabular-nums;\n  line-height: 1.5715;\n  list-style: none;\n  margin: 0;\n  padding: 0;\n  pointer-events: auto;\n  position: absolute;\n  right: 0;\n  top: 0;\n  width: 100%;\n  z-index: 1;\n}\n\n.mlflow-ui-container .rc-image-preview-operations-operation {\n  cursor: pointer;\n  margin-left: 12px;\n  padding: 12px;\n}\n\n.mlflow-ui-container .rc-image-preview-operations-operation-disabled {\n  color: hsla(0, 0%, 100%, 0.25);\n  pointer-events: none;\n}\n\n.mlflow-ui-container .rc-image-preview-operations-operation:last-of-type {\n  margin-left: 0;\n}\n\n.mlflow-ui-container .rc-image-preview-operations-icon {\n  font-size: 18px;\n}\n\n.mlflow-ui-container .rc-image-preview-switch-left,\n.mlflow-ui-container .rc-image-preview-switch-right ", ' align-items: center;\n  background: rgba(0, 0, 0, 0.1);\n  box-sizing: border-box;\n  color: rgba(0, 0, 0, 0.85);\n  color: hsla(0, 0%, 100%, 0.85);\n  display: flex;\n  flex-direction: row-reverse;\n  font-size: 14px;\n  font-variant: tabular-nums;\n  line-height: 1.5715;\n  list-style: none;\n  margin: 0;\n  padding: 0;\n  pointer-events: auto;\n  position: absolute;\n  right: 0;\n  top: 0;\n  width: 100%;\n  z-index: 1;\n}\n\n.mlflow-ui-container .rc-image-preview-operations-operation {\n  cursor: pointer;\n  margin-left: 12px;\n  padding: 12px;\n}\n\n.mlflow-ui-container .rc-image-preview-operations-operation-disabled {\n  color: hsla(0, 0%, 100%, 0.25);\n  pointer-events: none;\n}\n\n.mlflow-ui-container .rc-image-preview-operations-operation:last-of-type {\n  margin-left: 0;\n}\n\n.mlflow-ui-container .rc-image-preview-operations-icon {\n  font-size: 18px;\n}\n\n.mlflow-ui-container .rc-image-preview-switch-left,\n.mlflow-ui-container .rc-image-preview-switch-right {\n  align-items: center;\n  background: rgba(0, 0, 0, 0.1);\n  border-radius: 50%;\n  color: hsla(0, 0%, 100%, 0.85);\n  cursor: pointer;\n  display: flex;\n  height: 44px;\n  justify-content: center;\n  margin-top: -22px;\n  pointer-events: auto;\n  position: absolute;\n  right: 10px;\n  top: 50%;\n  width: 44px;\n  z-index: 1;\n}\n\n.mlflow-ui-container .rc-image-preview-switch-left-disabled,\n.mlflow-ui-container .rc-image-preview-switch-right-disabled {\n  color: hsla(0, 0%, 100%, 0.25);\n  cursor: not-allowed;\n}\n\n.mlflow-ui-container .rc-image-preview-switch-left-disabled > .anticon,\n.mlflow-ui-container .rc-image-preview-switch-right-disabled > .anticon {\n  cursor: not-allowed;\n}\n\n.mlflow-ui-container .rc-image-preview-switch-left > .anticon,\n.mlflow-ui-container .rc-image-preview-switch-right > .anticon {\n  font-size: 18px;\n}\n\n.mlflow-ui-container .rc-image-preview-switch-left {\n  left: 10px;\n}\n\n.mlflow-ui-container .rc-image-preview-switch-right {\n  right: 10px;\n}\n\n.mlflow-ui-container .fade-enter,\n.mlflow-ui-container .fade-appear {\n  animation-duration: 0.3s;\n  animation-fill-mode: both;\n  animation-play-state: paused;\n}\n.mlflow-ui-container .fade-leave {\n  animation-duration: 0.3s;\n  animation-fill-mode: both;\n  animation-play-state: paused;\n}\n.mlflow-ui-container .fade-enter.fade-enter-active,\n.mlflow-ui-container .fade-appear.fade-appear-active {\n  animation-name: mlflow-rcImageFadeIn;\n  animation-play-state: running;\n}\n.mlflow-ui-container .fade-leave.fade-leave-active {\n  animation-name: mlflow-rcImageFadeOut;\n  animation-play-state: running;\n  pointer-events: none;\n}\n.mlflow-ui-container .fade-enter,\n.mlflow-ui-container .fade-appear {\n  opacity: 0;\n  animation-timing-function: linear;\n}\n.mlflow-ui-container .fade-leave {\n  animation-timing-function: linear;\n}\n\n@keyframes mlflow-rcImageFadeIn {\n  0% {\n    opacity: 0;\n  }\n  100% {\n    opacity: 1;\n  }\n}\n\n@keyframes mlflow-rcImageFadeOut {\n  0% {\n    opacity: 1;\n  }\n  100% {\n    opacity: 0;\n  }\n}\n\n.mlflow-ui-container .zoom-enter,\n.mlflow-ui-container .zoom-appear {\n  animation-duration: 0.3s;\n  animation-fill-mode: both;\n  animation-play-state: paused;\n}\n.mlflow-ui-container .zoom-leave {\n  animation-duration: 0.3s;\n  animation-fill-mode: both;\n  animation-play-state: paused;\n}\n.mlflow-ui-container .zoom-enter.zoom-enter-active,\n.mlflow-ui-container .zoom-appear.zoom-appear-active {\n  animation-name: mlflow-rcImageZoomIn;\n  animation-play-state: running;\n}\n.mlflow-ui-container .zoom-leave.zoom-leave-active {\n  animation-name: mlflow-rcImageZoomOut;\n  animation-play-state: running;\n  pointer-events: none;\n}\n.mlflow-ui-container .zoom-enter,\n.mlflow-ui-container .zoom-appear {\n  transform: scale(0);\n  opacity: 0;\n  animation-timing-function: cubic-bezier(0.08, ', 'linear;\n}\n\n@keyframes mlflow-rcImageFadeIn {\n  0% {\n    opacity: 0;\n  }\n  100% {\n    opacity: 1;\n  }\n}\n\n@keyframes mlflow-rcImageFadeOut {\n  0% {\n    opacity: 1;\n  }\n  100% {\n    opacity: 0;\n  }\n}\n\n.mlflow-ui-container .zoom-enter,\n.mlflow-ui-container .zoom-appear {\n  animation-duration: 0.3s;\n  animation-fill-mode: both;\n  animation-play-state: paused;\n}\n.mlflow-ui-container .zoom-leave {\n  animation-duration: 0.3s;\n  animation-fill-mode: both;\n  animation-play-state: paused;\n}\n.mlflow-ui-container .zoom-enter.zoom-enter-active,\n.mlflow-ui-container .zoom-appear.zoom-appear-active {\n  animation-name: mlflow-rcImageZoomIn;\n  animation-play-state: running;\n}\n.mlflow-ui-container .zoom-leave.zoom-leave-active {\n  animation-name: mlflow-rcImageZoomOut;\n  animation-play-state: running;\n  pointer-events: none;\n}\n.mlflow-ui-container .zoom-enter,\n.mlflow-ui-container .zoom-appear {\n  transform: scale(0);\n  opacity: 0;\n  animation-timing-function: cubic-bezier(0.08, 0.82, 0.17, 1);\n}\n.mlflow-ui-container .zoom-leave {\n  animation-timing-function: cubic-bezier(0.78, 0.14, 0.15, 0.86);\n}\n\n@keyframes mlflow-rcImageZoomIn {\n  0% {\n    transform: scale(0.2);\n    opacity: 0;\n  }\n  100% {\n    transform: scale(1);\n    opacity: 1;\n  }\n}\n\n@keyframes mlflow-rcImageZoomOut {\n  0% {\n    transform: scale(1);\n  }\n  100% {\n    transform: scale(0.2);\n    opacity: 0;\n  }\n}\n', "input,\nselect,\noption,\nbutton,\ntextarea,\nbody,\nthead {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, 'Noto Sans', sans-serif,\n    'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n}\n\nbody,\nthead {\n  font-size: 13px;\n  line-height: 18px;\n  font-weight: 400;\n  box-shadow: none;\n}\n\nhtml,\nbody,\npre,\ncode {\n  margin: 0;\n  padding: 0;\n}\n\nbody {\n  min-height: 100vh;\n}\n\nhtml {\n  overflow-y: hidden;\n}\n", '# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n**For contribution guidelines, code standards, and additional development information not covered here, please refer to [CONTRIBUTING.md](./CONTRIBUTING.md).**\n\n## Repository Overview\n\nMLflow is an open-source platform for managing the end-to-end machine learning lifecycle. It provides tools for:\n\n- Experiment tracking\n- Model versioning and deployment\n- LLM observability and tracing\n- Model evaluation\n- Prompt management\n\n## Quick Start: Development Server\n\n### Start the Full Development Environment (Recommended)\n\n```bash\n# Kill any existing servers\npkill -f "mlflow server" || true; pkill -f "yarn start" || true\n\n# Start both MLflow backend and React frontend dev servers\nnohup uv run bash dev/run-dev-server.sh > /tmp/mlflow-dev-server.log 2>&1 &\n\n# Monitor the logs\ntail -f /tmp/mlflow-dev-server.log\n\n# Servers will be available at:\n# - MLflow backend: http://localhost:5000\n# - React frontend: http://localhost:3000\n```\n\nThis uses `uv` (fast Python package manager) to automatically manage dependencies and run the development environment.\n\n### Start Development Server with Databricks Backend\n\nTo run the MLflow dev server that proxies requests to a Databricks workspace:\n\n```bash\n# IMPORTANT: All four environment variables below are REQUIRED for proper Databricks backend operation\n# Set them in this exact order:\nexport DATABRICKS_HOST="https://your-workspace.databricks.com"  # Your Databricks workspace URL\nexport DATABRICKS_TOKEN="your-databricks-token"                # Your Databricks personal access token\nexport MLFLOW_TRACKING_URI="databricks"                        # Must be set to "databricks"\nexport MLFLOW_REGISTRY_URI="databricks-uc"                     # Use "databricks-uc" for Unity Catalog, or "databricks" for workspace model registry\n\n# Start the dev server with these environment variables\nnohup uv run bash dev/run-dev-server.sh > /tmp/mlflow-dev-server.log 2>&1 &\n\n# Monitor the logs\ntail -f /tmp/mlflow-dev-server.log\n\n# The MLflow server will now proxy tracking and model registry requests to Databricks\n# Access the UI at http://localhost:3000 to see your Databricks experiments and models\n```\n\n**Note**: The MLflow server acts as a proxy, forwarding API requests to your Databricks workspace while serving the local React frontend. This allows you to develop and test UI changes against real Databricks data.\n\n## Development Commands\n\n### Testing\n\n```bash\n# First-time setup: ', '    # Use "databricks-uc" for Unity Catalog, or "databricks" for workspace model registry\n\n# Start the dev server with these environment variables\nnohup uv run bash dev/run-dev-server.sh > /tmp/mlflow-dev-server.log 2>&1 &\n\n# Monitor the logs\ntail -f /tmp/mlflow-dev-server.log\n\n# The MLflow server will now proxy tracking and model registry requests to Databricks\n# Access the UI at http://localhost:3000 to see your Databricks experiments and models\n```\n\n**Note**: The MLflow server acts as a proxy, forwarding API requests to your Databricks workspace while serving the local React frontend. This allows you to develop and test UI changes against real Databricks data.\n\n## Development Commands\n\n### Testing\n\n```bash\n# First-time setup: Install test dependencies\nuv sync\nuv pip install -r requirements/test-requirements.txt\n\n# Run Python tests\nuv run pytest tests/\n\n# Run specific test file\nuv run pytest tests/test_version.py\n\n# Run JavaScript tests\nyarn --cwd mlflow/server/js test\n```\n\n### Code Quality\n\n```bash\n# Python linting and formatting with Ruff\nuv run ruff check . --fix         # Lint with auto-fix\nuv run ruff format .              # Format code\n\n# Check for MLflow spelling typos\nuv run bash dev/mlflow-typo.sh .\n\n# JavaScript linting and formatting\nyarn --cwd mlflow/server/js lint\nyarn --cwd mlflow/server/js prettier:check\nyarn --cwd mlflow/server/js prettier:fix\n\n# Type checking\nyarn --cwd mlflow/server/js type-check\n\n# Run all checks\nyarn --cwd mlflow/server/js check-all\n```\n\n### Special Testing\n\n```bash\n# Run tests with minimal dependencies (skinny client)\nuv run bash dev/run-python-skinny-tests.sh\n\n# Test in Docker container\nuv run bash dev/run-test-container.sh\n```\n\n### Documentation\n\n```bash\n# Build documentation site (needs gateway extras for API doc generation)\nuv run --all-extras bash dev/build-docs.sh --build-api-docs\n\n# Build with R docs included\nuv run --all-extras bash dev/build-docs.sh --build-api-docs --with-r-docs\n\n# Serve documentation locally (after building)\ncd docs && yarn serve --port 8080\n```\n\n## Important Files\n\n- `pyproject.toml`: Package configuration and tool settings\n- `.python-version`: Minimum Python version (3.10)\n- `requirements/`: Dependency specifications\n- `mlflow/ml-package-versions.yml`: Supported ML framework versions\n\n## Common Development Tasks\n\n### Modifying the UI\n\nSee `mlflow/server/js/` for frontend development.\n\n## Language-Specific Style Guides\n\n- [Python](/dev/guides/python.md)\n\n## Git Workflow\n\n### Committing Changes\n\n**IMPORTANT**: After making your commits, run pre-commit hooks on your PR changes to ensure code quality:\n\n```bash\n# Make your commit first (with DCO sign-off)\ngit commit -s -m "Your commit message"\n\n# Then check all files changed in your PR\nuv run pre-commit run --from-ref origin/master --to-ref HEAD\n\n# Fix any issues and amend your ', '--with-r-docs\n\n# Serve documentation locally (after building)\ncd docs && yarn serve --port 8080\n```\n\n## Important Files\n\n- `pyproject.toml`: Package configuration and tool settings\n- `.python-version`: Minimum Python version (3.10)\n- `requirements/`: Dependency specifications\n- `mlflow/ml-package-versions.yml`: Supported ML framework versions\n\n## Common Development Tasks\n\n### Modifying the UI\n\nSee `mlflow/server/js/` for frontend development.\n\n## Language-Specific Style Guides\n\n- [Python](/dev/guides/python.md)\n\n## Git Workflow\n\n### Committing Changes\n\n**IMPORTANT**: After making your commits, run pre-commit hooks on your PR changes to ensure code quality:\n\n```bash\n# Make your commit first (with DCO sign-off)\ngit commit -s -m "Your commit message"\n\n# Then check all files changed in your PR\nuv run pre-commit run --from-ref origin/master --to-ref HEAD\n\n# Fix any issues and amend your commit if needed\ngit add <fixed files>\ngit commit --amend -s\n\n# Re-run pre-commit to verify fixes\nuv run pre-commit run --from-ref origin/master --to-ref HEAD\n\n# Only push once all checks pass\ngit push origin <your-branch>\n```\n\nThis workflow ensures you only check files you\'ve actually modified in your PR, avoiding false positives from unrelated files.\n\n**IMPORTANT**: You MUST sign all commits with DCO (Developer Certificate of Origin). Always use the `-s` flag:\n\n```bash\n# REQUIRED: Always use -s flag when committing\ngit commit -s -m "Your commit message"\n\n# This will NOT work - missing -s flag\n# git commit -m "Your commit message"  âŒ\n```\n\nCommits without DCO sign-off will be rejected by CI.\n\n**Frontend Changes**: If your PR touches any code in `mlflow/server/js/`, you MUST run `yarn check-all` before committing:\n\n```bash\nyarn --cwd mlflow/server/js check-all\n```\n\n### Creating Pull Requests\n\nFollow [the PR template](./.github/pull_request_template.md) when creating pull requests. The template will automatically appear when you create a PR on GitHub.\n\n### Checking CI Status\n\nUse GitHub CLI to check for failing CI:\n\n```bash\n# Check workflow runs for current branch\ngh run list --branch $(git branch --show-current)\n\n# View details of a specific run\ngh run view <run-id>\n\n# Watch a run in progress\ngh run watch\n```\n\n## Pre-commit Hooks\n\nThe repository uses pre-commit for code quality. Install hooks with:\n\n```bash\nuv run pre-commit install --install-hooks\n```\n\nRun pre-commit manually:\n\n```bash\n# Run on all files\nuv run pre-commit run --all-files\n\n# Run on all files, skipping hooks that require external tools\nSKIP=taplo,typos,conftest uv run pre-commit run --all-files\n\n# Run on specific files\nuv run pre-commit run --files path/to/file.py\n\n# Run a specific hook\nuv run pre-commit run ruff --all-files\n```\n\nThis runs Ruff, typos checker, and other tools automatically before commits.\n\n**Note about external ', "failing CI:\n\n```bash\n# Check workflow runs for current branch\ngh run list --branch $(git branch --show-current)\n\n# View details of a specific run\ngh run view <run-id>\n\n# Watch a run in progress\ngh run watch\n```\n\n## Pre-commit Hooks\n\nThe repository uses pre-commit for code quality. Install hooks with:\n\n```bash\nuv run pre-commit install --install-hooks\n```\n\nRun pre-commit manually:\n\n```bash\n# Run on all files\nuv run pre-commit run --all-files\n\n# Run on all files, skipping hooks that require external tools\nSKIP=taplo,typos,conftest uv run pre-commit run --all-files\n\n# Run on specific files\nuv run pre-commit run --files path/to/file.py\n\n# Run a specific hook\nuv run pre-commit run ruff --all-files\n```\n\nThis runs Ruff, typos checker, and other tools automatically before commits.\n\n**Note about external tools**: Some pre-commit hooks require external tools that aren't Python packages:\n\n- `taplo` - TOML formatter\n- `typos` - Spell checker\n- `conftest` - Policy testing tool\n\nTo install these tools:\n\n```bash\n# Install all tools at once (recommended)\nuv run bin/install.py\n```\n\nThis automatically downloads and installs the correct versions of all external tools to the `bin/` directory. The tools work on both Linux and ARM Macs.\n\nThese tools are optional. Use `SKIP=taplo,typos,conftest` if they're not installed.\n\n**Note**: If the typos hook fails, you only need to fix typos in code that was changed by your PR, not pre-existing typos in the codebase.\n", '### Evaluation Criteria\n\nWhen evaluating potential new MLflow committers, the following criteria will be considered:\n\n- **Code Contributions**: Should have multiple non-trivial code contributions accepted and committed to the MLflow codebase. This demonstrates the ability to produce quality code aligned with the project\'s standards.\n- **Technical Expertise**: Should demonstrate a deep understanding of MLflow\'s architecture and design principles, evidenced by making appropriate design choices and technical recommendations. History of caring about code quality, testing, maintainability, and ability to critically evaluate technical artifacts (PRs, designs, etc.) and provide constructive suggestions for improvement.\n- **Subject Matter Breadth**: Contributions and learnings span multiple areas of the codebase, APIs, and integration points rather than a narrow niche.\n- **Community Participation**: Active participation for at least 3 months prior to nomination by authoring code contributions and engaging in the code review process. Involvement in mailing lists, Slack channels, Stack Overflow, and GitHub issues is valued but not strictly required.\n- **Communication**: Should maintain a constructive tone in communications, be receptive to feedback, and collaborate well with existing committers and other community members.\n- **Project Commitment**: Demonstrate commitment to MLflow\'s long-term success, uphold project principles and values, and willingness to pitch in for "unglamorous" work.\n\n### Committership Nomination\n\n- Any current MLflow committer can nominate a contributor for committership by emailing MLflow\'s TSC members with a nomination packet.\n- The nomination packet should provide details on the nominee\'s salient contributions, as well as justification on how they meet the evaluation criteria. Links to GitHub activity, mailing list threads, and other artifacts should be included.\n- In addition to the nominator, every nomination must have a seconder -- a separate committer who advocates for the nominee. The seconder should be a more senior committer (active committer for >1 year) familiar with the nominee\'s work.\n- It is the nominator\'s responsibility to identify a willing seconder and include their recommendation in the nomination packet.\n- If no eligible seconder is available or interested, it may indicate insufficient support to proceed with the nomination at that time. This ensures there are two supporting committers invested in each nomination - the nominator and the seconder. The seconder\'s seniority and familiarity with the situation ', "to the nominator, every nomination must have a seconder -- a separate committer who advocates for the nominee. The seconder should be a more senior committer (active committer for >1 year) familiar with the nominee's work.\n- It is the nominator's responsibility to identify a willing seconder and include their recommendation in the nomination packet.\n- If no eligible seconder is available or interested, it may indicate insufficient support to proceed with the nomination at that time. This ensures there are two supporting committers invested in each nomination - the nominator and the seconder. The seconder's seniority and familiarity with the situation also help build more consensus among the TSC members during evaluation.\n\n### Evaluation Process\n\n- When a committer nomination is made, the TSC members closely review the proposal and evaluate the nominee's qualifications.\n- Throughout the review, the nominator is responsible for addressing any questions from the TSC, and providing clarification or additional evidence as requested by TSC members.\n- After adequate discussion (~1 week), the nominator calls for a formal consensus check among the TSC.\n- A positive consensus requires at least 2 TSC +1 binding votes and no vetoes.\n- Any vetoes must be accompanied by a clear rationale that can be debated.\n- If consensus is not achieved, the nomination is rejected at that time.\n- If consensus fails, the nominator summarizes substantive feedback and remaining gaps to the nominee for their growth and potential re-nomination later. Nomination can be tried again in 3 months after addressing any gaps identified.\n\n### Onboarding a new committer\n\n- Upon a positive consensus being reached, one of the TSC members will extend the formal invitation to the nominee to become a committer. They also field the private initial response from the nominee on willingness to accept.\n- If the proposal is accepted, the nominator grants them the commit access and the new committer will be:\n  - Added to the committer list in the README.md\n  - Announced on the MLflow mailing lists, Slack channels, and the MLflow website\n  - Spotlighted through a post on the MLflow LinkedIn and X handles\n- The nominator will work with the new committer to ", 'a positive consensus being reached, one of the TSC members will extend the formal invitation to the nominee to become a committer. They also field the private initial response from the nominee on willingness to accept.\n- If the proposal is accepted, the nominator grants them the commit access and the new committer will be:\n  - Added to the committer list in the README.md\n  - Announced on the MLflow mailing lists, Slack channels, and the MLflow website\n  - Spotlighted through a post on the MLflow LinkedIn and X handles\n- The nominator will work with the new committer to identify well-scoped initial areas for the new committer to focus on, such as improvements to a specific component.\n- The nominator will also set up periodic 1:1 mentorship check-ins with the new committer over their first month to provide guidance where needed.\n', '# Issue Policy\n\nThe MLflow Issue Policy outlines the categories of MLflow GitHub issues and discusses the guidelines & processes\nassociated with each type of issue.\n\nBefore filing an issue, make sure to [search for related issues](https://github.com/mlflow/mlflow/issues) and check if\nthey address yours.\n\nFor support (ex. "How do I do X?"), please ask on [Stack Overflow](https://stackoverflow.com/questions/tagged/mlflow).\n\n## Issue Categories\n\nOur policy is that GitHub issues fall into one of the following categories:\n\n1. Feature Requests\n2. Bug reports\n3. Documentation fixes\n4. Installation issues\n\nEach category has its own GitHub issue template. Please do not delete the issue template unless you are certain your\nissue is outside its scope.\n\n### Feature Requests\n\n#### Guidelines\n\nFeature requests that are likely to be accepted:\n\n- Are minimal in scope (note that it\'s always easier to add additional functionality later than remove functionality)\n- Are extensible (e.g. if adding an integration with an ML framework, is it possible to add similar integrations with other frameworks?)\n- Have user impact & value that justifies the maintenance burden of supporting the feature moving forwards. The\n  [JQuery contributor guide](https://contribute.jquery.org/open-source/#contributing-something-new) has an excellent discussion on this.\n\n#### Lifecycle\n\nFeature requests typically go through the following lifecycle:\n\n1. A feature request GitHub Issue is submitted, which contains a high-level description of the proposal and its motivation.\n   We encourage requesters to provide an overview of the feature\'s implementation as well, if possible.\n2. The [issue is triaged](ISSUE_TRIAGE.rst) to identify whether more information is needed from the author, give an indication of priority, and route feature requests to appropriate committers.\n3. The feature request is discussed with a committer. The committer will provide input on the implementation overview or\n   ask for a more detailed design, if applicable.\n4. After discussion & agreement on the feature request and its implementation, an implementation owner is identified.\n5. The implementation owner begins developing the feature and ultimately files associated pull requests against the\n   MLflow Repository or packages the feature as an MLflow Plugin.\n\n### Bug reports\n\n#### Guidelines\n\nIn order to ensure that maintainers are able to assist in any reported bug:\n\n- Ensure that the bug report template is filled out in its entirety with appropriate levels of detail, particularly in the `Code to reproduce ', "will provide input on the implementation overview or\n   ask for a more detailed design, if applicable.\n4. After discussion & agreement on the feature request and its implementation, an implementation owner is identified.\n5. The implementation owner begins developing the feature and ultimately files associated pull requests against the\n   MLflow Repository or packages the feature as an MLflow Plugin.\n\n### Bug reports\n\n#### Guidelines\n\nIn order to ensure that maintainers are able to assist in any reported bug:\n\n- Ensure that the bug report template is filled out in its entirety with appropriate levels of detail, particularly in the `Code to reproduce issue` section.\n- Verify that the bug you are reporting meets one of the following criteria:\n  - A recent release of MLflow does not support the operation you are doing that an earlier release did (a regression).\n  - A [documented feature](https://mlflow.org/docs/latest/index.html) or functionality does not work properly by executing a provided example from the docs.\n  - Any exception raised is directly from MLflow and is not the result of an underlying package's exception (e.g., don't file an issue that MLflow can't log a model that can't be trained due to a tensorflow Exception)\n- Make a best effort to diagnose and troubleshoot the issue prior to filing.\n- Verify that the environment that you're experiencing the bug in is supported as defined in the docs.\n- Validate that MLflow supports the functionality that you're having an issue with. _A lack of a feature does not constitute a bug_.\n- Read the docs on the feature for the issue that you're reporting. If you're certain that you're following documented guidelines, please file a bug report.\n\nBug reports typically go through the following lifecycle:\n\n1. A bug report GitHub Issue is submitted, which contains a high-level description of the bug and information required to reproduce it.\n2. The [bug report is triaged](ISSUE_TRIAGE.rst) to identify whether more information is needed from the author, give an indication of priority, and route to request appropriate committers.\n3. An MLflow committer reproduces the bug and provides feedback about how to implement a fix.\n4. After an approach has been agreed upon, an owner ", "on the feature for the issue that you're reporting. If you're certain that you're following documented guidelines, please file a bug report.\n\nBug reports typically go through the following lifecycle:\n\n1. A bug report GitHub Issue is submitted, which contains a high-level description of the bug and information required to reproduce it.\n2. The [bug report is triaged](ISSUE_TRIAGE.rst) to identify whether more information is needed from the author, give an indication of priority, and route to request appropriate committers.\n3. An MLflow committer reproduces the bug and provides feedback about how to implement a fix.\n4. After an approach has been agreed upon, an owner for the fix is identified. MLflow committers may choose to adopt\n   ownership of severe bugs to ensure a timely fix.\n5. The fix owner begins implementing the fix and ultimately files associated pull requests.\n\n### Documentation fixes\n\nDocumentation issues typically go through the following lifecycle:\n\n1. A documentation GitHub Issue is submitted, which contains a description of the issue and its location(s) in the MLflow documentation.\n2. The [issue is triaged](ISSUE_TRIAGE.rst) to identify whether more information is needed from the author, give an indication of priority, and route the request to appropriate committers.\n3. An MLflow committer confirms the documentation issue and provides feedback about how to implement a fix.\n4. After an approach has been agreed upon, an owner for the fix is identified. MLflow committers may choose to adopt\n   ownership of severe documentation issues to ensure a timely fix.\n5. The fix owner begins implementing the fix and ultimately files associated pull requests.\n\n### Installation issues\n\nInstallation issues typically go through the following lifecycle:\n\n1. An installation GitHub Issue is submitted, which contains a description of the issue and the platforms its affects.\n2. The [issue is triaged](ISSUE_TRIAGE.rst) to identify whether more information is needed from the author, give an indication of priority, and route the issue to appropriate committers.\n3. An MLflow committer confirms the installation issue and provides feedback about how to implement a fix.\n4. After an approach has been agreed upon, an owner for the fix is identified. MLflow committers may choose to adopt\n   ownership of severe installation issues to ensure a ", 'requests.\n\n### Installation issues\n\nInstallation issues typically go through the following lifecycle:\n\n1. An installation GitHub Issue is submitted, which contains a description of the issue and the platforms its affects.\n2. The [issue is triaged](ISSUE_TRIAGE.rst) to identify whether more information is needed from the author, give an indication of priority, and route the issue to appropriate committers.\n3. An MLflow committer confirms the installation issue and provides feedback about how to implement a fix.\n4. After an approach has been agreed upon, an owner for the fix is identified. MLflow committers may choose to adopt\n   ownership of severe installation issues to ensure a timely fix.\n5. The fix owner begins implementing the fix and ultimately files associated pull requests.\n', '<h1 align="center" style="border-bottom: none">\n    <a href="https://mlflow.org/">\n        <img alt="MLflow logo" src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/logo.svg" width="200" />\n    </a>\n</h1>\n<h2 align="center" style="border-bottom: none">Open-Source Platform for Productionizing AI</h2>\n\nMLflow is an open-source developer platform to build AI/LLM applications and models with confidence. Enhance your AI applications with end-to-end **experiment tracking**, **observability**, and **evaluations**, all in one integrated platform.\n\n<div align="center">\n\n[![Python SDK](https://img.shields.io/pypi/v/mlflow)](https://pypi.org/project/mlflow/)\n[![PyPI Downloads](https://img.shields.io/pypi/dm/mlflow)](https://pepy.tech/projects/mlflow)\n[![License](https://img.shields.io/github/license/mlflow/mlflow)](https://github.com/mlflow/mlflow/blob/main/LICENSE)\n<a href="https://twitter.com/intent/follow?screen_name=mlflow" target="_blank">\n<img src="https://img.shields.io/twitter/follow/mlflow?logo=X&color=%20%23f5f5f5"\n      alt="follow on X(Twitter)"></a>\n<a href="https://www.linkedin.com/company/mlflow-org/" target="_blank">\n<img src="https://custom-icon-badges.demolab.com/badge/LinkedIn-0A66C2?logo=linkedin-white&logoColor=fff"\n      alt="follow on LinkedIn"></a>\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/mlflow/mlflow)\n\n</div>\n\n<div align="center">\n   <div>\n      <a href="https://mlflow.org/"><strong>Website</strong></a> Â·\n      <a href="https://mlflow.org/docs/latest/index.html"><strong>Docs</strong></a> Â·\n      <a href="https://github.com/mlflow/mlflow/issues/new/choose"><strong>Feature Request</strong></a> Â·\n      <a href="https://mlflow.org/blog"><strong>News</strong></a> Â·\n      <a href="https://www.youtube.com/@mlflowoss"><strong>YouTube</strong></a> Â·\n      <a href="https://lu.ma/mlflow?k=c"><strong>Events</strong></a>\n   </div>\n</div>\n\n<br>\n\n## ðŸš€ Installation\n\nTo install the MLflow Python package, run the following command:\n\n```\npip install mlflow\n```\n\n## ðŸ“¦ Core Components\n\nMLflow is **the only platform that provides a unified solution for all your AI/ML needs**, including LLMs, Agents, Deep Learning, and traditional machine learning.\n\n### ðŸ’¡ For LLM / GenAI Developers\n\n<table>\n  <tr>\n    <td>\n    <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-tracing.png" alt="Tracing" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/llms/tracing/index.html"><strong>ðŸ” Tracing / Observability</strong></a>\n        <br><br>\n        <div>Trace the internal states of your LLM/agentic applications for debugging quality issues and monitoring performance with ease.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/tracing/quickstart/python-openai/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n    <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-llm-eval.png" alt="LLM Evaluation" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/genai/eval-monitor/"><strong>ðŸ“Š LLM Evaluation</strong></a>\n        <br><br>\n        <div>A suite of automated model evaluation tools, seamlessly integrated with experiment tracking to compare ', 'debugging quality issues and monitoring performance with ease.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/tracing/quickstart/python-openai/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n    <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-llm-eval.png" alt="LLM Evaluation" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/genai/eval-monitor/"><strong>ðŸ“Š LLM Evaluation</strong></a>\n        <br><br>\n        <div>A suite of automated model evaluation tools, seamlessly integrated with experiment tracking to compare across multiple versions.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/eval-monitor/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-prompt.png" alt="Prompt Management">\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/prompt-registry/"><strong>ðŸ¤– Prompt Management</strong></a>\n        <br><br>\n        <div>Version, track, and reuse prompts across your organization, helping maintain consistency and improve collaboration in prompt development.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-registry/create-and-edit-prompts/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-logged-model.png" alt="MLflow Hero">\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/version-tracking/"><strong>ðŸ“¦ App Version Tracking</strong></a>\n        <br><br>\n        <div>MLflow keeps track of many moving parts in your AI applications, such as models, prompts, tools, and code, with end-to-end lineage.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/version-tracking/quickstart/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n</table>\n\n### ðŸŽ“ For Data Scientists\n\n<table>\n  <tr>\n    <td colspan="2" align="center" >\n     ', '     <br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/version-tracking/"><strong>ðŸ“¦ App Version Tracking</strong></a>\n        <br><br>\n        <div>MLflow keeps track of many moving parts in your AI applications, such as models, prompts, tools, and code, with end-to-end lineage.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/version-tracking/quickstart/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n</table>\n\n### ðŸŽ“ For Data Scientists\n\n<table>\n  <tr>\n    <td colspan="2" align="center" >\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-experiment.png" alt="Tracking" width=50%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/ml/tracking/"><strong>ðŸ“ Experiment Tracking</strong></a>\n        <br><br>\n        <div>Track your models, parameters, metrics, and evaluation results in ML experiments and compare them using an interactive UI.</div><br>\n        <a href="https://mlflow.org/docs/latest/ml/tracking/quickstart/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-model-registry.png" alt="Model Registry" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/ml/model-registry/"><strong>ðŸ’¾ Model Registry</strong></a>\n        <br><br>\n        <div> A centralized model store designed to collaboratively manage the full lifecycle and deployment of machine learning models.</div><br>\n        <a href="https://mlflow.org/docs/latest/ml/model-registry/tutorial/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-deployment.png" alt="Deployment" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/ml/deployment/"><strong>ðŸš€ Deployment</strong></a>\n        <br><br>\n        <div> Tools for seamless model deployment to batch ', 'store designed to collaboratively manage the full lifecycle and deployment of machine learning models.</div><br>\n        <a href="https://mlflow.org/docs/latest/ml/model-registry/tutorial/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-deployment.png" alt="Deployment" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/ml/deployment/"><strong>ðŸš€ Deployment</strong></a>\n        <br><br>\n        <div> Tools for seamless model deployment to batch and real-time scoring on platforms like Docker, Kubernetes, Azure ML, and AWS SageMaker.</div><br>\n        <a href="https://mlflow.org/docs/latest/ml/deployment/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n</table>\n\n## ðŸŒ Hosting MLflow Anywhere\n\n<div align="center" >\n  <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-providers.png" alt="Providers" width=100%>\n</div>\n\nYou can run MLflow in many different environments, including local machines, on-premise servers, and cloud infrastructure.\n\nTrusted by thousands of organizations, MLflow is now offered as a managed service by most major cloud providers:\n\n- [Amazon SageMaker](https://aws.amazon.com/sagemaker-ai/experiments/)\n- [Azure ML](https://learn.microsoft.com/en-us/azure/machine-learning/concept-mlflow?view=azureml-api-2)\n- [Databricks](https://www.databricks.com/product/managed-mlflow)\n- [Nebius](https://nebius.com/services/managed-mlflow)\n\nFor hosting MLflow on your own infrastructure, please refer to [this guidance](https://mlflow.org/docs/latest/ml/tracking/#tracking-setup).\n\n## ðŸ—£ï¸ Supported Programming Languages\n\n- [Python](https://pypi.org/project/mlflow/)\n- [TypeScript / JavaScript](https://www.npmjs.com/package/mlflow-tracing)\n- [Java](https://mvnrepository.com/artifact/org.mlflow/mlflow-client)\n- [R](https://cran.r-project.org/web/packages/mlflow/readme/README.html)\n\n## ðŸ”— Integrations\n\nMLflow is natively integrated with many popular machine learning frameworks and GenAI libraries.\n\n![Integrations](https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-integrations.png)\n\n## Usage Examples\n\n### Experiment Tracking ([Doc](https://mlflow.org/docs/latest/ml/tracking/))\n\nThe following examples trains a simple regression model with scikit-learn, while enabling MLflow\'s [autologging](https://mlflow.org/docs/latest/tracking/autolog.html) feature for experiment tracking.\n\n```python\nimport mlflow\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Enable MLflow\'s automatic experiment tracking for scikit-learn\nmlflow.sklearn.autolog()\n\n# Load the training dataset\ndb = load_diabetes()\nX_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n\nrf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\n# MLflow triggers logging automatically upon model fitting\nrf.fit(X_train, y_train)\n```\n\nOnce the above code finishes, run the following command in a separate terminal and access the MLflow UI via the printed URL. An MLflow **Run** should be automatically created, which tracks the training dataset, hyper parameters, performance metrics, the trained model, dependencies, and even more.\n\n```\nmlflow ui\n```\n\n### Evaluating Models ([Doc](https://mlflow.org/docs/latest/model-evaluation/index.html))\n\nThe following example runs automatic evaluation for question-answering tasks with several built-in metrics.\n\n```python\nimport mlflow\nimport ', 'import train_test_split\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Enable MLflow\'s automatic experiment tracking for scikit-learn\nmlflow.sklearn.autolog()\n\n# Load the training dataset\ndb = load_diabetes()\nX_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n\nrf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\n# MLflow triggers logging automatically upon model fitting\nrf.fit(X_train, y_train)\n```\n\nOnce the above code finishes, run the following command in a separate terminal and access the MLflow UI via the printed URL. An MLflow **Run** should be automatically created, which tracks the training dataset, hyper parameters, performance metrics, the trained model, dependencies, and even more.\n\n```\nmlflow ui\n```\n\n### Evaluating Models ([Doc](https://mlflow.org/docs/latest/model-evaluation/index.html))\n\nThe following example runs automatic evaluation for question-answering tasks with several built-in metrics.\n\n```python\nimport mlflow\nimport pandas as pd\n\n# Evaluation set contains (1) input question (2) model outputs (3) ground truth\ndf = pd.DataFrame(\n    {\n        "inputs": ["What is MLflow?", "What is Spark?"],\n        "outputs": [\n            "MLflow is an innovative fully self-driving airship powered by AI.",\n            "Sparks is an American pop and rock duo formed in Los Angeles.",\n        ],\n        "ground_truth": [\n            "MLflow is an open-source platform for productionizing AI.",\n            "Apache Spark is an open-source, distributed computing system.",\n        ],\n    }\n)\neval_dataset = mlflow.data.from_pandas(\n    df, predictions="outputs", targets="ground_truth"\n)\n\n# Start an MLflow Run to record the evaluation results to\nwith mlflow.start_run(run_name="evaluate_qa"):\n    # Run automatic evaluation with a set of built-in metrics for question-answering models\n    results = mlflow.evaluate(\n        data=eval_dataset,\n        model_type="question-answering",\n    )\n\nprint(results.tables["eval_results_table"])\n```\n\n### Observability ([Doc](https://mlflow.org/docs/latest/llms/tracing/index.html))\n\nMLflow Tracing provides LLM observability for various GenAI libraries such as OpenAI, LangChain, LlamaIndex, DSPy, AutoGen, and more. To enable auto-tracing, call `mlflow.xyz.autolog()` before running your models. Refer to the documentation for customization ', '],\n    }\n)\neval_dataset = mlflow.data.from_pandas(\n    df, predictions="outputs", targets="ground_truth"\n)\n\n# Start an MLflow Run to record the evaluation results to\nwith mlflow.start_run(run_name="evaluate_qa"):\n    # Run automatic evaluation with a set of built-in metrics for question-answering models\n    results = mlflow.evaluate(\n        data=eval_dataset,\n        model_type="question-answering",\n    )\n\nprint(results.tables["eval_results_table"])\n```\n\n### Observability ([Doc](https://mlflow.org/docs/latest/llms/tracing/index.html))\n\nMLflow Tracing provides LLM observability for various GenAI libraries such as OpenAI, LangChain, LlamaIndex, DSPy, AutoGen, and more. To enable auto-tracing, call `mlflow.xyz.autolog()` before running your models. Refer to the documentation for customization and manual instrumentation.\n\n```python\nimport mlflow\nfrom openai import OpenAI\n\n# Enable tracing for OpenAI\nmlflow.openai.autolog()\n\n# Query OpenAI LLM normally\nresponse = OpenAI().chat.completions.create(\n    model="gpt-4o-mini",\n    messages=[{"role": "user", "content": "Hi!"}],\n    temperature=0.1,\n)\n```\n\nThen navigate to the "Traces" tab in the MLflow UI to find the trace records OpenAI query.\n\n## ðŸ’­ Support\n\n- For help or questions about MLflow usage (e.g. "how do I do X?") visit the [documentation](https://mlflow.org/docs/latest/index.html).\n- In the documentation, you can ask the question to our AI-powered chat bot. Click on the **"Ask AI"** button at the right bottom.\n- Join the [virtual events](https://lu.ma/mlflow?k=c) like office hours and meetups.\n- To report a bug, file a documentation issue, or submit a feature request, please [open a GitHub issue](https://github.com/mlflow/mlflow/issues/new/choose).\n- For release announcements and other discussions, please subscribe to our mailing list (mlflow-users@googlegroups.com)\n  or join us on [Slack](https://mlflow.org/slack).\n\n## ðŸ¤ Contributing\n\nWe happily welcome contributions to MLflow!\n\n- Submit [bug reports](https://github.com/mlflow/mlflow/issues/new?template=bug_report_template.yaml) and [feature requests](https://github.com/mlflow/mlflow/issues/new?template=feature_request_template.yaml)\n- Contribute for [good-first-issues](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) and [help-wanted](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22)\n- Writing about MLflow and sharing your experience\n\nPlease see our [contribution guide](CONTRIBUTING.md) to learn more about contributing to MLflow.\n\n## â­ï¸ Star History\n\n<a href="https://star-history.com/#mlflow/mlflow&Date">\n <picture>\n   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date&theme=dark" />\n   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date" />\n   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date" />\n </picture>\n</a>\n\n## âœï¸ Citation\n\nIf you use MLflow in your research, please cite it using the "Cite this repository" button at the top of the [GitHub repository page](https://github.com/mlflow/mlflow), which will provide you with citation formats including APA and BibTeX.\n\n## ðŸ‘¥ Core Members\n\nMLflow is currently maintained by the following core members with significant contributions from ', '[good-first-issues](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) and [help-wanted](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22)\n- Writing about MLflow and sharing your experience\n\nPlease see our [contribution guide](CONTRIBUTING.md) to learn more about contributing to MLflow.\n\n## â­ï¸ Star History\n\n<a href="https://star-history.com/#mlflow/mlflow&Date">\n <picture>\n   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date&theme=dark" />\n   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date" />\n   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date" />\n </picture>\n</a>\n\n## âœï¸ Citation\n\nIf you use MLflow in your research, please cite it using the "Cite this repository" button at the top of the [GitHub repository page](https://github.com/mlflow/mlflow), which will provide you with citation formats including APA and BibTeX.\n\n## ðŸ‘¥ Core Members\n\nMLflow is currently maintained by the following core members with significant contributions from hundreds of exceptionally talented community members.\n\n- [Ben Wilson](https://github.com/BenWilson2)\n- [Corey Zumar](https://github.com/dbczumar)\n- [Daniel Lok](https://github.com/daniellok-db)\n- [Gabriel Fu](https://github.com/gabrielfu)\n- [Harutaka Kawamura](https://github.com/harupy)\n- [Serena Ruan](https://github.com/serena-ruan)\n- [Tomu Hirata](https://github.com/TomeHirata)\n- [Weichen Xu](https://github.com/WeichenXu123)\n- [Yuki Watanabe](https://github.com/B-Step62)\n', '# Security Policy\n\nMLflow and its community take security bugs seriously. We appreciate efforts to improve the security of MLflow\nand follow the [GitHub coordinated disclosure of security vulnerabilities](https://docs.github.com/en/code-security/security-advisories/about-coordinated-disclosure-of-security-vulnerabilities#about-reporting-and-disclosing-vulnerabilities-in-projects-on-github)\nfor responsible disclosure and prompt mitigation. We are committed to working with security researchers to\nresolve the vulnerabilities they discover.\n\n## Supported Versions\n\nThe latest version of MLflow has continued support. If a critical vulnerability is found in the current version\nof MLflow, we may opt to backport patches to previous versions.\n\n## Reporting a Vulnerability\n\nWhen finding a security vulnerability in MLflow, please perform the following actions:\n\n- [Open an issue](https://github.com/mlflow/mlflow/issues/new?assignees=&labels=bug&template=bug_report_template.md&title=%5BBUG%5D%20Security%20Vulnerability) on the MLflow repository. Ensure that you use `[BUG] Security Vulnerability` as the title and _do not_ mention any vulnerability details in the issue post.\n- Send a notification [email](mailto:mlflow-oss-maintainers@googlegroups.com) to `mlflow-oss-maintainers@googlegroups.com` that contains, at a minimum:\n  - The link to the filed issue stub.\n  - Your GitHub handle.\n  - Detailed information about the security vulnerability, evidence that supports the relevance of the finding and any reproducibility instructions for independent confirmation.\n\nThis first stage of reporting is to ensure that a rapid validation can occur without wasting the time and effort of a reporter. Future communication and vulnerability resolution will be conducted after validating\nthe veracity of the reported issue.\n\nAn MLflow maintainer will, after validating the report:\n\n- Acknowledge the [bug](ISSUE_POLICY.md#bug-reports) during [triage](ISSUE_TRIAGE.rst)\n- Mark the issue as `priority/critical-urgent`\n- Open a draft [GitHub Security Advisory](https://docs.github.com/en/code-security/security-advisories/creating-a-security-advisory)\n  to discuss the vulnerability details in private.\n\nThe private Security Advisory will be used to confirm the issue, prepare a fix, and publicly disclose it after the fix has been released.\n', '## MLflow Dev Scripts\n\nThis directory contains automation scripts for MLflow developers and the build infrastructure.\n\n## Job Statuses\n\n[![Examples Action Status](https://img.shields.io/github/actions/workflow/status/mlflow/dev/examples.yml.svg?branch=master&event=schedule&label=Examples&style=for-the-badge&logo=github)](https://github.com/mlflow/dev/actions/workflows/examples.yml?query=workflow%3AExamples+event%3Aschedule)\n[![Cross Version Tests Status](https://img.shields.io/github/actions/workflow/status/mlflow/dev/cross-version-tests.yml.svg?branch=master&event=schedule&label=Cross%20version%20tests&style=for-the-badge&logo=github)](https://github.com/mlflow/dev/actions/workflows/cross-version-tests.yml?query=workflow%3A%22Cross+version+tests%22+event%3Aschedule)\n[![R-devel Action Status](https://img.shields.io/github/actions/workflow/status/mlflow/dev/r.yml.svg?branch=master&event=schedule&label=r-devel&style=for-the-badge&logo=github)](https://github.com/mlflow/dev/actions/workflows/r.yml?query=workflow%3AR+event%3Aschedule)\n[![Test Requirements Status](https://img.shields.io/github/actions/workflow/status/mlflow/dev/requirements.yml.svg?branch=master&event=schedule&label=test%20requirements&logo=github&style=for-the-badge)](https://github.com/mlflow/dev/actions/workflows/requirements.yml?query=workflow%3A%22Test+requirements%22+event%3Aschedule)\n[![Push Images Status](https://img.shields.io/github/actions/workflow/status/mlflow/mlflow/push-images.yml.svg?event=release&label=push-images&logo=github&style=for-the-badge)](https://github.com/mlflow/mlflow/actions/workflows/push-images.yml?query=event%3Arelease)\n[![Slow Tests Status](https://img.shields.io/github/actions/workflow/status/mlflow/dev/slow-tests.yml.svg?branch=master&event=schedule&label=slow-tests&logo=github&style=for-the-badge)](https://github.com/mlflow/dev/actions/workflows/slow-tests.yml?query=event%3Aschedule)\n[![Website E2E Tests Status](https://img.shields.io/github/actions/workflow/status/mlflow/mlflow-website/e2e.yml.svg?branch=main&event=schedule&label=website-e2e&logo=github&style=for-the-badge)](https://github.com/mlflow/mlflow-website/actions/workflows/e2e.yml?query=event%3Aschedule)\n', '# Typos\n\nA quick guide on how to use [`typos`](https://github.com/crate-ci/typos) to find, fix, and ignore typos.\n\n## Installation\n\n```sh\n# Replace `<version>` with the version installed in `dev/install-typos.sh`.\nbrew install typos-cli@<version>\n\n```\n\nSee https://github.com/crate-ci/typos?tab=readme-ov-file#install for other installation methods.\n\n## Finding typos\n\n```sh\npre-commit run --all-files typos\n```\n\n## Fixing typos\n\nYou can fix typos either manually or by running the following command:\n\n```sh\ntypos --write-changes [PATH]\n```\n\n## Ignoring false positives\n\nThere are two ways to ignore false positives:\n\n### Option 1: Ignore a line/block containing false positives\n\nThis option is preferred if the false positive is a one-off.\n\n```python\n# Ignore a line containing a typo:\n\n"<false_positive>"  # spellchecker: disable-line\n\n# Ignore a block containing typos:\n\n# spellchecker: off\n"<false_positive>"\n"<another_false_positive>"\n# spellchecker: on\n```\n\n### Option 2: Extend the ignore list in [`pyproject.toml`](../pyproject.toml)\n\nThis option is preferred if the false positive is common across multiple files/lines.\n\n```toml\n# pyproject.toml\n\n[tool.typos.default]\nextend-ignore-re = [\n  ...,\n  "false_positive",\n]\n```\n\n## Found a typo, but `typos` doesn\'t recognize it?\n\n`typos` only recognizes typos that are in its dictionary.\nIf you find a typo that `typos` doesn\'t recognize,\nyou can extend the `extend-words` list in [`pyproject.toml`](../pyproject.toml).\n\n```toml\n# pyproject.toml\n\n[tool.typos.default.extend-words]\n...\nmflow = "mlflow"\n```\n', '# Clint\n\nA custom linter for mlflow to enforce rules that ruff doesn\'t cover.\n\n## Installation\n\n```\npip install -e dev/clint\n```\n\n## Usage\n\n```bash\nclint file.py ...\n```\n\n## Integrating with Visual Studio Code\n\n1. Install [the Pylint extension](https://marketplace.visualstudio.com/items?itemName=ms-python.pylint)\n2. Add the following setting in your `settings.json` file:\n\n```json\n{\n  "pylint.path": ["${interpreter}", "-m", "clint"]\n}\n```\n\n## Ignoring Rules for Specific Files or Lines\n\n**To ignore a rule on a specific line (recommended):**\n\n```python\nfoo()  # clint: disable=<rule_name>\n```\n\nReplace `<rule_name>` with the actual rule you want to disable.\n\n**To ignore a rule for an entire file:**\n\nAdd the file path to the `exclude` list in your `pyproject.toml`:\n\n```toml\n[tool.clint]\nexclude = [\n  # ...existing entries...\n  "path/to/file.py",\n]\n```\n\n## Testing\n\n```bash\npytest --confcutdir dev/clint dev/clint\n```\n', '# Python Style Guide\n\nThis guide documents Python coding conventions that go beyond what [ruff](https://docs.astral.sh/ruff/) and [clint](../../dev/clint/) can enforce. The practices below require human judgment to implement correctly and improve code readability, maintainability, and testability across the MLflow codebase.\n\n## Avoid Redundant Test Docstrings\n\nOmit docstrings that merely echo the function name without adding value. Test names should be self-documenting.\n\n```python\n# Bad\ndef test_foo():\n    """Test foo"""\n    ...\n\n\n# Good\ndef test_foo():\n    ...\n```\n\n## Use Type Hints for All Functions\n\nAdd type hints to all function parameters and return values. This enables better IDE support, catches bugs early, and serves as inline documentation.\n\n```python\n# Bad\ndef foo(s):\n    return len(s)\n\n\n# Good\ndef foo(s: str) -> int:\n    return len(s)\n```\n\n### Exceptions\n\n**Test functions:** The `-> None` return type can be omitted for test functions since they implicitly return `None` and the return value is not used.\n\n```python\n# Acceptable\ndef test_foo(s: str):\n    ...\n\n\n# Also acceptable (but not required)\ndef test_foo(s: str) -> None:\n    ...\n```\n\n**`__init__` methods:** The `-> None` return type can be omitted for `__init__` methods since they always return `None` by definition.\n\n```python\n# Acceptable\nclass Foo:\n    def __init__(self, s: str):\n        ...\n\n\n# Also acceptable (but not required)\nclass Foo:\n    def __init__(self, s: str) -> None:\n        ...\n```\n\n## Minimize Try-Catch Block Scope\n\nWrap only the specific operations that can raise exceptions. Keep safe operations outside the try block to improve debugging and avoid masking unexpected errors.\n\n```python\n# Bad\ntry:\n    never_fails()\n    can_fail()\nexcept ...:\n    handle_error()\n\n# Good\nnever_fails()\ntry:\n    can_fail()\nexcept ...:\n    handle_error()\n```\n\n## Use Dataclasses Instead of Complex Tuples\n\nReplace tuples with 3+ elements with named dataclasses. This improves code clarity, prevents positional argument errors, and enables type checking on individual fields.\n\n```python\n# Bad\ndef get_user() -> tuple[str, int, str]:\n    return "Alice", 30, "Engineer"\n\n\n# Good\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass User:\n    name: str\n    age: int\n    occupation: str\n\n\ndef get_user() -> User:\n    return User(name="Alice", age=30, occupation="Engineer")\n```\n\n## Use next() to Find First ', 'Bad\ntry:\n    never_fails()\n    can_fail()\nexcept ...:\n    handle_error()\n\n# Good\nnever_fails()\ntry:\n    can_fail()\nexcept ...:\n    handle_error()\n```\n\n## Use Dataclasses Instead of Complex Tuples\n\nReplace tuples with 3+ elements with named dataclasses. This improves code clarity, prevents positional argument errors, and enables type checking on individual fields.\n\n```python\n# Bad\ndef get_user() -> tuple[str, int, str]:\n    return "Alice", 30, "Engineer"\n\n\n# Good\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass User:\n    name: str\n    age: int\n    occupation: str\n\n\ndef get_user() -> User:\n    return User(name="Alice", age=30, occupation="Engineer")\n```\n\n## Use next() to Find First Match Instead of Loop-and-Break\n\nUse the `next()` builtin function with a generator expression to find the first item that matches a condition. This is more concise and functional than manually looping with break statements.\n\n```python\n# Bad\nresult = None\nfor item in items:\n    if item.name == "target":\n        result = item\n        break\n\n# Good\nresult = next((item for item in items if item.name == "target"), None)\n```\n\n## Always Verify Mock Calls with Assertions\n\nEvery mocked function must have an assertion (`assert_called`, `assert_called_once`, etc.) to verify it was invoked correctly. Without assertions, tests may pass even when the mocked code isn\'t executed.\n\n```python\nfrom unittest import mock\n\n\n# Bad\ndef test_foo():\n    with mock.patch("foo.bar"):\n        calls_bar()\n\n\n# Good\ndef test_bar():\n    with mock.patch("foo.bar") as mock_bar:\n        calls_bar()\n        mock_bar.assert_called_once()\n```\n\n## Set Mock Behaviors in Patch Declaration\n\nDefine `return_value` and `side_effect` directly in the `patch()` call rather than assigning them afterward. This keeps mock configuration explicit and reduces setup code.\n\n```python\nfrom unittest import mock\n\n\n# Bad\ndef test_foo():\n    with mock.patch("foo.bar") as mock_bar:\n        mock_bar.return_value = 42\n        calls_bar()\n\n    with mock.patch("foo.bar") as mock_bar:\n        mock_bar.side_effect = Exception("Error")\n        calls_bar()\n\n\n# Good\ndef test_foo():\n    with mock.patch("foo.bar", return_value=42) as mock_bar:\n      ', ' mock_bar.assert_called_once()\n```\n\n## Set Mock Behaviors in Patch Declaration\n\nDefine `return_value` and `side_effect` directly in the `patch()` call rather than assigning them afterward. This keeps mock configuration explicit and reduces setup code.\n\n```python\nfrom unittest import mock\n\n\n# Bad\ndef test_foo():\n    with mock.patch("foo.bar") as mock_bar:\n        mock_bar.return_value = 42\n        calls_bar()\n\n    with mock.patch("foo.bar") as mock_bar:\n        mock_bar.side_effect = Exception("Error")\n        calls_bar()\n\n\n# Good\ndef test_foo():\n    with mock.patch("foo.bar", return_value=42) as mock_bar:\n        calls_bar()\n\n    with mock.patch("foo.bar", side_effect=Exception("Error")) as mock_bar:\n        calls_bar()\n```\n\n## Use Pytest\'s Monkeypatch for Directory Changes\n\nUse `monkeypatch.chdir()` instead of manual `os.chdir()` with try/finally blocks. Pytest automatically restores the original directory after the test, preventing side effects.\n\n```python\nimport os\nimport pytest\n\n\n# Bad\ndef test_foo():\n    cwd = os.getcwd()\n    try:\n        os.chdir("some/directory")\n    finally:\n        os.chdir(cwd)\n\n\n# Good\ndef test_foo(monkeypatch: pytest.MonkeyPatch):\n    monkeypatch.chdir("some/directory")\n```\n\n## Parametrize Tests with Multiple Input Cases\n\nUse `@pytest.mark.parametrize` to test multiple inputs instead of repeating assertions. This creates separate test cases for each input, making failures easier to diagnose and tests more maintainable.\n\n```python\n# Bad\ndef test_foo():\n    assert foo("a") == 0\n    assert foo("b") == 1\n    assert foo("c") == 2\n\n\n# Good\n@pytest.mark.parametrize(\n    ("input", "expected"),\n    [\n        ("a", 0),\n        ("b", 1),\n        ("c", 2),\n    ],\n)\ndef test_foo(input: str, expected: int):\n    assert foo(input) == expected\n```\n\n## Use Pytest\'s Monkeypatch for Mocking Environment Variables\n\nUse `monkeypatch.setenv()` and `monkeypatch.delenv()` instead of `mock.patch.dict()` for environment variables. Pytest\'s monkeypatch fixture automatically restores the original environment after the test, providing cleaner and more reliable test isolation.\n\n```python\n# Bad - Setting environment variables\ndef test_foo():\n    with mock.patch.dict("os.environ", {"FOO": "True"}):\n        ...\n\n\n# Bad - ', '[\n        ("a", 0),\n        ("b", 1),\n        ("c", 2),\n    ],\n)\ndef test_foo(input: str, expected: int):\n    assert foo(input) == expected\n```\n\n## Use Pytest\'s Monkeypatch for Mocking Environment Variables\n\nUse `monkeypatch.setenv()` and `monkeypatch.delenv()` instead of `mock.patch.dict()` for environment variables. Pytest\'s monkeypatch fixture automatically restores the original environment after the test, providing cleaner and more reliable test isolation.\n\n```python\n# Bad - Setting environment variables\ndef test_foo():\n    with mock.patch.dict("os.environ", {"FOO": "True"}):\n        ...\n\n\n# Bad - Removing environment variables\ndef test_bar():\n    with mock.patch.dict("os.environ", {}, clear=True):\n        ...\n\n\n# Good - Setting environment variables\ndef test_foo(monkeypatch: pytest.MonkeyPatch):\n    monkeypatch.setenv("FOO", "True")\n    ...\n\n\n# Good - Removing environment variables\ndef test_bar(monkeypatch: pytest.MonkeyPatch):\n    # raising=False prevents KeyError if FOO doesn\'t exist\n    monkeypatch.delenv("FOO", raising=False)\n    ...\n```\n\n## Use Pytest\'s tmp_path Fixture for Temporary Files\n\nUse `tmp_path` fixture instead of manual `tempfile.TemporaryDirectory()` for handling temporary files and directories in tests. Pytest automatically cleans up the temporary directory after the test, provides better test isolation, and integrates seamlessly with pytest\'s fixture system.\n\n```python\n# Bad\nimport tempfile\n\n\ndef test_foo():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        ...\n\n\n# Good\nfrom pathlib import Path\n\n\ndef test_foo(tmp_path: Path):\n    ...\n```\n', '# MLflow Proto To GraphQL Autogeneration\n\n## What is this\n\nThe system in `dev/proto_to_graphql` parses proto rpc definitions and generates graphql schema based on the proto rpc definition. The goal of this system is to quickly generate base GraphQL schema and resolver code so that we can easily take advantage of the data joining functionalities of GraphQL.\n\nThe autogenerated schema and resolver are in the following file: `mlflow/server/graphql/autogenerated_graphql_schema.py`\n\nThe autogenerated schema and resolvers are referenced and can be extended in this file `mlflow/server/graphql/graphql_schema_extensions.py`\n\nYou can run `python ./dev/proto_to_graphql/code_generator.py` or `./dev/generate-protos.sh` to trigger the codegen process.\n\n## FAQs\n\n### How to onboard a new rpc to GraphQL\n\n- In your proto rpc definition, add `option (graphql) = {};` and re-run `./dev/generate-protos.sh`. You should see the changes in the generated schema. [Example](https://github.com/mlflow/mlflow/pull/11215/files#diff-8ab2ad3109b67a713e147edf557d4da88853563398ce354cc895bb5930950dc5R175).\n- In `mlflow/server/handlers.py`, identify the handler function for your rpc, for example `_get_run`, make sure there exists a corresponding `get_run_impl` function that takes in a `request_message` and returns a response messages that is of the generated service_pb proto type. If no such function exists, you can easily extract it out like in this [example](https://github.com/mlflow/mlflow/pull/11215/files#diff-5c10a4e2ca47745f06fa9e7201087acfc102849756cb8d85e774a5ac468cb037R1779-R1795).\n- Test manually with a localhost server, as well as adding a unit test in `tests/tracking/test_rest_tracking.py`. [Example](https://github.com/mlflow/mlflow/pull/11215/files#diff-2ec8756f67a20ecbaeec2d2c5e7bf33310a50c015fc3aa487e27100fc4c2f9a7R1771-R1802).\n\n### How to customize a generated query/mutation to join multiple rpc endpoints\n\nThe proto to graphql autogeneration only supports 1 to 1 mapping from proto rpc to graphql operation. However, the power of GraphQL is to join multiple rpc endpoints together as one query. So we often would like to customize or extend the autogenerated operations to join these multiple endpoints.\n\nFor example, we would like to query data about `Experiment`, `ModelVersions` and `Run` in one query by extending the `MlflowRun` object.\n\n```\nquery testQuery {\n    mlflowGetRun(input: {runId: "my-id"}) {\n        run {\n            experiment {\n                name\n            }\n            modelVersions {\n         ', 'example, we would like to query data about `Experiment`, `ModelVersions` and `Run` in one query by extending the `MlflowRun` object.\n\n```\nquery testQuery {\n    mlflowGetRun(input: {runId: "my-id"}) {\n        run {\n            experiment {\n                name\n            }\n            modelVersions {\n                name\n            }\n        }\n    }\n}\n```\n\nTo achieve joins, follow the steps below:\n\n- Make sure the rpcs you would like to join are already onboarded to GraphQL by following the `How to onboard a new rpc to GraphQL` section\n- Identify the class you would like to extend in `autogenerated_graphql_schema.py` and create a new class that inherits the target class, put it in `graphql_schema_extensions.py`. Add the new fields and the resolver function as you intended. [Example](https://github.com/mlflow/mlflow/pull/11173/files#diff-9e4f7bdf4d7f9d362338bed9ce6607a51b8f520ee605e2fd4c9bda5e43cb617cR21-R31)\n- Run `python ./dev/proto_to_graphql/code_generator.py` or `./dev/generate-protos.sh`, you should see the autogenerated schema being updated to reference the extension class you just created.\n- Add a test case in `tests/tracking/test_rest_tracking.py` [Example](https://github.com/mlflow/mlflow/pull/11173/files#diff-2ec8756f67a20ecbaeec2d2c5e7bf33310a50c015fc3aa487e27100fc4c2f9a7R1771-R1795)\n\n### How to generate typescript types for a GraphQL operation\n\nTo generate typescript types, first make sure the generated schema is up-to-date by running `python ./dev/proto_to_graphql/code_generator.py`\n\nThen write your new query or mutation in the mlflow/server/js/src folder, after that run the following commands:\n\n- cd mlflow/server/js\n- yarn graphql-codegen\n\nYou should be able to see the generated types in `mlflow/server/js/src/graphql/__generated__/`\n', "# MLflow with Docker Compose (PostgreSQL + MinIO)\n\nThis directory provides a **Docker Compose** setup for running **MLflow** locally with a **PostgreSQL** backend store and **MinIO** (S3-compatible) artifact storage. It's intended for quick evaluation and local development.\n\n---\n\n## Overview\n\n- **MLflow Tracking Server** â€” exposed on your host (default `http://localhost:5000`).\n- **PostgreSQL** â€” persists MLflow's metadata (experiments, runs, params, metrics).\n- **MinIO** â€” stores run artifacts via an S3-compatible API.\n\nCompose automatically reads configuration from a local `.env` file in this directory.\n\n---\n\n## Prerequisites\n\n- **Git**\n- **Docker** and **Docker Compose**\n  - Windows/macOS: [Docker Desktop](https://www.docker.com/products/docker-desktop/)\n  - Linux: Docker Engine + the `docker compose` plugin\n\nVerify your setup:\n\n```bash\ndocker --version\ndocker compose version\n```\n\n---\n\n## 1. Clone the Repository\n\n```bash\ngit clone https://github.com/mlflow/mlflow.git\ncd docker-compose\n```\n\n---\n\n## 2. Configure Environment\n\nCopy the example environment file and modify as needed:\n\n```bash\ncp .env.dev.example .env\n```\n\nThe `.env` file defines container image tags, ports, credentials, and storage configuration. Open it and review values before starting the stack.\n\n**Common variables** :\n\n- **MLflow**\n  - `MLFLOW_PORT=5000` â€” host port for the MLflow UI/API\n  - `MLFLOW_DEFAULT_ARTIFACT_ROOT=s3://mlflow/` â€” artifact store URI\n  - `MLFLOW_S3_ENDPOINT_URL=http://minio:9000` â€” S3 endpoint (inside the Compose network)\n- **PostgreSQL**\n  - `POSTGRES_USER=mlflow`\n  - `POSTGRES_PASSWORD=mlflow`\n  - `POSTGRES_DB=mlflow`\n- **MinIO (S3-compatible)**\n  - `MINIO_ROOT_USER=minio`\n  - `MINIO_ROOT_PASSWORD=minio123`\n  - `MINIO_HOST=minio`\n  - `MINIO_PORT=9000`\n  - `MINIO_BUCKET=mlflow`\n\n---\n\n## 3. Launch the Stack\n\n```bash\ndocker compose up -d\n```\n\nThis:\n\n- Builds/pulls images as needed\n- Creates a user-defined network\n- Starts **postgres**, **minio**, and **mlflow** containers\n\nCheck status:\n\n```bash\ndocker compose ps\n```\n\nView logs (useful on first run):\n\n```bash\ndocker compose logs -f\n```\n\n---\n\n## 4. Access MLflow\n\nOpen the MLflow UI:\n\n- **URL**: `http://localhost:5000` (or the port set in `.env`)\n\nYou can now create experiments, run training scripts, and log metrics, parameters, and artifacts to this local MLflow instance.\n\n---\n\n## 5. Shutdown\n\nTo stop and remove the containers and network:\n\n```bash\ndocker compose down\n```\n\n> Data is preserved in Docker **volumes**. To remove volumes as well (irreversible), run:\n>\n> ```bash\n> docker compose down -v\n> ```\n\n---\n\n## Tips & Troubleshooting\n\n- **Verify connectivity**  \n  If MLflow can't write artifacts, confirm your S3 settings:\n\n  - `MLFLOW_DEFAULT_ARTIFACT_ROOT` points to your MinIO bucket (e.g., `s3://mlflow/`)\n  - `MLFLOW_S3_ENDPOINT_URL` is reachable from the MLflow container (often `http://minio:9000`)\n\n- **Resetting the environment**  \n  If you want a clean slate, stop the stack and remove volumes:\n\n  ", "training scripts, and log metrics, parameters, and artifacts to this local MLflow instance.\n\n---\n\n## 5. Shutdown\n\nTo stop and remove the containers and network:\n\n```bash\ndocker compose down\n```\n\n> Data is preserved in Docker **volumes**. To remove volumes as well (irreversible), run:\n>\n> ```bash\n> docker compose down -v\n> ```\n\n---\n\n## Tips & Troubleshooting\n\n- **Verify connectivity**  \n  If MLflow can't write artifacts, confirm your S3 settings:\n\n  - `MLFLOW_DEFAULT_ARTIFACT_ROOT` points to your MinIO bucket (e.g., `s3://mlflow/`)\n  - `MLFLOW_S3_ENDPOINT_URL` is reachable from the MLflow container (often `http://minio:9000`)\n\n- **Resetting the environment**  \n  If you want a clean slate, stop the stack and remove volumes:\n\n  ```bash\n  docker compose down -v\n  docker compose up -d\n  ```\n\n- **Logs**\n\n  - MLflow server: `docker compose logs -f mlflow`\n  - PostgreSQL: `docker compose logs -f postgres`\n  - MinIO: `docker compose logs -f minio`\n\n- **Port conflicts**  \n  If `5000` (or any other port) is in use, change it in `.env` and restart:\n  ```bash\n  docker compose down\n  docker compose up -d\n  ```\n\n---\n\n## How It Works (at a Glance)\n\n- MLflow uses **PostgreSQL** as the _backend store_ for experiment/run metadata.\n- MLflow uses **MinIO** as the _artifact store_ via S3 APIs.\n- Docker Compose wires services on a shared network; MLflow talks to PostgreSQL and MinIO by container name (e.g., `postgres`, `minio`).\n\n---\n\n## Next Steps\n\n- Point your training scripts to this server:\n  ```bash\n  export MLFLOW_TRACKING_URI=http://localhost:5000\n  ```\n- Start logging runs with `mlflow.start_run()` (Python) or the MLflow CLI.\n- Customize the `.env` and `docker-compose.yml` to fit your local workflow (e.g., change image tags, add volumes, etc.).\n\n---\n\n**You now have a fully local MLflow stack with persistent metadata and artifact storageâ€”ideal for development and experimentation.**\n", "## MLflow examples\n\n### Quick Start example\n\n- `quickstart/mlflow_tracking.py` is a basic example to introduce MLflow concepts.\n\n## Tutorials\n\nVarious examples that depict MLflow tracking, project, and serving use cases.\n\n- `h2o` depicts how MLflow can be use to track various random forest architectures to train models\n  for predicting wine quality.\n- `hyperparam` shows how to do hyperparameter tuning with MLflow and some popular optimization libraries.\n- `keras` modifies\n  [a Keras classification example](https://github.com/keras-team/keras/blob/ed07472bc5fc985982db355135d37059a1f887a9/examples/reuters_mlp.py)\n  and uses MLflow's `mlflow.tensorflow.autolog()` API to automatically log metrics and parameters\n  to MLflow during training.\n- `multistep_workflow` is an end-to-end of a data ETL and ML training pipeline built as an MLflow\n  project. The example shows how parts of the workflow can leverage from previously run steps.\n- `pytorch` uses CNN on MNIST dataset for character recognition. The example logs TensorBoard events\n  and stores (logs) them as MLflow artifacts.\n- `remote_store` has a usage example of REST based backed store for tracking.\n- `r_wine` demonstrates how to log parameters, metrics, and models from R.\n- `sklearn_elasticnet_diabetes` uses the sklearn diabetes dataset to predict diabetes progression\n  using ElasticNet.\n- `sklearn_elasticnet_wine_quality` is an example for MLflow projects. This uses the Wine\n  Quality dataset and Elastic Net to predict quality. The example uses `MLproject` to set up a\n  Conda environment, define parameter types and defaults, entry point for training, etc.\n- `sklearn_logistic_regression` is a simple MLflow example with hooks to log training data to MLflow\n  tracking server.\n- `supply_chain_security` shows how to strengthen the security of ML projects against supply-chain attacks by enforcing hash checks on Python packages.\n- `tensorflow` contains end-to-end one run examples from train to predict for TensorFlow 2.8+ It includes usage of MLflow's\n  `mlflow.tensorflow.autolog()` API, which captures TensorBoard data and logs to MLflow with no code change.\n- `docker` demonstrates how to create and run an MLflow project using docker (rather than conda)\n  to manage project dependencies\n- `johnsnowlabs` gives you access to [20.000+ state-of-the-art enterprise NLP models in 200+ languages](https://nlp.johnsnowlabs.com/models) for medical, finance, legal and many more domains.\n\n## Demos\n\n- `demos` folder contains notebooks used during MLflow presentations.\n", "# Basic authentication example\n\nThis example demonstrates the authentication and authorization feature of MLflow.\n\nTo run this example,\n\n1. Start the tracking server\n   ```shell\n   mlflow ui --app-name=basic-auth\n   ```\n2. Go to `http://localhost:5000/signup` and register two users:\n   - `(user_a, password_a)`\n   - `(user_b, password_b)`\n3. Run the script\n   ```shell\n   python auth.py\n   ```\n   Expected output:\n   ```\n   2023/05/02 14:03:58 INFO mlflow.tracking.fluent: Experiment with name 'experiment_a' does not exist. Creating a new experiment.\n   {}\n   API request to endpoint /api/2.0/mlflow/runs/create failed with error code 403 != 200. Response body: 'Permission denied'\n   ```\n", '# MLflow 3 Examples\n\n## Pre-requisites\n\nBefore running the examples, run the following command to install mlflow 3.0:\n\n```sh\npip install git+https://github.com/mlflow/mlflow.git@mlflow-3\n```\n', '# MLflow Deployments\n\nThe examples provided within this directory show how to get started with MLflow Deployments using:\n\n- Databricks (see the `databricks` subdirectory)\n', '### MLflow evaluation Examples\n\nThe examples in this directory demonstrate how to use the `mlflow.evaluate()` API. Specifically,\nthey show how to evaluate a PyFunc model on a specified dataset using the builtin default evaluator\nand specified extra metrics, where the resulting metrics & artifacts are logged to MLflow Tracking.\nThey also show how to specify validation thresholds for the resulting metrics to validate the quality\nof your model. See full list of examples below:\n\n- Example `evaluate_on_binary_classifier.py` evaluates an xgboost `XGBClassifier` model on dataset loaded by\n  `shap.datasets.adult`.\n- Example `evaluate_on_multiclass_classifier.py` evaluates a scikit-learn `LogisticRegression` model on dataset\n  generated by `sklearn.datasets.make_classification`.\n- Example `evaluate_on_regressor.py` evaluate as scikit-learn `LinearRegression` model on dataset loaded by\n  `sklearn.datasets.fetch_california_housing`\n- Example `evaluate_with_custom_metrics.py` evaluates a scikit-learn `LinearRegression`\n  model with a custom metric function on dataset loaded by `sklearn.datasets.fetch_california_housing`\n- Example `evaluate_with_custom_metrics_comprehensive.py` evaluates a scikit-learn `LinearRegression` model\n  with a comprehensive list of custom metric functions on dataset loaded by `sklearn.datasets.fetch_california_housing`\n- Example `evaluate_with_model_validation.py` trains both a candidate xgboost `XGBClassifier` model\n  and a baseline `DummyClassifier` model on dataset loaded by `shap.datasets.adult`. Then, it validates\n  the candidate model against specified thresholds on both builtin and extra metrics and the dummy model.\n\n#### Prerequisites\n\n```\npip install scikit-learn xgboost shap>=0.40 matplotlib\n```\n\n#### How to run the examples\n\nRun in this directory with Python.\n\n```sh\npython evaluate_on_binary_classifier.py\npython evaluate_on_multiclass_classifier.py\npython evaluate_on_regressor.py\npython evaluate_with_custom_metrics.py\npython evaluate_with_custom_metrics_comprehensive.py\npython evaluate_with_model_vaidation.py\n```\n', '# MLflow AI Gateway\n\nThe examples provided within this directory show how to get started with individual providers and at least\none of the supported endpoint types. When configuring an instance of the MLflow AI Gateway, multiple providers,\ninstances of endpoint types, and model versions can be specified for each query endpoint on the server.\n\n## Example configuration files\n\nWithin this directory are example config files for each of the supported providers. If using these as a guide\nfor configuring a large number of endpoints, ensure that the placeholder names (i.e., "completions", "chat", "embeddings")\nare modified to prevent collisions. These names are provided for clarity only for the examples and real-world\nuse cases should define a relevant and meaningful endpoint name to eliminate ambiguity and minimize the chances of name collisions.\n\n# Getting Started with MLflow AI Gateway for OpenAI\n\nThis guide will walk you through the installation and basic setup of the MLflow AI Gateway.\nWithin sub directories of this examples section, you can find specific executable examples\nthat can be used to validate a given provider\'s configuration through the MLflow AI Gateway.\nLet\'s get started.\n\n## Step 1: Installing the MLflow AI Gateway\n\nThe MLflow AI Gateway is best installed from PyPI. Open your terminal and use the following pip command:\n\n```sh\n# Installation from PyPI\npip install \'mlflow[genai]\'\n```\n\nFor those interested in development or in using the most recent build of the MLflow AI Gateway, you may choose to install from the fork of the repository:\n\n```sh\n# Installation from the repository\npip install -e \'.[genai]\'\n```\n\n## Step 2: Configuring Endpoints\n\nEach provider has a distinct set of allowable endpoint types (i.e., chat, completions, etc) and\nspecific requirements for the initialization of the endpoints to interface with their services.\nFor full examples of configurations and supported endpoint types, see:\n\n- [OpenAI](openai/config.yaml)\n- [MosaicML](mosaicml/config.yaml)\n- [Anthropic](anthropic/config.yaml)\n- [Cohere](cohere/config.yaml)\n- [AI21 Labs](ai21labs/config.yaml)\n- [PaLM](palm/config.yaml)\n- [AzureOpenAI](azure_openai/config.yaml)\n- [Mistral](mistral/config.yaml)\n- [TogetherAI](togetherai/config.yaml)\n\n## Step 3: Setting Access Keys\n\nSee information on specific methods of obtaining and setting the access keys within the provider-specific documentation within this directory.\n\n## Step 4: Starting the MLflow AI Gateway\n\nWith the MLflow configuration file in place and access key(s) set, you can now start the MLflow AI Gateway.\nReplace `<provider>` with the actual path to the MLflow configuration file for the provider of your choice:\n\n```sh\nmlflow ', "(i.e., chat, completions, etc) and\nspecific requirements for the initialization of the endpoints to interface with their services.\nFor full examples of configurations and supported endpoint types, see:\n\n- [OpenAI](openai/config.yaml)\n- [MosaicML](mosaicml/config.yaml)\n- [Anthropic](anthropic/config.yaml)\n- [Cohere](cohere/config.yaml)\n- [AI21 Labs](ai21labs/config.yaml)\n- [PaLM](palm/config.yaml)\n- [AzureOpenAI](azure_openai/config.yaml)\n- [Mistral](mistral/config.yaml)\n- [TogetherAI](togetherai/config.yaml)\n\n## Step 3: Setting Access Keys\n\nSee information on specific methods of obtaining and setting the access keys within the provider-specific documentation within this directory.\n\n## Step 4: Starting the MLflow AI Gateway\n\nWith the MLflow configuration file in place and access key(s) set, you can now start the MLflow AI Gateway.\nReplace `<provider>` with the actual path to the MLflow configuration file for the provider of your choice:\n\n```sh\nmlflow gateway start --config-path examples/gateway/<provider>/config.yaml --port 7000\n\n# For example:\nmlflow gateway start --config-path examples/gateway/openai/config.yaml --port 7000\n```\n\n## Step 5: Accessing the Interactive API Documentation\n\nWith the MLflow AI Gateway up and running, access its interactive API documentation by navigating to the following URL:\n\nhttp://127.0.0.1:7000/docs\n\n## Step 6: Sending Test Requests\n\nAfter successfully setting up the MLflow AI Gateway, you can send a test request using the provided Python script.\nReplace <provider> with the name of the provider example test script that you'd like to use:\n\n```sh\npython examples/gateway/<provider>/example.py\n```\n", "## Example endpoint configuration for AI21 Labs\n\nTo set up your MLflow configuration file, include a single endpoint for the completions endpoint as shown in the [AI21 labs configuration](config.yaml) YAML file.\n\n## Obtaining and Setting the AI21 Labs API Key\n\nTo obtain an AI21 Labs API key, you need to create an account and subscribe to the service at [AI21 Labs](https://studio.ai21.com/account/api-key?source=docs).\n\nAfter obtaining the key, you can export it to your environment variables. Make sure to replace the '...' with your actual API key:\n\n```sh\nexport AI21LABS_API_KEY=...\n```\n", "## Example endpoint configuration for Anthropic\n\nTo set up your MLflow configuration file, include a single endpoint for the completions endpoint as shown in the [anthropic configuration](config.yaml) YAML file.\n\n## Obtaining and Setting the Anthropic API Key\n\nTo obtain an Anthropic API key, you need to create an account and subscribe to the service at [Anthropic](https://docs.anthropic.com/claude/docs/getting-access-to-claude).\n\nAfter obtaining the key, you can export it to your environment variables. Make sure to replace the '...' with your actual API key:\n\n```sh\nexport ANTHROPIC_API_KEY=...\n```\n", "## Example endpoint configuration for Azure OpenAI\n\nThe following example configuration shows the 3 supported endpoints for Azure OpenAI: chat, completions, and embeddings.\nAdditionally, it illustrates the two separate api types that are supported for this service.\n\n- `azure` api type: uses a generated token that is applied by setting the API token key directly to an environment variable\n- `azuread` api type: uses Azure Active Directory for supplying the active directory key to be used to an environment variable\n\nDepending on how your users will be interacting with the MLflow AI Gateway, a single access paradigm (either `azure` **or** `azuread` is recommended, not a mix of both).\n\nSee the [Azure OpenAI configuration](config.yaml) YAML file for example configurations showing all supported endpoint types and the different token access types.\n\n## Setting the Azure OpenAI API Key\n\nIn order to get access to the Azure OpenAI service, [see the documentation](https://azure.microsoft.com/en-us/products/cognitive-services/openai-service) guidance in the cognitive services portal.\nWith the key, export it to your environment variables.\n\nReplace the '...' with your actual API key:\n\n```sh\nexport OPENAI_API_KEY=...\n```\n\n## Validating the Azure OpenAI endpoint\n\nSee the [OpenAI Example](../openai/example.py) for testing the Azure OpenAI endpoints. The usage is identical to the standard OpenAI integration from an API perspective.\n", '## Example endpoint configuration for Amazon Bedrock\n\nTo view an example of a Bedrock endpoint configuration, see [the configuration example](config.yaml) YAML file.\n\n## Credentials\n\nValid AWS credentials are required for this example. Set `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` to valid credentials, or run in an environment with those variables set.\n', "## Example endpoint configuration for Cohere\n\nTo see an example of specifying both the completions and the embeddings endpoints for Cohere, see [the configuration](config.yaml) YAML file.\n\nThis configuration file specifies two endpoints: 'completions' and 'embeddings', both using Cohere's models 'command' and 'embed-english-light-v2.0', respectively.\n\n## Setting a Cohere API Key\n\nThis example requires a [Cohere API key](https://docs.cohere.com/docs/going-live):\n\n```sh\nexport COHERE_API_KEY=...\n```\n", "## Example endpoint configuration for GEMINI\n\nTo see an example of specifying both the completions and embeddings endpoints for Gemini, see [the configuration](config.yaml) YAML file.\n\nThis configuration file specifies three endpoints: 'completions', 'embeddings', and 'chat', using Gemini's model gemini-2.0-flash for completions and chat and gemini-embedding-exp-03-07 for embeddings.\n\n## Setting a GEMINI API Key\n\nThis example requires a [GEMINI API key](https://ai.google.dev/gemini-api/docs/api-key):\n\n```sh\nexport GEMINI_API_KEY=...\n```\n", '## Example endpoint configuration for Huggingface Text Generation Inference\n\n[Huggingface Text Generation Inference (TGI)](https://huggingface.co/docs/text-generation-inference/index) is a comprehensive toolkit designed for deploying and serving Large Language Models (LLMs) efficiently. It offers optimized support for various popular open-source LLMs such as Llama, Falcon, StarCoder, BLOOM, and GPT-Neo. TGI comes with various built-in optimizations and features, such as:\n\n- Simple launcher to serve most popular LLMs\n- Tensor Parallelism for faster inference on multiple GPUs\n- Safetensors weight loading\n- Optimized transformers code for inference using Flash Attention and Paged Attention on the most popular architectures\n\nIt should be noted that only a [selection of models](https://huggingface.co/docs/text-generation-inference/supported_models) are optimized for TGI, which uses custom CUDA kernels for faster inference. You can add the flag `--disable-custom-kernels`` at the end of the docker run command if you wish to disable them. If the above list lacks the model you would like to serve, or in the case you created a custom created model, you can try to initialize and serve the model anyways. However, since the model is not optimized for TGI, performance is not guaranteed.\n\nFor a more detailed description of all features, please go to the [documentation](https://huggingface.co/docs/text-generation-inference/index).\n\n## Getting Started\n\n> **NOTE** This example is tested on a Linux Machine (Debian 11) with a NVIDIA A100 GPU.\n\nTo configure the MLflow AI Gateway with Huggingface Text Generation Inference, a few additional steps need to be followed. The initial step involves deploying a Huggingface model on the TGI server, which is illustrated in the next section.\n\nThe recommended approach for deploying the TGI server is by utilizing the [official Docker container](ghcr.io/huggingface/text-generation-inference:1.1.1). Docker is an open-source platform that provides a streamlined solution for automating the deployment, scaling, and management of applications through containers. These containers encompass all the essential dependencies required for seamless execution, including libraries, binaries, and configuration files. To install Docker, please refer to the [installation guide](https://docs.docker.com/get-docker/).\n\nBefore proceeding, it is important to verify that your machine has the appropriate hardware to initiate the server. TGI optimized models are compatible with NVIDIA A100, A10G, and T4 GPUs. While other GPU hardware may still provide performance advantages, certain operations such as flash attention and paged attention will ', 'utilizing the [official Docker container](ghcr.io/huggingface/text-generation-inference:1.1.1). Docker is an open-source platform that provides a streamlined solution for automating the deployment, scaling, and management of applications through containers. These containers encompass all the essential dependencies required for seamless execution, including libraries, binaries, and configuration files. To install Docker, please refer to the [installation guide](https://docs.docker.com/get-docker/).\n\nBefore proceeding, it is important to verify that your machine has the appropriate hardware to initiate the server. TGI optimized models are compatible with NVIDIA A100, A10G, and T4 GPUs. While other GPU hardware may still provide performance advantages, certain operations such as flash attention and paged attention will not be executed. If you intend to run the container on a machine lacking GPUs or CUDA support, you can eliminate the `--gpus all` flag and include `--disable-custom-kernels`. However, please note that the CPU is not the intended platform for the server, and this choice significantly impacts performance.\n\n#### Installing the NVIDIA Container Toolkit\n\nTo begin, the installation of the NVIDIA container toolkit is necessary. This toolkit is essential for running GPU-accelerated containers. Execute the following command to acquire all the requisite packages [ref the code]:\n\n```sh\ncurl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\\n  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\\n    sed \'s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g\' | \\\n    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list \\\n  && \\\n    sudo apt-get update\n```\n\nInstall the NVIDIA Container toolkit by running the following command.\n\n```\nsudo apt-get install -y nvidia-container-toolkit\n```\n\n#### Running the TGI server.\n\nAfter you installed the NVIDIA Container toolkit, you can run the following Docker command to to start a TGI server on your local machine on port `8000`. This will load a [falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) model on the TGI server.\n\n```\nmodel=tiiuae/falcon-7b-instruct\nvolume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run\ndocker run --gpus all --shm-size 1g -p 8000:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:1.1.1 --model-id $model\n```\n\nAfter the TGI server is deployed, run the following script to verify that it is working correctly:\n\n```\nimport requests\nheaders = {\n    "Content-Type": "application/json",\n}\ndata = {\n    \'inputs\': \'What is Deep Learning?\',\n    \'parameters\': {\n      ', 'you can run the following Docker command to to start a TGI server on your local machine on port `8000`. This will load a [falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) model on the TGI server.\n\n```\nmodel=tiiuae/falcon-7b-instruct\nvolume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run\ndocker run --gpus all --shm-size 1g -p 8000:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:1.1.1 --model-id $model\n```\n\nAfter the TGI server is deployed, run the following script to verify that it is working correctly:\n\n```\nimport requests\nheaders = {\n    "Content-Type": "application/json",\n}\ndata = {\n    \'inputs\': \'What is Deep Learning?\',\n    \'parameters\': {\n        \'max_new_tokens\': 20,\n    },\n}\nresponse = requests.post(\'http://127.0.0.1:8000/generate\', headers=headers, json=data)\nprint(response.json())\n# {\'generated_text\': \'\\nDeep learning is a branch of machine learning that uses artificial neural networks to learn and make decisions.\'}\n```\n\n## Update the config.yaml to add a new embeddings endpoint\n\nAfter you started the server, update the MLflow AI Gateway configuration file [config.yaml](config.yaml) and add the server as a new endpoint:\n\n```\nendpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: "huggingface-text-generation-inference"\n      name: llm\n      config:\n        hf_server_url: http://127.0.0.1:8000/generate\n```\n\n## Starting the MLflow AI Gateway\n\nAfter the configuration file is created, you can start the MLflow AI Gateway by running the following command:\n\n```\nmlflow gateway start --config-path examples/gateway/huggingface/config.yaml --port 7000\n```\n\n## Querying the endpoint\n\nSee the [example script](example.py) within this directory to see how to query the `falcon-7b-instruct` model that is served.\n\n## Setting the parameters of TGI\n\nWhen you make a request to the MLflow Depoyments server, the information you provide in the request body will be sent to TGI. This gives you more control over the output you receive from TGI. However, it\'s important to note that you cannot turn off `details` and `decoder_input_details`, as they are necessary for TGI endpoints to work correctly.\n', "## Example endpoint configuration for Mistral\n\nTo see an example of specifying both the completions and the embeddings endpoints for Mistral, see [the configuration](config.yaml) YAML file.\n\nThis configuration file specifies two endpoints: 'completions' and 'embeddings', both using Mistral's models 'mistral-tiny' and 'mistral-embed', respectively.\n\n## Setting a Mistral API Key\n\nThis example requires a [Mistral API key](https://docs.mistral.ai/):\n\n```sh\nexport MISTRAL_API_KEY=...\n```\n", '# Guide to using an MLflow served model with MLflow Deployments\n\nIn order to utilize MLflow Deployments with MLflow model serving, a few steps must be taken\nin addition to those for configuring access to SaaS models (such as Anthropic and OpenAI). The first and most obvious\nstep that must be taken prior to interfacing with an MLflow served model is that a model needs to be logged to the\nMLflow tracking server.\n\nAn important consideration for deciding whether to interface MLflow Deployments with a specific model is to evaluate the PyFunc interface that the model will\nreturn after being called for inference. Due to the fact that the MLflow AI Gateway defines a specific response signature, expectations for each endpoint type\'s payload contents\nmust be met in order for a endpoint to be valid.\n\nFor example, an embeddings endpoint (llm/v1/embeddings endpoint type) is designed to return embeddings data as a collection (a list) of floats that correspond to each of the\ninput strings that are sent for embeddings inference to a service. The expectation that the embeddings endpoint definition has is that the data is in a particular format. Specifically one that\nis capable of having the embeddings data extractable from a service response. Therefore, an MLflow model that returns data in the format below is perfectly valid.\n\n```json\n{\n  "predictions": [\n    [0.0, 0.1],\n    [1.0, 0.0]\n  ]\n}\n```\n\nHowever, a return value from a serving endpoint via a custom PyFunc of the form below will not work.\n\n```json\n{\n  "predictions": [\n    {\n      "embedding": [0.0, 0.1]\n    },\n    {\n      "embedding": [1.0, 0.0]\n    }\n  ]\n}\n```\n\nIt is important to note that the MLflow AI Gateway does not perform validation on a configured endpoint until the point of querying. Creating a endpoint that interfaces with the\nMLflow model server that is returning a payload that is incompatible with the configured endpoint type definition will raise 502 exceptions only when queried.\n\n> **NOTE:** It is important to validate the output response of a model served by MLflow to ensure compatibility with ', '    "embedding": [0.0, 0.1]\n    },\n    {\n      "embedding": [1.0, 0.0]\n    }\n  ]\n}\n```\n\nIt is important to note that the MLflow AI Gateway does not perform validation on a configured endpoint until the point of querying. Creating a endpoint that interfaces with the\nMLflow model server that is returning a payload that is incompatible with the configured endpoint type definition will raise 502 exceptions only when queried.\n\n> **NOTE:** It is important to validate the output response of a model served by MLflow to ensure compatibility with the MLflow Deployments endpoint definitions. Not all model outputs are compatible with given endpoint types.\n\n## Creating and logging an embeddings model\n\nTo start, we need a model that is capable of generating embeddings. For this example, we\'ll use\nthe `sentence_transformers` library and the corresponding MLflow flavor.\n\n```python\nfrom sentence_transformers import SentenceTransformer\nimport mlflow\n\n\nmodel = SentenceTransformer(model_name_or_path="all-MiniLM-L6-v2")\nartifact_path = "embeddings_model"\n\nwith mlflow.start_run():\n    model_info = mlflow.sentence_transformers.log_model(\n        model,\n        name=artifact_path,\n    )\n```\n\n## Generate the cli command for starting a local MLflow Model Serving endpoint for this embeddings model\n\n```python\nprint(f"mlflow models serve -m {model_info.model_uri} -h 127.0.0.1 -p 9020 --no-conda")\n```\n\nCopy the output from the print statement to the clipboard.\n\n## Starting the model server for the embeddings model\n\nWith the printed string from running the above command copied to the clipboard, open a new terminal\nand paste the string. Leave the terminal window open and running.\n\n```commandline\nmlflow models serve -m file:///Users/me/demos/mlruns/0/2bfcdcb66eaf4c88abe8e0c7bcab639e/artifacts/embeddings_model -h 127.0.0.1 -p 9020 --no-conda\n```\n\n## Update the config.yaml to add a new embeddings endpoint\n\nAfter assigning a valid port and ensuring that the model server starts correctly:\n\n```commandline\n2023/08/08 17:36:44 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor \'python_function\'\n2023/08/08 17:36:44 INFO mlflow.pyfunc.backend: === Running command \'exec uvicorn --host 127.0.0.1 --port 9020 --workers 1 mlflow.pyfunc.scoring_server.app:app\'\nINFO:     Started server process [6992]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:9020\n```\n\nThe scoring server is ready to receive traffic.\n\nUpdate the MLflow AI Gateway configuration file (config.yaml) with the new endpoint:\n\n```yaml\nendpoints:\n  ', 'file:///Users/me/demos/mlruns/0/2bfcdcb66eaf4c88abe8e0c7bcab639e/artifacts/embeddings_model -h 127.0.0.1 -p 9020 --no-conda\n```\n\n## Update the config.yaml to add a new embeddings endpoint\n\nAfter assigning a valid port and ensuring that the model server starts correctly:\n\n```commandline\n2023/08/08 17:36:44 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor \'python_function\'\n2023/08/08 17:36:44 INFO mlflow.pyfunc.backend: === Running command \'exec uvicorn --host 127.0.0.1 --port 9020 --workers 1 mlflow.pyfunc.scoring_server.app:app\'\nINFO:     Started server process [6992]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:9020\n```\n\nThe scoring server is ready to receive traffic.\n\nUpdate the MLflow AI Gateway configuration file (config.yaml) with the new endpoint:\n\n```yaml\nendpoints:\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: mlflow-model-serving\n      name: sentence-transformer\n      config:\n        model_server_url: http://127.0.0.1:9020\n```\n\nThe key component here is the `model_server_url`. For serving an MLflow LLM, this url must match to the service that you are specifying for the\nModel Serving server.\n\n> **NOTE:** The MLflow Model Server does not have to be running in order to update the configuration file or to start the MLflow AI Gateway. In order to respond to submitted queries, it is required to be running.\n\n## Creating and logging a fill mask model\n\nTo support an additional endpoint for generating a mask fill response from masked input text, we need to log an appropriate model.\nFor this tutorial example, we\'ll use a `transformers` `Pipeline` wrapping a `BertForMaskedLM` torch model and will log this pipeline using the MLflow `transformers` flavor.\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nimport mlflow\n\n\nlm_architecture = "bert-base-cased"\nartifact_path = "mask_fill_model"\n\ntokenizer = AutoTokenizer.from_pretrained(lm_architecture)\nmodel = AutoModelForMaskedLM.from_pretrained(lm_architecture)\n\ncomponents = {"model": model, "tokenizer": tokenizer}\n\nwith mlflow.start_run():\n    model_info = mlflow.transformers.log_model(\n        transformers_model=components,\n        name=artifact_path,\n    )\n```\n\n## Generate the cli command for starting a local MLflow Model Serving endpoint for this fill mask model\n\n```python\nprint(f"mlflow models serve -m {model_info.model_uri} -h 127.0.0.1 -p 9010 --no-conda")\n```\n\n## Starting the model server for the fill mask model\n\nUsing the command printed to stdout from above, open a new terminal (do not close ', 'pipeline using the MLflow `transformers` flavor.\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nimport mlflow\n\n\nlm_architecture = "bert-base-cased"\nartifact_path = "mask_fill_model"\n\ntokenizer = AutoTokenizer.from_pretrained(lm_architecture)\nmodel = AutoModelForMaskedLM.from_pretrained(lm_architecture)\n\ncomponents = {"model": model, "tokenizer": tokenizer}\n\nwith mlflow.start_run():\n    model_info = mlflow.transformers.log_model(\n        transformers_model=components,\n        name=artifact_path,\n    )\n```\n\n## Generate the cli command for starting a local MLflow Model Serving endpoint for this fill mask model\n\n```python\nprint(f"mlflow models serve -m {model_info.model_uri} -h 127.0.0.1 -p 9010 --no-conda")\n```\n\n## Starting the model server for the fill mask model\n\nUsing the command printed to stdout from above, open a new terminal (do not close the terminal that is currently running the embeddings model being served!)\nand paste the command.\n\n```commandline\nmlflow models serve -m file:///Users/me/demos/mlruns/0/bc8bdb7fb90c406eb95603a97742cef8/artifacts/mask_fill_model -h 127.0.0.1 -p 9010 --no-conda\n```\n\n## Update the config.yaml to add a new completions endpoint\n\nEnsure that the MLflow serving endpoint starts and is ready for traffic.\n\n```commandline\n2023/08/08 17:39:14 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor \'python_function\'\n2023/08/08 17:39:14 INFO mlflow.pyfunc.backend: === Running command \'exec uvicorn --host 127.0.0.1 --port 9010 --workers 1 mlflow.pyfunc.scoring_server.app:app\'\nINFO:     Started server process [6992]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:9010\n```\n\nAdd the entry to the MLflow AI Gateway configuration file. The final file should match [the config file](config.yaml)\n\n## Create a completions model using MPT-7B-instruct (optional, see notes below)\n\n> **NOTE:** If your system does not have a CUDA-compatible GPU and you have not installed torch with the appropriate CUDA libraries, it is not recommended to attempt to run this portion of the example.\n> The inference performance of the MPT-7B-instruct model running on CPU is very slow.\n> It is also not recommended to add this model to an MLflow model serving environment that does not have a sufficiently powerful GPU available.\n\n### Download the MPT-7B instruct model and tokenizer to a local directory cache\n\n```python\nfrom huggingface_hub import snapshot_download\n\nsnapshot_location = snapshot_download(\n    repo_id="mosaicml/mpt-7b-instruct", local_dir="mpt-7b"\n)\n```\n\n### Define the PyFunc model that will be used for the completions endpoint\n\n```python\nimport transformers\nimport mlflow\nimport torch\n\n\nclass MPT(mlflow.pyfunc.PythonModel):\n    def load_context(self, context):\n        """\n    ', 'attempt to run this portion of the example.\n> The inference performance of the MPT-7B-instruct model running on CPU is very slow.\n> It is also not recommended to add this model to an MLflow model serving environment that does not have a sufficiently powerful GPU available.\n\n### Download the MPT-7B instruct model and tokenizer to a local directory cache\n\n```python\nfrom huggingface_hub import snapshot_download\n\nsnapshot_location = snapshot_download(\n    repo_id="mosaicml/mpt-7b-instruct", local_dir="mpt-7b"\n)\n```\n\n### Define the PyFunc model that will be used for the completions endpoint\n\n```python\nimport transformers\nimport mlflow\nimport torch\n\n\nclass MPT(mlflow.pyfunc.PythonModel):\n    def load_context(self, context):\n        """\n        This method initializes the tokenizer and language model\n        using the specified model snapshot directory.\n        """\n        # Initialize tokenizer and language model\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n            context.artifacts["snapshot"], padding_side="left"\n        )\n\n        config = transformers.AutoConfig.from_pretrained(\n            context.artifacts["snapshot"], trust_remote_code=True\n        )\n        # Comment out this configuration setting if not running on a GPU or if triton is not installed.\n        # Note that triton dramatically improves the inference speed performance\n        config.attn_config["attn_impl"] = "triton"\n\n        self.model = transformers.AutoModelForCausalLM.from_pretrained(\n            context.artifacts["snapshot"],\n            config=config,\n            torch_dtype=torch.bfloat16,\n            trust_remote_code=True,\n        )\n\n        # NB: If you do not have a CUDA-capable device or have torch installed with CUDA support\n ', '       config.attn_config["attn_impl"] = "triton"\n\n        self.model = transformers.AutoModelForCausalLM.from_pretrained(\n            context.artifacts["snapshot"],\n            config=config,\n            torch_dtype=torch.bfloat16,\n            trust_remote_code=True,\n        )\n\n        # NB: If you do not have a CUDA-capable device or have torch installed with CUDA support\n        # this setting will not function correctly. Setting device to \'cpu\' is valid, but\n        # the performance will be very slow.\n        self.model.to(device="cuda")\n\n        self.model.eval()\n\n    def _build_prompt(self, instruction):\n        """\n        This method generates the prompt for the model.\n        """\n        INSTRUCTION_KEY = "### Instruction:"\n        RESPONSE_KEY = "### Response:"\n        INTRO_BLURB = (\n            "Below is an instruction that describes a task. "\n            "Write a response that appropriately completes the request."\n        )\n\n        return f"""{INTRO_BLURB}\n        {INSTRUCTION_KEY}\n        {instruction}\n        {RESPONSE_KEY}\n        """\n\n    def predict(self, context, model_input, params=None):\n        """\n        This method generates prediction for the given input.\n        """\n    ', ' "Write a response that appropriately completes the request."\n        )\n\n        return f"""{INTRO_BLURB}\n        {INSTRUCTION_KEY}\n        {instruction}\n        {RESPONSE_KEY}\n        """\n\n    def predict(self, context, model_input, params=None):\n        """\n        This method generates prediction for the given input.\n        """\n        prompt = model_input["prompt"][0]\n        temperature = model_input.get("temperature", [1.0])[0]\n        max_tokens = model_input.get("max_tokens", [100])[0]\n\n        # Build the prompt\n        prompt = self._build_prompt(prompt)\n\n        # Encode the input and generate prediction\n        # NB: Sending the tokenized inputs to the GPU here explicitly will not work if your system does not have CUDA support.\n        # If attempting to run this with only CPU support, change \'cuda\' to \'cpu\'\n        encoded_input = self.tokenizer.encode(prompt, return_tensors="pt").to("cuda")\n        output = self.model.generate(\n            encoded_input,\n            do_sample=True,\n            temperature=temperature,\n            max_new_tokens=max_tokens,\n        )\n\n        # Decode the prediction to text\n        generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n\n        # Removing the prompt from the generated text\n        prompt_length = len(self.tokenizer.encode(prompt, return_tensors="pt")[0])\n        generated_response = ', '        do_sample=True,\n            temperature=temperature,\n            max_new_tokens=max_tokens,\n        )\n\n        # Decode the prediction to text\n        generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n\n        # Removing the prompt from the generated text\n        prompt_length = len(self.tokenizer.encode(prompt, return_tensors="pt")[0])\n        generated_response = self.tokenizer.decode(\n            output[0][prompt_length:], skip_special_tokens=True\n        )\n\n        return {"candidates": [generated_response]}\n```\n\n### Specify the model signature, input example, and log the custom model\n\n```python\nimport pandas as pd\nimport mlflow\nfrom mlflow.models.signature import ModelSignature\nfrom mlflow.types import DataType, Schema, ColSpec\n\n# Define input and output schema\ninput_schema = Schema(\n    [\n        ColSpec(DataType.string, "prompt"),\n        ColSpec(DataType.double, "temperature"),\n        ColSpec(DataType.long, "max_tokens"),\n    ]\n)\noutput_schema = Schema([ColSpec(DataType.string, "candidates")])\nsignature = ModelSignature(inputs=input_schema, outputs=output_schema)\n\n\n# Define input example\ninput_example = pd.DataFrame(\n    {"prompt": ["What is machine learning?"], "temperature": [0.5], "max_tokens": [100]}\n)\n\nwith mlflow.start_run():\n    mlflow.pyfunc.log_model(\n        name="mpt-7b-instruct",\n        python_model=MPT(),\n        artifacts={"snapshot": snapshot_location},\n        pip_requirements=[\n            "torch",\n            "transformers",\n            "accelerate",\n            "einops",\n            "sentencepiece",\n        ],\n        input_example=input_example,\n        signature=signature,\n    )\n```\n\n## Starting the model server ', '       pip_requirements=[\n            "torch",\n            "transformers",\n            "accelerate",\n            "einops",\n            "sentencepiece",\n        ],\n        input_example=input_example,\n        signature=signature,\n    )\n```\n\n## Starting the model server for mpt-7B-instruct (Optional)\n\nDue to the size and complexity of the MPT-7B-instruct model, it is highly advised to only attempt to serve this model in an environment that has:\n\n- A powerful GPU that is capable of holding the model weights in GPU memory\n- triton installed\n\nIn order to initialize the MLflow Model Server for a large model such as MPT-7B, a slightly modified cli command must be used. Most notably, the timeout duration must be increased from the\ndefault of 60 seconds and it is highly recommended to utilize only a single Gunicorn worker (since each worker will load its own copy of the model, there is a distinct possibility of crashing the server environment with an out of memory fault).\n\n```commandline\nmlflow models serve -m file:///Users/me/demos/mlruns/0/92d017e23ca04ffa919a935ed54e9334/artifacts/mpt-7b-instruct -h 127.0.0.1 -p 9030 -t 1200 -w 1 --no-conda\n```\n\n## Update the config.yaml to add the MPT-7B-instruct endpoint (Optional)\n\n> **NOTE** If you are adding this endpoint for the example, you will have to manually edit the config.yaml. If the server that is running the MPT-7B-instruct custom PyFunc model\'s inference does not have GPU support,\n> the performance for inference will take a very long time (CPU inference with this model can take tens of minutes for a single query).\n\n```yaml\nendpoints:\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: mlflow-model-serving\n      name: sentence-transformer\n      config:\n        model_server_url: http://127.0.0.1:9020\n  - name: fillmask\n    endpoint_type: llm/v1/completions\n ', 'you will have to manually edit the config.yaml. If the server that is running the MPT-7B-instruct custom PyFunc model\'s inference does not have GPU support,\n> the performance for inference will take a very long time (CPU inference with this model can take tens of minutes for a single query).\n\n```yaml\nendpoints:\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: mlflow-model-serving\n      name: sentence-transformer\n      config:\n        model_server_url: http://127.0.0.1:9020\n  - name: fillmask\n    endpoint_type: llm/v1/completions\n    model:\n      provider: mlflow-model-serving\n      name: fill-mask\n      config:\n        model_server_url: http://127.0.0.1:9010\n  - name: mpt-instruct\n    endpoint_type: llm/v1/completions\n    model:\n      provider: mlflow-model-serving\n      name: mpt-7b-instruct\n      config:\n        model_server_url: http://127.0.0.1:9030\n```\n\n## Start the MLflow AI Gateway\n\nNow that both endpoints (or all 3, if adding in the optional MPT-7B-instruct model endpoint) are defined within the configuration YAML file and the Model Serving servers are ready to receive queries, we can start the MLflow AI Gateway.\n\n```sh\nmlflow gateway start --config-path examples/gateway/mlflow_serving/config.yaml --port 7000\n```\n\nIf adding the mpt-7b-instruct model, start the MLflow AI Gateway by directing the `--config-path` argument to the location of the `config.yaml` file that you\'ve created with the endpoint\'s addition.\n\n## Query the MLflow AI Gateway\n\nSee the [example script](example.py) within this directory to see how to query these two models that are being served.\n\n### Query the mpt-7B-instruct endpoint (Optional)\n\nIn order to query the mpt-7b-instruct model, the example shown in the script can be modified by adding an additional query call, as shown below:\n\n```python\n# Querying the optional mpt-7b-instruct endpoint\nresponse_mpt = query(\n    endpoint="mpt-instruct",\n    data={\n        "prompt": "What is the purpose of an attention mask in a transformers model?",\n        "temperature": 0.1,\n   ', 'endpoint\'s addition.\n\n## Query the MLflow AI Gateway\n\nSee the [example script](example.py) within this directory to see how to query these two models that are being served.\n\n### Query the mpt-7B-instruct endpoint (Optional)\n\nIn order to query the mpt-7b-instruct model, the example shown in the script can be modified by adding an additional query call, as shown below:\n\n```python\n# Querying the optional mpt-7b-instruct endpoint\nresponse_mpt = query(\n    endpoint="mpt-instruct",\n    data={\n        "prompt": "What is the purpose of an attention mask in a transformers model?",\n        "temperature": 0.1,\n        "max_tokens": 200,\n    },\n)\nprint(f"Fluent API response for mpt-instruct: {response_mpt}")\n```\n', "## Example endpoint configuration for MosaicML\n\nTo see an example of specifying both the completions and the embeddings endpoints for MosaicML, see [the configuration](config.yaml) YAML file.\n\nThis configuration file specifies three endpoints: 'completions', 'embeddings', and 'chat', using MosaicML's models 'mpt-7b-instruct', 'instructor-xl', and 'llama2-70b-chat', respectively.\n\n## Setting a MosaicML API Key\n\nThis example requires a [MosaicML API key](https://docs.mosaicml.com/en/latest/getting_started.html):\n\n```sh\nexport MOSAICML_API_KEY=...\n```\n", "## Example endpoint configuration for OpenAI\n\nTo view an example of OpenAI endpoint configurations, see [the configuration example](config.yaml) YAML file for OpenAI.\n\nThis configuration shows all 3 supported endpoint types: chat, completions, and embeddings.\n\n## Setting the OpenAI API Key\n\nAn OpenAI API key is required for the configuration. If you haven't already, obtain an [OpenAI API key](https://platform.openai.com/account/api-keys).\n\nWith the key, export it to your environment variables. Replace the '...' with your actual API key:\n\n```sh\nexport OPENAI_API_KEY=...\n```\n", "## Example endpoint configuration for PaLM\n\nTo see an example of specifying both the completions and the embeddings endpoints for PaLM, see [the configuration](config.yaml) YAML file.\n\nThis configuration file specifies three endpoints: 'completions', 'embeddings', and 'chat', using PaLM's models 'text-bison-001', 'embedding-gecko-001', and 'chat-bison-001', respectively.\n\n## Setting a PaLM API Key\n\nThis example requires a [PaLM API key](https://developers.generativeai.google/tutorials/setup):\n\n```sh\nexport PALM_API_KEY=...\n```\n", "## Example endpoint configuration for plugin provider\n\nTo see an example of specifying the chat endpoint for a plugin provider,\nsee [the configuration](config.yaml) YAML file.\n\nWe implement our plugin provider package `my_llm` under `./my-llm` folder. It implements the chat method.\n\nThis configuration file specifies one endpoint: 'chat', using the model 'my-model-0.1.2'.\n\n## Setting up the server\n\nFirst, install the provider package `my_llm`:\n\n```sh\npip install -e ./my-llm\n```\n\nThen, start the server:\n\n```sh\nMY_LLM_API_KEY=some-api-key mlflow gateway start --config-path config.yaml --port 7000\n```\n\nTo clean up the installed package after the example, run\n\n```sh\npip uninstall my_llm\n```\n", "## Example endpoint configuration for TogetherAI\n\nTo see an example of specifying both the completions and the embeddings endpoints for TogetherAI, see [the configuration](config.yaml) YAML file.\n\nThis configuration file specifies two endpoints: 'completions' and 'embeddings', both using TogetherAI's provided models 'mistralai/Mixtral-8x7B-v0.1' and 'togethercomputer/m2-bert-80M-8k-retrieval', respectively.\n\n## Setting a Mistral API Key\n\nThis example requires a [TogetherAI API key](https://docs.together.ai/docs/):\n\n```sh\nexport TOGETHERAI_API_KEY=...\n```\n", '# Unity Catalog Integration\n\nThis example demonstrates how to use the Unity Catalog (UC) integration with MLflow AI Gateway.\n\n## Pre-requisites\n\n1. Install the required packages:\n\n```bash\npip install mlflow openai databricks-sdk\n```\n\n2. Create the UC function used in `run.py` by running the following command on Databricks notebook:\n\n```\n%sql\n\nCREATE OR REPLACE FUNCTION\nmy.uc_func.add (\n  x INTEGER COMMENT \'The first number to add.\',\n  y INTEGER COMMENT \'The second number to add.\'\n)\nRETURNS INTEGER\nLANGUAGE SQL\nRETURN x + y\n```\n\nTo define your own function, see https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.html#create-function-sql-and-python.\n\n3. Create a SQL warehouse in Databricks by following the instructions at https://docs.databricks.com/en/compute/sql-warehouse/create.html.\n\n## Running the example script\n\nFirst, run the deployments server:\n\n```bash\n# Required to authenticate with Databricks. See https://docs.databricks.com/en/dev-tools/auth/index.html#supported-authentication-types-by-databricks-tool-or-sdk for other authentication methods.\nexport DATABRICKS_HOST="..."   # e.g. https://my.databricks.com\nexport DATABRICKS_TOKEN="..."\n\n# Required to execute UC functions. See https://docs.databricks.com/en/integrations/compute-details.html#get-connection-details-for-a-databricks-compute-resource for how to get the http path of your warehouse.\n# The last part of the http path is the warehouse ID.\n#\n# /sql/1.0/warehouses/1234567890123456\n#                     ^^^^^^^^^^^^^^^^\nexport DATABRICKS_WAREHOUSE_ID="..."\n\n# Enable Unity Catalog integration\nexport MLFLOW_ENABLE_UC_FUNCTIONS=true\n\nmlflow gateway start --config-path examples/gateway/openai/config.yaml --port 7000\n```\n\nOnce the server starts running, run the example script:\n\n```bash\n# Replace `my.uc_func.add` if your UC function has a different name\npython examples/gateway/uc_functions/run.py  --uc-function-name my.uc_func.add\n```\n', '# Examples for LightGBM Autologging\n\nLightGBM autologging functionalities are demonstrated through two examples. The first example in the `lightgbm_native` folder logs a Booster model trained by `lightgbm.train()`. The second example in the `lightgbm_sklearn` folder shows how autologging works for LightGBM scikit-learn models. The autologging for all LightGBM models is enabled via `mlflow.lightgbm.autolog()`.\n', '# LightGBM Example\n\nThis example trains a LightGBM classifier with the iris dataset and logs hyperparameters, metrics, and trained model.\n\n## Running the code\n\n```\npython train.py --colsample-bytree 0.8 --subsample 0.9\n```\n\nYou can try experimenting with different parameter values like:\n\n```\npython train.py --learning-rate 0.4 --colsample-bytree 0.7 --subsample 0.8\n```\n\nThen you can open the MLflow UI to track the experiments and compare your runs via:\n\n```\nmlflow ui\n```\n\n## Running the code as a project\n\n```\nmlflow run . -P learning_rate=0.2 -P colsample_bytree=0.8 -P subsample=0.9\n```\n', '# XGBoost Scikit-learn Model Example\n\nThis example trains an [`LightGBM.LGBMClassifier`](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html) with the diabetes dataset and logs hyperparameters, metrics, and trained model.\n\nLike the other LightGBM example, we enable autologging for LightGBM scikit-learn models via `mlflow.lightgbm.autolog()`. Saving / loading models also supports LightGBM scikit-learn models.\n\nYou can run this example using the following command:\n\n```shell\npython train.py\n```\n', '# MLflow LlamaIndex Workflow Example\n\nThis example demonstrates how to build and optimize a Retrieval-Augmented Generation (RAG) workflow using [LlamaIndex](https://www.llamaindex.ai/) integrated with [MLflow](https://mlflow.org/docs/latest/llms/llama-index/index.html). The example covers various retrieval strategies such as vector search, BM25, and web search, along with logging, model tracking, and performance evaluation in MLflow.\n\n![Hybrid RAG Concept](static/images/llama_index_workflow_hybrid_rag_concept.png)\n\n![Evaluation Result](static/images/llama_index_workflow_result_chart.png)\n\n## Set Up\n\nThis repository contains a complete workflow definition, a hands-on notebook, and a sample dataset for running experiments. To clone it to your working environment, use the following command:\n\n```shell\ngit clone https://github.com/mlflow/mlflow.git\n```\n\nAfter cloning the repository, set up the virtual environment by running:\n\n```\ncd mlflow/examples/llama_index/workflow\nchmod +x install.sh\n./install.sh\n```\n\nOnce the installation is complete, start Jupyter Notebook within the Poetry environment using:\n\n```\npoetry run jupyter notebook\n```\n', "# MLflow examples for LLM use cases\n\nThis directory includes several examples for tracking, evaluating, and scoring models with LLMs.\n\n## Summarization\n\nThe `summarization/summarization.py` script uses prompt engineering to build two summarization models for news articles with LangChain. It leverages the `mlflow.langchain` flavor to package and log the models to MLflow, `mlflow.evaluate()` to evaluate each model's performance on a small example dataset, and `mlflow.pyfunc.load_model()` to load and score the best packaged model on a new example article.\n\nTo run the example as an MLflow Project, simply execute the following command from this directory:\n\n```\n$ cd summarization && mlflow run .\n```\n\nTo run the example as a Python script, simply execute the following command from this directory:\n\n```\n$ cd summarization && python summarization.py\n```\n\nNote that this example requires MLflow 2.4.0 or greater to run. Additionally, you must have [LangChain](https://python.langchain.com/en/latest/index.html) and the [OpenAI Python client](https://pypi.org/project/openai/) installed in order to run the example. We also recommend installing the [Hugging Face Evaluate library](https://huggingface.co/docs/evaluate/index) to compute [ROUGE metrics](<https://en.wikipedia.org/wiki/ROUGE_(metric)>) for summary quality. Finally, you must specify a valid OpenAI API key in the `OPENAI_API_KEY` environment variable.\n\n## Question answering\n\nThe `question_answering/question_answering.py` script uses prompt engineering to build two models that answer questions about MLflow.\n\nIt leverages the `mlflow.openai` flavor to package and log the models to MLflow, `mlflow.evaluate()` to evaluate each model's performance on some example questions, and `mlflow.pyfunc.load_model()` to load and score the best packaged model on a new example question.\n\nTo run the example as an MLflow Project, simply execute the following command from this directory:\n\n```\n$ cd question_answering && mlflow run .\n```\n\nTo run the example as a Python script, simply execute the following command from this directory:\n\n```\n$ cd question_answering && python question_answering.py\n```\n\nNote that this example requires MLflow 2.4.0 or greater to run. Additionally, you must have the [OpenAI Python client](https://pypi.org/project/openai/), [tiktoken](https://pypi.org/project/tiktoken/), and [tenacity](https://pypi.org/project/tenacity/) installed in order to run the example. Finally, you must specify a valid OpenAI API key in the `OPENAI_API_KEY` environment variable.\n", '# MLflow Artifacts Example\n\nThis directory contains a set of files for demonstrating the MLflow Artifacts Service.\n\n## What does the MLflow Artifacts Service do?\n\nThe MLflow Artifacts Service serves as a proxy between the client and artifact storage (e.g. S3)\nand allows the client to upload, download, and list artifacts via REST API without configuring\na set of credentials required to access resources in the artifact storage (e.g. `AWS_ACCESS_KEY_ID`\nand `AWS_SECRET_ACCESS_KEY` for S3).\n\n## Quick start\n\nFirst, launch the tracking server with the artifacts service via `mlflow server`:\n\n```sh\n# Launch a tracking server with the artifacts service\n$ mlflow server \\\n    --backend-store-uri=mlruns \\\n    --artifacts-destination ./mlartifacts \\\n    --default-artifact-root http://localhost:5000/api/2.0/mlflow-artifacts/artifacts/experiments \\\n    --gunicorn-opts "--log-level debug"\n```\n\nNotes:\n\n- `--artifacts-destination` specifies the base artifact location from which to resolve artifact upload/download/list requests. In this examples, we\'re using a local directory `./mlartifacts`, but it can be changed to a s3 bucket or\n- `--default-artifact-root` points to the `experiments` directory of the artifacts service. Therefore, the default artifact location of a newly-created experiment is set to `./mlartifacts/experiments/<experiment_id>`.\n- `--gunicorn-opts "--log-level debug"` is specified to print out request logs but can be omitted if unnecessary.\n- `--artifacts-only` disables all other endpoints for the tracking server apart from those involved in listing, uploading, and downloading artifacts. This makes the MLflow server a single-purpose proxy for artifact handling only.\n\nThen, run `example.py` that performs upload, download, and list operations for artifacts:\n\n```\n$ MLFLOW_TRACKING_URI=http://localhost:5000 python example.py\n```\n\nAfter running the command above, the server should print out request logs for artifact operations:\n\n```diff\n...\n[2021-11-05 19:13:34 +0900] [92800] [DEBUG] POST /api/2.0/mlflow/runs/create\n[2021-11-05 19:13:34 +0900] [92800] [DEBUG] GET /api/2.0/mlflow/runs/get\n[2021-11-05 19:13:34 +0900] [92802] [DEBUG] PUT /api/2.0/mlflow-artifacts/artifacts/0/a1b2c3d4/artifacts/a.txt\n[2021-11-05 19:13:34 +0900] [92802] [DEBUG] PUT /api/2.0/mlflow-artifacts/artifacts/0/a1b2c3d4/artifacts/dir/b.txt\n[2021-11-05 19:13:34 +0900] [92802] [DEBUG] POST /api/2.0/mlflow/runs/update\n[2021-11-05 19:13:34 +0900] [92802] [DEBUG] GET /api/2.0/mlflow-artifacts/artifacts\n...\n```\n\nThe contents of the `mlartifacts` directory should look like this:\n\n```sh\n$ tree mlartifacts\nmlartifacts\nâ””â”€â”€ experiments\n    â””â”€â”€ 0  # experiment ID\n        â””â”€â”€ a1b2c3d4  # run ID\n            â””â”€â”€ artifacts\n                â”œâ”€â”€ a.txt\n      ', '/api/2.0/mlflow/runs/create\n[2021-11-05 19:13:34 +0900] [92800] [DEBUG] GET /api/2.0/mlflow/runs/get\n[2021-11-05 19:13:34 +0900] [92802] [DEBUG] PUT /api/2.0/mlflow-artifacts/artifacts/0/a1b2c3d4/artifacts/a.txt\n[2021-11-05 19:13:34 +0900] [92802] [DEBUG] PUT /api/2.0/mlflow-artifacts/artifacts/0/a1b2c3d4/artifacts/dir/b.txt\n[2021-11-05 19:13:34 +0900] [92802] [DEBUG] POST /api/2.0/mlflow/runs/update\n[2021-11-05 19:13:34 +0900] [92802] [DEBUG] GET /api/2.0/mlflow-artifacts/artifacts\n...\n```\n\nThe contents of the `mlartifacts` directory should look like this:\n\n```sh\n$ tree mlartifacts\nmlartifacts\nâ””â”€â”€ experiments\n    â””â”€â”€ 0  # experiment ID\n        â””â”€â”€ a1b2c3d4  # run ID\n            â””â”€â”€ artifacts\n                â”œâ”€â”€ a.txt\n                â””â”€â”€ dir\n                    â””â”€â”€ b.txt\n\n5 directories, 2 files\n```\n\nTo delete the logged artifacts, run the following command:\n\n```bash\nmlflow gc --backend-store-uri=mlruns --run-ids <run_id>\n```\n\n### Clean up\n\n```sh\n# Remove experiment and run data\n$ rm -rf mlruns\n\n# Remove artifacts\n$ rm -rf mlartifacts\n```\n\n## Advanced example using `docker-compose`\n\n[`docker-compose.yml`](./docker-compose.yml) provides a more advanced setup than the quick-start example above:\n\n- Tracking service uses PostgreSQL as a backend store.\n- Artifact service uses MinIO as a artifact store.\n- Tracking and artifacts services are running on different servers.\n\n```sh\n# Build services\n$ docker-compose build\n\n# Launch tracking and artifacts servers in the background\n$ docker-compose up -d\n\n# Run `example.py` in the client container\n$ docker-compose run -v ${PWD}/example.py:/app/example.py client python example.py\n```\n\nYou can view the logged artifacts on MinIO Console served at http://localhost:9001. The login username and password are `user` and `password`.\n\n### Clean up\n\n```sh\n# Remove containers, networks, volumes, and images\n$ docker-compose down --rmi all --volumes --remove-orphans\n```\n\n### Development\n\n```sh\n# Build services using the dev version of mlflow\n$ ./build.sh\n$ docker-compose run -v ${PWD}/example.py:/app/example.py client python example.py\n```\n', '# OpenAI Autologging Examples\n\n## Using OpenAI client\n\nThe recommended way of using `openai` is to instantiate a client\nusing `openai.OpenAI()`. You can run the following example to use\nautologging using such client.\n\nBefore running these examples, ensure that you have the following additional libraries installed:\n\n```shell\npip install tenacity tiktoken \'openai>=1.17\'\n```\n\nYou can run the example via your command prompt as follows:\n\n```shell\npython examples/openai/autologging/instantiated_client.py --api-key="your-api-key"\n```\n\n## Using module-level client\n\n`openai` exposes a module client instance that can be used to make requests.\nYou can run the following example to use autologging with the module client.\n\n```shell\nexport OPENAI_API_KEY="your-api-key"\npython examples/openai/autologging/module_client.py\n```\n', '# Pyfunc model example\n\nThis example demonstrates the use of a pyfunc model with custom inference logic.\nMore specifically:\n\n- train a simple classification model\n- create a _pyfunc_ model that encapsulates the classification model with an attached module for custom inference logic\n\n## Structure of this example\n\nThis examples contains a `train.py` file that trains a scikit-learn model with iris dataset and uses MLflow Tracking APIs to log the model. The nested **mlflow run** delivers the packaging of `pyfunc` model and `custom_code` module is attached\nto act as a custom inference logic layer in inference time.\n\n```\nâ”œâ”€â”€ train.py\nâ”œâ”€â”€ infer_model_code_path.py\nâ””â”€â”€ custom_code.py\n```\n\n## Running this example\n\n1. Train and log the model\n\n```\n$ python train.py\n```\n\nor train and log the model using inferred code paths\n\n```\n$ python infer_model_code_paths.py\n```\n\n2. Serve the pyfunc model\n\n```bash\n# Replace <pyfunc_run_id> with the run ID obtained in the previous step\n$ mlflow models serve -m "runs:/<pyfunc_run_id>/model" -p 5001\n```\n\n3. Send a request\n\n```\n$ curl http://127.0.0.1:5001/invocations -H \'Content-Type: application/json\' -d \'{\n  "dataframe_records": [[1, 1, 1, 1]]\n}\'\n```\n\nThe response should look like this:\n\n```\n[0]\n```\n', '# PySpark ML Autologging Examples\n\nThis directory contains examples for demonstrating how PySpark ML autologging works.\n\n| File                     | Description                        |\n| :----------------------- | :--------------------------------- |\n| `logistic_regression.py` | Train a `LogisticRegression` model |\n| `one_vs_rest.py`         | Train a `OneVsRest` model          |\n', '# PySpark ML connect Examples\n\nThis directory contains examples for demonstrating how to log PySpark ML connect model.\n\n| File          | Description                                           |\n| :------------ | :---------------------------------------------------- |\n| `pipeline.py` | Use mlflow to Log a PySpark ML connect pipeline model |\n', "## Ax Hyperparameter Optimization Example\n\nIn this example, we train a Pytorch Lightning model to classify Iris flower classification dataset.\nA parent run will be created during the training process,which would dump the baseline model and relevant parameters,metrics and model along with its summary,subsequently followed by a set of nested child runs, which will dump the trial results.\nThe best parameters would be dumped into the parent run once the experiments are completed.\n\n### Running the code\n\nTo run the example via MLflow, navigate to the `mlflow/examples/pytorch/AxHyperOptimizationPTL` directory and run the command\n\n```\nmlflow run .\n```\n\nThis will run `AxHyperOptimizationPTL.py` with the default set of parameters such as `max_epochs=3` and `total_trials=3`. You can see the default value in the `MLproject` file.\n\nIn order to run the file with custom parameters, run the command\n\n```\nmlflow run . -P max_epochs=X -P total_trials=Y\n```\n\nwhere `X` is your desired value for `max_epochs` and `Y` is your desired value for `total_trials`.\n\nIf you have the required modules for the file and would like to skip the creation of a conda environment, add the argument `--env-manager=local`.\n\n```\nmlflow run . --env-manager=local\n```\n\n### Viewing results in the MLflow UI\n\nOnce the code is finished executing, you can view the run's metrics, parameters, and details by running the command\n\n```\nmlflow ui\n```\n\nand navigating to [http://localhost:5000](http://localhost:5000).\n\nFor more details on MLflow tracking, see [the docs](https://www.mlflow.org/docs/latest/tracking.html#mlflow-tracking).\n\n### Passing custom training parameters\n\nThe parameters can be overridden via the command line:\n\n1. max_epochs - Number of epochs to train model. Training can be interrupted early via Ctrl+C\n2. total_trials - Number of experimental trials\n\nFor example:\n\n```\nmlflow run . -P max_epochs=3 -P total_trials=3\n```\n\nOr to run the training script directly with custom parameters:\n\n```\npython ax_hpo_iris.py --max_epochs 3 --total_trials 3\n```\n\nBy running the above mentioned script, the hyperparameters are logged into MLFLow as nested runs.\n\n![Ax HPO Runs](screenshots/ax_hpo.png)\n\nThe child run contains the details of the hyperparameters used during that particular trial.\n\n![Trial Run](screenshots/trial_run.png)\n\nAnd the parent run contains the details of the optimum parameters derived by running n trials.\n\n![Parent Run](screenshots/parent_run.png)\n\n## Logging to a custom tracking server\n\nTo configure MLflow to log to a custom (non-default) tracking location, set the `MLFLOW_TRACKING_URI` environment variable, e.g. via `export MLFLOW_TRACKING_URI=http://localhost:5000`. For more details, see [the docs](https://mlflow.org/docs/latest/tracking.html#where-runs-are-recorded).\n", '## BERT news classification example\n\nIn this example, we train a Pytorch Lightning model to classify news articles into "World", "Sports", "Business" and "Sci/Tech" categories. The code, adapted from this [repository](https://github.com/ricardorei/lightning-text-classification/blob/master/classifier.py), is almost entirely dedicated to model training, with the addition of a single `mlflow.pytorch.autolog()` call to enable automatic logging of params, metrics, and models.\n\n### Running the code\n\nTo run the example via MLflow, navigate to the `mlflow/examples/pytorch/BertNewsClassification` directory and run the command\n\n```bash\nmlflow run .\n```\n\nThis will run `bert_classification.py` with the default set of parameters such as `--max_epochs=5`. You can see the default value in the `MLproject` file.\n\nIn order to run the file with custom parameters, run the command\n\n```bash\nmlflow run . -P max_epochs=X\n```\n\nwhere `X` is your desired value for `max_epochs`.\n\nIf you have the required modules for the file and would like to skip the creation of a conda environment, add the argument `--env-manager=local`.\n\n```bash\nmlflow run . --env-manager=local\n```\n\n### Viewing results in the MLflow UI\n\nOnce the code is finished executing, you can view the run\'s metrics, parameters, and details by running the command\n\n```bash\nmlflow ui\n```\n\nand navigating to [http://localhost:5000](http://localhost:5000).\n\nFor more details on MLflow tracking, see [the docs](https://www.mlflow.org/docs/latest/tracking.html#mlflow-tracking).\n\n### Passing custom training parameters\n\nThe parameters can be overridden via the command line:\n\n1. max_epochs - Number of epochs to train model. Training can be interrupted early via Ctrl+C\n2. devices - Number of GPUs.\n3. strategy - [strategy](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#trainer-class-api) (e.g. "ddp" for the Distributed Data Parallel backend) to use for training. By default, no strategy is used.\n4. accelerator - [accelerator](https://lightning.ai/docs/pytorch/stable/extensions/accelerator.html) (e.g. "gpu" - for running in GPU environment. Set to "cpu" by default)\n5. batch_size - Input batch size for training\n6. num_workers - Number of worker threads to load training data\n7. lr - Learning rate\n\nFor example:\n\n```bash\nmlflow run . -P max_epochs=5 -P devices=1 -P batch_size=32 -P num_workers=2 -P learning_rate=0.01 -P strategy="ddp" -P accelerator=gpu\n```\n\nOr to run the training script directly with custom parameters:\n\n```bash\npython bert_classification.py \\\n    --trainer.max_epochs 5 \\\n    --trainer.devices 1 \\\n    --trainer.strategy "ddp" \\\n    --trainer.accelerator "gpu" \\\n    --data.batch_size 64 \\\n    --data.num_workers 3 \\\n    --data.num_samples 2000 \\\n    --model.lr 0.001 \\\n    --data.dataset "20newsgroups"\n```\n\n## ', 'Input batch size for training\n6. num_workers - Number of worker threads to load training data\n7. lr - Learning rate\n\nFor example:\n\n```bash\nmlflow run . -P max_epochs=5 -P devices=1 -P batch_size=32 -P num_workers=2 -P learning_rate=0.01 -P strategy="ddp" -P accelerator=gpu\n```\n\nOr to run the training script directly with custom parameters:\n\n```bash\npython bert_classification.py \\\n    --trainer.max_epochs 5 \\\n    --trainer.devices 1 \\\n    --trainer.strategy "ddp" \\\n    --trainer.accelerator "gpu" \\\n    --data.batch_size 64 \\\n    --data.num_workers 3 \\\n    --data.num_samples 2000 \\\n    --model.lr 0.001 \\\n    --data.dataset "20newsgroups"\n```\n\n## Logging to a custom tracking server\n\nTo configure MLflow to log to a custom (non-default) tracking location, set the MLFLOW_TRACKING_URI environment variable, e.g. via export MLFLOW_TRACKING_URI=http://localhost:5000/. For more details, see [the docs](https://mlflow.org/docs/latest/tracking.html#where-runs-are-recorded).\n', "## Using Captum and MLflow to interpret Pytorch models\n\nIn this example, we will demonstrate the basic features of the [Captum](https://captum.ai/) interpretability,and logging those features using mlflow library through an example model trained on the Titanic survival data.\nWe will first train a deep neural network on the data using PyTorch and use Captum to understand which of the features were most important and how the network reached its prediction.\n\nyou can get more details about used attributions methods used in this example\n\n1. [Titanic_Basic_Interpret](https://captum.ai/tutorials/Titanic_Basic_Interpret)\n2. [integrated-gradients](https://captum.ai/docs/algorithms#primary-attribution)\n3. [layer-attributions](https://captum.ai/docs/algorithms#layer-attribution)\n\n### Running the code\n\nTo run the example via MLflow, navigate to the `mlflow/examples/pytorch/CaptumExample` directory and run the command\n\n```\nmlflow run .\n```\n\nThis will run `Titanic_Captum_Interpret.py` with default parameter values, e.g. `--max_epochs=100` and `--use_pretrained_model False`. You can see the full set of parameters in the `MLproject` file within this directory.\n\nIn order to run the file with custom parameters, run the command\n\n```\nmlflow run . -P max_epochs=X\n```\n\nwhere `X` is your desired value for `max_epochs`.\n\nIf you have the required modules for the file and would like to skip the creation of a conda environment, add the argument `--env-manager=local`.\n\n```\nmlflow run . --env-manager=local\n```\n\n### Viewing results in the MLflow UI\n\nOnce the code is finished executing, you can view the run's metrics, parameters, and details by running the command\n\n```\nmlflow ui\n```\n\nand navigating to [http://localhost:5000](http://localhost:5000).\n\nFor more details on MLflow tracking, see [the docs](https://www.mlflow.org/docs/latest/tracking.html#mlflow-tracking).\n\n### Passing custom training parameters\n\nThe parameters can be overridden via the command line:\n\n1. max_epochs - Number of epochs to train model. Training can be interrupted early via Ctrl+C\n2. lr - Learning rate\n3. use_pretrained_model - If want to use pretrained model\n\nFor example:\n\n```\nmlflow run . -P max_epochs=5 -P learning_rate=0.01 -P use_pretrained_model=True\n```\n\nOr to run the training script directly with custom parameters:\n\n```sh\npython Titanic_Captum_Interpret.py \\\n    --max_epochs 50 \\\n    --lr 0.1\n```\n\n## Logging to a custom tracking server\n\nTo configure MLflow to log to a custom (non-default) tracking location, set the MLFLOW_TRACKING_URI environment variable, e.g. via export MLFLOW_TRACKING_URI=http://localhost:5000/. For more details, see [the docs](https://mlflow.org/docs/latest/tracking.html#where-runs-are-recorded).\n", '## Iterative Pruning\n\nPruning is the process of compressing a neural network that involves removing weights from a trained model.\nPruning techniques include removing the neurons within a specific layer, or setting the weights of connections that are already near zero to zero. This script applies the latter technique.\nPruning a model reduces its size, at the cost of worsened model accuracy.\n\nFor more information check - [Pytorch Pruning Tutorial](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html)\n\nIn this example, we train a model to classify MNIST handwritten digit recognition dataset, and then apply iterative pruning to compress the model. The initial model ("base model") along with its parameters, metrics and summary are stored in mlflow.\nSubsequently, the base model is pruned iteratively by using the custom\ninputs provided from the cli. Ax is a platform for optimizing any kind of experiment, including machine learning experiments,\nA/B tests, and simulations. [Ax](https://ax.dev/docs/why-ax.html) can optimize discrete configurations using multi-armed bandit optimization,\nand continuous (e.g., integer or floating point)-valued configurations using Bayesian optimization.\n\nThe objective function of the experiment trials is "test_accuracy" based on which the model is evaluated at each trial and the best set of parameters are derived.\nAXClient is used to provide the initial pruning percentage as well as decides the number\nof trails to be run. The summary of the pruned model is captured in a separate file and stored as an artifact in MLflow.\n\n### Running the code to Iteratively Prune the Trained Model\n\nRun the command\n\n`python iterative_prune_mnist.py --max_epochs 10 --total_trials 3`\n\nOnce the code is finished executing, you can view the run\'s metrics, parameters, and details by running the command\n\n```\nmlflow ui\n```\n\nand navigating to [http://localhost:5000](http://localhost:5000).\n\nIn the MLflow UI, the Base Model is stored as the Parent Run and the runs for each iterations of the pruning is logged as nested child runs, as shown in the\nsnippets below:\n\n![prune_ankan](https://user-images.githubusercontent.com/51693147/100785435-a66d6e80-3436-11eb-967a-c96b23625d1c.JPG)\n\nWe can compare the child runs in the UI, as given below:\n\n![prune_capture](https://user-images.githubusercontent.com/51693147/100785071-2515dc00-3436-11eb-8e3a-de2d569287e6.JPG)\n\nFor more information on MLflow tracking, click [here](https://www.mlflow.org/docs/latest/tracking.html#mlflow-tracking) to view documentation.\n', '## MNIST example with MLflow\n\nIn this example, we train a Pytorch Lightning model to predict handwritten digits, leveraging early stopping.\nThe code is almost entirely dedicated to model training, with the addition of a single `mlflow.pytorch.autolog()` call to enable automatic logging of params, metrics, and models,\nincluding the best model from early stopping.\n\n### Running the code\n\nTo run the example via MLflow, navigate to the `mlflow/examples/pytorch/MNIST` directory and run the command\n\n```\nmlflow run .\n```\n\nThis will run `mnist_autolog_example.py` with the default set of parameters such as `max_epochs=5`. You can see the default value in the `MLproject` file.\n\nIn order to run the file with custom parameters, run the command\n\n```\nmlflow run . -P max_epochs=X\n```\n\nwhere `X` is your desired value for `max_epochs`.\n\nIf you have the required modules for the file and would like to skip the creation of a conda environment, add the argument `--env-manager=local`.\n\n```\nmlflow run . --env-manager=local\n```\n\n### Viewing results in the MLflow UI\n\nOnce the code is finished executing, you can view the run\'s metrics, parameters, and details by running the command\n\n```\nmlflow ui\n```\n\nand navigating to [http://localhost:5000](http://localhost:5000).\n\nFor more details on MLflow tracking, see [the docs](https://www.mlflow.org/docs/latest/tracking.html#mlflow-tracking).\n\n### Passing custom training parameters\n\nThe parameters can be overridden via the command line:\n\n1. max_epochs - Number of epochs to train model. Training can be interrupted early via Ctrl+C\n2. devices - Number of GPUs.\n3. strategy - [strategy](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#trainer-class-api) (e.g. "ddp" for the Distributed Data Parallel backend) to use for training. By default, no strategy is used.\n4. accelerator - [accelerator](https://lightning.ai/docs/pytorch/stable/extensions/accelerator.html) (e.g. "gpu" - for running in GPU environment. Set to "cpu" by default)\n5. batch_size - Input batch size for training\n6. num_workers - Number of worker threads to load training data\n7. learning_rate - Learning rate\n\nFor example:\n\n```\nmlflow run . -P max_epochs=5 -P devices=1 -P batch_size=32 -P num_workers=2 -P learning_rate=0.01 -P strategy="ddp"\n```\n\nOr to run the training script directly with custom parameters:\n\n```sh\npython mnist_autolog_example.py \\\n    --trainer.max_epochs 5 \\\n    --trainer.devices 1 \\\n    --trainer.strategy "ddp" \\\n    --trainer.accelerator "gpu" \\\n    --data.batch_size 64 \\\n    --data.num_workers 3 \\\n    --model.learning_rate 0.001\n```\n\n## Logging to a custom tracking server\n\nTo configure MLflow to log to a custom (non-default) tracking location, set ', 'for training\n6. num_workers - Number of worker threads to load training data\n7. learning_rate - Learning rate\n\nFor example:\n\n```\nmlflow run . -P max_epochs=5 -P devices=1 -P batch_size=32 -P num_workers=2 -P learning_rate=0.01 -P strategy="ddp"\n```\n\nOr to run the training script directly with custom parameters:\n\n```sh\npython mnist_autolog_example.py \\\n    --trainer.max_epochs 5 \\\n    --trainer.devices 1 \\\n    --trainer.strategy "ddp" \\\n    --trainer.accelerator "gpu" \\\n    --data.batch_size 64 \\\n    --data.num_workers 3 \\\n    --model.learning_rate 0.001\n```\n\n## Logging to a custom tracking server\n\nTo configure MLflow to log to a custom (non-default) tracking location, set the MLFLOW_TRACKING_URI environment variable, e.g. via export MLFLOW_TRACKING_URI=http://localhost:5000/. For more details, see [the docs](https://mlflow.org/docs/latest/tracking.html#where-runs-are-recorded).\n', "## Iris classification example with MLflow\n\nThis example demonstrates training a classification model on the Iris dataset, scripting the model with TorchScript, logging the\nscripted model to MLflow using\n[`mlflow.pytorch.log_model`](https://mlflow.org/docs/latest/python_api/mlflow.pytorch.html#mlflow.pytorch.log_model), and\nloading it back for inference using\n[`mlflow.pytorch.load_model`](https://mlflow.org/docs/latest/python_api/mlflow.pytorch.html#mlflow.pytorch.load_model)\n\n### Running the code\n\nTo run the example via MLflow, navigate to the `mlflow/examples/pytorch/torchscript/IrisClassification` directory and run the command\n\n```\nmlflow run .\n```\n\nThis will run `iris_classification.py` with the default set of parameters such as `--max_epochs=5`. You can see the default value in the `MLproject` file.\n\nIn order to run the file with custom parameters, run the command\n\n```\nmlflow run . -P epochs=X\n```\n\nwhere `X` is your desired value for `epochs`.\n\nIf you have the required modules for the file and would like to skip the creation of a conda environment, add the argument `--env-manager=local`.\n\n```\nmlflow run . --env-manager=local\n```\n\nOnce the code is finished executing, you can view the run's metrics, parameters, and details by running the command\n\n```\nmlflow ui\n```\n\nand navigating to [http://localhost:5000](http://localhost:5000).\n\n## Running against a custom tracking server\n\nTo configure MLflow to log to a custom (non-default) tracking location, set the `MLFLOW_TRACKING_URI` environment variable, e.g. via `export MLFLOW_TRACKING_URI=http://localhost:5000/`. For more details, see [the docs](https://mlflow.org/docs/latest/tracking.html#where-runs-are-recorded)\n", "## MNIST example with MLflow\n\nThis example demonstrates training of MNIST handwritten recognition model and logging it as torch scripted model.\n`mlflow.pytorch.log_model()` is used to log the scripted model to MLflow and `mlflow.pytorch.load_model()` to load it from MLflow\n\n### Code related to MLflow:\n\nThis will log the TorchScripted model into MLflow and load the logged model.\n\n## Setting Tracking URI\n\nMLflow tracking URI can be set using the environment variable `MLFLOW_TRACKING_URI`\n\nExample: `export MLFLOW_TRACKING_URI=http://localhost:5000/`\n\nFor more details - https://mlflow.org/docs/latest/tracking.html#where-runs-are-recorded\n\n### Running the code\n\nTo run the example via MLflow, navigate to the `mlflow/examples/pytorch/torchscript/MNIST` directory and run the command\n\n```\nmlflow run .\n```\n\nThis will run `mnist_torchscript.py` with the default set of parameters such as `--max_epochs=5`. You can see the default value in the `MLproject` file.\n\nIn order to run the file with custom parameters, run the command\n\n```\nmlflow run . -P epochs=X\n```\n\nwhere `X` is your desired value for `epochs`.\n\nIf you have the required modules for the file and would like to skip the creation of a conda environment, add the argument `--env-manager=local`.\n\n```\nmlflow run . --env-manager=local\n```\n\nOnce the code is finished executing, you can view the run's metrics, parameters, and details by running the command\n\n```\nmlflow ui\n```\n\nand navigating to [http://localhost:5000](http://localhost:5000).\n\nFor more information on MLflow tracking, click [here](https://www.mlflow.org/docs/latest/tracking.html#mlflow-tracking) to view documentation.\n", '### Train and Publish Locally With RAPIDS and MLflow\n\n**[RAPIDS](https://rapids.ai/)** is a suite of open source libraries for GPU-accelerated analytics.\n\n**[RAPIDS cuML](https://github.com/rapidsai/cuml)** matches the scikit-learn API, so it can build on MLflow\'s existing support for scikit-learn-like models to support\npersistence and deployment."\n\nThe example workflows below train RAPIDs regression models to predict airline flight delays, using\nMLflow to log models and deploy them as local REST API endpoints for real-time inference. You can run them:\n\n- On a GPU-enabled instance for free in Colab. If following this approach, we recommend using the "Jupyter notebook workflow" below\n  and following the setup steps in [this Colab notebook](https://colab.research.google.com/drive/1rY7Ln6rEE1pOlfSHCYOVaqt8OvDO35J0#forceEdit=true&offline=true&sandboxMode=true) to configure your\n  environment.\n\n- On your own machine with an NVIDIA GPU and CUDA installed. See the [RAPIDS getting-started guide](https://rapids.ai/start.html)\n  for more details on necessary prerequisites for running the examples on your own machine.\n\n#### Jupyter Notebook Workflow\n\n[Jupyter Notebook](notebooks/rapids_mlflow.ipynb)\n\n#### CLI Based Workflow\n\n1. Create data\n   1. `cd examples/rapids/mlflow_project`\n      ```shell script\n      # Create iris.csv\n      python -c "from sklearn.datasets import load_iris; d = load_iris(as_frame=True); d.frame.to_csv(\'iris.csv\', index=False)"\n      ```\n1. Set MLflow tracking uri\n   1. ```shell script\n       export MLFLOW_TRACKING_URI=sqlite:////tmp/mlflow-db.sqlite\n      ```\n1. Train the model using a single run.\n   1. ```shell script\n      # Launch the job\n      mlflow run . -e simple\\\n               --experiment-name RAPIDS-CLI \\\n               -P max_depth=10 -P max_features=0.75 -P n_estimators=500 \\\n               -P conda-env=$PWD/envs/conda.yaml \\\n               -P fpath=iris.csv\n      ```\n1. Train the model with Hyperopt\n\n   1. ```shell script\n      # Launch the job\n      mlflow run . -e hyperopt \\\n   ', '     --experiment-name RAPIDS-CLI \\\n               -P max_depth=10 -P max_features=0.75 -P n_estimators=500 \\\n               -P conda-env=$PWD/envs/conda.yaml \\\n               -P fpath=iris.csv\n      ```\n1. Train the model with Hyperopt\n\n   1. ```shell script\n      # Launch the job\n      mlflow run . -e hyperopt \\\n               --experiment-name RAPIDS-CLI \\\n               -P conda-env=$PWD/envs/conda.yaml \\\n               -P fpath=iris.csv\n      ```\n   1. In the output, note: "Created version \'[VERSION]\' of model \'rapids_mlflow\'"\n\n1. Deploy your model\n\n   1. Deploy your model\n      1. `$ mlflow models serve --env-manager=local -m models:/rapids_mlflow_cli/[VERSION] -p 55755`\n\n1. Query the deployed model with test data `src/sample_server_query.sh` example script.\n   1. `bash src/sample_server_query.sh`\n', "# MLflow-Ray-Serve deployment plugin\n\nIn this example, we will first train a model to classify the Iris dataset using `sklearn`. Next, we will deploy our model on Ray Serve and then scale it up, all using the MLflow Ray Serve plugin.\n\nThe plugin supports both a command line interface and a Python API. Below we will use the command line interface. For the full API documentation, see https://www.mlflow.org/docs/latest/cli.html#mlflow-deployments and https://www.mlflow.org/docs/latest/python_api/mlflow.deployments.html.\n\n## Plugin Installation\n\nPlease follow the installation instructions for the Ray Serve deployment plugin: https://github.com/ray-project/mlflow-ray-serve\n\n## Instructions\n\nFirst, navigate to the directory for this example, `mlflow/examples/ray_serve/`.\n\nSecond, run `python train_model.py`. This trains and saves our classifier to the MLflow Model Registry and sets up automatic logging to MLflow. It also prints the mean squared error and the target names, which are species of iris:\n\n```\nMSE: 1.04\nTarget names:  ['setosa' 'versicolor' 'virginica']\n```\n\nNext, set the MLflow Tracking URI environment variable to the location where the Model Registry resides:\n\n`export MLFLOW_TRACKING_URI=sqlite:///mlruns.db`\n\nNow start a Ray cluster with the following command:\n\n`ray start --head`\n\nNext, start a long-running Ray Serve instance on your Ray cluster:\n\n`serve start`\n\nRay Serve is now running and ready to deploy MLflow models. The MLflow Ray Serve plugin features both a Python API as well as a command-line interface. For this example, we'll use the command line interface.\n\nFinally, we can deploy our model by creating an instance using the following command:\n\n`mlflow deployments create -t ray-serve -m models:/RayMLflowIntegration/1 --name iris:v1`\n\nThe `-t` parameter here is the deployment target, which in our case is Ray Serve. The `-m` parameter is the Model URI, which consists of the registered model name and version in the Model Registry.\n\nWe can now run a prediction on our deployed model as follows. The file `input.json` contains a sample input containing the sepal length, sepal width, petal length, petal width of a sample flower. Now we can get the prediction using the following command:\n\n`mlflow deployments predict -t ray-serve --name iris:v1 --input-path input.json`\n\nThis will output `[0]`, `[1]`, or `[2]`, corresponding to the species listed above in the target names.\n\nWe can scale our deployed model up to use several replicas, improving throughput:\n\n`mlflow deployments update -t ray-serve --name iris:v1 --config num_replicas=2`\n\nHere we only used 2 ", 'the registered model name and version in the Model Registry.\n\nWe can now run a prediction on our deployed model as follows. The file `input.json` contains a sample input containing the sepal length, sepal width, petal length, petal width of a sample flower. Now we can get the prediction using the following command:\n\n`mlflow deployments predict -t ray-serve --name iris:v1 --input-path input.json`\n\nThis will output `[0]`, `[1]`, or `[2]`, corresponding to the species listed above in the target names.\n\nWe can scale our deployed model up to use several replicas, improving throughput:\n\n`mlflow deployments update -t ray-serve --name iris:v1 --config num_replicas=2`\n\nHere we only used 2 replicas, but you can use as many as you like, depending on how many CPU cores are available in your Ray cluster.\n\nThe deployed model instance can be deleted as follows:\n\n`mlflow deployments delete -t ray-serve --name iris:v1`\n\nTo tear down the Ray cluster, run the following command:\n\n`ray stop`\n', '### MLflow restore model dependencies examples\n\nThe example "restore_model_dependencies_example.ipynb" in this directory illustrates\nhow you can use the `mlflow.pyfunc.get_model_dependencies` API to get the dependencies from a model URI\nand install them, restoring the exact python environment that was used to build the model.\n\n#### Prerequisites\n\n```\npip install scikit-learn\n```\n\n#### How to run the example\n\nUse jupyter to load the notebook "restore_model_dependencies_example.ipynb" and run the notebook.\n', '# SHAP Examples\n\nExamples demonstrating use of the `mlflow.shap` APIs for model explainability.\n\n| File                                                         | Task                      | Description                                                    |\n| :----------------------------------------------------------- | :------------------------ | :------------------------------------------------------------- |\n| [regression.py](regression.py)                               | Regression                | Log explanations for a LinearRegression model                  |\n| [binary_classification.py](binary_classification.py)         | Binary classification     | Log explanations for a binary RandomForestClassifier model     |\n| [multiclass_classification.py](multiclass_classification.py) | Multiclass classification | Log explanations for a multiclass RandomForestClassifier model |\n\n## Prerequisites\n\nRun the following command to install required packages:\n\n```\npip install mlflow scikit-learn shap matplotlib\n```\n\n## How to run the scripts\n\n```bash\npython <script_name>\n```\n\n## How to view the logged explanations:\n\n- Run `mlflow ui` to launch the MLflow UI.\n- Open http://127.0.0.1:5000 on your browser.\n- Click the latest run in the runs table.\n- Scroll down to the artifact viewer.\n- Open a folder named `model_explanations_shap`.\n', '# Examples for scikit-learn Autologging\n\n| File                                           | Description                                         |\n| :--------------------------------------------- | :-------------------------------------------------- |\n| [linear_regression.py](./linear_regression.py) | Train a [LinearRegression][lr] model                |\n| [pipeline.py](./pipeline.py)                   | Train a [Pipeline][pipe] model                      |\n| [grid_search_cv.py](./grid_search_cv.py)       | Perform a parameter search using [GridSearchCV][gs] |\n\n[lr]: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n[pipe]: https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n[gs]: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n', '# Scikit-learn ElasticNet Diabetes Example\n\nThis example trains an ElasticNet regression model for predicting diabetes progression. The example uses [matplotlib](https://matplotlib.org/), which requires different Python dependencies for Linux and OSX. The [linux](linux) and [osx](osx) subdirectories include appropriate MLflow projects for each respective platform.\n', '# Sktime Example\n\nThis example trains a `Sktime` NaiveForecaster model using the Longley dataset for\nforecasting with exogenous variables. It shows a custom model type implementation\nthat logs the training hyper-parameters, evaluation metrics and the trained model\nas an artifact.\n\n## Running the code\n\nRun the `train.py` module to create a new MLflow experiment and to\ncompute interval forecasts loading the trained model in native `sktime`\nflavor and `pyfunc` flavor:\n\n```\npython train.py\n```\n\nTo view the newly created experiment and logged artifacts open the MLflow UI:\n\n```\nmlflow ui\n```\n\n## Model serving\n\nThis section illustrates an example of serving the `pyfunc` flavor to a local REST\nAPI endpoint and subsequently requesting a prediction from the served model. To serve the model run the command below where you substitute the run id printed during execution of the `train.py` module:\n\n```\nmlflow models serve -m runs:/<run_id>/model --env-manager local --host 127.0.0.1\n\n```\n\nOpen a new terminal and run the `score_model.py` module to request a prediction from the served model (for more details read the [MLflow deployment API reference](https://mlflow.org/docs/latest/models.html#deploy-mlflow-models)):\n\n```\npython score_model.py\n```\n\n## Running the code as a project\n\nYou can also run the code as a project as follows:\n\n```\nmlflow run .\n\n```\n\n## Running unit tests\n\nThe `test_sktime_model_export.py` module includes a number of tests that can be\nexecuted as follows:\n\n```\npytest test_sktime_model_export.py\n\n```\n\nWhile these tests will depend on the specifics of each individual flavor and in particular the design of the model wrapper interface (e.g. `_SktimeModelWrapper`), the above module can provide some orientation\nfor the type of tests that can be useful when creating a new custom model flavor.\n', '### MLflow Spark UDF Examples\n\nThe examples in this directory illustrate how you can use the `mlflow.pyfunc.spark_udf` API for batch inference,\nincluding environment reproducibility capabilities with argument `env_manager="conda"`,\nwhich creates a spark UDF for model inference that executes in an environment containing the exact dependency\nversions used during training.\n\n- Example `spark_udf.py` runs a sklearn model inference via spark UDF\n  using a python environment containing the precise versions of dependencies used during model training.\n\n#### Prerequisites\n\n```\npip install scikit-learn\n```\n\n#### How to run the examples\n\nSimple example:\n\n```\npython spark_udf.py\n```\n\nSpark UDF example with input data of datetime type:\n\n```\npython spark_udf_datetime.py\n```\n\nSpark UDF example with input data of struct and array type:\n\n```\npython structs_and_arrays.py\n```\n\nSpark UDF example using prebuilt model environment:\n\n```\npython spark_udf_with_prebuilt_env.py\n```\n', "# Statsmodels Example\n\nThis example trains a Statsmodels OLS (Ordinary Least Squares) model with synthetically generated data\nand logs hyperparameters, metric (MSE), and trained model.\n\n## Running the code\n\n```\npython train.py --inverse-method qr\n```\n\nThe inverse method is the method used to compute the inverse matrix, and can be either qr or pinv (default).\n'pinv' uses the Moore-Penrose pseudoinverse to solve the least squares problem. 'qr' uses the QR factorization.\nYou can try experimenting with both, as well as omitting the --inverse-method argument.\n\nThen you can open the MLflow UI to track the experiments and compare your runs via:\n\n```\nmlflow ui\n```\n\n## Running the code as a project\n\n```\nmlflow run . -P inverse_method=qr\n\n```\n", '## MLflow automatic Logging with SynapseML\n\n[MLflow automatic logging](https://www.mlflow.org/docs/latest/tracking.html#automatic-logging) allows you to log metrics, parameters, and models without the need for explicit log statements.\nSynapseML supports autologging for every model in the library.\n\nInstall SynapseML library following this [guidance](https://microsoft.github.io/SynapseML/docs/getting_started/installation/)\n\nDefault mlflow [log_model_allowlist file](https://github.com/mlflow/mlflow/blob/master/mlflow/pyspark/ml/log_model_allowlist.txt) already includes some SynapseML models. To enable more models, you could use `mlflow.pyspark.ml.autolog(log_model_allowlist=YOUR_SET_OF_MODELS)` function, or follow the below guidance by specifying a link to the file and update spark configuration.\n\nTo enable autologging with your custom log_model_allowlist file:\n\n1. Put your customized log_model_allowlist file at a place that your code has access to. ([SynapseML official log_model_allowlist file](https://mmlspark.blob.core.windows.net/publicwasb/log_model_allowlist.txt))\n   For example:\n\n- In Synapse `wasb://<containername>@<accountname>.blob.core.windows.net/PATH_TO_YOUR/log_model_allowlist.txt`\n- In Databricks `/dbfs/FileStore/PATH_TO_YOUR/log_model_allowlist.txt`.\n\n2. Set spark configuration `spark.mlflow.pysparkml.autolog.logModelAllowlistFile` to the path of your `log_model_allowlist.txt` file.\n3. Call `mlflow.pyspark.ml.autolog()` before your training code to enable autologging for all supported models.\n\nNote:\n\nIf you want to support autologging of PySpark models not present in the log_model_allowlist file, you can add such models to the file.\n\n## Configuration process in Databricks as an example\n\n1. Install latest MLflow via `%pip install mlflow -u`\n2. Upload your customized `log_model_allowlist.txt` file to dbfs by clicking File/Upload Data button on Databricks UI.\n3. Set Cluster Spark configuration following [this documentation](https://docs.microsoft.com/en-us/azure/databricks/clusters/configure#spark-configuration)\n\n```\nspark.mlflow.pysparkml.autolog.logModelAllowlistFile /dbfs/FileStore/PATH_TO_YOUR/log_model_allowlist.txt\n```\n\n4. Run the following line before your training code executes.\n\n```python\nimport mlflow\n\nmlflow.pyspark.ml.autolog()\n```\n\nYou can customize how autologging works by supplying appropriate [parameters](https://www.mlflow.org/docs/latest/python_api/mlflow.pyspark.ml.html#mlflow.pyspark.ml.autolog).\n\n5. To find your experiment\'s results via the `Experiments` tab of the MLflow UI.\n   <img src="https://mmlspark.blob.core.windows.net/graphics/adb_experiments.png" width="1200" />\n\n## Example for ConditionalKNNModel\n\n```python\nfrom pyspark.ml.linalg import Vectors\nfrom synapse.ml.nn import ConditionalKNN\n\ndf = spark.createDataFrame(\n    [\n        (Vectors.dense(2.0, 2.0, 2.0), "foo", 1),\n        (Vectors.dense(2.0, 2.0, 4.0), "foo", 3),\n        (Vectors.dense(2.0, 2.0, 6.0), "foo", 4),\n        (Vectors.dense(2.0, 2.0, 8.0), "foo", 3),\n        (Vectors.dense(2.0, 2.0, 10.0), "foo", 1),\n        (Vectors.dense(2.0, 2.0, 12.0), "foo", 2),\n        (Vectors.dense(2.0, 2.0, 14.0), "foo", 0),\n        (Vectors.dense(2.0, 2.0, 16.0), "foo", 1),\n        (Vectors.dense(2.0, 2.0, 18.0), "foo", 3),\n  ', '2.0), "foo", 1),\n        (Vectors.dense(2.0, 2.0, 4.0), "foo", 3),\n        (Vectors.dense(2.0, 2.0, 6.0), "foo", 4),\n        (Vectors.dense(2.0, 2.0, 8.0), "foo", 3),\n        (Vectors.dense(2.0, 2.0, 10.0), "foo", 1),\n        (Vectors.dense(2.0, 2.0, 12.0), "foo", 2),\n        (Vectors.dense(2.0, 2.0, 14.0), "foo", 0),\n        (Vectors.dense(2.0, 2.0, 16.0), "foo", 1),\n        (Vectors.dense(2.0, 2.0, 18.0), "foo", 3),\n        (Vectors.dense(2.0, 2.0, 20.0), "foo", 0),\n        (Vectors.dense(2.0, 4.0, 2.0), "foo", 2),\n        (Vectors.dense(2.0, 4.0, 4.0), "foo", 4),\n        (Vectors.dense(2.0, 4.0, 6.0), "foo", 2),\n        (Vectors.dense(2.0, 4.0, 8.0), "foo", 2),\n        (Vectors.dense(2.0, 4.0, 10.0), "foo", 4),\n        (Vectors.dense(2.0, 4.0, 12.0), "foo", 3),\n        (Vectors.dense(2.0, 4.0, 14.0), "foo", 2),\n        (Vectors.dense(2.0, 4.0, 16.0), "foo", 1),\n        (Vectors.dense(2.0, 4.0, 18.0), "foo", 4),\n        (Vectors.dense(2.0, 4.0, 20.0), "foo", 4),\n    ],\n    ["features", "values", "labels"],\n)\n\ncnn = ConditionalKNN().setOutputCol("prediction")\ncnnm = cnn.fit(df)\n\ntest_df = spark.createDataFrame(\n    [\n        (Vectors.dense(2.0, 2.0, 2.0), "foo", 1, [0, 1]),\n        (Vectors.dense(2.0, 2.0, 4.0), "foo", 4, [0, 1]),\n        (Vectors.dense(2.0, 2.0, 6.0), "foo", 2, [0, 1]),\n        (Vectors.dense(2.0, 2.0, 8.0), "foo", 4, [0, 1]),\n        (Vectors.dense(2.0, 2.0, 10.0), "foo", 4, [0, 1]),\n    ],\n    ["features", "values", "labels", "conditioner"],\n)\n\ndisplay(cnnm.transform(test_df))\n```\n\nThis code should log one run with a ConditionalKNNModel artifact and its parameters.\n<img src="https://mmlspark.blob.core.windows.net/graphics/autologgingRunSample.png" width="1200" />\n', '# Examples for XGBoost Autologging\n\nTwo examples are provided to demonstrate XGBoost autologging functionalities. The `xgboost_native` folder contains an example that logs a Booster model trained by `xgboost.train()`. The `xgboost_sklearn` includes another example showing how autologging works for XGBoost scikit-learn models. In fact, there is no difference in turning on autologging for all XGBoost models. That is, `mlflow.xgboost.autolog()` works for all XGBoost models.\n', '# XGBoost Example\n\nThis example trains an XGBoost classifier with the iris dataset and logs hyperparameters, metrics, and trained model.\n\n## Running the code\n\n```\npython train.py --learning-rate 0.2 --colsample-bytree 0.8 --subsample 0.9\n```\n\nYou can try experimenting with different parameter values like:\n\n```\npython train.py --learning-rate 0.4 --colsample-bytree 0.7 --subsample 0.8\n```\n\nThen you can open the MLflow UI to track the experiments and compare your runs via:\n\n```\nmlflow ui\n```\n\n## Running the code as a project\n\n```\nmlflow run . -P learning_rate=0.2 -P colsample_bytree=0.8 -P subsample=0.9\n```\n', '# XGBoost Scikit-learn Model Example\n\nThis example trains an [`XGBoost.XGBRegressor`](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor) with the diabetes dataset and logs hyperparameters, metrics, and trained model.\n\nLike the other XGBoost example, we enable autologging for XGBoost scikit-learn models via `mlflow.xgboost.autolog()`. Saving / loading models also supports XGBoost scikit-learn models.\n\nYou can run this example using the following command:\n\n```\npython train.py\n```\n', '# MLflow Skinny\n\n`mlflow-skinny` a lightweight version of MLflow that is designed to be used in environments where you want to minimize the size of the package.\n\n## Core Files\n\n| File               | Description                                                                     |\n| ------------------ | ------------------------------------------------------------------------------- |\n| `mlflow`           | A symlink that points to the `mlflow` directory in the root of the repository.  |\n| `pyproject.toml`   | The package metadata. Autogenerate by [`dev/pyproject.py`](../dev/pyproject.py) |\n| `README_SKINNY.md` | The package description. Autogenerate by [`dev/skinny.py`](../dev/pyproject.py) |\n\n## Installation\n\n```sh\n# If you have a local clone of the repository\npip install ./libs/skinny\n\n# If you want to install the latest version from GitHub\npip install git+https://github.com/mlflow/mlflow.git#subdirectory=libs/skinny\n```\n', '<!--  Autogenerated by dev/pyproject.py. Do not edit manually.  -->\n\nðŸ“£ This is the `mlflow-skinny` package, a lightweight MLflow package without SQL storage, server, UI, or data science dependencies.\nAdditional dependencies can be installed to leverage the full feature set of MLflow. For example:\n\n- To use the `mlflow.sklearn` component of MLflow Models, install `scikit-learn`, `numpy` and `pandas`.\n- To use SQL-based metadata storage, install `sqlalchemy`, `alembic`, and `sqlparse`.\n- To use serving-based features, install `flask` and `pandas`.\n\n---\n\n<br>\n<br>\n\n<h1 align="center" style="border-bottom: none">\n    <a href="https://mlflow.org/">\n        <img alt="MLflow logo" src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/logo.svg" width="200" />\n    </a>\n</h1>\n<h2 align="center" style="border-bottom: none">Open-Source Platform for Productionizing AI</h2>\n\nMLflow is an open-source developer platform to build AI/LLM applications and models with confidence. Enhance your AI applications with end-to-end **experiment tracking**, **observability**, and **evaluations**, all in one integrated platform.\n\n<div align="center">\n\n[![Python SDK](https://img.shields.io/pypi/v/mlflow)](https://pypi.org/project/mlflow/)\n[![PyPI Downloads](https://img.shields.io/pypi/dm/mlflow)](https://pepy.tech/projects/mlflow)\n[![License](https://img.shields.io/github/license/mlflow/mlflow)](https://github.com/mlflow/mlflow/blob/main/LICENSE)\n<a href="https://twitter.com/intent/follow?screen_name=mlflow" target="_blank">\n<img src="https://img.shields.io/twitter/follow/mlflow?logo=X&color=%20%23f5f5f5"\n      alt="follow on X(Twitter)"></a>\n<a href="https://www.linkedin.com/company/mlflow-org/" target="_blank">\n<img src="https://custom-icon-badges.demolab.com/badge/LinkedIn-0A66C2?logo=linkedin-white&logoColor=fff"\n      alt="follow on LinkedIn"></a>\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/mlflow/mlflow)\n\n</div>\n\n<div align="center">\n   <div>\n      <a href="https://mlflow.org/"><strong>Website</strong></a> Â·\n      <a href="https://mlflow.org/docs/latest/index.html"><strong>Docs</strong></a> Â·\n      <a href="https://github.com/mlflow/mlflow/issues/new/choose"><strong>Feature Request</strong></a> Â·\n      <a href="https://mlflow.org/blog"><strong>News</strong></a> Â·\n      <a href="https://www.youtube.com/@mlflowoss"><strong>YouTube</strong></a> Â·\n      <a href="https://lu.ma/mlflow?k=c"><strong>Events</strong></a>\n   </div>\n</div>\n\n<br>\n\n## ðŸš€ Installation\n\nTo install the MLflow Python package, run the following command:\n\n```\npip install mlflow\n```\n\n## ðŸ“¦ Core Components\n\nMLflow is **the only platform that provides a unified solution for all your AI/ML needs**, including LLMs, Agents, Deep Learning, and traditional machine learning.\n\n### ðŸ’¡ For LLM / GenAI Developers\n\n<table>\n  <tr>\n    <td>\n    <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-tracing.png" alt="Tracing" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/llms/tracing/index.html"><strong>ðŸ” Tracing / Observability</strong></a>\n        <br><br>\n        <div>Trace the internal states of your LLM/agentic applications for debugging quality issues and monitoring performance with ease.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/tracing/quickstart/python-openai/">Getting Started â†’</a>\n        <br><br>\n ', 'Learning, and traditional machine learning.\n\n### ðŸ’¡ For LLM / GenAI Developers\n\n<table>\n  <tr>\n    <td>\n    <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-tracing.png" alt="Tracing" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/llms/tracing/index.html"><strong>ðŸ” Tracing / Observability</strong></a>\n        <br><br>\n        <div>Trace the internal states of your LLM/agentic applications for debugging quality issues and monitoring performance with ease.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/tracing/quickstart/python-openai/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n    <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-llm-eval.png" alt="LLM Evaluation" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/genai/eval-monitor/"><strong>ðŸ“Š LLM Evaluation</strong></a>\n        <br><br>\n        <div>A suite of automated model evaluation tools, seamlessly integrated with experiment tracking to compare across multiple versions.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/eval-monitor/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-prompt.png" alt="Prompt Management">\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/prompt-registry/"><strong>ðŸ¤– Prompt Management</strong></a>\n        <br><br>\n        <div>Version, track, and reuse prompts across your organization, helping maintain consistency and improve collaboration in prompt development.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-registry/create-and-edit-prompts/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-logged-model.png" alt="MLflow Hero">\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/version-tracking/"><strong>ðŸ“¦ App Version Tracking</strong></a>\n        <br><br>\n  ', ' <br><br>\n        <div>Version, track, and reuse prompts across your organization, helping maintain consistency and improve collaboration in prompt development.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-registry/create-and-edit-prompts/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-logged-model.png" alt="MLflow Hero">\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/version-tracking/"><strong>ðŸ“¦ App Version Tracking</strong></a>\n        <br><br>\n        <div>MLflow keeps track of many moving parts in your AI applications, such as models, prompts, tools, and code, with end-to-end lineage.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/version-tracking/quickstart/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n</table>\n\n### ðŸŽ“ For Data Scientists\n\n<table>\n  <tr>\n    <td colspan="2" align="center" >\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-experiment.png" alt="Tracking" width=50%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/ml/tracking/"><strong>ðŸ“ Experiment Tracking</strong></a>\n        <br><br>\n        <div>Track your models, parameters, metrics, and evaluation results in ML experiments and compare them using an interactive UI.</div><br>\n        <a href="https://mlflow.org/docs/latest/ml/tracking/quickstart/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-model-registry.png" alt="Model Registry" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/ml/model-registry/"><strong>ðŸ’¾ Model Registry</strong></a>\n        <br><br>\n        <div> A centralized model store designed to collaboratively manage the full lifecycle and deployment of machine learning models.</div><br>\n        <a href="https://mlflow.org/docs/latest/ml/model-registry/tutorial/">Getting Started â†’</a>\n   ', '   <br><br>\n    </div>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-model-registry.png" alt="Model Registry" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/ml/model-registry/"><strong>ðŸ’¾ Model Registry</strong></a>\n        <br><br>\n        <div> A centralized model store designed to collaboratively manage the full lifecycle and deployment of machine learning models.</div><br>\n        <a href="https://mlflow.org/docs/latest/ml/model-registry/tutorial/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-deployment.png" alt="Deployment" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/ml/deployment/"><strong>ðŸš€ Deployment</strong></a>\n        <br><br>\n        <div> Tools for seamless model deployment to batch and real-time scoring on platforms like Docker, Kubernetes, Azure ML, and AWS SageMaker.</div><br>\n        <a href="https://mlflow.org/docs/latest/ml/deployment/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n</table>\n\n## ðŸŒ Hosting MLflow Anywhere\n\n<div align="center" >\n  <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-providers.png" alt="Providers" width=100%>\n</div>\n\nYou can run MLflow in many different environments, including local machines, on-premise servers, and cloud infrastructure.\n\nTrusted by thousands of organizations, MLflow is now offered as a managed service by most major cloud providers:\n\n- [Amazon SageMaker](https://aws.amazon.com/sagemaker-ai/experiments/)\n- [Azure ML](https://learn.microsoft.com/en-us/azure/machine-learning/concept-mlflow?view=azureml-api-2)\n- [Databricks](https://www.databricks.com/product/managed-mlflow)\n- [Nebius](https://nebius.com/services/managed-mlflow)\n\nFor hosting MLflow on your own infrastructure, please refer to [this guidance](https://mlflow.org/docs/latest/ml/tracking/#tracking-setup).\n\n## ðŸ—£ï¸ Supported Programming Languages\n\n- [Python](https://pypi.org/project/mlflow/)\n- [TypeScript / JavaScript](https://www.npmjs.com/package/mlflow-tracing)\n- [Java](https://mvnrepository.com/artifact/org.mlflow/mlflow-client)\n- [R](https://cran.r-project.org/web/packages/mlflow/readme/README.html)\n\n## ðŸ”— Integrations\n\nMLflow is natively integrated with many popular machine learning frameworks and GenAI libraries.\n\n![Integrations](https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-integrations.png)\n\n## Usage Examples\n\n### Experiment Tracking ([Doc](https://mlflow.org/docs/latest/ml/tracking/))\n\nThe following examples trains a simple regression model with scikit-learn, while enabling MLflow\'s [autologging](https://mlflow.org/docs/latest/tracking/autolog.html) feature for experiment tracking.\n\n```python\nimport mlflow\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Enable MLflow\'s automatic experiment tracking for scikit-learn\nmlflow.sklearn.autolog()\n\n# Load the training dataset\ndb = load_diabetes()\nX_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n\nrf ', 'a managed service by most major cloud providers:\n\n- [Amazon SageMaker](https://aws.amazon.com/sagemaker-ai/experiments/)\n- [Azure ML](https://learn.microsoft.com/en-us/azure/machine-learning/concept-mlflow?view=azureml-api-2)\n- [Databricks](https://www.databricks.com/product/managed-mlflow)\n- [Nebius](https://nebius.com/services/managed-mlflow)\n\nFor hosting MLflow on your own infrastructure, please refer to [this guidance](https://mlflow.org/docs/latest/ml/tracking/#tracking-setup).\n\n## ðŸ—£ï¸ Supported Programming Languages\n\n- [Python](https://pypi.org/project/mlflow/)\n- [TypeScript / JavaScript](https://www.npmjs.com/package/mlflow-tracing)\n- [Java](https://mvnrepository.com/artifact/org.mlflow/mlflow-client)\n- [R](https://cran.r-project.org/web/packages/mlflow/readme/README.html)\n\n## ðŸ”— Integrations\n\nMLflow is natively integrated with many popular machine learning frameworks and GenAI libraries.\n\n![Integrations](https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-integrations.png)\n\n## Usage Examples\n\n### Experiment Tracking ([Doc](https://mlflow.org/docs/latest/ml/tracking/))\n\nThe following examples trains a simple regression model with scikit-learn, while enabling MLflow\'s [autologging](https://mlflow.org/docs/latest/tracking/autolog.html) feature for experiment tracking.\n\n```python\nimport mlflow\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Enable MLflow\'s automatic experiment tracking for scikit-learn\nmlflow.sklearn.autolog()\n\n# Load the training dataset\ndb = load_diabetes()\nX_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n\nrf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\n# MLflow triggers logging automatically upon model fitting\nrf.fit(X_train, y_train)\n```\n\nOnce the above code finishes, run the following command in a separate terminal and access the MLflow UI via the printed URL. An MLflow **Run** should be automatically created, which tracks the training dataset, hyper parameters, performance metrics, the trained model, dependencies, and even more.\n\n```\nmlflow ui\n```\n\n### Evaluating Models ([Doc](https://mlflow.org/docs/latest/model-evaluation/index.html))\n\nThe following example runs automatic evaluation for question-answering tasks with several built-in metrics.\n\n```python\nimport mlflow\nimport pandas as pd\n\n# Evaluation set contains (1) input question (2) model outputs (3) ground truth\ndf = pd.DataFrame(\n    {\n        "inputs": ["What is MLflow?", "What is Spark?"],\n        "outputs": [\n            "MLflow is an innovative fully self-driving airship powered by AI.",\n            "Sparks is an American pop and rock duo formed in Los Angeles.",\n        ],\n        "ground_truth": [\n            "MLflow is an open-source platform for productionizing AI.",\n            "Apache Spark is an open-source, distributed computing system.",\n        ],\n    }\n)\neval_dataset = mlflow.data.from_pandas(\n    df, predictions="outputs", targets="ground_truth"\n)\n\n# Start an MLflow Run to record the evaluation results to\nwith mlflow.start_run(run_name="evaluate_qa"):\n    ', 'is an American pop and rock duo formed in Los Angeles.",\n        ],\n        "ground_truth": [\n            "MLflow is an open-source platform for productionizing AI.",\n            "Apache Spark is an open-source, distributed computing system.",\n        ],\n    }\n)\neval_dataset = mlflow.data.from_pandas(\n    df, predictions="outputs", targets="ground_truth"\n)\n\n# Start an MLflow Run to record the evaluation results to\nwith mlflow.start_run(run_name="evaluate_qa"):\n    # Run automatic evaluation with a set of built-in metrics for question-answering models\n    results = mlflow.evaluate(\n        data=eval_dataset,\n        model_type="question-answering",\n    )\n\nprint(results.tables["eval_results_table"])\n```\n\n### Observability ([Doc](https://mlflow.org/docs/latest/llms/tracing/index.html))\n\nMLflow Tracing provides LLM observability for various GenAI libraries such as OpenAI, LangChain, LlamaIndex, DSPy, AutoGen, and more. To enable auto-tracing, call `mlflow.xyz.autolog()` before running your models. Refer to the documentation for customization and manual instrumentation.\n\n```python\nimport mlflow\nfrom openai import OpenAI\n\n# Enable tracing for OpenAI\nmlflow.openai.autolog()\n\n# Query OpenAI LLM normally\nresponse = OpenAI().chat.completions.create(\n    model="gpt-4o-mini",\n    messages=[{"role": "user", "content": "Hi!"}],\n    temperature=0.1,\n)\n```\n\nThen navigate to the "Traces" tab in the MLflow UI to find the trace records OpenAI query.\n\n## ðŸ’­ Support\n\n- For help or questions about MLflow usage (e.g. "how do I do X?") visit the [documentation](https://mlflow.org/docs/latest/index.html).\n- In the documentation, you can ask the question to our AI-powered chat bot. Click on the **"Ask AI"** button at the right bottom.\n- Join the [virtual events](https://lu.ma/mlflow?k=c) like office hours and meetups.\n- To report a bug, file a documentation issue, or submit a feature request, please [open a GitHub issue](https://github.com/mlflow/mlflow/issues/new/choose).\n- For release announcements and other discussions, please subscribe to our mailing list (mlflow-users@googlegroups.com)\n  or join us on [Slack](https://mlflow.org/slack).\n\n## ðŸ¤ Contributing\n\nWe happily welcome contributions to MLflow!\n\n- Submit [bug reports](https://github.com/mlflow/mlflow/issues/new?template=bug_report_template.yaml) and [feature requests](https://github.com/mlflow/mlflow/issues/new?template=feature_request_template.yaml)\n- Contribute for [good-first-issues](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) and [help-wanted](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22)\n- Writing about MLflow and sharing your experience\n\nPlease see our [contribution guide](CONTRIBUTING.md) to learn more about contributing to MLflow.\n\n## â­ï¸ Star History\n\n<a href="https://star-history.com/#mlflow/mlflow&Date">\n <picture>\n  ', 'chat bot. Click on the **"Ask AI"** button at the right bottom.\n- Join the [virtual events](https://lu.ma/mlflow?k=c) like office hours and meetups.\n- To report a bug, file a documentation issue, or submit a feature request, please [open a GitHub issue](https://github.com/mlflow/mlflow/issues/new/choose).\n- For release announcements and other discussions, please subscribe to our mailing list (mlflow-users@googlegroups.com)\n  or join us on [Slack](https://mlflow.org/slack).\n\n## ðŸ¤ Contributing\n\nWe happily welcome contributions to MLflow!\n\n- Submit [bug reports](https://github.com/mlflow/mlflow/issues/new?template=bug_report_template.yaml) and [feature requests](https://github.com/mlflow/mlflow/issues/new?template=feature_request_template.yaml)\n- Contribute for [good-first-issues](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) and [help-wanted](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22)\n- Writing about MLflow and sharing your experience\n\nPlease see our [contribution guide](CONTRIBUTING.md) to learn more about contributing to MLflow.\n\n## â­ï¸ Star History\n\n<a href="https://star-history.com/#mlflow/mlflow&Date">\n <picture>\n   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date&theme=dark" />\n   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date" />\n   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date" />\n </picture>\n</a>\n\n## âœï¸ Citation\n\nIf you use MLflow in your research, please cite it using the "Cite this repository" button at the top of the [GitHub repository page](https://github.com/mlflow/mlflow), which will provide you with citation formats including APA and BibTeX.\n\n## ðŸ‘¥ Core Members\n\nMLflow is currently maintained by the following core members with significant contributions from hundreds of exceptionally talented community members.\n\n- [Ben Wilson](https://github.com/BenWilson2)\n- [Corey Zumar](https://github.com/dbczumar)\n- [Daniel Lok](https://github.com/daniellok-db)\n- [Gabriel Fu](https://github.com/gabrielfu)\n- [Harutaka Kawamura](https://github.com/harupy)\n- [Serena Ruan](https://github.com/serena-ruan)\n- [Tomu Hirata](https://github.com/TomeHirata)\n- [Weichen Xu](https://github.com/WeichenXu123)\n- [Yuki Watanabe](https://github.com/B-Step62)\n', '# MLflow Tracing: An Open-Source SDK for Observability and Monitoring GenAI ApplicationsðŸ”\n\n[![Latest Docs](https://img.shields.io/badge/docs-latest-success.svg?style=for-the-badge)](https://mlflow.org/docs/latest/index.html)\n[![Apache 2 License](https://img.shields.io/badge/license-Apache%202-brightgreen.svg?style=for-the-badge&logo=apache)](https://github.com/mlflow/mlflow/blob/master/LICENSE.txt)\n[![Slack](https://img.shields.io/badge/slack-@mlflow--users-CF0E5B.svg?logo=slack&logoColor=white&labelColor=3F0E40&style=for-the-badge)](https://mlflow.org/community/#slack)\n[![Twitter](https://img.shields.io/twitter/follow/MLflow?style=for-the-badge&labelColor=00ACEE&logo=twitter&logoColor=white)](https://twitter.com/MLflow)\n\nMLflow Tracing (`mlflow-tracing`) is an open-source, lightweight Python package that only includes the minimum set of dependencies and functionality\nto instrument your code/models/agents with [MLflow Tracing Feature](https://mlflow.org/docs/latest/tracing). It is designed to be a perfect fit for production environments where you want:\n\n- **âš¡ï¸ Faster Deployment**: The package size and dependencies are significantly smaller than the full MLflow package, allowing for faster deployment times in dynamic environments such as Docker containers, serverless functions, and cloud-based applications.\n- **ðŸ”§ Simplified Dependency Management**: A smaller set of dependencies means less work keeping up with dependency updates, security patches, and breaking changes from upstream libraries.\n- **ðŸ“¦ Portability**: With the less number of dependencies, MLflow Tracing can be easily deployed across different environments and platforms, without worrying about compatibility issues.\n- **ðŸ”’ Fewer Security Risks**: Each dependency potentially introduces security vulnerabilities. By reducing the number of dependencies, MLflow Tracing minimizes the attack surface and reduces the risk of security breaches.\n\n## âœ¨ Features\n\n- [Automatic Tracing](https://mlflow.org/docs/latest/tracing/integrations/) for AI libraries (OpenAI, LangChain, DSPy, Anthropic, etc...). Follow the link for the full list of supported libraries.\n- [Manual instrumentation APIs](https://mlflow.org/docs/latest/tracing/api/manual-instrumentation) such as `@trace` decorator.\n- [Production Monitoring](https://mlflow.org/docs/latest/tracing/production)\n- Other tracing APIs such as `mlflow.set_trace_tag`, `mlflow.search_traces`, etc.\n\n## ðŸŒ Choose Backend\n\nThe MLflow Trace package is designed to work with a remote hosted MLflow server as a backend. This allows you to log your traces to a central location, making it easier to manage and analyze your traces. There are several different options for hosting your MLflow server, including:\n\n- [Databricks](https://docs.databricks.com/machine-learning/mlflow/managed-mlflow.html) - Databricks offers a FREE, fully managed MLflow server as a part of their platform. This is the easiest way to get started with MLflow tracing, without having to set up any infrastructure.\n- [Amazon SageMaker](https://aws.amazon.com/sagemaker-ai/experiments/) - MLflow on Amazon SageMaker is a fully managed service offered as part of the SageMaker platform by AWS, including tracing and other MLflow features such as model registry.\n- [Nebius](https://nebius.com/) - Nebius, a cutting-edge cloud platform for GenAI explorers, offers a fully managed MLflow server.\n- [Self-hosting](https://mlflow.org/docs/latest/tracking/#tracking_setup) - MLflow is a fully open-source project, allowing you to self-host your own MLflow ', 'your MLflow server, including:\n\n- [Databricks](https://docs.databricks.com/machine-learning/mlflow/managed-mlflow.html) - Databricks offers a FREE, fully managed MLflow server as a part of their platform. This is the easiest way to get started with MLflow tracing, without having to set up any infrastructure.\n- [Amazon SageMaker](https://aws.amazon.com/sagemaker-ai/experiments/) - MLflow on Amazon SageMaker is a fully managed service offered as part of the SageMaker platform by AWS, including tracing and other MLflow features such as model registry.\n- [Nebius](https://nebius.com/) - Nebius, a cutting-edge cloud platform for GenAI explorers, offers a fully managed MLflow server.\n- [Self-hosting](https://mlflow.org/docs/latest/tracking/#tracking_setup) - MLflow is a fully open-source project, allowing you to self-host your own MLflow server and keep your data private. This is a great option if you want to have full control over your data and infrastructure.\n\n## ðŸš€ Getting Started\n\n### Installation\n\nTo install the MLflow Python package, run the following command:\n\n```bash\npip install mlflow-tracing\n```\n\nTo install from the source code, run the following command:\n\n```bash\npip install git+https://github.com/mlflow/mlflow.git#subdirectory=libs/tracing\n```\n\n> **NOTE:** It is **not** recommended to co-install this package with the full MLflow package together, as it may cause version mismatches issues.\n\n### Connect to the MLflow Server\n\nTo connect to your MLflow server to log your traces, set the `MLFLOW_TRACKING_URI` environment variable or use the `mlflow.set_tracking_uri` function:\n\n```python\nimport mlflow\n\nmlflow.set_tracking_uri("databricks")\n# Specify the experiment to log the traces to\nmlflow.set_experiment("/Path/To/Experiment")\n```\n\n### Start Logging Traces\n\n```python\nimport openai\n\nclient = openai.OpenAI(api_key="<your-api-key>")\n\n# Enable auto-tracing for OpenAI\nmlflow.openai.autolog()\n\n# Call the OpenAI API as usual\nresponse = client.chat.completions.create(\n    model="gpt-4.1-mini",\n    messages=[{"role": "user", "content": "Hello, how are you?"}],\n)\n```\n\n## ðŸ“˜ Documentation\n\nOfficial documentation for MLflow Tracing can be found at [here](https://mlflow.org/docs/latest/tracing).\n\n## ðŸ›‘ Features _Not_ Included\n\nThe following MLflow features are not included in this package.\n\n- MLflow tracking server and UI.\n- MLflow\'s other tracking capabilities such as Runs, Model Registry, Projects, etc.\n- Evaluate models/agents and log evaluation results.\n\nTo leverage the full feature set of MLflow, install the full package by running `pip install mlflow`.\n', '<h1 align="center" style="border-bottom: none">\n    <div>\n        <a href="https://mlflow.org/"><picture>\n            <img alt="MLflow Logo" src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/logo.svg" width="200" />\n        </picture></a>\n        <br>\n        MLflow TypeScript SDK\n    </div>\n</h1>\n<h2 align="center" style="border-bottom: none"></h2>\n\n<p align="center">\n  <a href="https://github.com/mlflow/mlflow"><img src="https://img.shields.io/github/stars/mlflow/mlflow?style=social" alt="stars"></a>\n  <a href="https://www.npmjs.com/package/mlflow-tracing"><img src="https://img.shields.io/npm/v/mlflow-tracing.svg" alt="version"></a>\n  <a href="https://www.npmjs.com/package/mlflow-tracing"><img src="https://img.shields.io/npm/dt/mlflow-tracing.svg" alt="downloads"></a>\n  <a href="https://github.com/mlflow/mlflow/blob/main/LICENSE"><img src="https://img.shields.io/github/license/mlflow/mlflow" alt="license"></a>\n</p>\n\nMLflow Typescript SDK is a variant of the [MLflow Python SDK](https://github.com/mlflow/mlflow) that provides a TypeScript API for MLflow.\n\n> [!IMPORTANT]\n> MLflow Typescript SDK is catching up with the Python SDK. Currently only support [Tracing]() and [Feedback Collection]() features. Please raise an issue in Github if you need a feature that is not supported.\n\n## Packages\n\n| Package                                | NPM                                                                                                                                         | Description                                       ', "                                                            | Description                                                |\n| -------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------- |\n| [mlflow-tracing](./core)               | [![npm package](https://img.shields.io/npm/v/mlflow-tracing?style=flat-square)](https://www.npmjs.com/package/mlflow-tracing)               | The core tracing functionality and manual instrumentation. |\n| [mlflow-openai](./integrations/openai) | [![npm package](https://img.shields.io/npm/v/mlflow-tracing-openai?style=flat-square)](https://www.npmjs.com/package/mlflow-tracing-openai) | Auto-instrumentation integration for OpenAI.               |\n\n## Installation\n\n```bash\nnpm install mlflow-tracing\n```\n\n> [!NOTE]\n> MLflow Typescript SDK requires Node.js 20 or higher.\n\n## Quickstart\n\nStart MLflow Tracking Server if you don't have one already:\n\n```bash\npip install mlflow\nmlflow server --backend-store-uri sqlite:///mlruns.db --port 5000\n```\n\nSelf-hosting MLflow server requires Python 3.10 or higher. If you don't have one, you can also use [managed MLflow service](https://mlflow.org/#get-started) for free to get started quickly.\n\nInstantiate MLflow SDK in your application:\n\n```typescript\nimport * as mlflow from 'mlflow-tracing';\n\nmlflow.init({\n  trackingUri: 'http://localhost:5000',\n  experimentId: '<experiment-id>'\n});\n```\n\nCreate a trace:\n\n```typescript\n// Wrap a function with mlflow.trace to generate a span when the function is called.\n// MLflow will automatically record the function name, arguments, return value,\n// latency, and exception information to the span.\nconst getWeather = mlflow.trace(\n  (city: string) => {\n    return `The weather in ${city} is sunny`;\n  },\n  // Pass options to set span name. See https://mlflow.org/docs/latest/genai/tracing/app-instrumentation/typescript-sdk\n  // for the full list of options.\n  { name: 'get-weather' }\n);\ngetWeather('San Francisco');\n\n// Alternatively, start and end span manually\nconst span = mlflow.startSpan({ name: 'my-span' });\nspan.end();\n```\n\nView traces in MLflow UI:\n\n![MLflow Tracing UI](https://github.com/mlflow/mlflow/blob/891fed9a746477f808dd2b82d3abb2382293c564/docs/static/images/llms/tracing/quickstart/openai-tool-calling-trace-detail.png?raw=true)\n\n## Trace Usage\n\nMLflow Tracing empowers ", '\'<experiment-id>\'\n});\n```\n\nCreate a trace:\n\n```typescript\n// Wrap a function with mlflow.trace to generate a span when the function is called.\n// MLflow will automatically record the function name, arguments, return value,\n// latency, and exception information to the span.\nconst getWeather = mlflow.trace(\n  (city: string) => {\n    return `The weather in ${city} is sunny`;\n  },\n  // Pass options to set span name. See https://mlflow.org/docs/latest/genai/tracing/app-instrumentation/typescript-sdk\n  // for the full list of options.\n  { name: \'get-weather\' }\n);\ngetWeather(\'San Francisco\');\n\n// Alternatively, start and end span manually\nconst span = mlflow.startSpan({ name: \'my-span\' });\nspan.end();\n```\n\nView traces in MLflow UI:\n\n![MLflow Tracing UI](https://github.com/mlflow/mlflow/blob/891fed9a746477f808dd2b82d3abb2382293c564/docs/static/images/llms/tracing/quickstart/openai-tool-calling-trace-detail.png?raw=true)\n\n## Trace Usage\n\nMLflow Tracing empowers you throughout the end-to-end lifecycle of your application. Here\'s how it helps you at each step of the workflow, click on each section to learn more:\n\n<details>\n<summary><strong>ðŸ” Build & Debug</strong></summary>\n\n<table>\n<tr>\n<td width="60%">\n\n#### Smooth Debugging Experience\n\nMLflow\'s tracing capabilities provide deep insights into what happens beneath the abstractions of your application, helping you precisely identify where issues occur.\n\n[Learn more â†’](https://mlflow.org/docs/latest/genai/tracing/observe-with-traces)\n\n</td>\n<td width="40%">\n\n![Trace Debug](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-trace-debug.png)\n\n</td>\n</tr>\n</table>\n\n</details>\n\n<details>\n<summary><strong>ðŸ’¬ Human Feedback</strong></summary>\n\n<table>\n<tr>\n<td width="60%">\n\n#### Track Annotation and User Feedback Attached to Traces\n\nCollecting and managing feedback is essential for improving your application. MLflow Tracing allows you to attach user feedback and annotations directly to traces, creating a rich dataset for analysis.\n\nThis feedback data helps you understand user satisfaction, identify areas for improvement, and build better evaluation datasets based on real user interactions.\n\n[Learn more â†’](https://mlflow.org/docs/latest/genai/tracing/collect-user-feedback)\n\n</td>\n<td width="40%">\n\n![Human Feedback](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-human-feedback.png)\n\n</td>\n</tr>\n</table>\n\n</details>\n\n<details>\n<summary><strong>ðŸ“Š Evaluation</strong></summary>\n\n<table>\n<tr>\n<td width="60%">\n\n#### Systematic Quality Assessment Throughout Your Application\n\nEvaluating the performance of your application is crucial, but creating a reliable evaluation process can be challenging. Traces serve as a rich data source, helping you assess quality with precise metrics for all components.\n\nWhen combined with MLflow\'s evaluation capabilities, you get a seamless experience for assessing and improving your application\'s performance.\n\n[Learn more â†’](https://mlflow.org/docs/latest/genai/eval-monitor)\n\n</td>\n<td width="40%">\n\n![Evaluation](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-trace-evaluation.png)\n\n</td>\n</tr>\n</table>\n\n</details>\n\n<details>\n<summary><strong>ðŸš€ Production Monitoring</strong></summary>\n\n<table>\n<tr>\n<td width="60%">\n\n#### Monitor Applications with Your Favorite Observability Stack\n\nMachine learning projects don\'t end with the first launch. Continuous monitoring and incremental improvement are critical to long-term success.\n\nIntegrated with various observability platforms such as Databricks, Datadog, Grafana, and Prometheus, MLflow Tracing provides a comprehensive solution for monitoring your applications in production.\n\n[Learn more â†’](https://mlflow.org/docs/latest/genai/tracing/prod-tracing)\n\n</td>\n<td width="40%">\n\n![Monitoring](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-monitoring.png)\n\n</td>\n</tr>\n</table>\n\n</details>\n\n<details>\n<summary><strong>ðŸ“¦ Dataset Collection</strong></summary>\n\n<table>\n<tr>\n<td width="60%">\n\n#### Create High-Quality Evaluation Datasets from Production Traces\n\nTraces from production are ', 'rich data source, helping you assess quality with precise metrics for all components.\n\nWhen combined with MLflow\'s evaluation capabilities, you get a seamless experience for assessing and improving your application\'s performance.\n\n[Learn more â†’](https://mlflow.org/docs/latest/genai/eval-monitor)\n\n</td>\n<td width="40%">\n\n![Evaluation](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-trace-evaluation.png)\n\n</td>\n</tr>\n</table>\n\n</details>\n\n<details>\n<summary><strong>ðŸš€ Production Monitoring</strong></summary>\n\n<table>\n<tr>\n<td width="60%">\n\n#### Monitor Applications with Your Favorite Observability Stack\n\nMachine learning projects don\'t end with the first launch. Continuous monitoring and incremental improvement are critical to long-term success.\n\nIntegrated with various observability platforms such as Databricks, Datadog, Grafana, and Prometheus, MLflow Tracing provides a comprehensive solution for monitoring your applications in production.\n\n[Learn more â†’](https://mlflow.org/docs/latest/genai/tracing/prod-tracing)\n\n</td>\n<td width="40%">\n\n![Monitoring](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-monitoring.png)\n\n</td>\n</tr>\n</table>\n\n</details>\n\n<details>\n<summary><strong>ðŸ“¦ Dataset Collection</strong></summary>\n\n<table>\n<tr>\n<td width="60%">\n\n#### Create High-Quality Evaluation Datasets from Production Traces\n\nTraces from production are invaluable for building comprehensive evaluation datasets. By capturing real user interactions and their outcomes, you can create test cases that truly represent your application\'s usage patterns.\n\nThis comprehensive data capture enables you to create realistic test scenarios, validate model performance on actual usage patterns, and continuously improve your evaluation datasets.\n\n[Learn more â†’](https://mlflow.org/docs/latest/genai/tracing/search-traces#creating-evaluation-datasets)\n\n</td>\n<td width="40%">\n\n![Dataset Collection](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-trace-dataset.png)\n\n</td>\n</tr>\n</table>\n\n</details>\n\n## Documentation ðŸ“˜\n\nOfficial documentation for MLflow Typescript SDK can be found [here](https://mlflow.org/docs/latest/genai/tracing/app-instrumentation/typescript-sdk).\n\n## License\n\nThis project is licensed under the [Apache License 2.0](https://github.com/mlflow/mlflow/blob/master/LICENSE.txt).\n', "# MLflow Typescript SDK - Core\n\nThis is the core package of the [MLflow Typescript SDK](https://github.com/mlflow/mlflow/tree/main/libs/typescript). It is a skinny package that includes the core tracing functionality and manual instrumentation.\n\n| Package              | NPM                                                                                                                           | Description                                                |\n| -------------------- | ----------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------- |\n| [mlflow-tracing](./) | [![npm package](https://img.shields.io/npm/v/mlflow-tracing?style=flat-square)](https://www.npmjs.com/package/mlflow-tracing) | The core tracing functionality and manual instrumentation. |\n\n## Installation\n\n```bash\nnpm install mlflow-tracing\n```\n\n## Quickstart\n\nStart MLflow Tracking Server if you don't have one already:\n\n```bash\npip install mlflow\nmlflow server --backend-store-uri sqlite:///mlruns.db --port 5000\n```\n\nSelf-hosting MLflow server requires Python 3.10 or higher. If you don't have one, you can also use [managed MLflow service](https://mlflow.org/#get-started) for free to get started quickly.\n\nInstantiate MLflow SDK in your application:\n\n```typescript\nimport * as mlflow from 'mlflow-tracing';\n\nmlflow.init({\n  trackingUri: 'http://localhost:5000',\n  experimentId: '<experiment-id>'\n});\n```\n\nCreate a trace:\n\n```typescript\n// Wrap a function with mlflow.trace to generate a span when the function is called.\n// MLflow will automatically record the function name, arguments, return value,\n// latency, and exception information to the span.\nconst getWeather = mlflow.trace(\n  (city: string) => {\n    return `The weather in ${city} is sunny`;\n  },\n ", "mlflow\nmlflow server --backend-store-uri sqlite:///mlruns.db --port 5000\n```\n\nSelf-hosting MLflow server requires Python 3.10 or higher. If you don't have one, you can also use [managed MLflow service](https://mlflow.org/#get-started) for free to get started quickly.\n\nInstantiate MLflow SDK in your application:\n\n```typescript\nimport * as mlflow from 'mlflow-tracing';\n\nmlflow.init({\n  trackingUri: 'http://localhost:5000',\n  experimentId: '<experiment-id>'\n});\n```\n\nCreate a trace:\n\n```typescript\n// Wrap a function with mlflow.trace to generate a span when the function is called.\n// MLflow will automatically record the function name, arguments, return value,\n// latency, and exception information to the span.\nconst getWeather = mlflow.trace(\n  (city: string) => {\n    return `The weather in ${city} is sunny`;\n  },\n  // Pass options to set span name. See https://mlflow.org/docs/latest/genai/tracing/app-instrumentation/typescript-sdk\n  // for the full list of options.\n  { name: 'get-weather' }\n);\ngetWeather('San Francisco');\n\n// Alternatively, start and end span manually\nconst span = mlflow.startSpan({ name: 'my-span' });\nspan.end();\n```\n\n## Documentation ðŸ“˜\n\nOfficial documentation for MLflow Typescript SDK can be found [here](https://mlflow.org/docs/latest/genai/tracing/app-instrumentation/typescript-sdk).\n\n## License\n\nThis project is licensed under the [Apache License 2.0](https://github.com/mlflow/mlflow/blob/master/LICENSE.txt).\n", "# MLflow Typescript SDK - OpenAI\n\nSeamlessly integrate [MLflow Tracing](https://github.com/mlflow/mlflow/tree/main/libs/typescript) with OpenAI to automatically trace your OpenAI API calls.\n\n| Package             | NPM                                                                                                                                         | Description                                  |\n| ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |\n| [mlflow-openai](./) | [![npm package](https://img.shields.io/npm/v/mlflow-tracing-openai?style=flat-square)](https://www.npmjs.com/package/mlflow-tracing-openai) | Auto-instrumentation integration for OpenAI. |\n\n## Installation\n\n```bash\nnpm install mlflow-openai\n```\n\nThe package includes the [`mlflow-tracing`](https://github.com/mlflow/mlflow/tree/main/libs/typescript) package and `openai` package as peer dependencies. Depending on your package manager, you may need to install these two packages separately.\n\n## Quickstart\n\nStart MLflow Tracking Server if you don't have one already:\n\n```bash\npip install mlflow\nmlflow server --backend-store-uri sqlite:///mlruns.db --port 5000\n```\n\nSelf-hosting MLflow server requires Python 3.10 or higher. If you don't have one, you can also use [managed MLflow service](https://mlflow.org/#get-started) for free to get started quickly.\n\nInstantiate MLflow SDK in your application:\n\n```typescript\nimport * as mlflow from 'mlflow-tracing';\n\nmlflow.init({\n  trackingUri: 'http://localhost:5000',\n  experimentId: '<experiment-id>'\n});\n```\n\nCreate a trace:\n\n```typescript\nimport { OpenAI } from 'openai';\nimport { tracedOpenAI } from 'mlflow-openai';\n\n// Wrap the OpenAI client with the tracedOpenAI function\nconst client = tracedOpenAI(new OpenAI());\n\n// Invoke the client as usual\nconst response = await client.chat.completions.create({\n  model: 'o4-mini',\n  messages: [\n    { ", 'MLflow Tracking Server if you don\'t have one already:\n\n```bash\npip install mlflow\nmlflow server --backend-store-uri sqlite:///mlruns.db --port 5000\n```\n\nSelf-hosting MLflow server requires Python 3.10 or higher. If you don\'t have one, you can also use [managed MLflow service](https://mlflow.org/#get-started) for free to get started quickly.\n\nInstantiate MLflow SDK in your application:\n\n```typescript\nimport * as mlflow from \'mlflow-tracing\';\n\nmlflow.init({\n  trackingUri: \'http://localhost:5000\',\n  experimentId: \'<experiment-id>\'\n});\n```\n\nCreate a trace:\n\n```typescript\nimport { OpenAI } from \'openai\';\nimport { tracedOpenAI } from \'mlflow-openai\';\n\n// Wrap the OpenAI client with the tracedOpenAI function\nconst client = tracedOpenAI(new OpenAI());\n\n// Invoke the client as usual\nconst response = await client.chat.completions.create({\n  model: \'o4-mini\',\n  messages: [\n    { role: \'system\', content: \'You are a helpful weather assistant.\' },\n    { role: \'user\', content: "What\'s the weather like in Seattle?" }\n  ]\n});\n```\n\nView traces in MLflow UI:\n\n![MLflow Tracing UI](https://github.com/mlflow/mlflow/blob/891fed9a746477f808dd2b82d3abb2382293c564/docs/static/images/llms/tracing/quickstart/single-openai-trace-detail.png?raw=true)\n\n## Documentation ðŸ“˜\n\nOfficial documentation for MLflow Typescript SDK can be found [here](https://mlflow.org/docs/latest/genai/tracing/app-instrumentation/typescript-sdk).\n\n## License\n\nThis project is licensed under the [Apache License 2.0](https://github.com/mlflow/mlflow/blob/master/LICENSE.txt).\n', '---\nnamespace: genai\ndescription: Analyzes the traces logged in an MLflow experiment to find operational and quality issues automatically, generating a markdown report.\n---\n\n# Analyze Experiment\n\nAnalyzes traces in an MLflow experiment for quality issues, performance problems, and patterns.\n\n## Step 1: Setup and Configuration\n\n### 1.1 Collect Experiment Information\n\n- **REQUIRED FIRST**: Ask user "How do you want to authenticate to MLflow?"\n\n  **Option 1: Local/Self-hosted MLflow**\n\n  - Ask for tracking URI (one of):\n    - SQLite: `sqlite:////path/to/mlflow.db`\n    - PostgreSQL: `postgresql://user:password@host:port/database`\n    - MySQL: `mysql://user:password@host:port/database`\n    - File Store: `file:///path/to/mlruns` or just `/path/to/mlruns`\n  - Ask user to create an environment file (e.g., `mlflow.env`) containing:\n    ```\n    MLFLOW_TRACKING_URI=<provided_uri>\n    ```\n\n  **Option 2: Databricks**\n\n  - Ask which authentication method:\n    - **PAT Auth**: Request `DATABRICKS_HOST` and `DATABRICKS_TOKEN`\n    - **Profile Auth**: Request `DATABRICKS_CONFIG_PROFILE` name\n  - Ask user to create an environment file (e.g., `mlflow.env`) containing:\n\n    ```\n    # For PAT Auth:\n    MLFLOW_TRACKING_URI=databricks\n    DATABRICKS_HOST=<provided_host>\n    DATABRICKS_TOKEN=<provided_token>\n\n    # OR for Profile Auth:\n    MLFLOW_TRACKING_URI=databricks\n    DATABRICKS_CONFIG_PROFILE=<provided_profile>\n    ```\n\n  **Option 3: Environment Variables Already Set**\n\n  - Ask user "Do you already have MLflow environment variables set in your shell (bashrc/zshrc)?"\n  - If yes, test connection directly: `uv run python -m mlflow experiments search --max-results 10`\n  - If this works, skip env file creation and use commands without `--env-file` flag\n  - If not, fall back to Options 1 or 2\n\n- Ask user for the path to their environment file (if using Options 1-2)\n- Verify connection by listing experiments: `uv run --env-file <env_file_path> python -m mlflow experiments search --max-results 10`\n- **Option to search by name**: If user knows the experiment name, use `--filter-string` parameter:\n  - `uv run --env-file <env_file_path> python -m mlflow experiments search --filter-string "name LIKE \'%experiment_name%\'" --max-results 10`\n- Ask user for experiment ID or let them choose from the list\n- **WAIT for user response** - do not continue ', 'env file creation and use commands without `--env-file` flag\n  - If not, fall back to Options 1 or 2\n\n- Ask user for the path to their environment file (if using Options 1-2)\n- Verify connection by listing experiments: `uv run --env-file <env_file_path> python -m mlflow experiments search --max-results 10`\n- **Option to search by name**: If user knows the experiment name, use `--filter-string` parameter:\n  - `uv run --env-file <env_file_path> python -m mlflow experiments search --filter-string "name LIKE \'%experiment_name%\'" --max-results 10`\n- Ask user for experiment ID or let them choose from the list\n- **WAIT for user response** - do not continue until they provide the experiment ID\n- Add `MLFLOW_EXPERIMENT_ID=<experiment_id>` to their environment file\n- Run `uv run --env-file <env_file_path> python -m mlflow traces --help` to understand the CLI commands and options\n\n### 1.2 Test Trace Retrieval\n\n- Call `uv run --env-file <env_file_path> python -m mlflow traces search --max-results 5` to verify:\n  - Traces exist in the experiment\n  - CLI is working properly (using local MLflow installation)\n  - Database connection is valid\n- Extract sample trace IDs for testing\n- Get one full trace with `uv run --env-file <env_file_path> python -m mlflow traces get --trace-id <id>` to understand the data structure\n\n## Step 2: Analysis Phase\n\n### 2.1 Bulk Trace Collection\n\n- Search for a larger sample using `--max-results` parameter (start with 20-50 traces for initial analysis)\n- **IMPORTANT**: Use `--max-results` to limit results for users with hundreds of thousands of experiments/traces\n- Extract key fields: trace_id, state, execution_duration_ms, request_preview, response_preview\n\n### 2.1.5 Understand Agent Purpose and Capabilities\n\n- Analyze trace inputs/outputs to understand the agent\'s task:\n  - Extract trace inputs/outputs: `--extract-fields info.trace_metadata.\\`mlflow.traceInputs\\`,info.trace_metadata.\\`mlflow.traceOutputs\\``\n  - Examine these fields to understand:\n    - Types of questions users ask\n    - Types of responses the agent provides\n    - Common patterns in user interactions\n  - Identify available tools by examining spans with type "TOOL":\n    - What tools are available to the agent?\n    - What data sources can the agent access?\n    - What capabilities do these tools provide?\n- Generate a 1-paragraph agent description covering:\n  - **What ', 'inputs/outputs to understand the agent\'s task:\n  - Extract trace inputs/outputs: `--extract-fields info.trace_metadata.\\`mlflow.traceInputs\\`,info.trace_metadata.\\`mlflow.traceOutputs\\``\n  - Examine these fields to understand:\n    - Types of questions users ask\n    - Types of responses the agent provides\n    - Common patterns in user interactions\n  - Identify available tools by examining spans with type "TOOL":\n    - What tools are available to the agent?\n    - What data sources can the agent access?\n    - What capabilities do these tools provide?\n- Generate a 1-paragraph agent description covering:\n  - **What the agent\'s job is** (e.g., "a boating agent that answers questions about weather and helps users plan trips")\n  - **What data sources it has access to** (APIs, databases, etc.)\n- **Present this description to the user** and ask for confirmation/corrections\n- **WAIT for user response** - do not proceed until they confirm or provide corrections\n- **Ask if they want to focus the analysis on anything specific** (or do a general report)\n  - If they provide specific focus areas, use these as additional context for hypothesis formation\n  - Don\'t overfit to their focus - still do comprehensive analysis, but prioritize their areas of interest\n  - Their specific concerns should become hypotheses to validate/invalidate during analysis\n- **WAIT for user response** before proceeding to section 2.2\n- Use agent context + any specific focus areas for all subsequent hypothesis testing in sections 2.2+\n\n### 2.2 Operational Issues Analysis (Hypothesis-Driven Approach)\n\n**NOTE: Use MLflow CLI commands for trace exploration - DO NOT use inline Python scripts during this phase**\n\n**Show your thinking as you go**: Always explain your hypothesis development process including:\n\n- Current hypothesis being tested\n- Evidence found: ALWAYS show BOTH trace input (user request) AND trace output (agent response), plus tools called\n- Reasoning for supporting/refuting the hypothesis\n\nProcess traces in batches of 10, building and refining hypotheses with each batch:\n\n1. Form initial hypotheses from first batch\n2. With each new batch: validate, refute, or expand hypotheses\n3. Continue until patterns stabilize\n\n**After confirming ANY hypothesis (operational or quality)**: Track assessments for inclusion in final report:\n\n- **1:1 Correspondence**: Each assessment ', 'commands for trace exploration - DO NOT use inline Python scripts during this phase**\n\n**Show your thinking as you go**: Always explain your hypothesis development process including:\n\n- Current hypothesis being tested\n- Evidence found: ALWAYS show BOTH trace input (user request) AND trace output (agent response), plus tools called\n- Reasoning for supporting/refuting the hypothesis\n\nProcess traces in batches of 10, building and refining hypotheses with each batch:\n\n1. Form initial hypotheses from first batch\n2. With each new batch: validate, refute, or expand hypotheses\n3. Continue until patterns stabilize\n\n**After confirming ANY hypothesis (operational or quality)**: Track assessments for inclusion in final report:\n\n- **1:1 Correspondence**: Each assessment must correspond to ONE specific issue/hypothesis\n- Use snake_case names as assessment keys (e.g., `overly_verbose`, `tool_failure`, `rate_limited`, `slow_response`)\n- Track which traces exhibit each issue with detailed rationales\n- Document specifics like:\n\n  - For quality issues: exact character counts, repetition counts, unnecessary sections\n  - For operational issues: exact durations, error messages, timeout values\n\n- **Error Analysis**\n\n  - Filter for ERROR traces: `uv run --env-file <env_file_path> python -m mlflow traces search --filter "info.state = \'ERROR\'" --max-results 10`\n  - **Adjust --max-results as needed**: Start with 10-20, increase if you need more examples to identify patterns\n  - **Pattern Analysis Focus**: Identify WHY errors occur by examining:\n    - Tool/API failures in spans (look for spans with type "TOOL" that failed)\n    - Rate limiting responses from external APIs\n    - Authentication/permission errors\n    - Timeout patterns (compare execution_duration_ms)\n    - Input validation failures\n    - Resource unavailability (databases, services down)\n  - Example hypotheses to test:\n    - Certain types of queries consistently trigger tool failures\n    - Errors cluster around specific time ranges (service outages)\n    - Fast failures (~2s) indicate input validation vs slower failures (~30s) indicate timeouts\n    - Specific tools/APIs are unreliable and cause cascading failures\n    - Rate limiting from external services causes batch failures\n  - **Note**: You may discover other operational error patterns as you analyze the traces\n\n- **Performance Problems (High Latency Analysis)**\n ', 'failures\n    - Resource unavailability (databases, services down)\n  - Example hypotheses to test:\n    - Certain types of queries consistently trigger tool failures\n    - Errors cluster around specific time ranges (service outages)\n    - Fast failures (~2s) indicate input validation vs slower failures (~30s) indicate timeouts\n    - Specific tools/APIs are unreliable and cause cascading failures\n    - Rate limiting from external services causes batch failures\n  - **Note**: You may discover other operational error patterns as you analyze the traces\n\n- **Performance Problems (High Latency Analysis)**\n  - Filter for OK traces with high latency: `uv run --env-file <env_file_path> python -m mlflow traces search --filter "info.state = \'OK\'" --max-results 10`\n  - **Adjust --max-results as needed**: Start with 10-20, increase if you need more examples to identify patterns\n  - **Pattern Analysis Focus**: Identify WHY traces are slow by examining:\n    - Tool call duration patterns in spans\n    - Number of sequential vs parallel tool calls\n    - Specific slow APIs/tools (database queries, web requests, etc.)\n    - Cold start vs warm execution patterns\n    - Resource contention indicators\n  - Example hypotheses to test:\n    - Complex queries with multiple sequential tool calls have multiplicative latency\n    - Certain tools/APIs are consistent performance bottlenecks (>5s per call)\n    - First queries in sessions are slower due to cold start overhead\n    - Database queries without proper indexing cause delays\n    - Network timeouts or retries inflate execution time\n    - Parallel tool execution is not properly implemented\n  - **Note**: You may discover other performance patterns as you analyze the traces\n\n### 2.3 Quality Issues Analysis (Hypothesis-Driven Approach)\n\n**NOTE: Use MLflow CLI commands for trace exploration - DO NOT use inline Python scripts during this phase**\n\nFocus on response quality, not operational performance:\n\n- **Content Quality Issues**\n  - Sample both OK and ERROR traces\n  - Example hypotheses to test:\n    - Agent ', "start overhead\n    - Database queries without proper indexing cause delays\n    - Network timeouts or retries inflate execution time\n    - Parallel tool execution is not properly implemented\n  - **Note**: You may discover other performance patterns as you analyze the traces\n\n### 2.3 Quality Issues Analysis (Hypothesis-Driven Approach)\n\n**NOTE: Use MLflow CLI commands for trace exploration - DO NOT use inline Python scripts during this phase**\n\nFocus on response quality, not operational performance:\n\n- **Content Quality Issues**\n  - Sample both OK and ERROR traces\n  - Example hypotheses to test:\n    - Agent provides overly verbose responses for simple questions\n    - Some text/information is repeated unnecessarily across responses\n    - Conversation context carries over inappropriately\n    - Agent asks follow-up questions instead of attempting tasks\n    - Responses are inconsistent for similar queries\n    - Agent provides incorrect or outdated information\n    - Response format is inappropriate for the query type\n  - **Note**: You may discover other quality issues as you analyze the traces\n\n### 2.4 Strengths and Successes Analysis (Hypothesis-Driven Approach)\n\n**NOTE: Use MLflow CLI commands for trace exploration - DO NOT use inline Python scripts during this phase**\n\nProcess successful traces to identify what's working well:\n\n- **Successful Interactions**\n\n  - Filter for OK traces with good outcomes\n  - Example hypotheses to test:\n    - Agent provides comprehensive, helpful responses for complex queries\n    - Certain types of questions consistently get high-quality answers\n    - Tool usage is appropriate and effective for specific scenarios\n    - Response format is well-structured for particular use cases\n\n- **Effective Tool Usage**\n\n  - Examine traces where tools are used successfully\n  - Example hypotheses to test:\n    - Agent selects appropriate tools for different query types\n    - Multi-step tool usage produces better outcomes\n    - Certain tool combinations work particularly well together\n\n- **Quality Responses**\n  - Identify traces with good response quality\n  - Example hypotheses to test:\n  ", 'types of questions consistently get high-quality answers\n    - Tool usage is appropriate and effective for specific scenarios\n    - Response format is well-structured for particular use cases\n\n- **Effective Tool Usage**\n\n  - Examine traces where tools are used successfully\n  - Example hypotheses to test:\n    - Agent selects appropriate tools for different query types\n    - Multi-step tool usage produces better outcomes\n    - Certain tool combinations work particularly well together\n\n- **Quality Responses**\n  - Identify traces with good response quality\n  - Example hypotheses to test:\n    - Agent provides right level of detail for complex questions\n    - Safety/important information is appropriately included\n    - Agent successfully handles follow-up questions in context\n\n### 2.5 Generate Final Report\n\n- Ask user where to save the report (markdown file path, e.g., `experiment_analysis.md`)\n- **ONLY NOW use uv inline Python scripts for statistical calculations** - never compute stats manually\n- Inline Python scripts are ONLY for final math/statistics, NOT for trace exploration\n- Use `uv run --env-file <env_file_path> python -c "..."` for any Python calculations that need MLflow access\n- Generate a single comprehensive markdown report with:\n  - **Summary statistics** (computed via `uv run --env-file <env_file_path> python -c "..."` with collected trace data):\n    - Total traces analyzed\n    - Success rate (OK vs ERROR percentage)\n    - Average, median, p95 latency for successful traces\n    - Error rate distribution by duration (fast fails vs timeouts)\n  - **Operational Issues** (errors, latency, performance):\n    - For each confirmed operational hypothesis:\n      - Clear statement of the hypothesis\n      - Example trace IDs that support the hypothesis\n      - BOTH trace input (user request) AND trace output (agent response) excerpts from those traces\n      - Tools called (spans of type "TOOL") and their durations/failures\n      - Root cause analysis: WHY the issue occurs (rate limiting, API failures, timeouts, etc.)\n ', 'by duration (fast fails vs timeouts)\n  - **Operational Issues** (errors, latency, performance):\n    - For each confirmed operational hypothesis:\n      - Clear statement of the hypothesis\n      - Example trace IDs that support the hypothesis\n      - BOTH trace input (user request) AND trace output (agent response) excerpts from those traces\n      - Tools called (spans of type "TOOL") and their durations/failures\n      - Root cause analysis: WHY the issue occurs (rate limiting, API failures, timeouts, etc.)\n      - **Trace assessments**: List specific trace IDs that exhibit this issue with detailed rationales explaining why each trace demonstrates the pattern\n      - Quantitative evidence (frequency, timing patterns, etc.) - computed via Python\n  - **Quality Issues** (content problems, user experience):\n    - For each confirmed quality hypothesis:\n      - Clear statement of the hypothesis\n      - Example trace IDs that support the hypothesis\n      - BOTH trace input (user request) AND trace output (agent response) excerpts from those traces\n      - **Trace assessments**: List specific trace IDs that exhibit this issue with detailed rationales explaining why each trace demonstrates the pattern\n      - Quantitative evidence (frequency, assessment patterns, etc.) - computed via Python\n  - **Refuted Hypotheses** (briefly noted)\n  - Recommendations for improvement based on confirmed issues\n', '# MLflow Claude Code Integration\n\nThis module provides automatic tracing integration between Claude Code and MLflow.\n\n## Module Structure\n\n- **`config.py`** - Configuration management (settings files, environment variables)\n- **`hooks.py`** - Claude Code hook setup and management\n- **`cli.py`** - MLflow CLI commands (`mlflow autolog claude`)\n- **`tracing.py`** - Core tracing logic and processors\n- **`hooks/`** - Hook implementation handlers\n\n## Installation\n\n```bash\npip install mlflow\n```\n\n## Usage\n\nSet up Claude Code tracing in any project directory:\n\n```bash\n# Set up tracing in current directory\nmlflow autolog claude\n\n# Set up tracing in specific directory\nmlflow autolog claude ~/my-project\n\n# Set up with custom tracking URI\nmlflow autolog claude -u file://./custom-mlruns\nmlflow autolog claude -u sqlite:///mlflow.db\n\n# Set up with Databricks\nmlflow autolog claude -u databricks -e 123456789\n\n# Check status\nmlflow autolog claude --status\n\n# Disable tracing\nmlflow autolog claude --disable\n```\n\n## How it Works\n\n1. **Setup**: The `mlflow autolog claude` command configures Claude Code hooks in a `.claude/settings.json` file\n2. **Automatic Tracing**: When you use the `claude` command in the configured directory, your conversations are automatically traced to MLflow\n3. **View Traces**: Use `mlflow ui` to view your conversation traces\n\n## Configuration\n\nThe setup creates two types of configuration:\n\n### Claude Code Hooks\n\n- **PostToolUse**: Captures tool usage during conversations\n- **Stop**: Processes complete conversations into MLflow traces\n\n### Environment Variables\n\n- `MLFLOW_CLAUDE_TRACING_ENABLED=true`: Enables tracing\n- `MLFLOW_TRACKING_URI`: Where to store traces (defaults to local `.claude/mlflow/runs`)\n- `MLFLOW_EXPERIMENT_ID` or `MLFLOW_EXPERIMENT_NAME`: Which experiment to use\n\n## Examples\n\n### Basic Local Setup\n\n```bash\nmlflow autolog claude\ncd .\nclaude "help me write a function"\nmlflow ui --backend-store-uri sqlite:///mlflow.db\n```\n\n### Databricks Integration\n\n```bash\nmlflow autolog claude -u databricks -e 123456789\nclaude "analyze this data"\n# View traces in Databricks\n```\n\n### Custom Project Setup\n\n```bash\nmlflow autolog claude ~/my-ai-project -u sqlite:///mlflow.db -n "My AI Project"\ncd ~/my-ai-project\nclaude "refactor this code"\nmlflow ui --backend-store-uri sqlite:///mlflow.db\n```\n\n## Troubleshooting\n\n### Check Status\n\n```bash\nmlflow autolog claude --status\n```\n\n### Disable Tracing\n\n```bash\nmlflow autolog claude --disable\n```\n\n### View Raw Configuration\n\nThe configuration is stored in `.claude/settings.json`:\n\n```bash\ncat .claude/settings.json\n```\n\n## Requirements\n\n- Python 3.10+ (required by MLflow)\n- MLflow installed (`pip install mlflow`)\n- Claude Code CLI installed\n', '# MLflow Java Client\n\nJava client for [MLflow](https://mlflow.org) REST API.\nSee also the MLflow [Python API](https://mlflow.org/docs/latest/python_api/index.html)\nand [REST API](https://mlflow.org/docs/latest/rest-api.html).\n\n## Requirements\n\n- Java 1.8\n- Maven\n- Run the [MLflow Tracking Server 0.4.2](https://mlflow.org/docs/latest/tracking.html#running-a-tracking-server)\n\n## Build\n\n### Build with tests\n\nThe MLflow Java client tests require that MLflow is on the PATH (to start a local server),\nso it is recommended to run them from within a development conda environment.\n\nTo build a deployable JAR and run tests:\n\n```\nmvn package\n```\n\n## Run\n\nTo run a simple sample.\n\n```\njava -cp target/mlflow-java-client-0.4.2.jar \\\n  com.databricks.mlflow.client.samples.QuickStartDriver http://localhost:5001\n```\n\n## JSON Serialization\n\nMLflow Java client uses [Protobuf](https://developers.google.com/protocol-buffers/) 3.6.0 to serialize the JSON payload.\n\n- [service.proto](../mlflow/protos/service.proto) - Protobuf definition of data objects.\n- [com.databricks.api.proto.mlflow.Service.java](src/main/java/com/databricks/api/proto/mlflow/Service.java) - Generated Java classes of all data objects.\n- [generate_protos.py](generate_protos.py) - One time script to generate Service.java. If service.proto changes you will need to re-run this script.\n- Javadoc can be generated by running `mvn javadoc:javadoc`. The output will be in [target/site/apidocs/index.html](target/site/apidocs/index.html).\n  Here is the javadoc for [Service.java](target/site/apidocs/com/databricks/api/proto/mlflow/Service.html).\n\n## Java Client API\n\nSee [ApiClient.java](src/main/java/org/mlflow/client/ApiClient.java)\nand [Service.java domain objects](src/main/java/org/mlflow/api/proto/mlflow/Service.java).\n\n```\nRun getRun(String runId)\nRunInfo createRun()\nRunInfo createRun(String experimentId)\nRunInfo createRun(String experimentId, String appName)\nRunInfo createRun(CreateRun request)\nList<RunInfo> listRunInfos(String experimentId)\n\n\nList<Experiment> searchExperiments()\nGetExperiment.Response getExperiment(String experimentId)\nOptional<Experiment> getExperimentByName(String experimentName)\nlong createExperiment(String experimentName)\n\nvoid logParam(String runId, String key, String value)\nvoid logMetric(String runId, String key, float value)\nvoid setTerminated(String runId)\nvoid setTerminated(String runId, RunStatus status)\nvoid setTerminated(String runId, RunStatus status, long endTime)\nListArtifacts.Response listArtifacts(String runId, String path)\n```\n\n## Usage\n\n### Java Usage\n\nFor a simple example see [QuickStartDriver.java](src/main/java/org/mlflow/tracking/samples/QuickStartDriver.java).\nFor full examples of API coverage see the [tests](src/test/java/org/mlflow/tracking) such as [MlflowClientTest.java](src/test/java/org/mlflow/tracking/MlflowClientTest.java).\n\n```\npackage org.mlflow.tracking.samples;\n\nimport java.util.List;\nimport java.util.Optional;\n\nimport org.apache.log4j.Level;\nimport org.apache.log4j.LogManager;\n\nimport org.mlflow.api.proto.Service.*;\nimport org.mlflow.tracking.MlflowClient;\n\n/**\n * This is an example application which uses the MLflow Tracking API to create and manage\n * experiments and runs.\n */\npublic class QuickStartDriver {\n  public static void main(String[] args) throws Exception {\n    (new QuickStartDriver()).process(args);\n  }\n\n  void process(String[] args) throws Exception {\n    MlflowClient client;\n    if (args.length < 1) {\n      client = new MlflowClient();\n    } else {\n      client = new MlflowClient(args[0]);\n    }\n\n    boolean verbose = args.length >= 2 && "true".equals(args[1]);\n    if (verbose) {\n      LogManager.getLogger("org.mlflow.client").setLevel(Level.DEBUG);\n    }\n\n    System.out.println("====== createExperiment");\n    String expName ', 'public static void main(String[] args) throws Exception {\n    (new QuickStartDriver()).process(args);\n  }\n\n  void process(String[] args) throws Exception {\n    MlflowClient client;\n    if (args.length < 1) {\n      client = new MlflowClient();\n    } else {\n      client = new MlflowClient(args[0]);\n    }\n\n    boolean verbose = args.length >= 2 && "true".equals(args[1]);\n    if (verbose) {\n      LogManager.getLogger("org.mlflow.client").setLevel(Level.DEBUG);\n    }\n\n    System.out.println("====== createExperiment");\n    String expName = "Exp_" + System.currentTimeMillis();\n    String expId = client.createExperiment(expName);\n    System.out.println("createExperiment: expId=" + expId);\n\n    System.out.println("====== getExperiment");\n    GetExperiment.Response exp = client.getExperiment(expId);\n    System.out.println("getExperiment: " + exp);\n\n    System.out.println("====== searchExperiments");\n    List<Experiment> exps = client.searchExperiments();\n    System.out.println("#experiments: " + exps.size());\n    exps.forEach(e -> System.out.println("  Exp: " + e));\n\n    createRun(client, expId);\n\n    System.out.println("====== getExperiment again");\n    GetExperiment.Response exp2 = client.getExperiment(expId);\n    System.out.println("getExperiment: " + exp2);\n\n    System.out.println("====== getExperiment by name");\n    Optional<Experiment> exp3 = client.getExperimentByName(expName);\n    System.out.println("getExperimentByName: " + exp3);\n  }\n\n  void createRun(MlflowClient client, String expId) {\n    System.out.println("====== createRun");\n\n    // Create run\n    String sourceFile = "MyFile.java";\n    RunInfo runCreated = client.createRun(expId, sourceFile);\n    System.out.println("CreateRun: " + runCreated);\n    String runId = runCreated.getRunUuid();\n\n    // Log parameters\n    client.logParam(runId, "min_samples_leaf", "2");\n    client.logParam(runId, "max_depth", "3");\n\n    // Log metrics\n    client.logMetric(runId, "auc", 2.12F);\n    client.logMetric(runId, "accuracy_score", 3.12F);\n    client.logMetric(runId, "zero_one_loss", 4.12F);\n\n    // Update finished run\n    client.setTerminated(runId, RunStatus.FINISHED);\n\n    // Get run details\n    Run run = client.getRun(runId);\n    System.out.println("GetRun: " + run);\n    client.close();\n  }\n}\n```\n', '# mlflow: R interface for MLflow\n\n[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/mlflow)](https://cran.r-project.org/package=mlflow)\n\n- Install [MLflow](https://mlflow.org/) from R to track experiments\n  locally.\n- Connect to MLflow servers to share experiments with others.\n- Use MLflow to export models that can be served locally and remotely.\n\n## Prerequisites\n\nTo use the MLflow R API, you must install [the MLflow Python package](https://pypi.org/project/mlflow/).\n\n```bash\npip install mlflow\n```\n\nOptionally, you can set the `MLFLOW_PYTHON_BIN` and `MLFLOW_BIN` environment variables to specify\nthe Python and MLflow binaries to use. By default, the R client automatically finds them using\n`Sys.which("python")` and `Sys.which("mlflow")`.\n\n```bash\nexport MLFLOW_PYTHON_BIN=/path/to/bin/python\nexport MLFLOW_BIN=/path/to/bin/mlflow\n```\n\n## Installation\n\nInstall `mlflow` as follows:\n\n```r\ndevtools::install_github("mlflow/mlflow", subdir = "mlflow/R/mlflow")\n```\n\n## Development\n\nInstall the `mlflow` package as follows:\n\n```r\ndevtools::install_github("mlflow/mlflow", subdir = "mlflow/R/mlflow")\n```\n\nThen install the latest released `mlflow` runtime.\n\nHowever, currently, the development runtime of `mlflow` is also\nrequired; which means you also need to download or clone the `mlflow`\nGitHub repo:\n\n```bash\ngit clone https://github.com/mlflow/mlflow\n```\n\nAnd upgrade the runtime to the development version as follows:\n\n```bash\n# Upgrade to the latest development version\npip install -e <local github repo>\n```\n\n## Tracking\n\nMLflow Tracking allows you to logging parameters, code versions,\nmetrics, and output files when running R code and for later visualizing\nthe results.\n\nMLflow allows you to group runs under experiments, which can be useful\nfor comparing runs intended to tackle a particular task. You can create\nand activate a new experiment locally using `mlflow` as follows:\n\n```r\nlibrary(mlflow)\nmlflow_set_experiment("Test")\n```\n\nThen you can list view your experiments from MLflows user interface by\nrunning:\n\n```r\nmlflow_ui()\n```\n\n<img src="tools/readme/mlflow-user-interface.png" class="screenshot" width=520 />\n\nYou can also use a MLflow server to track and share experiments, see\n[running a tracking\nserver](https://www.mlflow.org/docs/latest/tracking.html#running-a-tracking-server),\nand then make use of this server by running:\n\n```r\nmlflow_set_tracking_uri("http://tracking-server:5000")\n```\n\nOnce the tracking url is defined, the experiments will be stored and\ntracked in the specified server which others will also be able to\naccess.\n\n## Projects\n\nAn MLflow Project is a format for packaging data science code in a\nreusable and reproducible way.\n\nMLflow projects can be [explicitly\ncreated](https://www.mlflow.org/docs/latest/projects.html#specifying-projects)\nor implicitly used by running `R` with `mlflow` from the terminal as\nfollows:\n\n```bash\nmlflow run examples/r_wine --entry-point train.R\n```\n\nNotice that is equivalent to running from `examples/r_wine`,\n\n```bash\nRscript -e "mlflow::mlflow_source(\'train.R\')"\n```\n\nand `train.R` performing training and logging as follows:\n\n```r\nlibrary(mlflow)\n\n# read parameters\ncolumn <- mlflow_log_param("column", 1)\n\n# log total rows\nmlflow_log_metric("rows", nrow(iris))\n\n# train model\nmodel <- lm(\n  Sepal.Width ~ x,\n  data.frame(Sepal.Width = iris$Sepal.Width, x = iris[,column])\n)\n\n# log models intercept\nmlflow_log_metric("intercept", model$coefficients[["(Intercept)"]])\n```\n\n### Parameters\n\nYou will often want to parameterize your scripts to support running and\ntracking multiple experiments. You ', 'others will also be able to\naccess.\n\n## Projects\n\nAn MLflow Project is a format for packaging data science code in a\nreusable and reproducible way.\n\nMLflow projects can be [explicitly\ncreated](https://www.mlflow.org/docs/latest/projects.html#specifying-projects)\nor implicitly used by running `R` with `mlflow` from the terminal as\nfollows:\n\n```bash\nmlflow run examples/r_wine --entry-point train.R\n```\n\nNotice that is equivalent to running from `examples/r_wine`,\n\n```bash\nRscript -e "mlflow::mlflow_source(\'train.R\')"\n```\n\nand `train.R` performing training and logging as follows:\n\n```r\nlibrary(mlflow)\n\n# read parameters\ncolumn <- mlflow_log_param("column", 1)\n\n# log total rows\nmlflow_log_metric("rows", nrow(iris))\n\n# train model\nmodel <- lm(\n  Sepal.Width ~ x,\n  data.frame(Sepal.Width = iris$Sepal.Width, x = iris[,column])\n)\n\n# log models intercept\nmlflow_log_metric("intercept", model$coefficients[["(Intercept)"]])\n```\n\n### Parameters\n\nYou will often want to parameterize your scripts to support running and\ntracking multiple experiments. You can define parameters with type under\na `params_example.R` example as follows:\n\n```r\nlibrary(mlflow)\n\n# define parameters\nmy_int <- mlflow_param("my_int", 1, "integer")\nmy_num <- mlflow_param("my_num", 1.0, "numeric")\n\n# log parameters\nmlflow_log_param("param_int", my_int)\nmlflow_log_param("param_num", my_num)\n```\n\nThen run `mlflow run` with custom parameters as\nfollows\n\n    mlflow run tests/testthat/examples/ --entry-point params_example.R -P my_int=10 -P my_num=20.0 -P my_str=XYZ\n\n    === Created directory /var/folders/ks/wm_bx4cn70s6h0r5vgqpsldm0000gn/T/tmpi6d2_wzf for downloading remote URIs passed to arguments of type \'path\' ===\n    === Running command \'source /miniconda2/bin/activate mlflow-da39a3ee5e6b4b0d3255bfef95601890afd80709 && Rscript -e "mlflow::mlflow_source(\'params_example.R\')" --args --my_int 10 --my_num 20.0 --my_str XYZ\' in run with ID \'191b489b2355450a8c3cc9bf96cb1aa3\' ===\n    === Run (ID \'191b489b2355450a8c3cc9bf96cb1aa3\') succeeded ===\n\nRun results that we can view with `mlflow_ui()`.\n\n## Models\n\nAn MLflow Model is a standard format for packaging machine learning\nmodels that can be used in a variety of downstream toolsâ€”for example,\nreal-time serving through a REST API or batch inference on Apache Spark.\nThey provide a convention to save a model in different "flavors" that\ncan be understood by different downstream tools.\n\nTo save a model use `mlflow_save_model()`. For instance, you can add the\nfollowing lines to the previous `train.R` script:\n\n```r\n# train model (...)\n\n# save model\nmlflow_save_model(\n  crate(~ stats::predict(model, .x), model)\n)\n```\n\nAnd trigger a run with that will also save your model as follows:\n\n```bash\nmlflow run train.R\n```\n\nEach MLflow Model is simply a directory containing arbitrary files,\ntogether with an MLmodel file in the root of the directory that can\ndefine multiple flavors that the model can be viewed in.\n\nThe directory containing the model looks as follows:\n\n```r\ndir("model")\n```\n\n    ## [1] "crate.bin" "MLmodel"\n\nand the model definition `model/MLmodel` like:\n\n```r\ncat(paste(readLines("model/MLmodel"), collapse = "\\n"))\n```\n\n   ', 'understood by different downstream tools.\n\nTo save a model use `mlflow_save_model()`. For instance, you can add the\nfollowing lines to the previous `train.R` script:\n\n```r\n# train model (...)\n\n# save model\nmlflow_save_model(\n  crate(~ stats::predict(model, .x), model)\n)\n```\n\nAnd trigger a run with that will also save your model as follows:\n\n```bash\nmlflow run train.R\n```\n\nEach MLflow Model is simply a directory containing arbitrary files,\ntogether with an MLmodel file in the root of the directory that can\ndefine multiple flavors that the model can be viewed in.\n\nThe directory containing the model looks as follows:\n\n```r\ndir("model")\n```\n\n    ## [1] "crate.bin" "MLmodel"\n\nand the model definition `model/MLmodel` like:\n\n```r\ncat(paste(readLines("model/MLmodel"), collapse = "\\n"))\n```\n\n    ## flavors:\n    ##   crate:\n    ##     version: 0.1.0\n    ##     model: crate.bin\n    ## time_created: 18-10-03T22:18:25.25.55\n    ## run_id: 4286a3d27974487b95b19e01b7b3caab\n\nLater on, the R model can be deployed which will perform predictions\nusing\n`mlflow_rfunc_predict()`:\n\n```r\nmlflow_rfunc_predict("model", data = data.frame(x = c(0.3, 0.2)))\n```\n\n    ## Warning in mlflow_snapshot_warning(): Running without restoring the\n    ## packages snapshot may not reload the model correctly. Consider running\n    ## \'mlflow_restore_snapshot()\' or setting the \'restore\' parameter to \'TRUE\'.\n\n    ## 3.400381396714573.40656987651099\n\n    ##        1        2\n    ## 3.400381 3.406570\n\n## Deployment\n\nMLflow provides tools for deployment on a local machine and several\nproduction environments. You can use these tools to easily apply your\nmodels in a production environment.\n\nYou can serve a model by running,\n\n```bash\nmlflow rfunc serve model\n```\n\nwhich is equivalent to\nrunning,\n\n```bash\nRscript -e "mlflow_rfunc_serve(\'model\')"\n```\n\n<img src="tools/readme/mlflow-serve-rfunc.png" class="screenshot" width=520 />\n\nYou can also run:\n\n```bash\nmlflow rfunc predict model data.json\n```\n\nwhich is equivalent to running,\n\n```bash\nRscript -e "mlflow_rfunc_predict(\'model\', \'data.json\')"\n```\n\n## Dependencies\n\nWhen running a project, `mlflow_snapshot()` is automatically called to\ngenerate a `r-dependencies.txt` file which contains a list of required\npackages and versions.\n\nHowever, restoring dependencies is not automatic since it\'s usually an\nexpensive operation. To restore dependencies run:\n\n```r\nmlflow_restore_snapshot()\n```\n\nNotice that the `MLFLOW_SNAPSHOT_CACHE` environment variable can be set\nto a cache directory to improve the time required to restore\ndependencies.\n\n## RStudio\n\nTo enable fast iteration while tracking with MLflow improvements over a\nmodel, [RStudio 1.2.897](https://dailies.rstudio.com/) an ', 'production environment.\n\nYou can serve a model by running,\n\n```bash\nmlflow rfunc serve model\n```\n\nwhich is equivalent to\nrunning,\n\n```bash\nRscript -e "mlflow_rfunc_serve(\'model\')"\n```\n\n<img src="tools/readme/mlflow-serve-rfunc.png" class="screenshot" width=520 />\n\nYou can also run:\n\n```bash\nmlflow rfunc predict model data.json\n```\n\nwhich is equivalent to running,\n\n```bash\nRscript -e "mlflow_rfunc_predict(\'model\', \'data.json\')"\n```\n\n## Dependencies\n\nWhen running a project, `mlflow_snapshot()` is automatically called to\ngenerate a `r-dependencies.txt` file which contains a list of required\npackages and versions.\n\nHowever, restoring dependencies is not automatic since it\'s usually an\nexpensive operation. To restore dependencies run:\n\n```r\nmlflow_restore_snapshot()\n```\n\nNotice that the `MLFLOW_SNAPSHOT_CACHE` environment variable can be set\nto a cache directory to improve the time required to restore\ndependencies.\n\n## RStudio\n\nTo enable fast iteration while tracking with MLflow improvements over a\nmodel, [RStudio 1.2.897](https://dailies.rstudio.com/) an be configured\nto automatically trigger `mlflow_run()` when sourced. This is enabled by\nincluding a `# !source mlflow::mlflow_run` comment at the top of the R\nscript as\nfollows:\n\n<img src="tools/readme/mlflow-source-rstudio.png" class="screenshot" width=520 />\n\n## Contributing\n\nSee the [MLflow contribution guidelines](https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md).\n', '# CLAUDE.md - MLflow Frontend Development\n\nThis file provides guidance to Claude Code when working with the MLflow frontend code in this directory.\n\n**For contribution guidelines, code standards, and additional development information not covered here, please refer to [CONTRIBUTING.md](../../../CONTRIBUTING.md).**\n\n## Consistency is Critical\n\n**IMPORTANT**: Always be consistent with the rest of the repository. This is extremely important!\n\nBefore implementing any feature:\n1. Read through similar files to understand their structure and patterns\n2. Do NOT invent new components if they already exist\n3. Use existing patterns and conventions found in the codebase\n4. Check for similar functionality that already exists\n\n## Development Server\n\n**IMPORTANT**: Always start the development server from the repository root for the best development experience with hot reload:\n\n```bash\n# MUST be run from the repository root\nnohup uv run bash dev/run-dev-server.sh > /tmp/mlflow-dev-server.log 2>&1 &\n\n# Monitor the logs\ntail -f /tmp/mlflow-dev-server.log\n\n# Servers will be available at:\n# - MLflow backend: http://localhost:5000\n# - React frontend: http://localhost:3000 (with hot reload)\n```\n\nThis provides fast edit-refresh for UI development - changes to React components will automatically reload in the browser.\n\n## Available Yarn Scripts\n\nWhen running from the repository root, use this pattern:\n\n```bash\n# Example: Run any yarn command from root\npushd mlflow/server/js && yarn <command>; popd\n```\n\nAvailable scripts:\n\n```bash\n# Development\nyarn start              # Start dev server (port 3000) with hot reload\nyarn build              # Build production bundle\n\n# Testing\nyarn test               # Run Jest tests\nyarn test:watch         # Run tests in watch mode\nyarn test:ci            # Run tests with coverage for CI\n\n# Code Quality\nyarn lint               # Run ESLint\nyarn lint:fix           # Run ESLint with auto-fix\nyarn prettier:check     # Check Prettier formatting\nyarn prettier:fix       # Fix Prettier formatting\nyarn type-check         # Run ', "        # Run tests in watch mode\nyarn test:ci            # Run tests with coverage for CI\n\n# Code Quality\nyarn lint               # Run ESLint\nyarn lint:fix           # Run ESLint with auto-fix\nyarn prettier:check     # Check Prettier formatting\nyarn prettier:fix       # Fix Prettier formatting\nyarn type-check         # Run TypeScript type checking\n\n# Combined Checks\nyarn check-all          # Run all checks (lint, prettier, i18n, type-check)\n\n# Other Commands\nyarn storybook          # Start Storybook for component development\nyarn build-storybook    # Build static Storybook\nyarn i18n:check         # Check i18n translations\n```\n\n### Before Committing\n\n**IMPORTANT**: Always run these checks and fix any remaining issues before committing:\n\n```bash\n# From repository root\npushd mlflow/server/js && yarn check-all; popd\n\n# Fix any issues that are reported\n```\n\n## UI Components and Design System\n\n### Use Databricks Design System Components\n\n**Always use components from `@databricks/design-system` when available.** Do not create custom components if they already exist in the design system.\n\nCommon components include:\n\n- `Button`, `IconButton` - for actions\n- `Input`, `Textarea`, `Select` - for form inputs  \n- `Modal`, `Drawer` - for overlays\n- `Table`, `TableRow`, `TableCell` - for data tables\n- `Tabs`, `TabPane` - for tabbed interfaces\n- `Alert`, `Notification` - for feedback\n- `Spinner`, `Skeleton` - for loading states\n- `Tooltip`, `Popover` - for additional information\n- `Card` - for content containers\n- `Typography` - for text styling\n\nExample import:\n\n```typescript\nimport { Button, Modal, Input } from '@databricks/design-system';\n```\n\n### Theme Usage\n\nUse the design system theme for consistent styling:\n\n```typescript\nimport { useDesignSystemTheme } from '@databricks/design-system';\n\nconst Component = () => {\n  const { theme } = useDesignSystemTheme();\n  \n  return (\n    <div style={{ \n      color: theme.colors.textPrimary,\n      padding: theme.spacing.md,\n      fontSize: theme.typography.fontSizeBase\n    }}>\n  ", 'tabbed interfaces\n- `Alert`, `Notification` - for feedback\n- `Spinner`, `Skeleton` - for loading states\n- `Tooltip`, `Popover` - for additional information\n- `Card` - for content containers\n- `Typography` - for text styling\n\nExample import:\n\n```typescript\nimport { Button, Modal, Input } from \'@databricks/design-system\';\n```\n\n### Theme Usage\n\nUse the design system theme for consistent styling:\n\n```typescript\nimport { useDesignSystemTheme } from \'@databricks/design-system\';\n\nconst Component = () => {\n  const { theme } = useDesignSystemTheme();\n  \n  return (\n    <div style={{ \n      color: theme.colors.textPrimary,\n      padding: theme.spacing.md,\n      fontSize: theme.typography.fontSizeBase\n    }}>\n      Content\n    </div>\n  );\n};\n```\n\n### Spacing Guidelines\n\n**ALWAYS use `theme.spacing` values instead of hard-coded pixel widths.** This ensures consistency and maintainability across the application.\n\n```typescript\n// âœ… GOOD - Use theme spacing\n<div style={{ \n  padding: theme.spacing.md,\n  marginBottom: theme.spacing.lg,\n  gap: theme.spacing.sm \n}} />\n\n// âŒ BAD - Avoid hard-coded pixels\n<div style={{ \n  padding: \'16px\',\n  marginBottom: \'24px\',\n  gap: \'8px\'\n}} />\n```\n\nCommon spacing values:\n- `theme.spacing.xs` - Extra small spacing (4px)\n- `theme.spacing.sm` - Small spacing (8px)\n- `theme.spacing.md` - Medium spacing (16px)\n- `theme.spacing.lg` - Large spacing (24px)\n- `theme.spacing.xl` - Extra large spacing (32px)\n\nFor custom spacing needs, use the spacing function:\n```typescript\n// When you need a specific multiple of the base unit\npadding: theme.spacing(2.5) // 20px (2.5 * 8px base unit)\n```\n\n### Finding the Right Component\n\nWhen looking for a component:\n\n1. First check `@databricks/design-system` imports in existing code\n2. Component names may not be exact (e.g., "dropdown" could be `Select`, `DialogCombobox`, or `DropdownMenu`)\n3. Look at similar UI patterns in the codebase for examples\n4. If multiple matches exist, choose based on the use case\n\n### Discovering Available Components Dynamically\n\nTo see ALL components available in the design system:\n\n```bash\n# From mlflow/server/js directory, check what\'s exported\ncat node_modules/@databricks/design-system/dist/design-system/index.d.ts\n\n# This file lists every component as: export * from \'./ComponentName\';\n# Each line represents a component you can import\n```\n\nThis is the definitive source for available components - more reliable than checking folders since it shows only what\'s publicly exported.\n\n### Viewing Component Documentation in Storybook\n\nYou can use Playwright to view the component documentation and examples in Storybook:\n\n```\nhttps://ui-infra.dev.databricks.com/storybook/js/packages/du-bois/index.html?path=/docs/primitives-<component-name>--docs\n```\n\nFor example:\n- Alert: `https://ui-infra.dev.databricks.com/storybook/js/packages/du-bois/index.html?path=/docs/primitives-alert--docs`\n- Button: `https://ui-infra.dev.databricks.com/storybook/js/packages/du-bois/index.html?path=/docs/primitives-button--docs`\n- Modal: `https://ui-infra.dev.databricks.com/storybook/js/packages/du-bois/index.html?path=/docs/primitives-modal--docs`\n\nUse ', "at similar UI patterns in the codebase for examples\n4. If multiple matches exist, choose based on the use case\n\n### Discovering Available Components Dynamically\n\nTo see ALL components available in the design system:\n\n```bash\n# From mlflow/server/js directory, check what's exported\ncat node_modules/@databricks/design-system/dist/design-system/index.d.ts\n\n# This file lists every component as: export * from './ComponentName';\n# Each line represents a component you can import\n```\n\nThis is the definitive source for available components - more reliable than checking folders since it shows only what's publicly exported.\n\n### Viewing Component Documentation in Storybook\n\nYou can use Playwright to view the component documentation and examples in Storybook:\n\n```\nhttps://ui-infra.dev.databricks.com/storybook/js/packages/du-bois/index.html?path=/docs/primitives-<component-name>--docs\n```\n\nFor example:\n- Alert: `https://ui-infra.dev.databricks.com/storybook/js/packages/du-bois/index.html?path=/docs/primitives-alert--docs`\n- Button: `https://ui-infra.dev.databricks.com/storybook/js/packages/du-bois/index.html?path=/docs/primitives-button--docs`\n- Modal: `https://ui-infra.dev.databricks.com/storybook/js/packages/du-bois/index.html?path=/docs/primitives-modal--docs`\n\nUse Playwright MCP to navigate to these URLs and see live examples, props documentation, and usage patterns.\n\n## Browser Testing with Playwright\n\nFor testing UI changes in a real browser, Claude Code can use the Playwright MCP (Model Context Protocol) integration.\n\n### Checking Playwright MCP Status\n\nTo check if Playwright MCP is available:\n\n- Look for browser testing tools in available MCP functions\n- Try using browser navigation or screenshot capabilities\n\n### Installing Playwright MCP\n\nIf Playwright MCP is not available and you need to test UI changes, you can install it:\n\n```bash\nclaude mcp add playwright npx '@playwright/mcp@latest'\n```\n\n**Note**: After installation, you must restart Claude Code for the integration to be available.\n\n### Using Playwright MCP\n\nOnce installed, you can:\n\n- Navigate to the development server\n- Take screenshots of UI components\n- Interact with forms and buttons\n- Verify UI changes are working correctly\n\nExample workflow:\n\n1. Make changes to React components\n2. Wait for hot reload (automatic)\n3. Use Playwright to navigate to `http://localhost:3000`\n4. Take screenshots or interact with the updated UI\n5. Verify the changes work as expected\n\n## Project Structure\n\n```text\nmlflow/server/js/\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ experiment-tracking/    # Experiment tracking UI\nâ”‚   â”œâ”€â”€ model-registry/         # Model registry UI  \nâ”‚   â”œâ”€â”€ common/                 # Shared components\nâ”‚   â”œâ”€â”€ shared/                 # Shared utilities\nâ”‚   â””â”€â”€ app.tsx          ", "the updated UI\n5. Verify the changes work as expected\n\n## Project Structure\n\n```text\nmlflow/server/js/\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ experiment-tracking/    # Experiment tracking UI\nâ”‚   â”œâ”€â”€ model-registry/         # Model registry UI  \nâ”‚   â”œâ”€â”€ common/                 # Shared components\nâ”‚   â”œâ”€â”€ shared/                 # Shared utilities\nâ”‚   â””â”€â”€ app.tsx                # Main app entry point\nâ”œâ”€â”€ vendor/                     # Third-party dependencies\nâ”œâ”€â”€ package.json               # Dependencies and scripts\nâ”œâ”€â”€ tsconfig.json              # TypeScript configuration\nâ”œâ”€â”€ webpack.config.js          # Webpack bundler config\nâ””â”€â”€ jest.config.js             # Jest test configuration\n```\n\n## Key Technologies\n\n- **React 18**: UI framework\n- **TypeScript**: Type safety\n- **Redux**: State management\n- **Apollo Client**: GraphQL client\n- **Ant Design (antd)**: UI component library\n- **AG-Grid**: Data table component\n- **Jest**: Testing framework\n- **React Testing Library**: Component testing\n- **Webpack**: Module bundler\n\n## Common Tasks\n\n### Adding a New Component\n\n1. Create component file in appropriate directory\n2. Add TypeScript types/interfaces\n3. Write component with hooks (functional components preferred)\n4. Add unit tests in same directory with `.test.tsx` extension\n5. Add to Storybook if it's a reusable component\n\n### Updating GraphQL Queries\n\n1. Modify query in relevant `.graphql` file\n2. Run codegen to update TypeScript types (if configured)\n3. Update components using the query\n\n### Testing Components\n\n```bash\n# Run tests for a specific component\nyarn test ComponentName\n\n# Run tests in watch mode for development\nyarn test --watch\n\n# Update snapshots if needed\nyarn test -u\n```\n\n### Debugging\n\n1. Use React Developer Tools browser extension\n2. Redux DevTools for state debugging\n3. Browser console for network requests\n4. Source maps are enabled in development mode\n\n## Code ", "directory\n2. Add TypeScript types/interfaces\n3. Write component with hooks (functional components preferred)\n4. Add unit tests in same directory with `.test.tsx` extension\n5. Add to Storybook if it's a reusable component\n\n### Updating GraphQL Queries\n\n1. Modify query in relevant `.graphql` file\n2. Run codegen to update TypeScript types (if configured)\n3. Update components using the query\n\n### Testing Components\n\n```bash\n# Run tests for a specific component\nyarn test ComponentName\n\n# Run tests in watch mode for development\nyarn test --watch\n\n# Update snapshots if needed\nyarn test -u\n```\n\n### Debugging\n\n1. Use React Developer Tools browser extension\n2. Redux DevTools for state debugging\n3. Browser console for network requests\n4. Source maps are enabled in development mode\n\n## Code Style\n\n- Use functional components with hooks\n- Prefer TypeScript strict mode\n- Follow existing patterns in the codebase\n- Use meaningful component and variable names\n- Add JSDoc comments for complex logic\n- Keep components small and focused\n\n## Best Practices\n\n### Data Fetching\n\n**Use React Query** for all API calls and data fetching:\n\n```typescript\n// Good: Using React Query\nconst { data, isLoading, error } = useQuery({\n  queryKey: ['experiments', experimentId],\n  queryFn: () => fetchExperiment(experimentId),\n});\n\n// Avoid: Manual fetch in useEffect\n// useEffect(() => { fetch(...) }, [])\n```\n\n### State Management\n\n**Avoid useEffect** when possible. Prefer deriving state with `useMemo`:\n\n```typescript\n// Good: Derive state with useMemo\nconst filteredRuns = useMemo(() => {\n  return runs.filter(run => run.status === 'active');\n}, [runs]);\n\n// Avoid: useEffect to update state\n// useEffect(() => {\n//   setFilteredRuns(runs.filter(run => run.status === 'active'));\n// }, [runs]);\n```\n\nUse `useEffect` only for:\n\n- Side effects (DOM manipulation, subscriptions)\n- Synchronizing with external systems\n- Cleanup operations\n\n## Performance Considerations\n\n- Use React.memo for expensive components\n- Implement virtualization for large lists (AG-Grid handles this)\n- Lazy load routes and heavy components\n", "# Jupter Notebook Trace UI Renderer\n\nThis directory contains a standalone notebook renderer that is built as a separate entry point from the main MLflow application.\n\n## Architecture\n\nThe notebook renderer is configured as a separate webpack entry point that generates its own HTML file and JavaScript bundle, completely independent of the main MLflow application.\n\n### Build Configuration\n\nThe webpack configuration in `craco.config.js` handles the dual-entry setup:\n\n1. **Entry Points**:\n\n   - `main`: The main MLflow application (`src/index.tsx`)\n   - `ml-model-trace-renderer`: The notebook renderer (`src/shared/web-shared/model-trace-explorer/oss-notebook-renderer/index.ts`)\n\n2. **Output Structure**:\n\n   ```\n   build/\n   â”œâ”€â”€ index.html                           # Main app HTML (excludes notebook renderer)\n   â”œâ”€â”€ static/js/main.[hash].js             # Main app bundle\n   â”œâ”€â”€ static/css/main.[hash].css           # Main app styles\n   â””â”€â”€ lib/notebook-trace-renderer/\n       â”œâ”€â”€ index.html                       # Notebook renderer HTML\n       â””â”€â”€ js/ml-model-trace-renderer.[hash].js  # Notebook renderer bundle\n   ```\n\n3. **Path Resolution**:\n   - Main app uses relative paths: `static-files/static/js/...`\n   - Notebook renderer uses absolute paths: `/static-files/lib/notebook-trace-renderer/js/...`\n   - Dynamic chunks use absolute paths: `/static-files/static/...` (via `__webpack_public_path__`)\n\n### Key Configuration Details\n\n#### Separate Entry Configuration\n\n```javascript\nwebpackConfig.entry = {\n  main: webpackConfig.entry, // Preserve original entry as 'main'\n  'ml-model-trace-renderer': path.resolve(\n    __dirname,\n    'src/shared/web-shared/model-trace-explorer/oss-notebook-renderer/index.ts',\n  ),\n};\n```\n\n#### Output Path Functions\n\n```javascript\nwebpackConfig.output = {\n  filename: (pathData) => {\n    return pathData.chunk.name === 'ml-model-trace-renderer'\n      ? 'lib/notebook-trace-renderer/js/[name].[contenthash].js'\n      : 'static/js/[name].[contenthash:8].js';\n  },\n  // ... similar for chunkFilename\n};\n```\n\n#### HTML Plugin Configuration\n\n- **Main app**: Excludes notebook renderer chunks via `excludeChunks: ['ml-model-trace-renderer']`\n- **Notebook renderer**: Includes only its own chunks via `chunks: ['ml-model-trace-renderer']`\n\n#### Runtime Path Override\n\nThe notebook renderer sets `__webpack_public_path__ = '/static-files/'` at ", "Configuration Details\n\n#### Separate Entry Configuration\n\n```javascript\nwebpackConfig.entry = {\n  main: webpackConfig.entry, // Preserve original entry as 'main'\n  'ml-model-trace-renderer': path.resolve(\n    __dirname,\n    'src/shared/web-shared/model-trace-explorer/oss-notebook-renderer/index.ts',\n  ),\n};\n```\n\n#### Output Path Functions\n\n```javascript\nwebpackConfig.output = {\n  filename: (pathData) => {\n    return pathData.chunk.name === 'ml-model-trace-renderer'\n      ? 'lib/notebook-trace-renderer/js/[name].[contenthash].js'\n      : 'static/js/[name].[contenthash:8].js';\n  },\n  // ... similar for chunkFilename\n};\n```\n\n#### HTML Plugin Configuration\n\n- **Main app**: Excludes notebook renderer chunks via `excludeChunks: ['ml-model-trace-renderer']`\n- **Notebook renderer**: Includes only its own chunks via `chunks: ['ml-model-trace-renderer']`\n\n#### Runtime Path Override\n\nThe notebook renderer sets `__webpack_public_path__ = '/static-files/'` at runtime to ensure dynamically loaded chunks use the correct absolute paths.\n\n## Files\n\n- `index.ts`: Entry point that sets webpack public path and bootstraps the renderer\n- `bootstrap.tsx`: Main renderer component\n- `index.html`: HTML template for the standalone renderer\n- `index.css`: Styles for the renderer\n\n## Usage\n\nThe notebook renderer is built automatically as part of the main build process:\n\n```bash\nyarn build\n```\n\nThis generates both the main application and the standalone notebook renderer, accessible at:\n\n- Main app: `/static-files/index.html`\n- Notebook renderer: `/static-files/lib/notebook-trace-renderer/index.html`\n\n## Development Notes\n\n- The renderer is completely independent of the main app - no shared runtime dependencies\n- Uses absolute paths to avoid complex relative path calculations\n- Webpack code splitting works correctly for both entry points\n- CSS extraction is configured separately for each entry point\n", "# MLflow Tracking database migrations\n\nThis directory contains configuration scripts and database migration logic for MLflow tracking\ndatabases, using the Alembic migration library (https://alembic.sqlalchemy.org). To run database\nmigrations, use the `mlflow db upgrade` CLI command. To add and modify database migration logic,\nsee the contributor guide at https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md.\n\nIf you encounter failures while executing migrations, please file a GitHub issue at\nhttps://github.com/mlflow/mlflow/issues.\n\n## Migration descriptions\n\n### 89d4b8295536_create_latest_metrics_table\n\nThis migration creates a `latest_metrics` table and populates it with the latest metric entry for\neach unique `(run_id, metric_key)` tuple. Latest metric entries are computed based on `step`,\n`timestamp`, and `value`.\n\nThis migration may take a long time for databases containing a large number of metric entries. You\ncan determine the total number of metric entries using the following query:\n\n```sql\nSELECT count(*) FROM metrics GROUP BY metrics.key, run_uuid;\n```\n\nAdditionally, query join latency during the migration increases with the number of unique\n`(run_id, metric_key)` tuples. You can determine the total number of unique tuples using\nthe following query:\n\n```sql\nSELECT count(*) FROM (\n   SELECT metrics.key, run_uuid FROM metrics GROUP BY run_uuid, metrics.key\n) unique_metrics;\n```\n\nFor reference, migrating a Tracking database with the following attributes takes roughly\n**three seconds** on MySQL 5.7:\n\n- `3702` unique metrics\n- `466860` total metric entries\n- `186` runs\n- An average of `125` entries per unique metric\n\n#### Recovering from a failed migration\n\nIf the **create_latest_metrics_table** migration fails, simply delete the `latest_metrics`\ntable from your Tracking database as follows:\n\n```sql\nDROP TABLE latest_metrics;\n```\n\nAlembic does not stamp the database with an updated version unless the corresponding migration\ncompletes successfully. Therefore, when this migration fails, the database remains on the\nprevious version, and deleting the `latest_metrics` table is sufficient to restore the database\nto its prior state.\n\nIf the migration fails to complete due to excessive latency, please try executing the\n`mlflow db upgrade` command on the same host machine where the database is running. This will\nreduce the overhead of the migration's queries and batch insert operation.\n", '# Instructions\n\nThis directory contains files to test MLflow tracking operations using the following databases:\n\n- PostgreSQL\n- MySQL\n- Microsoft SQL Server\n- SQLite\n\n## Prerequisites\n\n- Docker\n- Docker Compose V2\n\n## Build Services\n\n```bash\n# Build a service\nservice=mlflow-sqlite\n./tests/db/compose.sh build --build-arg DEPENDENCIES="$(python dev/extract_deps.py)" $service\n\n# Build all services\n./tests/db/compose.sh build --build-arg DEPENDENCIES="$(python dev/extract_deps.py)"\n```\n\n## Run Services\n\n```bash\n# Run a service (`pytest tests/db` is executed by default)\n./tests/db/compose.sh run --rm $service\n\n# Run all services\nfor service in $(./tests/db/compose.sh config --services | grep \'^mlflow-\')\ndo\n  ./tests/db/compose.sh run --rm "$service"\ndone\n\n# Run tests\n./tests/db/compose.sh run --rm $service pytest /path/to/directory/or/script\n\n# Run a python script\n./tests/db/compose.sh run --rm $service python /path/to/script\n```\n\n## Clean Up Services\n\n```bash\n# Clean up containers, networks, and volumes\n./tests/db/compose.sh down --volumes --remove-orphans\n\n# Clean up containers, networks, volumes, and images\n./tests/db/compose.sh down --volumes --remove-orphans --rmi all\n```\n\n## Other Useful Commands\n\n```bash\n# View database logs\n./tests/db/compose.sh logs --follow <database service>\n```\n', '# Adding `examples` unit tests to `pytest` test suite\n\nTwo types of test runs for code in `examples` directory are supported:\n\n- Examples run by `mlflow run`\n- Examples run by another command, such as the `python` interpreter\n\nEach of these types of runs are implemented using `@pytest.mark.parametrize` decorator. Adding a new\nexample to test involves updating the decorator list as described below.\n\nFor purpose of discussion, `new_example_dir` designates the\ndirectory the example code is found, i.e., it is located in `examples/new_example_dir`.\n\n## Examples that utilize `mlflow run` construct\n\nThe `@pytest.mark.mark.parametrize` decorator for `def test_mlflow_run_example(directory, params):`\nis updated.\n\nIf the example is executed by `cd examples/new_example_dir && mlflow run . -P param1=99 -P param2=3`, then\nthis `tuple` is added to the decorator list\n\n```\n("new_example_dir", ["-P", "param1=123", "-P", "param2=99"])\n```\n\nas shown below\n\n```\n@pytest.mark.parametrize(("directory", "params"), [\n    ("sklearn_elasticnet_wine", ["-P", "alpha=0.5"]),\n    (os.path.join("sklearn_elasticnet_diabetes", "linux"), []),\n    ("new_example_dir", ["-P", "param1=123", "-P", "param2=99"]),\n])\ndef test_mlflow_run_example(directory, params):\n```\n\nThe `tuple` for an example requiring no parameters is simply:\n\n```\n("new_example_dir", []),\n```\n\n## Examples that are executed with another command\n\nFor an example that is not run by `mlflow run`, the list in\n`@pytest.mark.parametrize` decorator for `test_command_example(tmpdir, directory, command):` is updated.\n\nExamples invoked by `cd examples/new_example_dir && python train.py` require this tuple added\nto the decorator\'s list\n\n```\n("new_example_dir", ["python", "train.py"]),\n```\n\nas shown below\n\n```\n@pytest.mark.parametrize(("directory", "command"), [\n    (\'sklearn_logistic_regression\', [\'python\', \'train.py\']),\n    (\'h2o\', [\'python\', \'random_forest.py\']),\n    (\'quickstart\', [\'python\', \'mlflow_tracking.py\']),\n    ("new_example_dir", ["python", "train.py"]),\n])\ndef test_command_example(tmpdir, directory, command):\n```\n\nIf the example requires arguments to run, i.e., `python train.py arg1 arg2`, then the\ntuple would look like this\n\n```\n(\'new_example_dir\', [\'python\', \'train.py\', \'arg1\', \'arg2\'])\n```\n', '# Historical Pyfunc Models\n\nThese serialized model files are used in backwards compatibility tests, so we can ensure that models logged with old versions of MLflow are still able to be loaded in newer versions.\n\nThese files were created by running the following:\n\n1. First, install the desired MLflow version with `$ pip install mlflow=={version_number}`\n2. Next, run the following script from MLflow root:\n\n```python\nimport mlflow\n\n\nclass MyModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input):\n        return model_input\n\n\nmodel = MyModel()\n\nmlflow.pyfunc.save_model(\n    python_model=model,\n    path=f"tests/resources/pyfunc_models/{mlflow.__version__}",\n)\n```\n', 'Copyright 2018 Databricks, Inc.  All rights reserved.\n\n\t\t\t\tApache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      "License" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      "Licensor" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      "Legal Entity" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      "control" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      "You" (or "Your") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      "Source" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      "Object" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, ', '  "You" (or "Your") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      "Source" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      "Object" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      "Work" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      "Derivative Works" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      "Contribution" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or ', 'include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      "Contribution" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, "submitted"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as "Not a Contribution."\n\n      "Contributor" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such ', '     on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n  ', '     cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a "NOTICE" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n     ', 'those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a "NOTICE" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n    ', '        notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an "AS IS" ', '6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an "AS IS" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of ', 'to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets "[]"\n      replaced with your own identifying information. (Don\'t include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class ', 'END OF TERMS AND CONDITIONS\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets "[]"\n      replaced with your own identifying information. (Don\'t include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same "printed page" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the "License");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an "AS IS" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n', '# Dev script dependencies\nclick\nruamel.yaml.clib!=0.2.7\nruamel.yaml\nrequests\npackaging\npydantic\npyyaml\ntoml\n', 'https://mlflow.org/docs/latest/auth/index.html\nhttps://mlflow.org/docs/latest/auth/python-api.html\nhttps://mlflow.org/docs/latest/cli.html\nhttps://mlflow.org/docs/latest/deep-learning/keras/quickstart/quickstart_keras.html\nhttps://mlflow.org/docs/latest/deep-learning/pytorch/guide/index.html\nhttps://mlflow.org/docs/latest/deep-learning/tensorflow/guide/index.html\nhttps://mlflow.org/docs/latest/deployment/index.html\nhttps://mlflow.org/docs/latest/getting-started/intro-quickstart/index.html\nhttps://mlflow.org/docs/latest/index.html\nhttps://mlflow.org/docs/latest/introduction/index.html\nhttps://mlflow.org/docs/latest/llms/custom-pyfunc-for-llms/index.html\nhttps://mlflow.org/docs/latest/llms/custom-pyfunc-for-llms/notebooks/custom-pyfunc-advanced-llm.html\nhttps://mlflow.org/docs/latest/llms/custom-pyfunc-for-llms/notebooks/index.html\nhttps://mlflow.org/docs/latest/llms/deployments/guides/index.html\nhttps://mlflow.org/docs/latest/llms/deployments/guides/step1-create-deployments.html\nhttps://mlflow.org/docs/latest/llms/deployments/guides/step2-query-deployments.html\nhttps://mlflow.org/docs/latest/llms/deployments/index.html\nhttps://mlflow.org/docs/latest/llms/deployments/uc_integration.html\nhttps://mlflow.org/docs/latest/llms/index.html\nhttps://mlflow.org/docs/latest/llms/langchain/autologging.html\nhttps://mlflow.org/docs/latest/llms/langchain/guide/index.html\nhttps://mlflow.org/docs/latest/llms/langchain/index.html\nhttps://mlflow.org/docs/latest/llms/langchain/notebooks/langchain-quickstart.html\nhttps://mlflow.org/docs/latest/llms/llama-index/index.html\nhttps://mlflow.org/docs/latest/llms/llm-evaluate/index.html\nhttps://mlflow.org/docs/latest/llms/openai/guide/index.html\nhttps://mlflow.org/docs/latest/llms/openai/index.html\nhttps://mlflow.org/docs/latest/llms/sentence-transformers/guide/index.html\nhttps://mlflow.org/docs/latest/llms/sentence-transformers/index.html\nhttps://mlflow.org/docs/latest/llms/tracing/index.html\nhttps://mlflow.org/docs/latest/llms/tracing/overview.html\nhttps://mlflow.org/docs/latest/llms/transformers/index.html\nhttps://mlflow.org/docs/latest/model-evaluation/index.html\nhttps://mlflow.org/docs/latest/model-registry.html\nhttps://mlflow.org/docs/latest/model/dependencies.html\nhttps://mlflow.org/docs/latest/model/notebooks/signature_examples.html\nhttps://mlflow.org/docs/latest/model/signatures.html\nhttps://mlflow.org/docs/latest/models.html\nhttps://mlflow.org/docs/latest/python_api/index.html\nhttps://mlflow.org/docs/latest/rest-api.html\nhttps://mlflow.org/docs/latest/system-metrics/index.html\nhttps://mlflow.org/docs/latest/tracking.html\nhttps://mlflow.org/docs/latest/tracking/artifacts-stores.html\nhttps://mlflow.org/docs/latest/tracking/autolog.html\nhttps://mlflow.org/docs/latest/tracking/backend-stores.html\nhttps://mlflow.org/docs/latest/tracking/data-api.html\nhttps://mlflow.org/docs/latest/tracking/server.html\nhttps://mlflow.org/docs/latest/tracking/tracking-api.html\nhttps://mlflow.org/docs/latest/tracking/tutorials/local-database.html\nhttps://mlflow.org/docs/latest/tracking/tutorials/remote-server.html', 'promptflow\njinja2\n', '../../LICENSE.txt', '../../LICENSE.txt', '# classification\npyspark.ml.classification.LinearSVCModel\npyspark.ml.classification.DecisionTreeClassificationModel\npyspark.ml.classification.GBTClassificationModel\npyspark.ml.classification.LogisticRegressionModel\npyspark.ml.classification.RandomForestClassificationModel\npyspark.ml.classification.NaiveBayesModel\n\n# clustering\npyspark.ml.clustering.BisectingKMeansModel\npyspark.ml.clustering.KMeansModel\npyspark.ml.clustering.GaussianMixtureModel\n\n# Regression\npyspark.ml.regression.AFTSurvivalRegressionModel\npyspark.ml.regression.DecisionTreeRegressionModel\npyspark.ml.regression.GBTRegressionModel\npyspark.ml.regression.GeneralizedLinearRegressionModel\npyspark.ml.regression.LinearRegressionModel\npyspark.ml.regression.RandomForestRegressionModel\n\n# Featurizer model\npyspark.ml.feature.BucketedRandomProjectionLSHModel\npyspark.ml.feature.ChiSqSelectorModel\npyspark.ml.feature.CountVectorizerModel\npyspark.ml.feature.IDFModel\npyspark.ml.feature.ImputerModel\npyspark.ml.feature.MaxAbsScalerModel\npyspark.ml.feature.MinHashLSHModel\npyspark.ml.feature.MinMaxScalerModel\npyspark.ml.feature.OneHotEncoderModel\npyspark.ml.feature.RobustScalerModel\npyspark.ml.feature.RFormulaModel\npyspark.ml.feature.StandardScalerModel\npyspark.ml.feature.StringIndexerModel\npyspark.ml.feature.VarianceThresholdSelectorModel\npyspark.ml.feature.VectorIndexerModel\npyspark.ml.feature.UnivariateFeatureSelectorModel\n\n# composite model\npyspark.ml.classification.OneVsRestModel\n\n# pipeline model\npyspark.ml.pipeline.PipelineModel\n\n# Hyper-parameter tuning\npyspark.ml.tuning.CrossValidatorModel\npyspark.ml.tuning.TrainValidationSplitModel\n\n# SynapeML models\nsynapse.ml.cognitive.*\nsynapse.ml.exploratory.*\nsynapse.ml.featurize.*\nsynapse.ml.geospatial.*\nsynapse.ml.image.*\nsynapse.ml.io.*\nsynapse.ml.isolationforest.*\nsynapse.ml.lightgbm.*\nsynapse.ml.nn.*\nsynapse.ml.opencv.*\nsynapse.ml.stages.*\nsynapse.ml.vw.*\n', 'pytest==8.4.0\n# transformers 4.51.0 has this issue:\n# https://github.com/huggingface/transformers/issues/37326\ntransformers!=4.51.0\n# https://github.com/BerriAI/litellm/issues/10373\nlitellm!=1.67.4\n# https://github.com/run-llama/llama_index/issues/18587\nllama-index-core!=0.12.34\n# https://github.com/mangiucugna/json_repair/issues/124\njson_repair!=0.45.0\n# https://github.com/huggingface/transformers/issues/38269\ntransformers!=4.52.2\ntransformers!=4.52.1\n# TODO(https://github.com/mlflow/mlflow/issues/15847): Remove this constraint when MLflow is ready for pyspark 4.0.0. Pyspark 3.5.6 has the same issue.\npyspark<3.5.6\n', '-r extra-ml-requirements.txt\n-r test-requirements.txt\n-r lint-requirements.txt\n-r doc-requirements.txt\n', '# Minimum version that works with Python 3.10\nsphinx==4.2.0\njinja2==3.0.3\n# to be compatible with jinja2==3.0.3\nflask<=2.2.5\nsphinx-autobuild\nsphinx-click\n# to be compatible with docutils==0.16\nsphinx-tabs==3.2.0\n# redirect handling\nsphinx-reredirects==0.1.3\n# Pin sphinxcontrib packages. Their newer versions are incompatible with sphinx==4.2.0.\nsphinxcontrib-applehelp<1.0.8\nsphinxcontrib-devhelp<1.0.6\nsphinxcontrib-htmlhelp<2.0.4\nsphinxcontrib-serializinghtml<1.1.10\nsphinxcontrib-qthelp<1.0.7\n', '-r doc-min-requirements.txt\ntensorflow-cpu<=2.12.0; platform_system!="Darwin" or platform_machine!="arm64"\ntensorflow-macos<=2.12.0; platform_system=="Darwin" and platform_machine=="arm64"\npyspark\ndatasets\n# nbsphinx and ipython are required for jupyter notebook rendering\nnbsphinx==0.8.8\n# ipython 8.7.0 is an incompatible release\nipython!=8.7.0\nkeras\ntorch>=1.11.0\ntorchvision>=0.12.0\nlightning>=1.8.1\nscrapy\nipywidgets>=8.1.1\n# incremental==24.7.0 requires setuptools>=61.0, which causes https://github.com/mlflow/mlflow/issues/8635\nincremental<24.7.0\n# this is an extra dependency for the auth app which\n# is not included in the core mlflow requirements\nFlask-WTF<2\n# required for testing polars dataset integration\npolars>=1\n# required for the genai evaluation example\nopenai\n', '## This file describes extra ML library dependencies that you, as an end user,\n## must install in order to use various MLflow Python modules.\n# Required by mlflow.spacy\n# TODO: Remove `<3.8` once we bump the minimim supported python version of MLflow to 3.9.\nspacy>=3.3.0,<3.8\n# Required by mlflow.tensorflow\ntensorflow>=2.10.0; platform_system!="Darwin" or platform_machine!="arm64"\ntensorflow-macos>=2.10.0; platform_system=="Darwin" and platform_machine=="arm64"\n# Required by mlflow.pytorch\ntorch>=1.11.0\ntorchvision>=0.12.0\nlightning>=1.8.1\n# Required by mlflow.xgboost\nxgboost>=0.82\n# Required by mlflow.lightgbm\nlightgbm\n# Required by mlflow.catboost\ncatboost\n# Required by mlflow.statsmodels\nstatsmodels\n# Required by mlflow.h2o\nh2o\n# Required by mlflow.onnx\nonnx>=1.11.0\nonnxruntime\ntf2onnx\n# Required by mlflow.spark and using Delta with MLflow Tracking datasets\npyspark\n# Required by mlflow.paddle\npaddlepaddle\n# Required by mlflow.prophet\n# NOTE: Prophet\'s whl build process will fail with dependencies not being present.\n#   Installation will default to setup.py in order to install correctly.\n#   To install in dev environment, ensure that gcc>=8 is installed to allow pystan\n#   to compile the model binaries. See: https://gcc.gnu.org/install/\n# Avoid 0.25 due to https://github.com/dr-prodigy/python-holidays/issues/1200\nholidays!=0.25\nprophet\n# Required by mlflow.shap\n# and shap evaluation functionality\nshap>=0.42.1\n# Required by mlflow.pmdarima\npmdarima\n# Required by mlflow.diviner\ndiviner\n# Required for using Hugging Face datasets with MLflow Tracking\n# Avoid datasets < 2.19.1 due to an incompatibility issue https://github.com/huggingface/datasets/issues/6737\ndatasets>=2.19.1\n# Required by mlflow.transformers\ntransformers\nsentencepiece\nsetfit\nlibrosa\nffmpeg\naccelerate\n# Required by mlflow.openai\nopenai\ntiktoken\ntenacity\n# Required by mlflow.llama_index\nllama_index\n# Required for an agent example of mlflow.llama_index\nllama-index-agent-openai\n# Required by mlflow.langchain\nlangchain\n# Required by mlflow.promptflow\npromptflow\n# Required by mlflow.sentence_transformers\nsentence-transformers\n# Required by mlflow.anthropic\nanthropic\n# Required by mlflow.ag2\nag2\n# Required by mlflow.dspy\n# In dspy 2.6.9, `dspy.__name__` is not \'dspy\', but \'dspy.__metadata__\',\n# which causes auto-logging tests to fail.\ndspy!=2.6.9\n# Required by mlflow.litellm\nlitellm\n# Required by mlflow.gemini\ngoogle-genai\n# Required by mlflow.groq\ngroq\n# Required by mlflow.mistral\nmistralai\n# Required by mlflow.autogen\nautogen-agentchat\n# Required by mlflow.semantic_kernel\nsemantic-kernel\n# Required by mlflow.agno\nagno\n# Required by mlflow.strands\nstrands-agents\n', 'ruff==0.12.10\nblack[jupyter]==23.7.0\nblacken-docs==1.18.0\npre-commit==4.0.1\ntoml==0.10.2\nmypy==1.17.1\npytest==8.4.0\npydantic==2.11.7\n-e ./dev/clint\n', '## Test-only dependencies\npytest\npytest-cov\n', '## Dependencies required to run tests\n# Required for testing utilities for parsing pip requirements\npip>=20.1\n## Test-only dependencies\npytest\npytest-asyncio\npytest-repeat\npytest-cov\npytest-timeout\npytest-localserver==0.5.0\nmoto>=4.2.0,<5,!=4.2.5\nazure-storage-blob>=12.0.0\nazure-storage-file-datalake>=12.9.1\nazure-identity>=1.6.1\npillow\nplotly\nkaleido\n# Required by tuning tests\nhyperopt\n# Required by evaluator tests\nshap\n# Required to evaluate language models in `mlflow.evaluate`\nevaluate\nnltk\nrouge_score\ntextstat\ntiktoken\n# Required by progress bar tests\nipywidgets\ntqdm\n# Required for LLM eval in `mlflow.evaluate`\nopenai\n# Required for showing pytest stats\npsutil\n# SQLAlchemy == 2.0.25 requires typing_extensions >= 4.6.0\ntyping_extensions>=4.6.0\n# Required for importing boto3 ClientError directly for testing\nbotocore>=1.34\npyspark\n# Required for testing the opentelemetry exporter of tracing\nopentelemetry-exporter-otlp-proto-grpc\nopentelemetry-exporter-otlp-proto-http\n# Required for testing mlflow.server.auth\nFlask-WTF<2\n# required for testing polars dataset integration\npolars>=1\n# required for testing mlflow.genai.optimize_prompt\ndspy\n', 'promptflow[azure]\npromptflow-tools\npython-dotenv\n', 'mlflow\ncloudpickle==2.2.1\nscikit-learn==1.5.2\n', 'bcrypt==3.2.0\ncloudpickle==2.0.0\nconfigparser==5.2.0\ncryptography==39.0.1\ndatabricks-feature-engineering==0.2.1\ndatabricks-rag-studio==0.2.0.dev0\nentrypoints==0.4\ngoogle-cloud-storage==2.11.0\ngrpcio-status==1.48.1\nlangchain==0.1.20\nmlflow[gateway]==2.12.2\nnumpy==1.23.5\npackaging==23.2\npandas==1.5.3\nprotobuf==4.24.0\npsutil==5.9.0\npyarrow==8.0.0\npydantic==1.10.6\npyyaml==6.0\nrequests==2.28.1\ntornado==6.1\n', 'bcrypt==3.2.0\ncloudpickle==2.0.0\nconfigparser==5.2.0\ncryptography==39.0.1\ndatabricks-feature-engineering==0.2.1\nentrypoints==0.4\ngoogle-cloud-storage==2.11.0\ngrpcio-status==1.48.1\nlangchain==0.1.20\nmlflow[gateway]==2.12.2\nnumpy==1.23.5\npackaging==23.2\npandas==1.5.3\nprotobuf==4.24.0\npsutil==5.9.0\npyarrow==8.0.0\npydantic==1.10.6\npyyaml==6.0\nrequests==2.28.1\ntornado==6.1\n', 'mlflow\nscikit-learn==1.4.2\n', 'mlflow\nscikit-learn==1.4.2\n', 'mlflow==2.7.1\ncloudpickle==2.2.1\n', 'mlflow==2.8.1\ncloudpickle==2.2.1\n', "MLflow Contributor Covenant Code of Conduct\n===========================================\n\n.. contents:: **Table of Contents**\n  :local:\n  :depth: 4\n\nOur Pledge\n##########\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\nOur Standards\n#############\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a professional setting\n\nOur Responsibilities\n####################\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\nScope\n#####\n\nThis Code of Conduct applies both within project spaces and in public spaces\nwhen an individual is representing the project or its community. Examples of\nrepresenting a project or community include using an official project e-mail\naddress, posting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event. Representation of a project may be\nfurther defined and clarified by project maintainers.\n\nEnforcement\n###########\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the Technical Steering Committee defined `here <https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#governance>`_.\nAll complaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter ", "individual is representing the project or its community. Examples of\nrepresenting a project or community include using an official project e-mail\naddress, posting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event. Representation of a project may be\nfurther defined and clarified by project maintainers.\n\nEnforcement\n###########\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the Technical Steering Committee defined `here <https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#governance>`_.\nAll complaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\nAttribution\n###########\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq\n", '=========================\nExtra MLflow Dependencies\n=========================\n\nWhen you `install the MLflow Python package <https://mlflow.org/docs/latest/quickstart.html#installing-mlflow>`_,\na set of core dependencies needed to use most MLflow functionality (tracking, projects, models APIs)\nis also installed.\n\nHowever, in order to use certain framework-specific MLflow APIs or configuration options,\nyou need to install additional, "extra" dependencies. For example, the model persistence APIs under\nthe ``mlflow.sklearn`` module require scikit-learn to be installed. Some of the most common MLflow\nextra dependencies can be installed via ``pip install mlflow[extras]``.\n\nThe full set of extra dependencies are documented, along with the modules that depend on them,\nin the following files:\n\n* extra-ml-requirements.txt: ML libraries needed to use model persistence and inference APIs\n* test-requirements.txt: Libraries required to use non-default artifact-logging and tracking server configurations\n', '\nThis document is a hands-on manual for doing issue and pull request triage for `MLflow issues\non GitHub <https://github.com/mlflow/mlflow/issues>`_ .\nThe purpose of triage is to speed up issue management and get community members faster responses.\n\nIssue and pull request triage has three steps:\n\n- assign one or more process labels (e.g. ``needs design`` or ``help wanted``),\n- mark a priority, and\n- label one or more relevant areas, languages, or integrations to help route issues to appropriate contributors or reviewers.\n\nThe remainder of the document describes the labels used in each of these steps and how to apply them.\n\nAssign appropriate process labels\n#######\nAssign at least one process label to every issue you triage.\n\n- ``needs author feedback``: We need input from the author of the issue or PR to proceed.\n- | ``needs design``: This feature is large or tricky enough that we think it warrants a design doc\n  | and review before someone begins implementation.\n- | ``needs committer feedback``: The issue has a design that is ready for committer review, or there is\n  | an issue or pull request that needs feedback from a committer about the approach or appropriateness\n  | of the contribution.\n- | ``needs review``: Use this label for issues that need a more detailed design review or pull\n  | requests ready for review (all questions answered, PR updated if requests have been addressed,\n  | tests passing).\n- ``help wanted``: We would like community help for this issue.\n- ``good first issue``: This would make a good first issue.\n\n\nAssign priority\n#######\n\nYou should assign a priority to each issue you triage. We use `kubernetes-style <https://github.com/\nkubernetes/community/blob/master/contributors/guide/issue-triage.md#define-priority>`_ priority\nlabels.\n\n- | ``priority/critical-urgent``: This is the highest priority and should be worked on by\n  | somebody right now. This should typically be reserved for things like security bugs,\n  | regressions, release blockers.\n- | ``priority/important-soon``: The issue is worked on by the community currently or will\n  | be very soon, ideally in time for the next release.\n- | ``priority/important-longterm``: Important over the long term, but may not be staffed or\n  | may need multiple releases to complete. Also used for things we know are on a\n  ', "priority to each issue you triage. We use `kubernetes-style <https://github.com/\nkubernetes/community/blob/master/contributors/guide/issue-triage.md#define-priority>`_ priority\nlabels.\n\n- | ``priority/critical-urgent``: This is the highest priority and should be worked on by\n  | somebody right now. This should typically be reserved for things like security bugs,\n  | regressions, release blockers.\n- | ``priority/important-soon``: The issue is worked on by the community currently or will\n  | be very soon, ideally in time for the next release.\n- | ``priority/important-longterm``: Important over the long term, but may not be staffed or\n  | may need multiple releases to complete. Also used for things we know are on a\n  | contributor's roadmap in the next few months. We can use this in conjunction with\n  | ``help wanted`` to mark issues we would like to get help with. If someone begins actively\n  | working on an issue with this label and we think it may be merged by the next release, change\n  | the priority to ``priority/important-soon``.\n- | ``priority/backlog``: We believe it is useful but don't see it being prioritized in the\n  | next few months. Use this for issues that are lower priority than ``priority/important-longterm``.\n  | We welcome community members to pick up a ``priority/backlog`` issue, but there may be some\n  | delay in getting support through design review or pull request feedback.\n- | ``priority/awaiting-more-evidence``: Lowest priority. Possibly useful, but not yet enough\n  | support to actually get it done. This is a good place to put issues that could be useful but\n  | require more evidence to demonstrate broad value. Don't use it as a way to say no.\n  | If we think it doesn't fit in MLflow, we should just say that and why.\n\nLabel relevant areas\n#######\n\nAssign one more labels for relevant component or interface surface areas, languages, or\nintegrations. As a principle, we aim to have the minimal set of labels needed to help route issues\nand PRs to appropriate contributors. For example, a ``language/python`` label would not be\nparticularly helpful for routing issues to committers, since most PRs involve Python code.\n``language/java`` and ``language/r`` make sense to have, as the clients ", 'but\n  | require more evidence to demonstrate broad value. Don\'t use it as a way to say no.\n  | If we think it doesn\'t fit in MLflow, we should just say that and why.\n\nLabel relevant areas\n#######\n\nAssign one more labels for relevant component or interface surface areas, languages, or\nintegrations. As a principle, we aim to have the minimal set of labels needed to help route issues\nand PRs to appropriate contributors. For example, a ``language/python`` label would not be\nparticularly helpful for routing issues to committers, since most PRs involve Python code.\n``language/java`` and ``language/r`` make sense to have, as the clients in these languages differ from the Python client and aren\'t maintained by many people. As with process labels, we\ntake inspiration from Kubernetes on naming conventions.\n\nComponents\n""""""""\n- ``area/artifacts``: Artifact stores and artifact logging\n- ``area/build``: Build and test infrastructure for MLflow\n- ``area/docs``: MLflow documentation pages\n- ``area/evaluation``: MLflow model evaluation features, evaluation metrics, and evaluation workflows\n- ``area/examples``: Example code\n- ``area/gateway``: AI Gateway service, Gateway client APIs, third-party Gateway integrations\n- ``area/model-registry``: Model Registry service, APIs, and the fluent client calls for Model Registry\n- ``area/models``: MLmodel format, model serialization/deserialization, flavors\n- ``area/projects``: MLproject format, project execution backends\n- ``area/prompt``: MLflow prompt engineering features, prompt templates, and prompt management\n- ``area/scoring``: MLflow Model server, model deployment tools, Spark UDFs\n- ``area/server-infra``: MLflow Tracking server backend\n- ``area/tracing``: MLflow Tracing features, tracing APIs, and LLM tracing functionality\n- ``area/tracking``: Tracking Service, tracking client APIs, autologging\n\nInterface Surface\n""""""""\n- ``area/uiux``: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- ``area/docker``: Docker use across MLflow\'s components, such as MLflow Projects and MLflow Models\n- ``area/sqlalchemy``: Use of SQLAlchemy in the Tracking Service or Model Registry\n- ``area/windows``: Windows support\n\nLanguage Surface\n""""""""\n- ``language/r``: R APIs and clients\n- ``language/java``: Java APIs and clients\n- ``language/new``: Proposals for new client languages\n\nIntegrations\n""""""""\n- ``integrations/azure``: Azure and Azure ML integrations\n- ``integrations/sagemaker``: SageMaker integrations\n- ``integrations/databricks``: Databricks integrations\n', "Dockerized Model Training with MLflow\n-------------------------------------\nThis directory contains an MLflow project that trains a linear regression model on the UC Irvine\nWine Quality Dataset. The project uses a Docker image to capture the dependencies needed to run\ntraining code. Running a project in a Docker environment (as opposed to Conda) allows for capturing\nnon-Python dependencies, e.g. Java libraries. In the future, we also hope to add tools to MLflow\nfor running Dockerized projects e.g. on a Kubernetes cluster for scale out.\n\nStructure of this MLflow Project\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThis MLflow project contains a ``train.py`` file that trains a scikit-learn model and uses\nMLflow Tracking APIs to log the model and its metadata (e.g., hyperparameters and metrics)\nfor later use and reference. ``train.py`` operates on the Wine Quality Dataset, which is included\nin ``wine-quality.csv``.\n\nMost importantly, the project also includes an ``MLproject`` file, which specifies the Docker\ncontainer environment in which to run the project using the ``docker_env`` field:\n\n.. code-block:: yaml\n\n  docker_env:\n    image:  mlflow-docker-example\n\nHere, ``image`` can be any valid argument to ``docker run``, such as the tag, ID or URL of a Docker\nimage (see `Docker docs <https://docs.docker.com/engine/reference/run/#general-form>`_). The above\nexample references a locally-stored image (``mlflow-docker-example``) by tag.\n\nFinally, the project includes a ``Dockerfile`` that is used to build the image referenced by the\n``MLproject`` file. The ``Dockerfile`` specifies library dependencies required by the project, such\nas ``mlflow`` and ``scikit-learn``.\n\nRunning this Example\n^^^^^^^^^^^^^^^^^^^^\n\nFirst, install MLflow (via ``pip install mlflow``) and install\n`Docker <https://www.docker.com/get-started>`_.\n\nThen, build the image for the project's Docker container environment. You must use the same image\nname that is given by the ``docker_env.image`` field of the MLproject file. In this example, the\nimage name is ``mlflow-docker-example``. Issue the following command to build an image with this\nname:\n\n.. code-block:: bash\n\n  docker build -t mlflow-docker-example -f Dockerfile .\n\nNote that the name if the image used in the ``docker build`` command, ``mlflow-docker-example``,\nmatches the name of the image referenced in the ``MLproject`` file.\n\nFinally, run the example project using ``mlflow run examples/docker -P alpha=0.5``.\n\n.. note::\n    If running this example on a Mac with Apple silicon, ensure that Docker Desktop is running and\n    that you are logged in to the Docker Desktop service.\n    If ", "MLproject file. In this example, the\nimage name is ``mlflow-docker-example``. Issue the following command to build an image with this\nname:\n\n.. code-block:: bash\n\n  docker build -t mlflow-docker-example -f Dockerfile .\n\nNote that the name if the image used in the ``docker build`` command, ``mlflow-docker-example``,\nmatches the name of the image referenced in the ``MLproject`` file.\n\nFinally, run the example project using ``mlflow run examples/docker -P alpha=0.5``.\n\n.. note::\n    If running this example on a Mac with Apple silicon, ensure that Docker Desktop is running and\n    that you are logged in to the Docker Desktop service.\n    If you are modifying the example ``DockerFile`` to specify older versions of ``scikit-learn``,\n    you should enable `Rosetta compatibility <https://docs.docker.com/desktop/settings/mac/#features-in-development>`_\n    in the Docker Desktop configuration settings to ensure that the appropriate ``cython`` compiler is used.\n\nWhat happens when the project is run?\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nRunning ``mlflow run examples/docker`` builds a new Docker image based on ``mlflow-docker-example``\nthat also contains our project code. The resulting image is tagged as\n``mlflow-docker-example-<git-version>`` where ``<git-version>`` is the git commit ID. After the image is\nbuilt, MLflow executes the default (main) project entry point within the container using ``docker run``.\n\nEnvironment variables, such as ``MLFLOW_TRACKING_URI``, are propagated inside the container during\nproject execution. When running against a local tracking URI, MLflow mounts the host system's\ntracking directory (e.g., a local ``mlruns`` directory) inside the container so that metrics and\nparams logged during project execution are accessible afterwards.\n", 'How To Train and Deploy Image Classifier with MLflow and Keras\n--------------------------------------------------------------\n\nIn this example we demonstrate how to train and deploy image classification models with MLflow.\nWe train a VGG16 deep learning model to classify flower species from photos using a `dataset\n<http://download.tensorflow.org/example_images/flower_photos.tgz>`_ available from `tensorflow.org\n<http://www.tensorflow.org>`_. Note that although we use Keras to train the model in this case,\na similar approach can be applied to other deep learning frameworks such as ``PyTorch``.\n\nThe MLflow model produced by running this example can be deployed to any MLflow supported endpoints.\nAll the necessary image preprocessing is packaged with the model. The model can therefore be applied\nto image data directly. All that is required in order to pass new data to the model is to encode the\nimage binary data as base64 encoded string in pandas DataFrame (standard interface for MLflow python\nfunction models). The included Python scripts demonstrate how the model can be deployed to a REST\nAPI endpoint for realtime evaluation or to Spark for batch scoring..\n\nIn order to include custom image pre-processing logic with the model, we define the model as a\ncustom python function model wrapping around the underlying Keras model. The wrapper provides\nnecessary preprocessing to convert input data into multidimensional arrays expected by the\nKeras model. The preprocessing logic is stored with the model as a code dependency. Here is an\nexample of the output model directory layout:\n\n.. code-block:: bash\n\n   tree model\n\n::\n\n   model\n   â”œâ”€â”€ MLmodel\n   â”œâ”€â”€ code\n   â”‚\xa0\xa0 â””â”€â”€ image_pyfunc.py\n   â”œâ”€â”€ data\n   â”‚\xa0\xa0 â””â”€â”€ image_model\n   â”‚\xa0\xa0     â”œâ”€â”€ conf.yaml\n   â”‚\xa0\xa0     â””â”€â”€ keras_model\n   â”‚\xa0\xa0         â”œâ”€â”€ MLmodel\n   â”‚\xa0\xa0         â”œâ”€â”€ conda.yaml\n   â”‚\xa0\xa0         â””â”€â”€ model.h5\n   â””â”€â”€ mlflow_env.yml\n\n\n\nThe example contains the following files:\n\n * MLproject\n   Contains definition of this project. Contains only one entry point to train the model.\n\n * conda.yaml\n   Defines project dependencies. NOTE: You might want to change tensorflow package to ', '  â”‚\xa0\xa0     â”œâ”€â”€ conf.yaml\n   â”‚\xa0\xa0     â””â”€â”€ keras_model\n   â”‚\xa0\xa0         â”œâ”€â”€ MLmodel\n   â”‚\xa0\xa0         â”œâ”€â”€ conda.yaml\n   â”‚\xa0\xa0         â””â”€â”€ model.h5\n   â””â”€â”€ mlflow_env.yml\n\n\n\nThe example contains the following files:\n\n * MLproject\n   Contains definition of this project. Contains only one entry point to train the model.\n\n * conda.yaml\n   Defines project dependencies. NOTE: You might want to change tensorflow package to tensorflow-gpu\n   if you have gpu(s) available.\n\n * train.py\n   Main entry point of the projects. Handles command line arguments and possibly downloads the\n   dataset.\n\n * image_pyfunc.py\n   The implementation of the model train and also of the outputed custom python flavor model. Note\n   that the same preprocessing code that is used during model training is packaged with the output\n   model and is used during scoring.\n\n * score_images_rest.py\n   Score an image or a directory of images using a model deployed to a REST endpoint.\n\n * score_images_spark.py\n   Score an image or a directory of images using model deployed to Spark.\n\n\n\nRunning this Example\n^^^^^^^^^^^^^^^^^^^^\n\nTo train the model, run the example as a standard MLflow project:\n\n\n.. code-block:: bash\n\n    mlflow run examples/flower_classifier\n\nThis will download the training dataset from ``tensorflow.org``, train a classifier using Keras and\nlog results with MLflow.\n\nTo test your model, run the included scoring scripts. For example, say your model was trained with\nrun_id ``101``.\n\n- To test REST api scoring do the following two steps:\n\n  1. Deploy the model as a local REST endpoint by running ``mlflow models serve``:\n\n    .. code-block:: bash\n\n        # deploy the model to local REST api endpoint\n        mlflow models serve --model-uri runs:/101/model --port 54321\n\n  1. Apply the model to new data using the provided score_images_rest.py script:\n\n    .. code-block:: bash\n\n     ', 'your model, run the included scoring scripts. For example, say your model was trained with\nrun_id ``101``.\n\n- To test REST api scoring do the following two steps:\n\n  1. Deploy the model as a local REST endpoint by running ``mlflow models serve``:\n\n    .. code-block:: bash\n\n        # deploy the model to local REST api endpoint\n        mlflow models serve --model-uri runs:/101/model --port 54321\n\n  1. Apply the model to new data using the provided score_images_rest.py script:\n\n    .. code-block:: bash\n\n        # score the deployed model\n        python score_images_rest.py --host http://127.0.0.1 --port 54321 /path/to/images/for/scoring\n\n\n- To test batch scoring in Spark, run score_images_spark.py to score the model in Spark like this:\n\n  .. code-block:: bash\n\n    python score_images_spark.py --model-uri runs:/101/model /path/to/images/for/scoring\n', 'Hyperparameter Tuning Example\n------------------------------\n\nExample of how to do hyperparameter tuning with MLflow and some popular optimization libraries.\n\nThis example tries to optimize the RMSE metric of a Keras deep learning model on a wine quality\ndataset. The Keras model is fitted by the ``train`` entry point and has two hyperparameters that we\ntry to optimize: ``learning-rate`` and ``momentum``. The input dataset is split into three parts: training,\nvalidation, and test. The training dataset is used to fit the model and the validation dataset is used to\nselect the best hyperparameter values, and the test set is used to evaluate expected performance and\nto verify that we did not overfit on the particular training and validation combination. All three\nmetrics are logged with MLflow and you can use the MLflow UI to inspect how they vary between different\nhyperparameter values.\n\nexamples/hyperparam/MLproject has 4 targets:\n  * train:\n    train a simple deep learning model on the wine-quality dataset from our tutorial.\n    It has 2 tunable hyperparameters: ``learning-rate`` and ``momentum``.\n    Contains examples of how Keras callbacks can be used for MLflow integration.\n  * random:\n    perform simple random search over the parameter space.\n  * hyperopt:\n    use `Hyperopt <https://github.com/hyperopt/hyperopt>`_ to optimize hyperparameters.\n\n\nRunning this Example\n^^^^^^^^^^^^^^^^^^^^\n\nYou can run any of the targets as a standard MLflow run.\n\n.. code-block:: bash\n\n    mlflow experiments create -n individual_runs\n\nCreates experiment for individual runs and return its experiment ID.\n\n.. code-block:: bash\n\n    mlflow experiments create -n hyper_param_runs\n\nCreates an experiment for hyperparam runs and return its experiment ID.\n\n.. code-block:: bash\n\n    mlflow run -e train --experiment-id <individual_runs_experiment_id> examples/hyperparam\n\nRuns the Keras deep learning training with default parameters and log it in experiment 1.\n\n.. code-block:: bash\n\n    mlflow run -e random --experiment-id <hyperparam_experiment_id> examples/hyperparam\n\n.. code-block:: bash\n\n    mlflow run -e hyperopt --experiment-id <hyperparam_experiment_id> examples/hyperparam\n\nRuns the hyperparameter tuning with either random search or Hyperopt and log the\nresults under ``hyperparam_experiment_id``.\n\nYou can compare these results by using ``mlflow ui``.\n', "Multistep Workflow Example\n--------------------------\nThis MLproject aims to be a fully self-contained example of how to\nchain together multiple different MLflow runs which each encapsulate\na transformation or training step, allowing a clear definition of the\ninterface between the steps, as well as allowing for caching and reuse\nof the intermediate results.\n\nAt a high level, our goal is to predict users' ratings of movie given\na history of their ratings for other movies. This example is based\non `this webinar <https://databricks.com/blog/2018/07/13/scalable-end-to-end-deep-learning-using-tensorflow-and-databricks-on-demand-webinar-and-faq-now-available.html>`_\nby @brookewenig and @smurching.\n\n.. image:: ../../docs/source/_static/images/tutorial-multistep-workflow.png?raw=true\n\nThere are four steps to this workflow:\n\n- **load_raw_data.py**: Downloads the MovieLens dataset\n  (a set of triples of user id, movie id, and rating) as a CSV and puts\n  it into the artifact store.\n\n- **etl_data.py**: Converts the MovieLens CSV from the\n  previous step into Parquet, dropping unnecessary columns along the way.\n  This reduces the input size from 500 MB to 49 MB, and allows columnar\n  access of the data.\n\n- **als.py**: Runs Alternating Least Squares for collaborative\n  filtering on the Parquet version of MovieLens to estimate the\n  movieFactors and userFactors. This produces a relatively accurate estimator.\n\n- **train_keras.py**: Trains a neural network on the\n  original data, supplemented by the ALS movie/userFactors -- we hope\n  this can improve upon the ALS estimations.\n\nWhile we can run each of these steps manually, here we have a driver\nrun, defined as **main** (main.py). This run will run\nthe steps in order, passing the results of one to the next.\nAdditionally, this run will attempt to determine if a sub-run has\nalready been executed successfully with the same parameters and, if so,\nreuse the cached results.\n\nRunning this Example\n^^^^^^^^^^^^^^^^^^^^\nIn order for the multistep workflow to find the other steps, you must\nexecute ``mlflow run`` from this directory. So, in order to find out if\nthe Keras model does in fact improve upon the ALS model, you can simply\nrun:\n\n.. code-block:: bash\n\n    cd examples/multistep_workflow\n    mlflow run .\n\n\nThis downloads and transforms the MovieLens dataset, trains an ALS\nmodel, and then trains a Keras model -- you can compare the results by\nusing ``mlflow ui``.\n\nYou can also try changing the number of ALS iterations or Keras hidden\nunits:\n\n.. code-block:: bash\n\n  ", 'the same parameters and, if so,\nreuse the cached results.\n\nRunning this Example\n^^^^^^^^^^^^^^^^^^^^\nIn order for the multistep workflow to find the other steps, you must\nexecute ``mlflow run`` from this directory. So, in order to find out if\nthe Keras model does in fact improve upon the ALS model, you can simply\nrun:\n\n.. code-block:: bash\n\n    cd examples/multistep_workflow\n    mlflow run .\n\n\nThis downloads and transforms the MovieLens dataset, trains an ALS\nmodel, and then trains a Keras model -- you can compare the results by\nusing ``mlflow ui``.\n\nYou can also try changing the number of ALS iterations or Keras hidden\nunits:\n\n.. code-block:: bash\n\n    mlflow run . -P als_max_iter=20 -P keras_hidden_units=50\n', 'mlflow REST API Example\n-----------------------\nThis simple example shows how you could use MLflow REST API to create new\nruns inside an experiment to log parameters/metrics.\n\nTo run this example code do the following:\n\nOpen a terminal and navigate to the ``/tmp`` directory and start the mlflow tracking server::\n\n  mlflow server\n\nIn another terminal window navigate to the ``mlflow/examples/rest_api`` directory.  Run the example code\nwith this command::\n\n  python mlflow_tracking_rest_api.py\n\nProgram options::\n\n  usage: mlflow_tracking_rest_api.py [-h] [--hostname HOSTNAME] [--port PORT]\n                                   [--experiment-id EXPERIMENT_ID]\n\n  MLflow REST API Example\n\n  optional arguments:\n    -h, --help            show this help message and exit\n    --hostname HOSTNAME   MLflow server hostname/ip (default: localhost)\n    --port PORT           MLflow server port number (default: 5000)\n    --experiment-id EXPERIMENT_ID\n                            Experiment ID (default: 0)\n', 'Python Package Anti-Tampering with MLflow\n-----------------------------------------\nThis directory contains an MLflow project showing how to harden the ML supply chain, and in particular\nhow to protect against Python package tampering by enforcing\n`hash checks <https://pip.pypa.io/en/latest/cli/pip_install/#hash-checking-mode>`_ on packages.\n\nRunning this Example\n^^^^^^^^^^^^^^^^^^^^\n\nFirst, install MLflow (via ``pip install mlflow``).\n\nThe model is trained locally by running:\n\n.. code-block:: bash\n\n  mlflow run .\n\nAt the end of the training, note the run ID (say ``e651fcd4dab140a2bd4d3745a32370ac``).\n\nThe model is served locally by running:\n\n.. code-block:: bash\n\n  mlflow models serve -m runs:/e651fcd4dab140a2bd4d3745a32370ac/model\n\nInference is performed by sending JSON POST requests to http://localhost:5000/invocations:\n\n.. code-block:: bash\n\n  curl -X POST -d "{\\"dataframe_split\\": {\\"data\\":[[0.0199132142,0.0506801187,0.1048086895,0.0700725447,-0.0359677813,-0.0266789028,-0.0249926566,-0.002592262,0.0037117382,0.0403433716]]}}" -H "Content-Type: application/json" http://localhost:5000/invocations\n\nWhich returns ``[235.11371081266924]``.\n\nStructure of this MLflow Project\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. code-block:: yaml\n\n  name: mlflow-supply-chain-security\n  channels:\n  - nodefaults\n  dependencies:\n  - python=3.9\n  - pip\n  - pip:\n    - --require-hashes\n    - -r requirements.txt\n\nThis ensures that all the package requirements referenced in ``requirements.txt`` have been pinned through both version and hash:\n\n.. code-block:: text\n\n  mlflow==1.20.2 \\\n      --hash=sha256:963c22532e82a93450674ab97d62f9e528ed0906b580fadb7c003e696197557c \\\n      --hash=sha256:b15ff0c7e5e64f864a0b40c99b9a582227315eca2065d9f831db9aeb8f24637b\n  numpy==1.21.4 \\\n      --hash=sha256:0b78ecfa070460104934e2caf51694ccd00f37d5e5dbe76f021b1b0b0d221823 \\\n  ...\n\nThat same conda environment is referenced when logging the model in ``train.py`` so the environment matches during inference:\n\n.. code-block:: python\n\n  mlflow.sklearn.log_model(\n      model,\n      name="model",\n      signature=mlflow.models.infer_signature(X_train[:10], y_train[:10]),\n      input_example=X_train[:10],\n      conda_env="conda.yaml",\n  )\n\nThe package requirements are managed in ``requirements.in``:\n\n.. code-block:: text\n\n  pandas==1.3.2\n  scikit-learn==0.24.2\n  mlflow==1.20.2\n\nThey are compiled using ``pip-tools`` to resolve all the package dependencies, their versions, and their hashes:\n\n.. code-block:: bash\n\n  pip install pip-tools\n  pip-compile --generate-hashes --output-file=requirements.txt requirements.in\n', '{\n  "kube-context": "docker-for-desktop",\n  "kube-job-template-path": "examples/docker/kubernetes_job_template.yaml",\n  "repository-uri": "username/mlflow-kubernetes-example"\n}\n', '[[4.6, 3.1, 1.5, 0.2]]\n', '{\n  "name": "mlflow-typescript",\n  "private": true,\n  "description": "TypeScript implementation of MLflow Tracing SDK. This is the root workspace package that includes all the public packages as sub-directories.",\n  "workspaces": [\n    "core",\n    "integrations/*"\n  ],\n  "scripts": {\n    "build": "npm run build:subpackages",\n    "build:subpackages": "npm run build:core && npm run build:integrations",\n    "build:core": "cd core && npm run build",\n    "build:integrations": "cd integrations/openai && npm run build",\n    "test": "npm run test:subpackages",\n    "test:subpackages": "npm run test:core && npm run test:integrations",\n    "test:core": "cd core && npm run test",\n    "test:integrations": "cd integrations/openai && npm run test",\n    "lint": "eslint . --ext .ts",\n    "lint:fix": "eslint . --ext .ts --fix",\n    "format": "prettier --write .",\n    "format:check": "prettier --check .",\n    "prepare": "npm run build"\n  },\n  "devDependencies": {\n    "typedoc": "^0.28.0"\n  }\n}\n', '{\n  "name": "mlflow-tracing",\n  "version": "0.1.0",\n  "description": "TypeScript implementation of MLflow Tracing SDK for LLM observability",\n  "repository": {\n    "type": "git",\n    "url": "https://github.com/mlflow/mlflow.git"\n  },\n  "homepage": "https://mlflow.org/",\n  "author": {\n    "name": "MLflow",\n    "url": "https://mlflow.org/"\n  },\n  "bugs": {\n    "url": "https://github.com/mlflow/mlflow/issues"\n  },\n  "license": "Apache-2.0",\n  "keywords": [\n    "mlflow",\n    "tracing",\n    "observability",\n    "opentelemetry",\n    "llm",\n    "javascript",\n    "typescript"\n  ],\n  "main": "dist/index.js",\n  "types": "dist/index.d.ts",\n  "scripts": {\n    "build": "tsc",\n    "test": "jest",\n    "lint": "eslint . --ext .ts",\n    "lint:fix": "eslint . --ext .ts --fix",\n    "format": "prettier --write .",\n    "format:check": "prettier --check ."\n  },\n  "dependencies": {\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/sdk-node": "^0.201.1",\n    "@types/json-bigint": "^1.0.4",\n    "fast-safe-stringify": "^2.1.1",\n    "ini": "^5.0.0",\n    "json-bigint": "^1.0.0"\n  },\n  "devDependencies": {\n    "@types/ini": "^4.1.1",\n    "@types/jest": "^29.5.3",\n    "@types/node": "^20.4.5",\n    "@typescript-eslint/eslint-plugin": "^6.21.0",\n    "@typescript-eslint/parser": "^6.21.0",\n    "openai": "^4.0.0",\n    "eslint": "^8.57.1",\n    "jest": "^29.6.2",\n    "msw": "^2.10.3",\n    "prettier": "^3.5.3",\n    "ts-jest": "^29.1.1",\n    "tsx": "^4.7.0",\n    "typescript": "^5.8.3",\n    "whatwg-fetch": "^3.6.20"\n  },\n  "engines": {\n    "node": ">=18"\n  },\n  "files": [\n    "dist/"\n  ]\n}\n', '{\n  "name": "mlflow-openai",\n  "version": "0.1.0",\n  "description": "OpenAI integration package for MLflow Tracing",\n  "repository": {\n    "type": "git",\n    "url": "https://github.com/mlflow/mlflow.git"\n  },\n  "homepage": "https://mlflow.org/",\n  "author": {\n    "name": "MLflow",\n    "url": "https://mlflow.org/"\n  },\n  "bugs": {\n    "url": "https://github.com/mlflow/mlflow/issues"\n  },\n  "license": "Apache-2.0",\n  "keywords": [\n    "mlflow",\n    "tracing",\n    "observability",\n    "opentelemetry",\n    "llm",\n    "openai",\n    "javascript",\n    "typescript"\n  ],\n  "main": "dist/index.js",\n  "types": "dist/index.d.ts",\n  "scripts": {\n    "build": "tsc",\n    "test": "jest",\n    "lint": "eslint . --ext .ts",\n    "lint:fix": "eslint . --ext .ts --fix",\n    "format": "prettier --write .",\n    "format:check": "prettier --check ."\n  },\n  "peerDependencies": {\n    "mlflow-tracing": "^0.1.0-rc.0",\n    "openai": "^4.0.0"\n  },\n  "devDependencies": {\n    "jest": "^29.6.2",\n    "typescript": "^5.8.3"\n  },\n  "engines": {\n    "node": ">=18"\n  },\n  "files": [\n    "dist/"\n  ]\n}\n', '{\n  "name": "@mlflow/mlflow",\n  "version": "0.1.0",\n  "scripts": {\n    "start": "craco start",\n    "storybook": "start-storybook -p 6006 -s public",\n    "build-storybook": "build-storybook -s public",\n    "test": "craco --max_old_space_size=8192 test --env=jsdom --colors --watchAll=false",\n    "test:watch": "yarn test --watch",\n    "test:ci": "CI=true craco test --env=jsdom --colors --forceExit --ci --coverage",\n    "lint": "eslint --ext js,jsx,ts,tsx src",\n    "lint:fix": "eslint --ext js,jsx,ts,tsx src --fix",\n    "type-check": "tsc --noEmit",\n    "prettier": "prettier",\n    "prettier:fix": "prettier . --write",\n    "prettier:check": "prettier . --check",\n    "i18n:check": "yarn i18n --lint",\n    "i18n": "node scripts/extract-i18n.js",\n    "check-all": "yarn lint && yarn prettier:check && yarn i18n:check && yarn type-check",\n    "knip": "knip --reporter markdown --preprocessor ./knip-preprocessor.ts",\n    "build": "craco --max_old_space_size=8192 build",\n    "graphql-codegen": "python ../../../dev/proto_to_graphql/code_generator.py && yarn graphql-codegen:clean && yarn graphql-codegen:base",\n    "graphql-codegen:base": "graphql-codegen --config ./src/graphql/graphql-codegen.ts",\n    "graphql-codegen:clean": "find . -path \'**/__generated__/*.ts\' | xargs rm"\n  },\n  "dependencies": {\n    "@ag-grid-community/client-side-row-model": "^27.2.1",\n    "@ag-grid-community/core": "^27.2.1",\n    "@ag-grid-community/react": "^27.2.1",\n    "@apollo/client": "^3.6.9",\n    "@craco/craco": "7.0.0-alpha.0",\n    "@databricks/design-system": "^1.12.20",\n    "@emotion/cache": "^11.11.0",\n    "@emotion/react": "^11.11.3",\n    "@tanstack/react-query": "^4.29.17",\n    "@tanstack/react-table": "^8.8.2",\n    "@tanstack/react-virtual": "^3.8.1",\n    "@types/react-virtualized": "^9.21.9",\n    "babel-jest": "^27.5.1",\n    "buffer": "^6.0.3",\n    "bytes": "3.0.0",\n    "classnames": "^2.2.6",\n    "cookie": "0.3.1",\n    "cronstrue": "^1.94.0",\n    "d3-array": "^3.2.4",\n    "d3-scale": "^2.1.0",\n    "dateformat": "3.0.3",\n    "diff": "5.1.0",\n    "file-saver": "^2.0.5",\n    "font-awesome": "4.7.0",\n    "graphql": "^15.5.0",\n    "http-proxy-middleware": "^1.0.3",\n    "immutable": "3.8.1",\n    "invariant": "^2.2.4",\n    "js-yaml": "^3.14.0",\n    "json-bigint": "databricks/json-bigint#a1defaf9cd8dd749f0fd4d5f83a22cd846789658",\n    "leaflet": "^1.5.1",\n    "lodash": "^4.17.21",\n    "moment": "^2.29.4",\n    ', '"buffer": "^6.0.3",\n    "bytes": "3.0.0",\n    "classnames": "^2.2.6",\n    "cookie": "0.3.1",\n    "cronstrue": "^1.94.0",\n    "d3-array": "^3.2.4",\n    "d3-scale": "^2.1.0",\n    "dateformat": "3.0.3",\n    "diff": "5.1.0",\n    "file-saver": "^2.0.5",\n    "font-awesome": "4.7.0",\n    "graphql": "^15.5.0",\n    "http-proxy-middleware": "^1.0.3",\n    "immutable": "3.8.1",\n    "invariant": "^2.2.4",\n    "js-yaml": "^3.14.0",\n    "json-bigint": "databricks/json-bigint#a1defaf9cd8dd749f0fd4d5f83a22cd846789658",\n    "leaflet": "^1.5.1",\n    "lodash": "^4.17.21",\n    "moment": "^2.29.4",\n    "pako": "0.2.7",\n    "papaparse": "^5.3.2",\n    "parcoord-es": "^2.2.10",\n    "pdfjs-dist": "^5.3.31",\n    "plotly.js": "2.5.1",\n    "prop-types": "^15.8.1",\n    "qs": "6.10.5",\n    "rc-image": "~5.2.4",\n    "react": "^18.2.0",\n    "react-dnd": "^15.1.1",\n    "react-dnd-html5-backend": "^15.1.2",\n    "react-dom": "^18.2.0",\n    "react-draggable": "^4.4.6",\n    "react-error-boundary": "^4.0.2",\n    "react-hook-form": "^7.36.0",\n    "react-iframe": "1.8.0",\n    "react-intl": "^6.0.4",\n    "react-markdown-10": "npm:react-markdown@10",\n    "react-mde": "^11.0.0",\n    "react-pdf": "^10.0.1",\n    "react-plotly.js": "^2.5.1",\n    "react-redux": "^7.2.5",\n    "react-resizable": "^3.0.4",\n    "react-router": "^6.4.0",\n    "react-router-dom": "^6.4.0",\n    "react-syntax-highlighter": "^15.4.5",\n    "react-transition-group": "^4.4.1",\n    "react-treebeard": "2.1.0",\n    "react-vega": "^7.6.0",\n    "react-virtual": "^2.10.4",\n    "react-virtualized": "^9.21.2",\n    "redux": "^4.1.1",\n    "redux-promise-middleware": "^5.1.1",\n    "redux-thunk": "^2.3.0",\n    "remark-gfm-4": "npm:remark-gfm@4",\n    "sanitize-html": "^1.18.5",\n    "showdown": "^1.8.6",\n    "stream-browserify": "^3.0.0",\n    "stylis": "^4.0.10",\n    "url": "^0.11.0",\n    "use-clipboard-copy": "^0.2.0",\n    "use-debounce": "^10.0.4",\n    "use-sync-external-store": "^1.2.0",\n    "wavesurfer.js": "^7.8.8",\n    "yup": "^1.6.1"\n  },\n  "devDependencies": {\n    "@babel/core": "^7.27.3",\n    "@babel/eslint-parser": "^7.22.15",\n    "@babel/preset-env": "^7.27.2",\n    "@babel/preset-react": "^7.27.1",\n    ', '"react-virtualized": "^9.21.2",\n    "redux": "^4.1.1",\n    "redux-promise-middleware": "^5.1.1",\n    "redux-thunk": "^2.3.0",\n    "remark-gfm-4": "npm:remark-gfm@4",\n    "sanitize-html": "^1.18.5",\n    "showdown": "^1.8.6",\n    "stream-browserify": "^3.0.0",\n    "stylis": "^4.0.10",\n    "url": "^0.11.0",\n    "use-clipboard-copy": "^0.2.0",\n    "use-debounce": "^10.0.4",\n    "use-sync-external-store": "^1.2.0",\n    "wavesurfer.js": "^7.8.8",\n    "yup": "^1.6.1"\n  },\n  "devDependencies": {\n    "@babel/core": "^7.27.3",\n    "@babel/eslint-parser": "^7.22.15",\n    "@babel/preset-env": "^7.27.2",\n    "@babel/preset-react": "^7.27.1",\n    "@babel/preset-typescript": "^7.27.1",\n    "@emotion/babel-plugin": "^11.11.0",\n    "@emotion/babel-preset-css-prop": "^11.11.0",\n    "@emotion/eslint-plugin": "^11.7.0",\n    "@formatjs/cli": "^4.2.15",\n    "@graphql-codegen/cli": "^5.0.0",\n    "@graphql-codegen/typescript": "^4.0.1",\n    "@graphql-codegen/typescript-operations": "^4.0.1",\n    "@jest/globals": "^30.0.2",\n    "@storybook/addon-actions": "^6.5.5",\n    "@storybook/addon-docs": "^6.5.5",\n    "@storybook/addon-essentials": "^6.5.5",\n    "@storybook/addon-links": "^6.5.5",\n    "@storybook/builder-webpack5": "6.5.5",\n    "@storybook/manager-webpack5": "6.5.5",\n    "@storybook/node-logger": "^6.5.5",\n    "@storybook/preset-create-react-app": "^4.1.0",\n    "@storybook/react": "^6.5.5",\n    "@testing-library/dom": "^10.4.0",\n    "@testing-library/jest-dom": "^6.4.2",\n    "@testing-library/react": "^16.1.0",\n    "@testing-library/user-event": "^14.5.2",\n    "@types/d3-array": "^3.2.1",\n    "@types/d3-scale": "^2.1.0",\n    "@types/d3-selection": "^1.3.0",\n    "@types/diff": "^5.1.0",\n    "@types/file-saver": "^2.0.3",\n    "@types/invariant": "^2.2.35",\n    "@types/jest": "^29.5.14",\n    "@types/pako": "^2.0.0",\n    "@types/plotly.js": "^1.54.21",\n    "@types/react": "^17.0.50",\n    "@types/react-dom": "^17.0.17",\n    "@types/react-plotly.js": "^2.5.0",\n    "@types/react-resizable": "^3.0.3",\n    "@types/react-router": "^5.1.20",\n    "@types/react-router-dom": "^5.3.3",\n    "@types/react-transition-group": "^4.4.4",\n    "@types/stylis": "^4.0.1",\n    "@types/use-sync-external-store": "^0.0.3",\n    "@typescript-eslint/eslint-plugin": "^5.28.0",\n    "@typescript-eslint/parser": "^5.28.0",\n    "@wojtekmaj/enzyme-adapter-react-17": "^0.6.3",\n    "argparse": "^2.0.1",\n    "babel-plugin-formatjs": "^10.2.14",\n    "babel-plugin-react-require": "^3.1.3",\n    "confusing-browser-globals": "^1.0.11",\n    "enzyme": "^3.11.0",\n    "eslint": "^8.25.0",\n    "eslint-config-prettier": "^8.5.0",\n    ', '"@types/plotly.js": "^1.54.21",\n    "@types/react": "^17.0.50",\n    "@types/react-dom": "^17.0.17",\n    "@types/react-plotly.js": "^2.5.0",\n    "@types/react-resizable": "^3.0.3",\n    "@types/react-router": "^5.1.20",\n    "@types/react-router-dom": "^5.3.3",\n    "@types/react-transition-group": "^4.4.4",\n    "@types/stylis": "^4.0.1",\n    "@types/use-sync-external-store": "^0.0.3",\n    "@typescript-eslint/eslint-plugin": "^5.28.0",\n    "@typescript-eslint/parser": "^5.28.0",\n    "@wojtekmaj/enzyme-adapter-react-17": "^0.6.3",\n    "argparse": "^2.0.1",\n    "babel-plugin-formatjs": "^10.2.14",\n    "babel-plugin-react-require": "^3.1.3",\n    "confusing-browser-globals": "^1.0.11",\n    "enzyme": "^3.11.0",\n    "eslint": "^8.25.0",\n    "eslint-config-prettier": "^8.5.0",\n    "eslint-config-standard": "10.2.1",\n    "eslint-import-resolver-webpack": "0.8.4",\n    "eslint-loader": "2.1.1",\n    "eslint-plugin-chai-expect": "1.1.1",\n    "eslint-plugin-chai-friendly": "^0.7.2",\n    "eslint-plugin-cypress": "^2.12.1",\n    "eslint-plugin-flowtype": "^8.0.3",\n    "eslint-plugin-formatjs": "^3.1.5",\n    "eslint-plugin-import": "^2.26.0",\n    "eslint-plugin-jest": "^26.5.3",\n    "eslint-plugin-jsx-a11y": "^6.7.1",\n    "eslint-plugin-no-lookahead-lookbehind-regexp": "^0.1.0",\n    "eslint-plugin-no-only-tests": "^2.6.0",\n    "eslint-plugin-node": "5.2.1",\n    "eslint-plugin-prettier": "^4.0.0",\n    "eslint-plugin-promise": "3.6.0",\n    "eslint-plugin-react": "^7.30.0",\n    "eslint-plugin-react-hooks": "^4.6.0",\n    "eslint-plugin-standard": "3.0.1",\n    "eslint-plugin-testing-library": "^6.1.0",\n    "fast-glob": "^3.2.11",\n    "graphql-codegen-typescript-operation-types": "^2.0.1",\n    "jest-canvas-mock": "^2.2.0",\n    "jest-localstorage-mock": "^2.3.0",\n    "knip": "^5.30.2",\n    "msw": "^1.2.3",\n    "prettier": "^2.8.0",\n    "react-17": "npm:react@^17.0.2",\n    "react-dom-17": "npm:react-dom@^17.0.2",\n    "react-scripts": "5.0.0",\n    "react-test-renderer-17": "npm:react-test-renderer@^17.0.2",\n    "redux-mock-store": "^1.5.3",\n    "resolve": "^1.22.1",\n    "stream-browserify": "^3.0.0",\n    "tsconfig-paths-webpack-plugin": "^4.0.1",\n    "typescript": "^5.8.3",\n    "webpack": "^5.69.0",\n    "whatwg-fetch": "^3.6.17"\n  },\n  "private": true,\n  "engines": {\n    "node": "^22.16.0"\n  },\n  "resolutions": {\n    "@floating-ui/dom@^0.5.3": "patch:@floating-ui/dom@npm%3A0.5.4#yarn/patches/@floating-ui-dom-0.5.4.diff",\n    "@types/react": "^17.0.50",\n    "@emotion/react": "11.11.0",\n    "@types/react-plotly.js/@types/plotly.js": "^1.54.6",\n    "d3-transition": "3.0.1",\n    "react-dev-utils/fork-ts-checker-webpack-plugin": "6.5.3",\n    "postcss-preset-env/autoprefixer": "10.4.5",\n    "rc-virtual-list@^3.2.0": "patch:rc-virtual-list@npm%3A3.2.0#yarn/patches/rc-virtual-list-npm-3.2.0-5efaefc12e.patch",\n    "rc-virtual-list@^3.0.3": "patch:rc-virtual-list@npm%3A3.2.0#yarn/patches/rc-virtual-list-npm-3.2.0-5efaefc12e.patch",\n ', '"react-test-renderer-17": "npm:react-test-renderer@^17.0.2",\n    "redux-mock-store": "^1.5.3",\n    "resolve": "^1.22.1",\n    "stream-browserify": "^3.0.0",\n    "tsconfig-paths-webpack-plugin": "^4.0.1",\n    "typescript": "^5.8.3",\n    "webpack": "^5.69.0",\n    "whatwg-fetch": "^3.6.17"\n  },\n  "private": true,\n  "engines": {\n    "node": "^22.16.0"\n  },\n  "resolutions": {\n    "@floating-ui/dom@^0.5.3": "patch:@floating-ui/dom@npm%3A0.5.4#yarn/patches/@floating-ui-dom-0.5.4.diff",\n    "@types/react": "^17.0.50",\n    "@emotion/react": "11.11.0",\n    "@types/react-plotly.js/@types/plotly.js": "^1.54.6",\n    "d3-transition": "3.0.1",\n    "react-dev-utils/fork-ts-checker-webpack-plugin": "6.5.3",\n    "postcss-preset-env/autoprefixer": "10.4.5",\n    "rc-virtual-list@^3.2.0": "patch:rc-virtual-list@npm%3A3.2.0#yarn/patches/rc-virtual-list-npm-3.2.0-5efaefc12e.patch",\n    "rc-virtual-list@^3.0.3": "patch:rc-virtual-list@npm%3A3.2.0#yarn/patches/rc-virtual-list-npm-3.2.0-5efaefc12e.patch",\n    "rc-virtual-list@^3.0.1": "patch:rc-virtual-list@npm%3A3.2.0#yarn/patches/rc-virtual-list-npm-3.2.0-5efaefc12e.patch"\n  },\n  "//": "homepage is hard to configure without resorting to env variables and doesn\'t play nicely with other webpack settings. This field should be removed.",\n  "homepage": "static-files",\n  "browserslist": [\n    "defaults"\n  ],\n  "babel": {\n    "env": {\n      "test": {\n        "plugins": [\n          [\n            "babel-plugin-formatjs",\n            {\n              "idInterpolationPattern": "[sha512:contenthash:base64:6]",\n              "removeDefaultMessage": false\n            }\n          ]\n        ]\n      }\n    }\n  }\n}\n', '{\n  "short_name": "React App",\n  "name": "Create React App Sample",\n  "icons": [\n    {\n      "src": "favicon.ico",\n      "sizes": "64x64 32x32 24x24 16x16",\n      "type": "image/x-icon"\n    }\n  ],\n  "start_url": "./index.html",\n  "display": "standalone",\n  "theme_color": "#000000",\n  "background_color": "#ffffff"\n}\n', '{\n  "columns": [\n    "company_name",\n    "company_goal",\n    "prompt",\n    "output",\n    "MLFLOW_model",\n    "MLFLOW_route_type",\n    "MLFLOW_latency"\n  BROKEN', '{\n  "columns": [\n    "company_name",\n    "company_goal",\n    "prompt",\n    "output",\n    "MLFLOW_model",\n    "MLFLOW_route_type",\n    "MLFLOW_latency"\n  ],\n  "data": [\n    [\n      "Abc",\n      "bottom line revenue",\n      "You are a marketing consultant for a technology company. Develop a marketing strategy report for Abc aiming to bottom line revenue",\n      " Here is an outline for a marketing strategy report aimed at increasing bottom line revenue for the technology company Abc:\\n\\nIntroduction \\n- Brief background on Abc - products/services, target markets, competitive landscape\\n- Objective of report: Provide recommendations to increase bottom line revenue through marketing strategies\\n\\nMarket Analysis\\n- Trends in Abc\'s industry and target markets\\n- Customer analysis - demographics, psychographics, buying behavior \\n- Competitor analysis - positioning, pricing,",\n      "",\n      "",\n      "7788.29999999702"\n    ],\n    [\n      "XYZ Company",\n      "Increase top-line revenue",\n      "You are a marketing consultant for a technology company. Develop a marketing strategy report for XYZ Company aiming to Increase top-line revenue",\n      " Here is an outline for a marketing strategy report aimed at increasing top-line revenue for XYZ Company:\\n\\nXYZ Company \\nMarketing Strategy Report\\n\\nExecutive Summary\\n- Brief overview of key recommendations to increase revenue \\n\\nCurrent Situation Analysis\\n- Background on XYZ Company\'s products/services, target customers, competitive landscape\\n- Analysis of current marketing efforts and sales performance \\n\\nOpportunities for Growth \\n- New customer segments to target\\n- Additional products/services to meet customer needs",\n      "claude-2",\n      "llm/v1/completions",\n      "11563.60000000149"\n    ]\n  ]\n}\n', '{\n  "columns": [0, 1],\n  "data": [\n    ["a", "b"],\n    [1, 2]\n  ]\n}\n', '{\n  "current": {\n    "path": "/static/lib/ml-model-trace-renderer/index.html"\n  },\n  "2": {\n    "path": "/static/lib/ml-model-trace-renderer/2/index.html",\n    "commit": "93d4afc7a2e876f74cbba56c2db3d05edc91e872"\n  },\n  "oss": {\n    "path": "/static/lib/ml-model-trace-renderer/oss/index.html",\n    "commit": "b5595f5c6263c1c8e3614d85eb1d233d28789bb9"\n  },\n  "3": {\n    "path": "/static/lib/ml-model-trace-renderer/3/index.html",\n    "commit": "93d4afc7a2e876f74cbba56c2db3d05edc91e872"\n  }\n}\n', '{\n  "bd263e2b04b04460a40c1acae72a18ae": {\n    "metric_1": -2.5797830282214,\n    "metric_0": -2.434975966906267,\n    "metric_3": -0.9688077263066934,\n    "metric_2": -1.4438003481072212\n  },\n  "55461e2180fb40338072c04ff86fd0f9": {\n    "metric_1": -2.2857451912150792,\n    "metric_0": 3.4173603047073176,\n    "metric_3": -0.24019895935855473,\n    "metric_2": -0.7097425052930393\n  },\n  "123810810b234e9b8b97fb1e00abd9aa": {\n    "metric_1": 0.7308706035999548,\n    "metric_0": 3.0994891921059544,\n    "metric_3": 1.9819820891007573,\n    "metric_2": 0.48569560278784785\n  },\n  "66fbc3c813944c1a80d2336849c6e72f": {\n    "metric_1": -2.672718621393841,\n    "metric_0": -1.7902590711838267,\n    "metric_3": 2.477982822663786,\n    "metric_2": 1.273023064731822\n  },\n  "83698fafa8714bd3929b9c38bf6cdee8": {\n    "metric_1": -2.292131339453693,\n    "metric_0": 3.3064205155126096,\n    "metric_3": -0.46183104891365634,\n    "metric_2": 0.8465206209214458\n  },\n  "79461e9d7aa24e18a626f61b047315c8": {\n    "metric_1": -0.006443567706641673,\n    "metric_0": 1.8136489475666746,\n    "metric_3": 2.6700440103809013,\n    "metric_2": 0.1556304999295106\n  },\n  "6870761df41f4350adbd37f2a18eb641": {\n    "metric_1": -0.24614682477958416,\n    "metric_0": -1.5486858485543848,\n    "metric_3": -2.742733532695466,\n    "metric_2": 3.0344898094132358\n  },\n  "eb124e3ef9c04109a65372aab4222307": {\n    "metric_1": -1.9754772100389224,\n    "metric_0": -0.6234240819980461,\n    "metric_3": -2.4020972270978844,\n    "metric_2": 2.7962318576455436\n  },\n  "39c152b25fb04c08b3cf4a4c8ebccb7c": {\n    "metric_1": -2.149649101035413,\n    "metric_0": 0.7310583410679579,\n    "metric_3": 1.1244662209560552,\n    "metric_2": -2.0257045260054656\n  },\n  "1b92583de96c4fd88ff5b04e866aff8c": {\n    "metric_1": -0.02959618784025775,\n    "metric_0": -0.2931365711894838,\n    "metric_3": 3.1567281048311546,\n    "metric_2": 2.9203148639651495\n  },\n  "3c28149e5ab44efb8d7f4b3c11ab0cb1": {\n    "metric_1": -2.0944626407865288,\n    "metric_0": 3.779234079600906,\n    "metric_3": -2.9467929849482024,\n    "metric_2": -1.4704105222912913\n  },\n  "1afafa10e69a4083bb88a96b23547b7b": {\n    "metric_1": -0.10343236992763893,\n    "metric_0": 2.28372065230554,\n    "metric_3": 2.0481770225881615,\n    "metric_2": -1.1601122975618\n  },\n  "5f9c5e48ff844c0498199b390ca9c1a4": {\n    "metric_1": 2.4422641299267553,\n    "metric_0": -1.8107502356154743,\n    "metric_3": 3.6679213677343423,\n    "metric_2": -1.8061767363300147\n  },\n  "0ffbde81192e43a48482434219cc4458": {\n    "metric_1": -1.5559004670240322,\n    "metric_0": 0.8721310864910716,\n    "metric_3": 2.5822778193072846,\n    "metric_2": 0.6969033758109711\n  ', '},\n  "3c28149e5ab44efb8d7f4b3c11ab0cb1": {\n    "metric_1": -2.0944626407865288,\n    "metric_0": 3.779234079600906,\n    "metric_3": -2.9467929849482024,\n    "metric_2": -1.4704105222912913\n  },\n  "1afafa10e69a4083bb88a96b23547b7b": {\n    "metric_1": -0.10343236992763893,\n    "metric_0": 2.28372065230554,\n    "metric_3": 2.0481770225881615,\n    "metric_2": -1.1601122975618\n  },\n  "5f9c5e48ff844c0498199b390ca9c1a4": {\n    "metric_1": 2.4422641299267553,\n    "metric_0": -1.8107502356154743,\n    "metric_3": 3.6679213677343423,\n    "metric_2": -1.8061767363300147\n  },\n  "0ffbde81192e43a48482434219cc4458": {\n    "metric_1": -1.5559004670240322,\n    "metric_0": 0.8721310864910716,\n    "metric_3": 2.5822778193072846,\n    "metric_2": 0.6969033758109711\n  },\n  "f9b1cb0a470e4f9b98bd4d36a51e4d31": {\n    "metric_1": 2.9364188950633814,\n    "metric_0": -0.2344822065779013,\n    "metric_3": 3.8849138907360397,\n    "metric_2": -2.524561921321326\n  },\n  "99d7cdc4d77f4103937bbb9a70c5d4c8": {\n    "metric_1": 0.8548586864241012,\n    "metric_0": 0.9891059238008051,\n    "metric_3": 1.7255056515257134,\n    "metric_2": -0.958042549612474\n  },\n  "f662d9fdd072422899f1a91dca132e45": {\n    "metric_eq_ts_step": 4.7\n  },\n  "8c27fb8d3d734fc988eafc7e130af2c1": {\n    "metric_1": 0.2568792149591985,\n    "metric_0": -2.9251778063945633,\n    "metric_3": 3.5795601350695536,\n    "metric_2": 2.2358743640336654\n  },\n  "46830b05f4914ce980ee960921474308": {\n    "metric_1": -2.4512019495254855,\n    "metric_0": -1.7363712773941686,\n    "metric_3": -0.6764695293332332,\n    "metric_2": 0.902757467400038\n  },\n  "2bf6a4001dda47a89bd0dd1b638900c8": {\n    "metric_1": 1.4881757006157033,\n    "metric_0": -2.184521948675489,\n    "metric_3": 1.5024111371215891,\n    "metric_2": -0.9992038125180369\n  },\n  "bf3f29f1c16741e8b2de46b8af7f26db": {\n    "metric_1": -2.856136683397856,\n    "metric_0": 1.7351206329209488,\n    "metric_3": 3.3574106252540243,\n    "metric_2": 1.898369877601997\n  },\n  "8a56867ed3af4ead84ac5c215c43d2f2": {\n    "metric_1": -1.6845465954407057,\n    "metric_0": 2.799113454790449,\n    "metric_3": -1.9554854183785264,\n    "metric_2": 1.2596708758008042\n  },\n  "0e77b745f78c4985825c66aafb15f45e": {\n    "metric_1": -0.5966921249245285,\n    "metric_0": 0.38303481722942134,\n    "metric_3": 1.4751754578912797,\n    "metric_2": -2.935488516752223\n  },\n  "f8a6361f177a40b291751d743ec0cd52": {\n    "metric_1": -2.52066603748033,\n    "metric_0": -0.7281695813441598,\n    "metric_3": 0.3529736105779344,\n    "metric_2": -1.603741841379303\n  },\n  "8c6d7c99dc014b15b5ccec784110b83f": {\n    "metric_1": -1.2231009149094496,\n    "metric_0": 1.4079626574511428,\n  ', '  "metric_3": 3.3574106252540243,\n    "metric_2": 1.898369877601997\n  },\n  "8a56867ed3af4ead84ac5c215c43d2f2": {\n    "metric_1": -1.6845465954407057,\n    "metric_0": 2.799113454790449,\n    "metric_3": -1.9554854183785264,\n    "metric_2": 1.2596708758008042\n  },\n  "0e77b745f78c4985825c66aafb15f45e": {\n    "metric_1": -0.5966921249245285,\n    "metric_0": 0.38303481722942134,\n    "metric_3": 1.4751754578912797,\n    "metric_2": -2.935488516752223\n  },\n  "f8a6361f177a40b291751d743ec0cd52": {\n    "metric_1": -2.52066603748033,\n    "metric_0": -0.7281695813441598,\n    "metric_3": 0.3529736105779344,\n    "metric_2": -1.603741841379303\n  },\n  "8c6d7c99dc014b15b5ccec784110b83f": {\n    "metric_1": -1.2231009149094496,\n    "metric_0": 1.4079626574511428,\n    "metric_3": 2.5892028641452907,\n    "metric_2": 3.643033981543657\n  },\n  "9af4b84d78524c4ab08161e7b5f7f2dc": {\n    "metric_1": 0.2702968551842675,\n    "metric_0": 0.11248586282952555,\n    "metric_3": -1.533809108651962,\n    "metric_2": 1.678493181317803\n  }\n}\n', '{\n  "kube-context": "docker-for-desktop",\n  "kube-job-template-path": "examples/docker/kubernetes_job_template.yaml",\n  "repository-uri": "username/mlflow-kubernetes-example"\n}\n', '{ "messages": [{ "role": "user", "content": "What is Retrieval-augmented Generation?" }] }\n', 'build_dependencies:\n  - pip\ndependencies:\n  - diviner\n  - mlflow>=1.24.1\n', 'apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: "{replaced with MLflow Project name}"\n  namespace: mlflow\nspec:\n  ttlSecondsAfterFinished: 100\n  backoffLimit: 0\n  template:\n    spec:\n      containers:\n        - name: "{replaced with MLflow Project name}"\n          image: "{replaced with URI of Docker image created during Project execution}"\n          command: ["{replaced with MLflow Project entry point command}"]\n          resources:\n            limits:\n              memory: 512Mi\n            requests:\n              memory: 256Mi\n      restartPolicy: Never\n', 'build_dependencies:\n  - pip==22.2.2\ndependencies:\n  - mlflow>=1.6\n  - pandas==1.5.0\n  - scikit-learn==1.1.3\n  - tensorflow==2.10.0\n  - pillow==9.2.0\n', 'endpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: ai21labs\n      name: j2-mid\n      config:\n        ai21labs_api_key: $AI21LABS_API_KEY\n', 'endpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: anthropic\n      name: claude-1.3-100k\n      config:\n        anthropic_api_key: $ANTHROPIC_API_KEY\n', 'endpoints:\n  - name: chat\n    endpoint_type: llm/v1/chat\n    model:\n      provider: openai\n      name: gpt-4o-mini\n      config:\n        openai_api_type: "azure"\n        openai_api_key: $OPENAI_API_KEY\n        openai_deployment_name: "{your_deployment_name}"\n        openai_api_base: "https://{your_resource_name}-azureopenai.openai.azure.com/"\n        openai_api_version: "2023-05-15"\n\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: openai\n      name: gpt-4o-mini\n      config:\n        openai_api_type: "azuread"\n        openai_api_key: $AZURE_AAD_TOKEN\n        openai_deployment_name: "{your_deployment_name}"\n        openai_api_base: "https://{your_resource_name}-azureopenai.openai.azure.com/"\n        openai_api_version: "2023-05-15"\n\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: openai\n      name: text-embedding-ada-002\n      config:\n        openai_api_type: "azure"\n        openai_api_key: $OPENAI_API_KEY\n        openai_deployment_name: "{your_deployment_name}"\n        openai_api_base: "https://{your_resource_name}-azureopenai.openai.azure.com/"\n        openai_api_version: "2023-05-15"\n', 'endpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: amazon-bedrock\n      name: amazon.titan-tg1-large\n      config:\n        aws_config:\n          aws_region: us-east-1\n          aws_access_key_id: $AWS_ACCESS_KEY_ID\n          aws_secret_access_key: $AWS_SECRET_ACCESS_KEY\n', 'endpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: cohere\n      name: command\n      config:\n        cohere_api_key: $COHERE_API_KEY\n\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: cohere\n      name: embed-english-light-v2.0\n      config:\n        cohere_api_key: $COHERE_API_KEY\n', 'endpoints:\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: gemini\n      name: gemini-embedding-exp-03-07\n      config:\n        gemini_api_key: $GEMINI_API_KEY\n\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: gemini\n      name: gemini-2.0-flash\n      config:\n        gemini_api_key: $GEMINI_API_KEY\n\n  - name: chat\n    endpoint_type: llm/v1/chat\n    model:\n      provider: gemini\n      name: gemini-2.0-flash\n      config:\n        gemini_api_key: $GEMINI_API_KEY\n', 'endpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: "huggingface-text-generation-inference"\n      name: falcon-7b-instruct\n      config:\n        hf_server_url: http://127.0.0.1:8080\n', 'endpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: mistral\n      name: mistral-tiny\n      config:\n        mistral_api_key: $MISTRAL_API_KEY\n\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: mistral\n      name: mistral-embed\n      config:\n        mistral_api_key: $MISTRAL_API_KEY\n', 'endpoints:\n  - name: fillmask\n    endpoint_type: llm/v1/completions\n    model:\n      provider: mlflow-model-serving\n      name: mask-fill\n      config:\n        model_server_url: http://127.0.0.1:9010\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: mlflow-model-serving\n      name: sentence-transformer\n      config:\n        model_server_url: http://127.0.0.1:9020\n', 'endpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: mosaicml\n      name: mpt-7b-instruct\n      config:\n        mosaicml_api_key: $MOSAICML_API_KEY\n\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: mosaicml\n      name: instructor-xl\n      config:\n        mosaicml_api_key: $MOSAICML_API_KEY\n\n  - name: chat\n    endpoint_type: llm/v1/chat\n    model:\n      provider: mosaicml\n      name: llama2-70b-chat\n      config:\n        mosaicml_api_key: $MOSAICML_API_KEY\n', 'endpoints:\n  - name: chat\n    endpoint_type: llm/v1/chat\n    model:\n      provider: openai\n      name: gpt-4o-mini\n      config:\n        openai_api_key: $OPENAI_API_KEY\n    limit:\n      renewal_period: minute\n      calls: 10\n\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: openai\n      name: gpt-4o-mini\n      config:\n        openai_api_key: $OPENAI_API_KEY\n\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: openai\n      name: text-embedding-ada-002\n      config:\n        openai_api_key: $OPENAI_API_KEY\n', 'endpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: palm\n      name: text-bison-001\n      config:\n        palm_api_key: $PALM_API_KEY\n\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: palm\n      name: embedding-gecko-001\n      config:\n        palm_api_key: $PALM_API_KEY\n\n  - name: chat\n    endpoint_type: llm/v1/chat\n    model:\n      provider: palm\n      name: chat-bison-001\n      config:\n        palm_api_key: $PALM_API_KEY\n', 'endpoints:\n  - name: chat\n    endpoint_type: llm/v1/chat\n    model:\n      provider: my_llm\n      name: my-model-0.1.2\n      config:\n        my_llm_api_key: $MY_LLM_API_KEY\n', 'endpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: togetherai\n      name: mistralai/Mixtral-8x7B-v0.1\n      config:\n        togetherai_api_key: $TOGETHERAI_API_KEY\n\n  - name: chat\n    endpoint_type: llm/v1/chat\n    model:\n      provider: togetherai\n      name: mistralai/Mixtral-8x7B-Instruct-v0.1\n      config:\n        togetherai_api_key: $TOGETHERAI_API_KEY\n\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: togetherai\n      name: togethercomputer/m2-bert-80M-8k-retrieval\n      config:\n        togetherai_api_key: $TOGETHERAI_API_KEY\n', 'build_dependencies:\n  - pip\ndependencies:\n  - h2o\n  - mlflow>=1.0\n  - numpy\n  - pandas\n', 'build_dependencies:\n  - pip\ndependencies:\n  - numpy\n  - click\n  - pandas\n  - scipy\n  - scikit-learn\n  - tensorflow==2.10.0\n  - matplotlib\n  - mlflow>=1.6\n  - hyperopt\n  - protobuf<4.0.0\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow>=1.6.0\n  - matplotlib\n  - lightgbm\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow>=1.6.0\n  - matplotlib\n  - lightgbm\n  - cloudpickle>=2.0.0\n', 'python: "3.10"\nbuild_dependencies:\n  - pip\ndependencies:\n  - openai>=0.27.2\n  - tiktoken>=0.4.0\n  - tenacity>=8.2.2\n  - mlflow>=2.4.0\n', 'python: "3.10"\nbuild_dependencies:\n  - pip\ndependencies:\n  - langchain>=0.0.244\n  - openai>=0.27.2\n  - evaluate>=0.4.0\n  - mlflow>=2.4.0\n  - tiktoken>=0.4.0\n', 'build_dependencies:\n  - pip\ndependencies:\n  - tensorflow==1.15.2\n  - keras==2.2.4\n  - mlflow>=1.0\n  - pyspark\n  - requests\n  - click\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - paddlepaddle==2.1.0\n  - cloudpickle==1.6.0\n', 'build_dependencies:\n  - pip\ndependencies:\n  - pmdarima\n  - mlflow>=1.23.1\n', '$schema: https://azuremlschemas.azureedge.net/promptflow/latest/Flow.schema.json\n\ninputs:\n  text:\n    type: string\n    default: Hello World!\n\noutputs:\n  output:\n    type: string\n    reference: ${llm.output}\n\nnodes:\n  - name: hello_prompt\n    type: python\n    source:\n      type: code\n      path: render_template.py\n    inputs:\n      text: ${inputs.text}\n      template: |\n        system:\n        Your task is to generate what I ask.\n        user:\n        Write a simple {{text}} program that displays the greeting message.\n  - name: llm\n    type: python\n    source:\n      type: code\n      path: hello.py\n    inputs:\n      prompt: ${hello_prompt.output}\n      deployment_name: gpt-4o-mini\n      max_tokens: "120"\nenvironment:\n  image: mcr.microsoft.com/azureml/promptflow/promptflow-runtime:latest\n  python_requirements_txt: requirements.txt\n', 'build_dependencies:\n  - pip\ndependencies:\n  - prophet>=1.0.1\n', 'build_dependencies:\n  - pip\ndependencies:\n  - torch\n  - torchvision\n  - mlflow\n  - tensorboardX\n', 'channels:\n  - conda-forge\ndependencies:\n  - python=3.8.2\n  - pip\n  - pip:\n      - mlflow\n      - lightning==2.0.0\n      - jsonargparse[signatures]>=4.17.0\n      # typeguard is used for type validation in the ax-platform code base. 3.0.0 release has\n      # breaking changes that need to be resolved in ax. Remove this pin when\n      # https://github.com/facebook/Ax/issues/1509 is addressed\n      - typeguard<3.0.0\n      - ax-platform\n      - torchvision>=0.15.1\n      - torch>=2.0\n      # gyptorch 1.9.x is incompatible with the versions of botorch\n      # required by many versions of pytorch\n      - gpytorch<1.9.0\n      - protobuf<4.0.0\n      # Pinning pandas version less than 1.4.4 due to https://github.com/facebook/Ax/issues/1153\n      - pandas<=1.4.4\n      # Numpy>=2 is not compatible with pandas<=1.4.4\n      - numpy<2\n      # TODO: Remove this requirement once ax-platform achieves compatibility with SQLAlchemy 2.x\n      - sqlalchemy<2\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - lightning==2.0.0\n  - jsonargparse[signatures]>=4.17.0\n  # typeguard is used for type validation in the ax-platform code base. 3.0.0 release has\n  # breaking changes that need to be resolved in ax. Remove this pin when\n  # https://github.com/facebook/Ax/issues/1509 is addressed\n  - typeguard<3.0.0\n  - ax-platform\n  - torchvision>=0.15.1\n  - torch>=2.0\n  # gyptorch 1.9.x is incompatible with the versions of botorch\n  # required by many versions of pytorch\n  - gpytorch<1.9.0\n  - protobuf<4.0.0\n  # Pinning pandas version less than 1.4.4 due to https://github.com/facebook/Ax/issues/1153\n  - pandas<=1.4.4\n  # Numpy>=2 is not compatible with pandas<=1.4.4\n  - numpy<2\n  # TODO: Remove this requirement once ax-platform achieves compatibility with SQLAlchemy 2.x\n  - sqlalchemy<2\n', 'channels:\n  - conda-forge\ndependencies:\n  - python=3.8.2\n  - pip\n  - pip:\n      - mlflow\n      - scikit-learn\n      - cloudpickle==1.6.0\n      - boto3\n      - transformers>=4.0.0\n      - pandas\n      - numpy<2.0\n      - torch>=2.0.0\n      - torchdata>=0.6.0\n      - torchtext==0.15.1\n      - lightning==2.0.0\n      - jsonargparse[signatures]>=4.17.0\n      - protobuf<4.0.0\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - scikit-learn\n  - cloudpickle==1.6.0\n  - boto3\n  - transformers>=4.0.0\n  - pandas\n  - numpy<2.0\n  - torch>=2.0\n  - torchdata\n  - torchtext==0.16.2\n  - lightning\n  - jsonargparse[signatures]>=4.17.0\n  - protobuf<4.0.0\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - pandas\n  - scipy\n  - captum\n  - boto3\n  - scikit-learn\n  - prettytable\n  - ipython\n  - torch\n', 'channels:\n  - conda-forge\ndependencies:\n  - python=3.8.2\n  - pip\n  - pip:\n      - mlflow\n      - torchvision>=0.15.1\n      - cloudpickle==1.6.0\n      - lightning==2.0.0\n      - jsonargparse[signatures]>=4.17.0\n      # typeguard is used for type validation in the ax-platform code base. 3.0.0 release has\n      # breaking changes that need to be resolved in ax. Remove this pin when\n      # https://github.com/facebook/Ax/issues/1509 is addressed\n      - typeguard<3.0.0\n      - ax-platform\n      - prettytable\n      - torch>=2.0\n      - protobuf<4.0.0\n      # gyptorch 1.9.x is incompatible with the versions of botorch\n      # required by many versions of pytorch\n      - gpytorch<1.9.0\n      # Pinning pandas version less than 1.4.4 due to https://github.com/facebook/Ax/issues/1153\n      - pandas<=1.4.4\n      # Numpy>=2 is not compatible with pandas<=1.4.4\n      - numpy<2\n      # ax-platform 0.2.x is not yet compatible with SQLAlchemy 2.x\n      # TODO: Remove this requirement once ax-platform achieves compatibility with SQLAlchemy 2.x\n      - sqlalchemy<2\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - torchvision>=0.15.1\n  - cloudpickle==1.6.0\n  - lightning==2.0.0\n  - jsonargparse[signatures]>=4.17.0\n  # typeguard is used for type validation in the ax-platform code base. 3.0.0 release has\n  # breaking changes that need to be resolved in ax. Remove this pin when\n  # https://github.com/facebook/Ax/issues/1509 is addressed\n  - typeguard<3.0.0\n  - ax-platform\n  - prettytable\n  - torch>=2.0\n  # gyptorch 1.9.x is incompatible with the versions of botorch\n  # required by many versions of pytorch\n  - gpytorch<1.9.0\n  - protobuf<4.0.0\n  # Pinning pandas version less than 1.4.4 due to https://github.com/facebook/Ax/issues/1153\n  - pandas<=1.4.4\n  # Numpy>=2 is not compatible with pandas<=1.4.4\n  - numpy<2\n  # ax-platform 0.2.x is not yet compatible with SQLAlchemy 2.x\n  # TODO: Remove this requirement once ax-platform achieves compatibility with SQLAlchemy 2.x\n  - sqlalchemy<2\n', 'channels:\n  - conda-forge\ndependencies:\n  - python=3.8.2\n  - pip\n  - pip:\n      - mlflow\n      - torchvision>=0.15.1\n      - torch>=2.0\n      - lightning==2.0.0\n      - jsonargparse[signatures]>=4.17.0\n      - protobuf<4.0.0\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - torchvision>=0.15.1\n  - torch>=2.0\n  - lightning==2.0.0\n  - jsonargparse[signatures]>=4.17.0\n  - protobuf<4.0.0\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - torch==1.8.0\n  - torchvision==0.9.1\n  - pytorch-lightning==1.0.2\n', 'build_dependencies:\n  - pip\ndependencies:\n  - scikit-learn\n  - cloudpickle==1.6.0\n  - boto3\n  - torchvision>=0.9.1\n  - torch>=1.9.0\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - cloudpickle==1.6.0\n  - boto3\n  - torchvision>=0.9.1\n  - torch>=1.9.0\n', 'name: mlflow\nchannels:\n  - rapidsai\n  - nvidia\n  - conda-forge\n  - defaults\ndependencies:\n  - _libgcc_mutex=0.1=conda_forge\n  - _openmp_mutex=4.5=1_llvm\n  - arrow-cpp=0.15.0=py37h090bef1_2\n  - bokeh=2.1.0=py37hc8dfbb8_0\n  - boost-cpp=1.70.0=h8e57a91_2\n  - brotli=1.0.7=he1b5a44_1002\n  - bzip2=1.0.8=h516909a_2\n  - c-ares=1.15.0=h516909a_1001\n  - ca-certificates=2020.4.5.2=hecda079_0\n  - certifi=2020.4.5.2=py37hc8dfbb8_0\n  - click=7.1.2=pyh9f0ad1d_0\n  - cloudpickle=1.4.1=py_0\n  - cudatoolkit=10.2.89=h6bb024c_0\n  - cudf=0.14.0=py37_0\n  - cudnn=7.6.5=cuda10.2_0\n  - cuml=0.14.0=cuda10.2_py37_0\n  - cupy=7.5.0=py37h940342b_0\n  - cytoolz=0.10.1=py37h516909a_0\n  - dask=2.18.1=py_0\n  - dask-core=2.18.1=py_0\n  - dask-cudf=0.14.0=py37_0\n  - distributed=2.18.0=py37hc8dfbb8_0\n  - dlpack=0.2=he1b5a44_1\n  - double-conversion=3.1.5=he1b5a44_2\n  - fastavro=0.23.4=py37h8f50634_0\n  - fastrlock=0.5=py37h3340039_0\n  - freetype=2.10.2=he06d7ca_0\n  - fsspec=0.7.4=py_0\n  - gflags=2.2.2=he1b5a44_1002\n  - glog=0.4.0=h49b9bf7_3\n  - grpc-cpp=1.23.0=h18db393_0\n  - heapdict=1.0.1=py_0\n  - icu=64.2=he1b5a44_1\n  - jinja2=2.11.2=pyh9f0ad1d_0\n  - joblib=0.15.1=py_0\n  - jpeg=9d=h516909a_0\n  - ld_impl_linux-64=2.33.1=h53a641e_7\n  - libblas=3.8.0=16_openblas\n  - libcblas=3.8.0=16_openblas\n  - libcudf=0.14.0=cuda10.2_0\n  - libcuml=0.14.0=cuda10.2_0\n  - libcumlprims=0.14.1=cuda10.2_0\n  - libedit=3.1.20181209=hc058e9b_0\n  - libevent=2.1.10=h72c5cf5_0\n  - libffi=3.3=he6710b0_1\n  - libgcc-ng=9.2.0=h24d8f2e_2\n  - libgfortran-ng=7.5.0=hdf63c60_6\n  - libhwloc=2.1.0=h3c4fd83_0\n  - libiconv=1.15=h516909a_1006\n  - liblapack=3.8.0=16_openblas\n  - libllvm8=8.0.1=hc9558a2_0\n  - libnvstrings=0.14.0=cuda10.2_0\n  - libopenblas=0.3.9=h5ec1e0e_0\n  - libpng=1.6.37=hed695b0_1\n  - libprotobuf=3.8.0=h8b12597_0\n  - librmm=0.14.0=cuda10.2_0\n  - libstdcxx-ng=9.1.0=hdf63c60_0\n  - libtiff=4.1.0=hfc65ed5_0\n  - libxml2=2.9.10=hee79883_0\n  - llvm-openmp=10.0.0=hc9558a2_0\n  - llvmlite=0.32.1=py37h5202443_0\n  - locket=0.2.0=py_2\n  - lz4-c=1.8.3=he1b5a44_1001\n  - markupsafe=1.1.1=py37h8f50634_1\n  - msgpack-python=1.0.0=py37h99015e2_1\n  - nccl=2.6.4.1=hc6a2c23_0\n  - ncurses=6.2=he6710b0_1\n  - numba=0.49.1=py37h0da4684_0\n  - numpy=1.17.5=py37h95a1406_0\n  - nvstrings=0.14.0=py37_0\n  - olefile=0.46=py_0\n  - openssl=1.1.1g=h516909a_0\n  - packaging=20.4=pyh9f0ad1d_0\n  - pandas=0.25.3=py37hb3f55d8_0\n  - parquet-cpp=1.5.1=2\n  - partd=1.1.0=py_0\n  - pillow=5.3.0=py37h00a061d_1000\n  - pip=20.1.1=py37_1\n  - psutil=5.7.0=py37h8f50634_1\n  - pyarrow=0.15.0=py37h8b68381_1\n  - pyparsing=2.4.7=pyh9f0ad1d_0\n  - python=3.8.13=h12debd9_0\n  - python-dateutil=2.8.1=py_0\n  - python_abi=3.8=2_cp38\n  - pytz=2020.1=pyh9f0ad1d_0\n  - pyyaml=5.3.1=py37h8f50634_0\n  - re2=2020.04.01=he1b5a44_0\n  - readline=8.0=h7b6447c_0\n  - rmm=0.14.0=py37_0\n  - setuptools=47.3.0=py37_0\n  - six=1.15.0=pyh9f0ad1d_0\n  - snappy=1.1.8=he1b5a44_2\n  - sortedcontainers=2.2.2=pyh9f0ad1d_0\n  - spdlog=1.6.1=hc9558a2_0\n  - sqlite=3.31.1=h62c20be_1\n  - tblib=1.6.0=py_0\n  - thrift-cpp=0.12.0=hf3afdfd_1004\n  - tk=8.6.8=hbc83047_0\n  - toolz=0.10.0=py_0\n  - tornado=6.0.4=py37h8f50634_1\n  - typing_extensions=3.7.4.2=py_0\n  - ucx=1.8.0+gf6ec8d4=cuda10.2_20\n  - ucx-py=0.14.0+gf6ec8d4=py37_0\n  - uriparser=0.9.3=he1b5a44_1\n  - wheel=0.34.2=py37_0\n  - xz=5.2.5=h7b6447c_0\n  - yaml=0.2.5=h516909a_0\n  - zict=2.0.0=py_0\n  - zlib=1.2.11=h7b6447c_3\n  - zstd=1.4.3=h3b9ef0a_0\n  - pip:\n    ', 'psutil=5.7.0=py37h8f50634_1\n  - pyarrow=0.15.0=py37h8b68381_1\n  - pyparsing=2.4.7=pyh9f0ad1d_0\n  - python=3.8.13=h12debd9_0\n  - python-dateutil=2.8.1=py_0\n  - python_abi=3.8=2_cp38\n  - pytz=2020.1=pyh9f0ad1d_0\n  - pyyaml=5.3.1=py37h8f50634_0\n  - re2=2020.04.01=he1b5a44_0\n  - readline=8.0=h7b6447c_0\n  - rmm=0.14.0=py37_0\n  - setuptools=47.3.0=py37_0\n  - six=1.15.0=pyh9f0ad1d_0\n  - snappy=1.1.8=he1b5a44_2\n  - sortedcontainers=2.2.2=pyh9f0ad1d_0\n  - spdlog=1.6.1=hc9558a2_0\n  - sqlite=3.31.1=h62c20be_1\n  - tblib=1.6.0=py_0\n  - thrift-cpp=0.12.0=hf3afdfd_1004\n  - tk=8.6.8=hbc83047_0\n  - toolz=0.10.0=py_0\n  - tornado=6.0.4=py37h8f50634_1\n  - typing_extensions=3.7.4.2=py_0\n  - ucx=1.8.0+gf6ec8d4=cuda10.2_20\n  - ucx-py=0.14.0+gf6ec8d4=py37_0\n  - uriparser=0.9.3=he1b5a44_1\n  - wheel=0.34.2=py37_0\n  - xz=5.2.5=h7b6447c_0\n  - yaml=0.2.5=h516909a_0\n  - zict=2.0.0=py_0\n  - zlib=1.2.11=h7b6447c_3\n  - zstd=1.4.3=h3b9ef0a_0\n  - pip:\n      - alembic==1.4.2\n      - attrs==19.3.0\n      - backcall==0.2.0\n      - bleach==3.1.5\n      - chardet==3.0.4\n      - cycler==0.10.0\n      - databricks-cli==0.11.0\n      - decorator==4.4.2\n      - defusedxml==0.6.0\n      - docker==4.2.1\n      - entrypoints==0.3\n      - flask==1.1.2\n      - future==0.18.2\n      - gitdb==4.0.5\n      - gitpython==3.1.3\n      - gorilla==0.3.0\n      - gunicorn==20.0.4\n      - hyperopt==0.2.4\n      - idna==2.9\n      - importlib-metadata==1.6.1\n      - ipykernel==5.3.0\n      - ipython==7.15.0\n      - ipython-genutils==0.2.0\n      - ipywidgets==7.5.1\n      - itsdangerous==1.1.0\n      - jedi==0.17.0\n      - json5==0.9.5\n      - jsonschema==3.2.0\n      - jupyter==1.0.0\n      - jupyter-client==6.1.3\n      - jupyter-console==6.1.0\n      - jupyter-core==4.6.3\n      - jupyterlab==2.1.4\n      - jupyterlab-server==1.1.5\n      - kiwisolver==1.2.0\n      - lab==6.0\n  ', 'ipython==7.15.0\n      - ipython-genutils==0.2.0\n      - ipywidgets==7.5.1\n      - itsdangerous==1.1.0\n      - jedi==0.17.0\n      - json5==0.9.5\n      - jsonschema==3.2.0\n      - jupyter==1.0.0\n      - jupyter-client==6.1.3\n      - jupyter-console==6.1.0\n      - jupyter-core==4.6.3\n      - jupyterlab==2.1.4\n      - jupyterlab-server==1.1.5\n      - kiwisolver==1.2.0\n      - lab==6.0\n      - mako==1.1.3\n      - matplotlib==3.2.2\n      - mistune==0.8.4\n      - mlflow==1.8.0\n      - nbconvert==5.6.1\n      - nbformat==5.0.7\n      - networkx==2.4\n      - notebook==6.0.3\n      - pandocfilters==1.4.2\n      - parso==0.7.0\n      - pexpect==4.8.0\n      - pickleshare==0.7.5\n      - prometheus-client==0.8.0\n      - prometheus-flask-exporter==0.14.1\n      - prompt-toolkit==3.0.5\n      - protobuf==3.12.2\n      - ptyprocess==0.6.0\n      - pygments==2.6.1\n      - pyrsistent==0.16.0\n      - python-editor==1.0.4\n      - pyzmq==19.0.1\n      - qtconsole==4.7.4\n      - qtpy==1.9.0\n      - querystring-parser==1.2.4\n      - requests==2.24.0\n      - scikit-learn==0.23.1\n      - scipy==1.4.1\n      - send2trash==1.5.0\n      - simplejson==3.17.0\n      - sklearn==0.0\n      - smmap==3.0.4\n      - sqlalchemy==1.3.13\n      - sqlparse==0.4.2\n      - tabulate==0.8.7\n      - terminado==0.8.3\n      - ', ' - qtconsole==4.7.4\n      - qtpy==1.9.0\n      - querystring-parser==1.2.4\n      - requests==2.24.0\n      - scikit-learn==0.23.1\n      - scipy==1.4.1\n      - send2trash==1.5.0\n      - simplejson==3.17.0\n      - sklearn==0.0\n      - smmap==3.0.4\n      - sqlalchemy==1.3.13\n      - sqlparse==0.4.2\n      - tabulate==0.8.7\n      - terminado==0.8.3\n      - testpath==0.4.4\n      - threadpoolctl==2.1.0\n      - tqdm==4.46.1\n      - traitlets==4.3.3\n      - txt2tags==3.7\n      - urllib3==1.25.9\n      - wcwidth==0.2.4\n      - webencodings==0.5.1\n      - websocket-client==0.57.0\n      - werkzeug==1.0.1\n      - widgetsnbextension==3.5.1\n      - zipp==3.1.0\n', 'name: mlflow\nchannels:\n  - rapidsai\n  - nvidia\n  - conda-forge\n  - defaults\ndependencies:\n  - _libgcc_mutex=0.1=conda_forge\n  - _openmp_mutex=4.5=1_llvm\n  - arrow-cpp=0.15.0=py37h090bef1_2\n  - bokeh=2.1.0=py37hc8dfbb8_0\n  - boost-cpp=1.70.0=h8e57a91_2\n  - brotli=1.0.7=he1b5a44_1002\n  - bzip2=1.0.8=h516909a_2\n  - c-ares=1.15.0=h516909a_1001\n  - ca-certificates=2020.4.5.2=hecda079_0\n  - certifi=2020.4.5.2=py37hc8dfbb8_0\n  - click=7.1.2=pyh9f0ad1d_0\n  - cloudpickle=1.4.1=py_0\n  - cudatoolkit=10.2.89=h6bb024c_0\n  - cudf=0.14.0=py37_0\n  - cudnn=7.6.5=cuda10.2_0\n  - cuml=0.14.0=cuda10.2_py37_0\n  - cupy=7.5.0=py37h940342b_0\n  - cytoolz=0.10.1=py37h516909a_0\n  - dask=2.18.1=py_0\n  - dask-core=2.18.1=py_0\n  - dask-cudf=0.14.0=py37_0\n  - distributed=2.18.0=py37hc8dfbb8_0\n  - dlpack=0.2=he1b5a44_1\n  - double-conversion=3.1.5=he1b5a44_2\n  - fastavro=0.23.4=py37h8f50634_0\n  - fastrlock=0.5=py37h3340039_0\n  - freetype=2.10.2=he06d7ca_0\n  - fsspec=0.7.4=py_0\n  - gflags=2.2.2=he1b5a44_1002\n  - glog=0.4.0=h49b9bf7_3\n  - grpc-cpp=1.23.0=h18db393_0\n  - heapdict=1.0.1=py_0\n  - icu=64.2=he1b5a44_1\n  - jinja2=2.11.2=pyh9f0ad1d_0\n  - joblib=0.15.1=py_0\n  - jpeg=9d=h516909a_0\n  - ld_impl_linux-64=2.33.1=h53a641e_7\n  - libblas=3.8.0=16_openblas\n  - libcblas=3.8.0=16_openblas\n  - libcudf=0.14.0=cuda10.2_0\n  - libcuml=0.14.0=cuda10.2_0\n  - libcumlprims=0.14.1=cuda10.2_0\n  - libedit=3.1.20181209=hc058e9b_0\n  - libevent=2.1.10=h72c5cf5_0\n  - libffi=3.3=he6710b0_1\n  - libgcc-ng=9.2.0=h24d8f2e_2\n  - libgfortran-ng=7.5.0=hdf63c60_6\n  - libhwloc=2.1.0=h3c4fd83_0\n  - libiconv=1.15=h516909a_1006\n  - liblapack=3.8.0=16_openblas\n  - libllvm8=8.0.1=hc9558a2_0\n  - libnvstrings=0.14.0=cuda10.2_0\n  - libopenblas=0.3.9=h5ec1e0e_0\n  - libpng=1.6.37=hed695b0_1\n  - libprotobuf=3.8.0=h8b12597_0\n  - librmm=0.14.0=cuda10.2_0\n  - libstdcxx-ng=9.1.0=hdf63c60_0\n  - libtiff=4.1.0=hfc65ed5_0\n  - libxml2=2.9.10=hee79883_0\n  - llvm-openmp=10.0.0=hc9558a2_0\n  - llvmlite=0.32.1=py37h5202443_0\n  - locket=0.2.0=py_2\n  - lz4-c=1.8.3=he1b5a44_1001\n  - markupsafe=1.1.1=py37h8f50634_1\n  - msgpack-python=1.0.0=py37h99015e2_1\n  - nccl=2.6.4.1=hc6a2c23_0\n  - ncurses=6.2=he6710b0_1\n  - numba=0.49.1=py37h0da4684_0\n  - numpy=1.17.5=py37h95a1406_0\n  - nvstrings=0.14.0=py37_0\n  - olefile=0.46=py_0\n  - openssl=1.1.1g=h516909a_0\n  - packaging=20.4=pyh9f0ad1d_0\n  - pandas=0.25.3=py37hb3f55d8_0\n  - parquet-cpp=1.5.1=2\n  - partd=1.1.0=py_0\n  - pillow=5.3.0=py37h00a061d_1000\n  - pip=20.1.1=py37_1\n  - psutil=5.7.0=py37h8f50634_1\n  - pyarrow=0.15.0=py37h8b68381_1\n  - pyparsing=2.4.7=pyh9f0ad1d_0\n  - python=3.8.13=h12debd9_0\n  - python-dateutil=2.8.1=py_0\n  - python_abi=3.8=2_cp38\n  - pytz=2020.1=pyh9f0ad1d_0\n  - pyyaml=5.3.1=py37h8f50634_0\n  - re2=2020.04.01=he1b5a44_0\n  - readline=8.0=h7b6447c_0\n  - rmm=0.14.0=py37_0\n  - setuptools=47.3.0=py37_0\n  - six=1.15.0=pyh9f0ad1d_0\n  - snappy=1.1.8=he1b5a44_2\n  - sortedcontainers=2.2.2=pyh9f0ad1d_0\n  - spdlog=1.6.1=hc9558a2_0\n  - sqlite=3.31.1=h62c20be_1\n  - tblib=1.6.0=py_0\n  - thrift-cpp=0.12.0=hf3afdfd_1004\n  - tk=8.6.8=hbc83047_0\n  - toolz=0.10.0=py_0\n  - tornado=6.0.4=py37h8f50634_1\n  - typing_extensions=3.7.4.2=py_0\n  - ucx=1.8.0+gf6ec8d4=cuda10.2_20\n  - ucx-py=0.14.0+gf6ec8d4=py37_0\n  - uriparser=0.9.3=he1b5a44_1\n  - wheel=0.34.2=py37_0\n  - xz=5.2.5=h7b6447c_0\n  - yaml=0.2.5=h516909a_0\n  - zict=2.0.0=py_0\n  - zlib=1.2.11=h7b6447c_3\n  - zstd=1.4.3=h3b9ef0a_0\n  - pip:\n    ', 'psutil=5.7.0=py37h8f50634_1\n  - pyarrow=0.15.0=py37h8b68381_1\n  - pyparsing=2.4.7=pyh9f0ad1d_0\n  - python=3.8.13=h12debd9_0\n  - python-dateutil=2.8.1=py_0\n  - python_abi=3.8=2_cp38\n  - pytz=2020.1=pyh9f0ad1d_0\n  - pyyaml=5.3.1=py37h8f50634_0\n  - re2=2020.04.01=he1b5a44_0\n  - readline=8.0=h7b6447c_0\n  - rmm=0.14.0=py37_0\n  - setuptools=47.3.0=py37_0\n  - six=1.15.0=pyh9f0ad1d_0\n  - snappy=1.1.8=he1b5a44_2\n  - sortedcontainers=2.2.2=pyh9f0ad1d_0\n  - spdlog=1.6.1=hc9558a2_0\n  - sqlite=3.31.1=h62c20be_1\n  - tblib=1.6.0=py_0\n  - thrift-cpp=0.12.0=hf3afdfd_1004\n  - tk=8.6.8=hbc83047_0\n  - toolz=0.10.0=py_0\n  - tornado=6.0.4=py37h8f50634_1\n  - typing_extensions=3.7.4.2=py_0\n  - ucx=1.8.0+gf6ec8d4=cuda10.2_20\n  - ucx-py=0.14.0+gf6ec8d4=py37_0\n  - uriparser=0.9.3=he1b5a44_1\n  - wheel=0.34.2=py37_0\n  - xz=5.2.5=h7b6447c_0\n  - yaml=0.2.5=h516909a_0\n  - zict=2.0.0=py_0\n  - zlib=1.2.11=h7b6447c_3\n  - zstd=1.4.3=h3b9ef0a_0\n  - pip:\n      - alembic==1.4.2\n      - attrs==19.3.0\n      - backcall==0.2.0\n      - bleach==3.1.5\n      - chardet==3.0.4\n      - cycler==0.10.0\n      - databricks-cli==0.11.0\n      - decorator==4.4.2\n      - defusedxml==0.6.0\n      - docker==4.2.1\n      - entrypoints==0.3\n      - flask==1.1.2\n      - future==0.18.2\n      - gitdb==4.0.5\n      - gitpython==3.1.3\n      - gorilla==0.3.0\n      - gunicorn==20.0.4\n      - hyperopt==0.2.4\n      - idna==2.9\n      - importlib-metadata==1.6.1\n      - ipykernel==5.3.0\n      - ipython==7.15.0\n      - ipython-genutils==0.2.0\n      - ipywidgets==7.5.1\n      - itsdangerous==1.1.0\n      - jedi==0.17.0\n      - json5==0.9.5\n      - jsonschema==3.2.0\n      - jupyter==1.0.0\n      - jupyter-client==6.1.3\n      - jupyter-console==6.1.0\n      - jupyter-core==4.6.3\n      - jupyterlab==2.1.4\n      - jupyterlab-server==1.1.5\n      - kiwisolver==1.2.0\n      - lab==6.0\n  ', 'ipython==7.15.0\n      - ipython-genutils==0.2.0\n      - ipywidgets==7.5.1\n      - itsdangerous==1.1.0\n      - jedi==0.17.0\n      - json5==0.9.5\n      - jsonschema==3.2.0\n      - jupyter==1.0.0\n      - jupyter-client==6.1.3\n      - jupyter-console==6.1.0\n      - jupyter-core==4.6.3\n      - jupyterlab==2.1.4\n      - jupyterlab-server==1.1.5\n      - kiwisolver==1.2.0\n      - lab==6.0\n      - mako==1.1.3\n      - matplotlib==3.2.2\n      - mistune==0.8.4\n      - mlflow==1.8.0\n      - nbconvert==5.6.1\n      - nbformat==5.0.7\n      - networkx==2.4\n      - notebook==6.0.3\n      - pandocfilters==1.4.2\n      - parso==0.7.0\n      - pexpect==4.8.0\n      - pickleshare==0.7.5\n      - prometheus-client==0.8.0\n      - prometheus-flask-exporter==0.14.1\n      - prompt-toolkit==3.0.5\n      - protobuf==3.12.2\n      - ptyprocess==0.6.0\n      - pygments==2.6.1\n      - pyrsistent==0.16.0\n      - python-editor==1.0.4\n      - pyzmq==19.0.1\n      - qtconsole==4.7.4\n      - qtpy==1.9.0\n      - querystring-parser==1.2.4\n      - requests==2.24.0\n      - scikit-learn==0.23.1\n      - scipy==1.4.1\n      - send2trash==1.5.0\n      - simplejson==3.17.0\n      - sklearn==0.0\n      - smmap==3.0.4\n      - sqlalchemy==1.3.13\n      - sqlparse==0.4.2\n      - tabulate==0.8.7\n      - terminado==0.8.3\n      - ', ' - qtconsole==4.7.4\n      - qtpy==1.9.0\n      - querystring-parser==1.2.4\n      - requests==2.24.0\n      - scikit-learn==0.23.1\n      - scipy==1.4.1\n      - send2trash==1.5.0\n      - simplejson==3.17.0\n      - sklearn==0.0\n      - smmap==3.0.4\n      - sqlalchemy==1.3.13\n      - sqlparse==0.4.2\n      - tabulate==0.8.7\n      - terminado==0.8.3\n      - testpath==0.4.4\n      - threadpoolctl==2.1.0\n      - tqdm==4.46.1\n      - traitlets==4.3.3\n      - txt2tags==3.7\n      - urllib3==1.25.9\n      - wcwidth==0.2.4\n      - webencodings==0.5.1\n      - websocket-client==0.57.0\n      - werkzeug==1.0.1\n      - widgetsnbextension==3.5.1\n      - zipp==3.1.0\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - numpy\n  - matplotlib\n  - pandas\n  - scipy\n  - scikit-learn\n  - cloudpickle\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow>=1.0\n  - cloudpickle\n  - numpy\n  - matplotlib\n  - pandas\n  - scikit-learn\n', 'name: tutorial\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.9\n  - pip\n  - pip:\n      - scikit-learn==1.4.2\n      - mlflow>=1.0\n      - pandas\n', 'build_dependencies:\n  - pip\ndependencies:\n  - scikit-learn==1.4.2\n  - mlflow>=1.0\n  - pandas\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow>=1.0\n  - scipy\n  - scikit-learn\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow<3,>=2.1\n  - sktime==0.16.0\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow>=1.0\n  - spacy==3.8.2\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - statsmodels\n  - scikit-learn\n', 'python: "3.10"\nbuild_dependencies:\n  - pip\ndependencies:\n  - numpy\n  - pandas\n  - scipy\n  - scikit-learn\n  - mlflow\n', 'name: tutorial\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.8\n  - pip\n  - pip:\n      - mlflow>=2.0\n      - tensorflow>=2.8\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow>=2.0\n  - tensorflow>=2.8\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - scikit-learn\n  - matplotlib\n  - xgboost\n', 'build_dependencies:\n  - pip\ndependencies:\n  - mlflow\n  - pandas\n  - scikit-learn\n  - xgboost\n', '# These are the core requirements for the complete MLflow platform, which augments\n# the skinny client functionality with support for running the MLflow Tracking\n# Server & UI. It also adds project backends such as Docker and Kubernetes among\n# other capabilities. When we release a new major/minor version, this file is\n# automatically updated as a part of the release process.\n\nalembic:\n  pip_release: alembic\n  max_major_version: 1\n  # alembic 1.10.0 contains a regression: https://github.com/sqlalchemy/alembic/issues/1195\n  unsupported: ["1.10.0"]\n\ndocker:\n  pip_release: docker\n  minimum: "4.0.0"\n  max_major_version: 7\n\nflask:\n  pip_release: Flask\n  max_major_version: 3\n\nnumpy:\n  pip_release: numpy\n  max_major_version: 2\n\nscipy:\n  pip_release: scipy\n  max_major_version: 1\n\npandas:\n  pip_release: pandas\n  max_major_version: 2\n\nsqlalchemy:\n  pip_release: sqlalchemy\n  minimum: "1.4.0"\n  max_major_version: 2\n\ncryptography:\n  pip_release: cryptography\n  minimum: "43.0.0"\n  max_major_version: 45\n\ngunicorn:\n  pip_release: gunicorn\n  max_major_version: 23\n  markers: "platform_system != \'Windows\'"\n\nwaitress:\n  pip_release: waitress\n  max_major_version: 3\n  markers: "platform_system == \'Windows\'"\n\nscikit-learn:\n  pip_release: scikit-learn\n  max_major_version: 1\n\npyarrow:\n  pip_release: pyarrow\n  minimum: "4.0.0"\n  max_major_version: 21\n\nmatplotlib:\n  pip_release: matplotlib\n  max_major_version: 3\n\ngraphene:\n  pip_release: graphene\n  max_major_version: 3\n\nfastmcp:\n  pip_release: fastmcp\n  minimum: "2.0.0"\n  max_major_version: 2\n', '# These are the extra requirements for MLflow Gateway, which can be installed\n# on top of the core requirements using `pip install mlflow[gateway]`.\n# When we release a new major/minor version, this file is automatically updated\n# as a part of the release process.\n\nfastapi:\n  pip_release: fastapi\n  max_major_version: 0\n\nuvicorn:\n  pip_release: uvicorn\n  extras:\n    - standard\n  max_major_version: 0\n\nwatchfiles:\n  pip_release: watchfiles\n  max_major_version: 1\n\naiohttp:\n  pip_release: aiohttp\n  max_major_version: 3\n\nboto3:\n  pip_release: boto3\n  minimum: "1.28.56"\n  max_major_version: 1\n\ntiktoken:\n  pip_release: tiktoken\n  max_major_version: 0\n\nslowapi:\n  pip_release: slowapi\n  max_major_version: 0\n  minimum: "0.1.9"\n', '# Minimal requirements for the skinny MLflow client which provides a limited\n# subset of functionality such as: RESTful client functionality for Tracking and\n# Model Registry, as well as support for Project execution against local backends\n# and Databricks. When we release a new major/minor version, this file is automatically\n# updated as a part of the release process.\n\nclick:\n  pip_release: click\n  minimum: "7.0"\n  max_major_version: 8\n\ncloudpickle:\n  pip_release: cloudpickle\n  max_major_version: 3\n\npython-dotenv:\n  pip_release: python-dotenv\n  minimum: "0.19.0"\n  max_major_version: 1\n\ngitpython:\n  pip_release: gitpython\n  minimum: "3.1.9"\n  max_major_version: 3\n\npyyaml:\n  pip_release: pyyaml\n  minimum: "5.1"\n  max_major_version: 6\n\nprotobuf:\n  pip_release: protobuf\n  minimum: "3.12.0"\n  max_major_version: 6\n\nrequests:\n  pip_release: requests\n  minimum: "2.17.3"\n  max_major_version: 2\n\npackaging:\n  pip_release: packaging\n  max_major_version: 25\n\nimportlib_metadata:\n  pip_release: importlib_metadata\n  # Automated dependency detection in MLflow Models relies on\n  # `importlib_metadata.packages_distributions` to resolve a module name to its package name\n  # (e.g. \'sklearn\' -> \'scikit-learn\'). importlib_metadata 3.7.0 or newer supports this function:\n  # https://github.com/python/importlib_metadata/blob/main/CHANGES.rst#v370\n  minimum: "3.7.0"\n  max_major_version: 8\n  unsupported: ["4.7.0"]\n\nsqlparse:\n  pip_release: sqlparse\n  # Lower bound sqlparse for: https://github.com/andialbrecht/sqlparse/pull/567\n  minimum: "0.4.0"\n  max_major_version: 0\n\n# Required for tracing\ncachetools:\n  pip_release: cachetools\n  minimum: "5.0.0"\n  max_major_version: 6\n\n# 1.9.0 is the minimum supported version as NoOpTracer was introduced in 1.9.0\nopentelemetry-api:\n  pip_release: opentelemetry-api\n  minimum: "1.9.0"\n  max_major_version: 2\n\nopentelemetry-sdk:\n  pip_release: opentelemetry-sdk\n  minimum: "1.9.0"\n  max_major_version: 2\n\nopentelemetry-proto:\n  pip_release: opentelemetry-proto\n  minimum: "1.9.0"\n  max_major_version: 2\n\ndatabricks-sdk:\n  pip_release: databricks-sdk\n  minimum: "0.20.0"\n  max_major_version: 0\n\npydantic:\n  pip_release: pydantic\n  minimum: "1.10.8"\n  max_major_version: 2\n\ntyping-extensions:\n  pip_release: typing-extensions\n  minimum: "4.0.0"\n  max_major_version: 4\n\nfastapi:\n  pip_release: fastapi\n  max_major_version: 0\n\nuvicorn:\n  pip_release: uvicorn\n  max_major_version: 0\n', '# Minimal requirements for the MLflow Tracing package. It is a lightweight\n# package that only includes the minimum set of dependencies and functionality\n# to instrument code/models/agents with MLflow Tracing.\n# When we release a new major/minor version, this file is automatically\n# updated as a part of the release process.\n\nprotobuf:\n  pip_release: protobuf\n  minimum: "3.12.0"\n  max_major_version: 6\n\npackaging:\n  pip_release: packaging\n  max_major_version: 25\n\n# Required for tracing\ncachetools:\n  pip_release: cachetools\n  minimum: "5.0.0"\n  max_major_version: 6\n\n# 1.9.0 is the minimum supported version as NoOpTracer was introduced in 1.9.0\nopentelemetry-api:\n  pip_release: opentelemetry-api\n  minimum: "1.9.0"\n  max_major_version: 2\n\nopentelemetry-sdk:\n  pip_release: opentelemetry-sdk\n  minimum: "1.9.0"\n  max_major_version: 2\n\nopentelemetry-proto:\n  pip_release: opentelemetry-proto\n  minimum: "1.9.0"\n  max_major_version: 2\n\ndatabricks-sdk:\n  pip_release: databricks-sdk\n  minimum: "0.20.0"\n  max_major_version: 0\n\npydantic:\n  pip_release: pydantic\n  minimum: "1.10.8"\n  max_major_version: 2\n', 'llm:\n  model_name: "gpt-4o-mini"\n  temperature: 0.7\n', 'embedding_model_query_instructions: "Represent this sentence for searching relevant passages:"\nllm_model: "databricks-dbrx-instruct"\nllm_prompt_template: "You are a trustful assistant."\nretriever_config:\n  k: 5\n  use_mmr: false\nllm_parameters:\n  temperature: 0.01\n  max_tokens: 500\nllm_prompt_template_variables:\n  - "chat_history"\n  - "context"\n  - "question"\n', 'embedding_model_query_instructions: "Represent this sentence for searching relevant passages:"\nllm_model: "databricks-dbrx-instruct"\nllm_prompt_template: "You are a trustful assistant. Answer concisely and clearly."\nretriever_config:\n  k: 5\n  use_mmr: false\nllm_parameters:\n  temperature: 0.01\n  max_tokens: 200\nllm_prompt_template_variables:\n  - "chat_history"\n  - "context"\n  - "question"\n', '$schema: https://azuremlschemas.azureedge.net/promptflow/latest/Flow.schema.json\n\ninputs:\n  text:\n    type: string\n    default: Hello World!\n\noutputs:\n  output:\n    type: string\n    reference: ${llm.output}\n\nnodes:\n  - name: hello_prompt\n    type: python\n    source:\n      type: code\n      path: render_template.py\n    inputs:\n      text: ${inputs.text}\n      template: |\n        system:\n        Your task is to generate what I ask.\n        user:\n        Write a simple {{text}} program that displays the greeting message.\n  - name: llm\n    type: python\n    source:\n      type: code\n      path: echo.py\n    inputs:\n      prompt: ${hello_prompt.output}\nenvironment:\n  image: mcr.microsoft.com/azureml/promptflow/promptflow-runtime:latest\n  python_requirements_txt: requirements.txt\nadditional_includes:\n  - ../additional_file\n', 'channels:\n  - conda-forge\ndependencies:\n  - python=3.10.16\n  - pip<=23.0.1\n  - pip:\n      - mlflow\n      - cloudpickle==2.2.1\n      - scikit-learn==1.5.2\nname: mlflow-env\n', 'python: 3.10.16\nbuild_dependencies:\n  - pip==23.0.1\n  - setuptools==58.1.0\n  - wheel==0.42.0\ndependencies:\n  - -r requirements.txt\n', 'channels:\n  - conda-forge\ndependencies:\n  - python=3.10.12\n  - pip<=22.3.1\n  - pip:\n      - bcrypt==3.2.0\n      - cloudpickle==2.0.0\n      - configparser==5.2.0\n      - cryptography==39.0.1\n      - databricks-feature-engineering==0.2.1\n      - entrypoints==0.4\n      - google-cloud-storage==2.11.0\n      - grpcio-status==1.48.1\n      - langchain==0.1.20\n      - mlflow[gateway]==2.12.2\n      - numpy==1.23.5\n      - packaging==23.2\n      - pandas==1.5.3\n      - protobuf==4.24.0\n      - psutil==5.9.0\n      - pyarrow==8.0.0\n      - pydantic==1.10.6\n      - pyyaml==6.0\n      - requests==2.28.1\n      - tornado==6.1\nname: mlflow-env\n', 'python: 3.10.12\nbuild_dependencies:\n  - pip==22.3.1\n  - setuptools==65.6.3\n  - wheel==0.38.4\ndependencies:\n  - -r requirements.txt\n', 'channels:\n  - conda-forge\ndependencies:\n  - python=3.10.12\n  - pip<=22.3.1\n  - pip:\n      - bcrypt==3.2.0\n      - cloudpickle==2.0.0\n      - configparser==5.2.0\n      - cryptography==39.0.1\n      - databricks-feature-engineering==0.2.1\n      - entrypoints==0.4\n      - google-cloud-storage==2.11.0\n      - grpcio-status==1.48.1\n      - langchain==0.1.20\n      - mlflow[gateway]==2.12.2\n      - numpy==1.23.5\n      - packaging==23.2\n      - pandas==1.5.3\n      - protobuf==4.24.0\n      - psutil==5.9.0\n      - pyarrow==8.0.0\n      - pydantic==1.10.6\n      - pyyaml==6.0\n      - requests==2.28.1\n      - tornado==6.1\nname: mlflow-env\n', 'python: 3.10.12\nbuild_dependencies:\n  - pip==22.3.1\n  - setuptools==65.6.3\n  - wheel==0.38.4\ndependencies:\n  - -r requirements.txt\n', '# Adding a comment here to distinguish the hash of this conda.yaml from the hashes of existing\n# conda.yaml files in the test environment.\nname: tutorial\nchannels:\n  - defaults\ndependencies:\n  - python=3.10\n  - pip:\n      - -e ../../../\n      - psutil\n', 'name: virtualenv-conda\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.10.16\n  - numpy\n  - pip\n  - pip:\n      - mlflow\n      - scikit-learn==1.4.2\n', 'python: "3.10.16"\nbuild_dependencies:\n  - pip\ndependencies:\n  - -r requirements.txt\n', 'python: "3.10.16"\nbuild_dependencies:\n  - pip\ndependencies:\n  - -r requirements.txt\n', 'channels:\n  - conda-forge\ndependencies:\n  - python=3.9.13\n  - pip<=23.3\n  - pip:\n      - mlflow==2.7.1\n      - cloudpickle==2.2.1\nname: mlflow-env\n', 'python: 3.9.13\nbuild_dependencies:\n  - pip==23.3\n  - setuptools\n  - wheel==0.41.2\ndependencies:\n  - -r requirements.txt\n', 'channels:\n  - conda-forge\ndependencies:\n  - python=3.9.13\n  - pip<=23.3\n  - pip:\n      - mlflow==2.8.1\n      - cloudpickle==2.2.1\nname: mlflow-env\n', 'python: 3.9.13\nbuild_dependencies:\n  - pip==23.3\n  - setuptools\n  - wheel==0.41.2\ndependencies:\n  - -r requirements.txt\n', 'volumes:\n  pgdata:\n  minio-data:\n\nservices:\n  postgres:\n    image: postgres:15\n    container_name: mlflow-postgres\n    environment:\n      POSTGRES_USER: ${POSTGRES_USER}\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n      POSTGRES_DB: ${POSTGRES_DB}\n    volumes:\n      - pgdata:/var/lib/postgresql/data\n    ports:\n      - "5432:5432"\n    healthcheck:\n      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]\n      interval: 5s\n      timeout: 3s\n      retries: 10\n\n  minio:\n    image: minio/minio:latest\n    container_name: mlflow-minio\n    environment:\n      MINIO_ROOT_USER: ${MINIO_ROOT_USER}\n      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}\n    volumes:\n      - minio-data:/data\n    command: server /data --console-address ":9001"\n    ports:\n      - "9000:9000"\n      - "9001:9001"\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${MINIO_PORT}/minio/health/live"]\n      interval: 5s\n      timeout: 3s\n      retries: 20\n\n  create-bucket:\n    image: minio/mc:latest\n    container_name: mlflow-create-bucket\n    depends_on:\n      minio:\n        condition: service_healthy\n    entrypoint: >\n      /bin/sh -c \'\n        mc alias set myminio http://${MINIO_HOST}:${MINIO_PORT} \\\n          ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD} &&\n        mc mb --ignore-existing myminio/${MINIO_BUCKET:-mlflow}\n      \'\n    restart: "no"\n\n  mlflow:\n    image: ghcr.io/mlflow/mlflow:${MLFLOW_VERSION}\n    container_name: mlflow-server\n    depends_on:\n      postgres:\n        condition: service_healthy\n      minio:\n        condition: service_healthy\n      create-bucket:\n       ', '  mc alias set myminio http://${MINIO_HOST}:${MINIO_PORT} \\\n          ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD} &&\n        mc mb --ignore-existing myminio/${MINIO_BUCKET:-mlflow}\n      \'\n    restart: "no"\n\n  mlflow:\n    image: ghcr.io/mlflow/mlflow:${MLFLOW_VERSION}\n    container_name: mlflow-server\n    depends_on:\n      postgres:\n        condition: service_healthy\n      minio:\n        condition: service_healthy\n      create-bucket:\n        condition: service_completed_successfully\n    environment:\n      # Backend store URI built from vars\n      MLFLOW_BACKEND_STORE_URI: ${MLFLOW_BACKEND_STORE_URI}\n\n      # S3/MinIO settings\n      MLFLOW_S3_ENDPOINT_URL: ${MLFLOW_S3_ENDPOINT_URL}\n      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}\n      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}\n      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION}\n      MLFLOW_S3_IGNORE_TLS: "true"\n\n      # Server host/port\n      MLFLOW_HOST: ${MLFLOW_HOST}\n      MLFLOW_PORT: ${MLFLOW_PORT}\n\n    command: >\n      /bin/bash -c "\n        pip install --no-cache-dir psycopg2-binary boto3 &&\n        mlflow server \\\n          --backend-store-uri ${MLFLOW_BACKEND_STORE_URI} \\\n          --default-artifact-root ${MLFLOW_DEFAULT_ARTIFACT_ROOT} \\\n          --host ${MLFLOW_HOST} \\\n          --port ${MLFLOW_PORT}\n      "\n    ports:\n      - "${MLFLOW_PORT}:${MLFLOW_PORT}"\n    healthcheck:\n      test:\n        [\n          "CMD",\n          "python",\n          "-c",\n          "import urllib.request; ', '\\\n          --host ${MLFLOW_HOST} \\\n          --port ${MLFLOW_PORT}\n      "\n    ports:\n      - "${MLFLOW_PORT}:${MLFLOW_PORT}"\n    healthcheck:\n      test:\n        [\n          "CMD",\n          "python",\n          "-c",\n          "import urllib.request; urllib.request.urlopen(\'http://localhost:${MLFLOW_PORT}/health\')",\n        ]\n      interval: 10s\n      timeout: 5s\n      retries: 30\n\nnetworks:\n  default:\n    name: mlflow-network\n', 'version: "3"\nservices:\n  minio:\n    image: minio/minio\n    expose:\n      - "9000"\n    ports:\n      - "9000:9000"\n      # MinIO Console is available at http://localhost:9001\n      - "9001:9001"\n    environment:\n      MINIO_ROOT_USER: "user"\n      MINIO_ROOT_PASSWORD: "password"\n    healthcheck:\n      test: timeout 5s bash -c \':> /dev/tcp/127.0.0.1/9000\' || exit 1\n      interval: 1s\n      timeout: 10s\n      retries: 5\n    # Note there is no bucket by default\n    command: server /data --console-address ":9001"\n\n  minio-create-bucket:\n    image: minio/mc\n    depends_on:\n      minio:\n        condition: service_healthy\n    entrypoint: >\n      bash -c "\n      mc alias set minio http://minio:9000 user password &&\n      if ! mc ls minio/bucket; then\n        mc mb minio/bucket\n      else\n        echo \'bucket already exists\'\n      fi\n      "\n\n  artifacts-server:\n    build:\n      context: .\n      dockerfile: "${DOCKERFILE:-Dockerfile}"\n    depends_on:\n      - minio-create-bucket\n    expose:\n      - "5500"\n    ports:\n      - "5500:5500"\n    environment:\n      MLFLOW_S3_ENDPOINT_URL: http://minio:9000\n      AWS_ACCESS_KEY_ID: "user"\n      AWS_SECRET_ACCESS_KEY: "password"\n    command: >\n      mlflow server\n      --host 0.0.0.0\n      --port 5500\n      --artifacts-destination s3://bucket\n      --gunicorn-opts "--log-level debug"\n   ', '  depends_on:\n      - minio-create-bucket\n    expose:\n      - "5500"\n    ports:\n      - "5500:5500"\n    environment:\n      MLFLOW_S3_ENDPOINT_URL: http://minio:9000\n      AWS_ACCESS_KEY_ID: "user"\n      AWS_SECRET_ACCESS_KEY: "password"\n    command: >\n      mlflow server\n      --host 0.0.0.0\n      --port 5500\n      --artifacts-destination s3://bucket\n      --gunicorn-opts "--log-level debug"\n      --artifacts-only\n\n  postgres:\n    image: postgres\n    restart: always\n    environment:\n      POSTGRES_DB: db\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n\n  tracking-server:\n    build:\n      context: .\n      dockerfile: "${DOCKERFILE:-Dockerfile}"\n    depends_on:\n      - postgres\n      - artifacts-server\n    expose:\n      - "5000"\n    ports:\n      # MLflow UI is available at http://localhost:5000\n      - "5000:5000"\n    command: >\n      mlflow server\n      --host 0.0.0.0\n      --port 5000\n      --backend-store-uri postgresql://user:password@postgres:5432/db\n      --default-artifact-root http://artifacts-server:5500/api/2.0/mlflow-artifacts/artifacts/experiments\n      --gunicorn-opts "--log-level debug"\n\n  client:\n    build:\n      context: .\n      dockerfile: "${DOCKERFILE:-Dockerfile}"\n    depends_on:\n      - tracking-server\n    environment:\n      MLFLOW_TRACKING_URI: http://tracking-server:5000\n', 'services:\n  base:\n    image: mlflow-base\n    build:\n      context: .\n    volumes:\n      - ${PWD}:/mlflow/home\n    working_dir: /mlflow/home\n    entrypoint: /mlflow/home/tests/db/entrypoint.sh\n    command: pytest tests/db\n    environment:\n      DISABLE_RESET_MLFLOW_URI_FIXTURE: "true"\n\n  postgresql:\n    image: postgres\n    restart: always\n    environment:\n      POSTGRES_DB: mlflowdb\n      POSTGRES_USER: mlflowuser\n      POSTGRES_PASSWORD: mlflowpassword\n\n  mlflow-postgresql:\n    depends_on:\n      - postgresql\n    extends:\n      service: base\n    environment:\n      MLFLOW_TRACKING_URI: postgresql://mlflowuser:mlflowpassword@postgresql:5432/mlflowdb\n      INSTALL_MLFLOW_FROM_REPO: true\n\n  migration-postgresql:\n    depends_on:\n      - postgresql\n    extends:\n      service: base\n    environment:\n      MLFLOW_TRACKING_URI: postgresql://mlflowuser:mlflowpassword@postgresql:5432/mlflowdb\n    command: tests/db/check_migration.sh\n\n  mysql:\n    image: mysql\n    restart: always\n    environment:\n      MYSQL_ROOT_PASSWORD: root-password\n      MYSQL_DATABASE: mlflowdb\n      MYSQL_USER: mlflowuser\n      MYSQL_PASSWORD: mlflowpassword\n\n  mlflow-mysql:\n    extends:\n      service: base\n    depends_on:\n      - mysql\n    environment:\n      MLFLOW_TRACKING_URI: mysql://mlflowuser:mlflowpassword@mysql:3306/mlflowdb?charset=utf8mb4\n      INSTALL_MLFLOW_FROM_REPO: true\n\n  migration-mysql:\n    extends:\n      service: base\n    depends_on:\n      - mysql\n    environment:\n      MLFLOW_TRACKING_URI: mysql://mlflowuser:mlflowpassword@mysql:3306/mlflowdb?charset=utf8mb4\n    command: tests/db/check_migration.sh\n\n  mssql:\n    image: mcr.microsoft.com/mssql/server\n    restart: always\n    environment:\n      ACCEPT_EULA: Y\n      SA_PASSWORD: "1Secure*Password1"\n\n  mlflow-mssql:\n    depends_on:\n      - mssql\n    extends:\n  ', '   MLFLOW_TRACKING_URI: mysql://mlflowuser:mlflowpassword@mysql:3306/mlflowdb?charset=utf8mb4\n      INSTALL_MLFLOW_FROM_REPO: true\n\n  migration-mysql:\n    extends:\n      service: base\n    depends_on:\n      - mysql\n    environment:\n      MLFLOW_TRACKING_URI: mysql://mlflowuser:mlflowpassword@mysql:3306/mlflowdb?charset=utf8mb4\n    command: tests/db/check_migration.sh\n\n  mssql:\n    image: mcr.microsoft.com/mssql/server\n    restart: always\n    environment:\n      ACCEPT_EULA: Y\n      SA_PASSWORD: "1Secure*Password1"\n\n  mlflow-mssql:\n    depends_on:\n      - mssql\n    extends:\n      service: base\n    platform: linux/amd64\n    image: mlflow-mssql\n    build:\n      context: .\n      dockerfile: Dockerfile.mssql\n    environment:\n      MLFLOW_TRACKING_URI: mssql+pyodbc://mlflowuser:Mlfl*wpassword1@mssql/mlflowdb?driver=ODBC+Driver+17+for+SQL+Server\n      INSTALL_MLFLOW_FROM_REPO: true\n\n  migration-mssql:\n    depends_on:\n      - mssql\n    extends:\n      service: base\n    platform: linux/amd64\n    image: mlflow-mssql\n    build:\n      context: .\n      dockerfile: Dockerfile.mssql\n    environment:\n      # We could try using ODBC Driver 18 and append `LongAsMax=Yes` to fix error for sqlalchemy<2.0:\n      # [ODBC Driver 17 for SQL Server][SQL Server]The data types varchar and ntext are incompatible in the equal to operator\n      # https://docs.sqlalchemy.org/en/20/dialects/mssql.html#avoiding-sending-large-string-parameters-as-text-ntext\n      MLFLOW_TRACKING_URI: mssql+pyodbc://mlflowuser:Mlfl*wpassword1@mssql/mlflowdb?driver=ODBC+Driver+17+for+SQL+Server\n    command: tests/db/check_migration.sh\n\n  mlflow-sqlite:\n    extends:\n      service: base\n    environment:\n      MLFLOW_TRACKING_URI: "sqlite:////tmp/mlflowdb"\n      INSTALL_MLFLOW_FROM_REPO: true\n\n  migration-sqlite:\n    extends:\n      service: base\n    environment:\n      MLFLOW_TRACKING_URI: "sqlite:////tmp/mlflowdb"\n    command: tests/db/check_migration.sh\n', 'prompt_with_history_str: "Here is a history between you and a human: {chat_history}\\nNow, please answer this question: {question}"\n', 'llm_prompt_template: "Answer the following question based on the context: {context}\\nQuestion: {question}"\nembedding_size: 5\nresponse: "Databricks"\nnot_used_array:\n  - 1\n  - 2\n  - 3\n', 'use_gpu: True\ntemperature: 0.9\ntimeout: 300\n', 'llm_prompt_template: "Answer the following question based on the context: {context}\\nQuestion: {question}"\nembedding_size: 5\nresponse: "Databricks"\n']}}}
2025-09-05 20:09:11,161 - INFO - api.data_pipeline - data_pipeline.py:805 - Loaded 21886 documents from existing database
2025-09-05 20:09:11,161 - INFO - api.rag - rag.py:371 - Loaded 21886 documents for retrieval
2025-09-05 20:09:11,163 - WARNING - api.rag - rag.py:285 - Document 3500 has empty embedding vector, skipping
2025-09-05 20:09:11,163 - WARNING - api.rag - rag.py:285 - Document 3501 has empty embedding vector, skipping
2025-09-05 20:09:11,163 - WARNING - api.rag - rag.py:285 - Document 3502 has empty embedding vector, skipping
2025-09-05 20:09:11,164 - WARNING - api.rag - rag.py:285 - Document 3503 has empty embedding vector, skipping
2025-09-05 20:09:11,164 - WARNING - api.rag - rag.py:285 - Document 3504 has empty embedding vector, skipping
2025-09-05 20:09:11,164 - WARNING - api.rag - rag.py:285 - Document 3505 has empty embedding vector, skipping
2025-09-05 20:09:11,164 - WARNING - api.rag - rag.py:285 - Document 3506 has empty embedding vector, skipping
2025-09-05 20:09:11,164 - WARNING - api.rag - rag.py:285 - Document 3507 has empty embedding vector, skipping
2025-09-05 20:09:11,164 - WARNING - api.rag - rag.py:285 - Document 3508 has empty embedding vector, skipping
2025-09-05 20:09:11,164 - WARNING - api.rag - rag.py:285 - Document 3509 has empty embedding vector, skipping
2025-09-05 20:09:11,165 - WARNING - api.rag - rag.py:285 - Document 3510 has empty embedding vector, skipping
2025-09-05 20:09:11,165 - WARNING - api.rag - rag.py:285 - Document 3511 has empty embedding vector, skipping
2025-09-05 20:09:11,165 - WARNING - api.rag - rag.py:285 - Document 3512 has empty embedding vector, skipping
2025-09-05 20:09:11,165 - WARNING - api.rag - rag.py:285 - Document 3513 has empty embedding vector, skipping
2025-09-05 20:09:11,166 - WARNING - api.rag - rag.py:285 - Document 3514 has empty embedding vector, skipping
2025-09-05 20:09:11,166 - WARNING - api.rag - rag.py:285 - Document 3515 has empty embedding vector, skipping
2025-09-05 20:09:11,166 - WARNING - api.rag - rag.py:285 - Document 3516 has empty embedding vector, skipping
2025-09-05 20:09:11,166 - WARNING - api.rag - rag.py:285 - Document 3517 has empty embedding vector, skipping
2025-09-05 20:09:11,166 - WARNING - api.rag - rag.py:285 - Document 3518 has empty embedding vector, skipping
2025-09-05 20:09:11,166 - WARNING - api.rag - rag.py:285 - Document 3519 has empty embedding vector, skipping
2025-09-05 20:09:11,166 - WARNING - api.rag - rag.py:285 - Document 3520 has empty embedding vector, skipping
2025-09-05 20:09:11,167 - WARNING - api.rag - rag.py:285 - Document 3521 has empty embedding vector, skipping
2025-09-05 20:09:11,167 - WARNING - api.rag - rag.py:285 - Document 3522 has empty embedding vector, skipping
2025-09-05 20:09:11,167 - WARNING - api.rag - rag.py:285 - Document 3523 has empty embedding vector, skipping
2025-09-05 20:09:11,167 - WARNING - api.rag - rag.py:285 - Document 3524 has empty embedding vector, skipping
2025-09-05 20:09:11,167 - WARNING - api.rag - rag.py:285 - Document 3525 has empty embedding vector, skipping
2025-09-05 20:09:11,167 - WARNING - api.rag - rag.py:285 - Document 3526 has empty embedding vector, skipping
2025-09-05 20:09:11,167 - WARNING - api.rag - rag.py:285 - Document 3527 has empty embedding vector, skipping
2025-09-05 20:09:11,167 - WARNING - api.rag - rag.py:285 - Document 3528 has empty embedding vector, skipping
2025-09-05 20:09:11,167 - WARNING - api.rag - rag.py:285 - Document 3529 has empty embedding vector, skipping
2025-09-05 20:09:11,168 - WARNING - api.rag - rag.py:285 - Document 3530 has empty embedding vector, skipping
2025-09-05 20:09:11,168 - WARNING - api.rag - rag.py:285 - Document 3531 has empty embedding vector, skipping
2025-09-05 20:09:11,168 - WARNING - api.rag - rag.py:285 - Document 3532 has empty embedding vector, skipping
2025-09-05 20:09:11,168 - WARNING - api.rag - rag.py:285 - Document 3533 has empty embedding vector, skipping
2025-09-05 20:09:11,168 - WARNING - api.rag - rag.py:285 - Document 3534 has empty embedding vector, skipping
2025-09-05 20:09:11,169 - WARNING - api.rag - rag.py:285 - Document 3535 has empty embedding vector, skipping
2025-09-05 20:09:11,169 - WARNING - api.rag - rag.py:285 - Document 3536 has empty embedding vector, skipping
2025-09-05 20:09:11,169 - WARNING - api.rag - rag.py:285 - Document 3537 has empty embedding vector, skipping
2025-09-05 20:09:11,169 - WARNING - api.rag - rag.py:285 - Document 3538 has empty embedding vector, skipping
2025-09-05 20:09:11,169 - WARNING - api.rag - rag.py:285 - Document 3539 has empty embedding vector, skipping
2025-09-05 20:09:11,169 - WARNING - api.rag - rag.py:285 - Document 3540 has empty embedding vector, skipping
2025-09-05 20:09:11,169 - WARNING - api.rag - rag.py:285 - Document 3541 has empty embedding vector, skipping
2025-09-05 20:09:11,169 - WARNING - api.rag - rag.py:285 - Document 3542 has empty embedding vector, skipping
2025-09-05 20:09:11,169 - WARNING - api.rag - rag.py:285 - Document 3543 has empty embedding vector, skipping
2025-09-05 20:09:11,170 - WARNING - api.rag - rag.py:285 - Document 3544 has empty embedding vector, skipping
2025-09-05 20:09:11,170 - WARNING - api.rag - rag.py:285 - Document 3545 has empty embedding vector, skipping
2025-09-05 20:09:11,170 - WARNING - api.rag - rag.py:285 - Document 3546 has empty embedding vector, skipping
2025-09-05 20:09:11,170 - WARNING - api.rag - rag.py:285 - Document 3547 has empty embedding vector, skipping
2025-09-05 20:09:11,170 - WARNING - api.rag - rag.py:285 - Document 3548 has empty embedding vector, skipping
2025-09-05 20:09:11,170 - WARNING - api.rag - rag.py:285 - Document 3549 has empty embedding vector, skipping
2025-09-05 20:09:11,170 - WARNING - api.rag - rag.py:285 - Document 3550 has empty embedding vector, skipping
2025-09-05 20:09:11,170 - WARNING - api.rag - rag.py:285 - Document 3551 has empty embedding vector, skipping
2025-09-05 20:09:11,171 - WARNING - api.rag - rag.py:285 - Document 3552 has empty embedding vector, skipping
2025-09-05 20:09:11,171 - WARNING - api.rag - rag.py:285 - Document 3553 has empty embedding vector, skipping
2025-09-05 20:09:11,171 - WARNING - api.rag - rag.py:285 - Document 3554 has empty embedding vector, skipping
2025-09-05 20:09:11,171 - WARNING - api.rag - rag.py:285 - Document 3555 has empty embedding vector, skipping
2025-09-05 20:09:11,171 - WARNING - api.rag - rag.py:285 - Document 3556 has empty embedding vector, skipping
2025-09-05 20:09:11,171 - WARNING - api.rag - rag.py:285 - Document 3557 has empty embedding vector, skipping
2025-09-05 20:09:11,171 - WARNING - api.rag - rag.py:285 - Document 3558 has empty embedding vector, skipping
2025-09-05 20:09:11,171 - WARNING - api.rag - rag.py:285 - Document 3559 has empty embedding vector, skipping
2025-09-05 20:09:11,171 - WARNING - api.rag - rag.py:285 - Document 3560 has empty embedding vector, skipping
2025-09-05 20:09:11,172 - WARNING - api.rag - rag.py:285 - Document 3561 has empty embedding vector, skipping
2025-09-05 20:09:11,172 - WARNING - api.rag - rag.py:285 - Document 3562 has empty embedding vector, skipping
2025-09-05 20:09:11,172 - WARNING - api.rag - rag.py:285 - Document 3563 has empty embedding vector, skipping
2025-09-05 20:09:11,172 - WARNING - api.rag - rag.py:285 - Document 3564 has empty embedding vector, skipping
2025-09-05 20:09:11,172 - WARNING - api.rag - rag.py:285 - Document 3565 has empty embedding vector, skipping
2025-09-05 20:09:11,172 - WARNING - api.rag - rag.py:285 - Document 3566 has empty embedding vector, skipping
2025-09-05 20:09:11,172 - WARNING - api.rag - rag.py:285 - Document 3567 has empty embedding vector, skipping
2025-09-05 20:09:11,172 - WARNING - api.rag - rag.py:285 - Document 3568 has empty embedding vector, skipping
2025-09-05 20:09:11,173 - WARNING - api.rag - rag.py:285 - Document 3569 has empty embedding vector, skipping
2025-09-05 20:09:11,173 - WARNING - api.rag - rag.py:285 - Document 3570 has empty embedding vector, skipping
2025-09-05 20:09:11,173 - WARNING - api.rag - rag.py:285 - Document 3571 has empty embedding vector, skipping
2025-09-05 20:09:11,173 - WARNING - api.rag - rag.py:285 - Document 3572 has empty embedding vector, skipping
2025-09-05 20:09:11,173 - WARNING - api.rag - rag.py:285 - Document 3573 has empty embedding vector, skipping
2025-09-05 20:09:11,173 - WARNING - api.rag - rag.py:285 - Document 3574 has empty embedding vector, skipping
2025-09-05 20:09:11,173 - WARNING - api.rag - rag.py:285 - Document 3575 has empty embedding vector, skipping
2025-09-05 20:09:11,173 - WARNING - api.rag - rag.py:285 - Document 3576 has empty embedding vector, skipping
2025-09-05 20:09:11,173 - WARNING - api.rag - rag.py:285 - Document 3577 has empty embedding vector, skipping
2025-09-05 20:09:11,174 - WARNING - api.rag - rag.py:285 - Document 3578 has empty embedding vector, skipping
2025-09-05 20:09:11,174 - WARNING - api.rag - rag.py:285 - Document 3579 has empty embedding vector, skipping
2025-09-05 20:09:11,174 - WARNING - api.rag - rag.py:285 - Document 3580 has empty embedding vector, skipping
2025-09-05 20:09:11,174 - WARNING - api.rag - rag.py:285 - Document 3581 has empty embedding vector, skipping
2025-09-05 20:09:11,174 - WARNING - api.rag - rag.py:285 - Document 3582 has empty embedding vector, skipping
2025-09-05 20:09:11,174 - WARNING - api.rag - rag.py:285 - Document 3583 has empty embedding vector, skipping
2025-09-05 20:09:11,174 - WARNING - api.rag - rag.py:285 - Document 3584 has empty embedding vector, skipping
2025-09-05 20:09:11,174 - WARNING - api.rag - rag.py:285 - Document 3585 has empty embedding vector, skipping
2025-09-05 20:09:11,175 - WARNING - api.rag - rag.py:285 - Document 3586 has empty embedding vector, skipping
2025-09-05 20:09:11,175 - WARNING - api.rag - rag.py:285 - Document 3587 has empty embedding vector, skipping
2025-09-05 20:09:11,175 - WARNING - api.rag - rag.py:285 - Document 3588 has empty embedding vector, skipping
2025-09-05 20:09:11,175 - WARNING - api.rag - rag.py:285 - Document 3589 has empty embedding vector, skipping
2025-09-05 20:09:11,175 - WARNING - api.rag - rag.py:285 - Document 3590 has empty embedding vector, skipping
2025-09-05 20:09:11,175 - WARNING - api.rag - rag.py:285 - Document 3591 has empty embedding vector, skipping
2025-09-05 20:09:11,175 - WARNING - api.rag - rag.py:285 - Document 3592 has empty embedding vector, skipping
2025-09-05 20:09:11,175 - WARNING - api.rag - rag.py:285 - Document 3593 has empty embedding vector, skipping
2025-09-05 20:09:11,175 - WARNING - api.rag - rag.py:285 - Document 3594 has empty embedding vector, skipping
2025-09-05 20:09:11,176 - WARNING - api.rag - rag.py:285 - Document 3595 has empty embedding vector, skipping
2025-09-05 20:09:11,176 - WARNING - api.rag - rag.py:285 - Document 3596 has empty embedding vector, skipping
2025-09-05 20:09:11,176 - WARNING - api.rag - rag.py:285 - Document 3597 has empty embedding vector, skipping
2025-09-05 20:09:11,176 - WARNING - api.rag - rag.py:285 - Document 3598 has empty embedding vector, skipping
2025-09-05 20:09:11,176 - WARNING - api.rag - rag.py:285 - Document 3599 has empty embedding vector, skipping
2025-09-05 20:09:11,176 - WARNING - api.rag - rag.py:285 - Document 3600 has empty embedding vector, skipping
2025-09-05 20:09:11,176 - WARNING - api.rag - rag.py:285 - Document 3601 has empty embedding vector, skipping
2025-09-05 20:09:11,176 - WARNING - api.rag - rag.py:285 - Document 3602 has empty embedding vector, skipping
2025-09-05 20:09:11,176 - WARNING - api.rag - rag.py:285 - Document 3603 has empty embedding vector, skipping
2025-09-05 20:09:11,177 - WARNING - api.rag - rag.py:285 - Document 3604 has empty embedding vector, skipping
2025-09-05 20:09:11,177 - WARNING - api.rag - rag.py:285 - Document 3605 has empty embedding vector, skipping
2025-09-05 20:09:11,177 - WARNING - api.rag - rag.py:285 - Document 3606 has empty embedding vector, skipping
2025-09-05 20:09:11,177 - WARNING - api.rag - rag.py:285 - Document 3607 has empty embedding vector, skipping
2025-09-05 20:09:11,177 - WARNING - api.rag - rag.py:285 - Document 3608 has empty embedding vector, skipping
2025-09-05 20:09:11,177 - WARNING - api.rag - rag.py:285 - Document 3609 has empty embedding vector, skipping
2025-09-05 20:09:11,177 - WARNING - api.rag - rag.py:285 - Document 3610 has empty embedding vector, skipping
2025-09-05 20:09:11,177 - WARNING - api.rag - rag.py:285 - Document 3611 has empty embedding vector, skipping
2025-09-05 20:09:11,177 - WARNING - api.rag - rag.py:285 - Document 3612 has empty embedding vector, skipping
2025-09-05 20:09:11,178 - WARNING - api.rag - rag.py:285 - Document 3613 has empty embedding vector, skipping
2025-09-05 20:09:11,178 - WARNING - api.rag - rag.py:285 - Document 3614 has empty embedding vector, skipping
2025-09-05 20:09:11,178 - WARNING - api.rag - rag.py:285 - Document 3615 has empty embedding vector, skipping
2025-09-05 20:09:11,178 - WARNING - api.rag - rag.py:285 - Document 3616 has empty embedding vector, skipping
2025-09-05 20:09:11,178 - WARNING - api.rag - rag.py:285 - Document 3617 has empty embedding vector, skipping
2025-09-05 20:09:11,179 - WARNING - api.rag - rag.py:285 - Document 3618 has empty embedding vector, skipping
2025-09-05 20:09:11,179 - WARNING - api.rag - rag.py:285 - Document 3619 has empty embedding vector, skipping
2025-09-05 20:09:11,179 - WARNING - api.rag - rag.py:285 - Document 3620 has empty embedding vector, skipping
2025-09-05 20:09:11,179 - WARNING - api.rag - rag.py:285 - Document 3621 has empty embedding vector, skipping
2025-09-05 20:09:11,179 - WARNING - api.rag - rag.py:285 - Document 3622 has empty embedding vector, skipping
2025-09-05 20:09:11,179 - WARNING - api.rag - rag.py:285 - Document 3623 has empty embedding vector, skipping
2025-09-05 20:09:11,180 - WARNING - api.rag - rag.py:285 - Document 3624 has empty embedding vector, skipping
2025-09-05 20:09:11,180 - WARNING - api.rag - rag.py:285 - Document 3625 has empty embedding vector, skipping
2025-09-05 20:09:11,180 - WARNING - api.rag - rag.py:285 - Document 3626 has empty embedding vector, skipping
2025-09-05 20:09:11,180 - WARNING - api.rag - rag.py:285 - Document 3627 has empty embedding vector, skipping
2025-09-05 20:09:11,180 - WARNING - api.rag - rag.py:285 - Document 3628 has empty embedding vector, skipping
2025-09-05 20:09:11,180 - WARNING - api.rag - rag.py:285 - Document 3629 has empty embedding vector, skipping
2025-09-05 20:09:11,181 - WARNING - api.rag - rag.py:285 - Document 3630 has empty embedding vector, skipping
2025-09-05 20:09:11,181 - WARNING - api.rag - rag.py:285 - Document 3631 has empty embedding vector, skipping
2025-09-05 20:09:11,181 - WARNING - api.rag - rag.py:285 - Document 3632 has empty embedding vector, skipping
2025-09-05 20:09:11,181 - WARNING - api.rag - rag.py:285 - Document 3633 has empty embedding vector, skipping
2025-09-05 20:09:11,181 - WARNING - api.rag - rag.py:285 - Document 3634 has empty embedding vector, skipping
2025-09-05 20:09:11,181 - WARNING - api.rag - rag.py:285 - Document 3635 has empty embedding vector, skipping
2025-09-05 20:09:11,182 - WARNING - api.rag - rag.py:285 - Document 3636 has empty embedding vector, skipping
2025-09-05 20:09:11,182 - WARNING - api.rag - rag.py:285 - Document 3637 has empty embedding vector, skipping
2025-09-05 20:09:11,182 - WARNING - api.rag - rag.py:285 - Document 3638 has empty embedding vector, skipping
2025-09-05 20:09:11,182 - WARNING - api.rag - rag.py:285 - Document 3639 has empty embedding vector, skipping
2025-09-05 20:09:11,182 - WARNING - api.rag - rag.py:285 - Document 3640 has empty embedding vector, skipping
2025-09-05 20:09:11,183 - WARNING - api.rag - rag.py:285 - Document 3641 has empty embedding vector, skipping
2025-09-05 20:09:11,183 - WARNING - api.rag - rag.py:285 - Document 3642 has empty embedding vector, skipping
2025-09-05 20:09:11,183 - WARNING - api.rag - rag.py:285 - Document 3643 has empty embedding vector, skipping
2025-09-05 20:09:11,183 - WARNING - api.rag - rag.py:285 - Document 3644 has empty embedding vector, skipping
2025-09-05 20:09:11,183 - WARNING - api.rag - rag.py:285 - Document 3645 has empty embedding vector, skipping
2025-09-05 20:09:11,183 - WARNING - api.rag - rag.py:285 - Document 3646 has empty embedding vector, skipping
2025-09-05 20:09:11,183 - WARNING - api.rag - rag.py:285 - Document 3647 has empty embedding vector, skipping
2025-09-05 20:09:11,183 - WARNING - api.rag - rag.py:285 - Document 3648 has empty embedding vector, skipping
2025-09-05 20:09:11,184 - WARNING - api.rag - rag.py:285 - Document 3649 has empty embedding vector, skipping
2025-09-05 20:09:11,184 - WARNING - api.rag - rag.py:285 - Document 3650 has empty embedding vector, skipping
2025-09-05 20:09:11,184 - WARNING - api.rag - rag.py:285 - Document 3651 has empty embedding vector, skipping
2025-09-05 20:09:11,184 - WARNING - api.rag - rag.py:285 - Document 3652 has empty embedding vector, skipping
2025-09-05 20:09:11,184 - WARNING - api.rag - rag.py:285 - Document 3653 has empty embedding vector, skipping
2025-09-05 20:09:11,184 - WARNING - api.rag - rag.py:285 - Document 3654 has empty embedding vector, skipping
2025-09-05 20:09:11,184 - WARNING - api.rag - rag.py:285 - Document 3655 has empty embedding vector, skipping
2025-09-05 20:09:11,184 - WARNING - api.rag - rag.py:285 - Document 3656 has empty embedding vector, skipping
2025-09-05 20:09:11,185 - WARNING - api.rag - rag.py:285 - Document 3657 has empty embedding vector, skipping
2025-09-05 20:09:11,185 - WARNING - api.rag - rag.py:285 - Document 3658 has empty embedding vector, skipping
2025-09-05 20:09:11,185 - WARNING - api.rag - rag.py:285 - Document 3659 has empty embedding vector, skipping
2025-09-05 20:09:11,185 - WARNING - api.rag - rag.py:285 - Document 3660 has empty embedding vector, skipping
2025-09-05 20:09:11,185 - WARNING - api.rag - rag.py:285 - Document 3661 has empty embedding vector, skipping
2025-09-05 20:09:11,185 - WARNING - api.rag - rag.py:285 - Document 3662 has empty embedding vector, skipping
2025-09-05 20:09:11,185 - WARNING - api.rag - rag.py:285 - Document 3663 has empty embedding vector, skipping
2025-09-05 20:09:11,185 - WARNING - api.rag - rag.py:285 - Document 3664 has empty embedding vector, skipping
2025-09-05 20:09:11,185 - WARNING - api.rag - rag.py:285 - Document 3665 has empty embedding vector, skipping
2025-09-05 20:09:11,186 - WARNING - api.rag - rag.py:285 - Document 3666 has empty embedding vector, skipping
2025-09-05 20:09:11,186 - WARNING - api.rag - rag.py:285 - Document 3667 has empty embedding vector, skipping
2025-09-05 20:09:11,186 - WARNING - api.rag - rag.py:285 - Document 3668 has empty embedding vector, skipping
2025-09-05 20:09:11,186 - WARNING - api.rag - rag.py:285 - Document 3669 has empty embedding vector, skipping
2025-09-05 20:09:11,186 - WARNING - api.rag - rag.py:285 - Document 3670 has empty embedding vector, skipping
2025-09-05 20:09:11,186 - WARNING - api.rag - rag.py:285 - Document 3671 has empty embedding vector, skipping
2025-09-05 20:09:11,186 - WARNING - api.rag - rag.py:285 - Document 3672 has empty embedding vector, skipping
2025-09-05 20:09:11,186 - WARNING - api.rag - rag.py:285 - Document 3673 has empty embedding vector, skipping
2025-09-05 20:09:11,187 - WARNING - api.rag - rag.py:285 - Document 3674 has empty embedding vector, skipping
2025-09-05 20:09:11,187 - WARNING - api.rag - rag.py:285 - Document 3675 has empty embedding vector, skipping
2025-09-05 20:09:11,187 - WARNING - api.rag - rag.py:285 - Document 3676 has empty embedding vector, skipping
2025-09-05 20:09:11,187 - WARNING - api.rag - rag.py:285 - Document 3677 has empty embedding vector, skipping
2025-09-05 20:09:11,187 - WARNING - api.rag - rag.py:285 - Document 3678 has empty embedding vector, skipping
2025-09-05 20:09:11,187 - WARNING - api.rag - rag.py:285 - Document 3679 has empty embedding vector, skipping
2025-09-05 20:09:11,187 - WARNING - api.rag - rag.py:285 - Document 3680 has empty embedding vector, skipping
2025-09-05 20:09:11,187 - WARNING - api.rag - rag.py:285 - Document 3681 has empty embedding vector, skipping
2025-09-05 20:09:11,187 - WARNING - api.rag - rag.py:285 - Document 3682 has empty embedding vector, skipping
2025-09-05 20:09:11,188 - WARNING - api.rag - rag.py:285 - Document 3683 has empty embedding vector, skipping
2025-09-05 20:09:11,188 - WARNING - api.rag - rag.py:285 - Document 3684 has empty embedding vector, skipping
2025-09-05 20:09:11,188 - WARNING - api.rag - rag.py:285 - Document 3685 has empty embedding vector, skipping
2025-09-05 20:09:11,188 - WARNING - api.rag - rag.py:285 - Document 3686 has empty embedding vector, skipping
2025-09-05 20:09:11,188 - WARNING - api.rag - rag.py:285 - Document 3687 has empty embedding vector, skipping
2025-09-05 20:09:11,188 - WARNING - api.rag - rag.py:285 - Document 3688 has empty embedding vector, skipping
2025-09-05 20:09:11,188 - WARNING - api.rag - rag.py:285 - Document 3689 has empty embedding vector, skipping
2025-09-05 20:09:11,188 - WARNING - api.rag - rag.py:285 - Document 3690 has empty embedding vector, skipping
2025-09-05 20:09:11,189 - WARNING - api.rag - rag.py:285 - Document 3691 has empty embedding vector, skipping
2025-09-05 20:09:11,189 - WARNING - api.rag - rag.py:285 - Document 3692 has empty embedding vector, skipping
2025-09-05 20:09:11,189 - WARNING - api.rag - rag.py:285 - Document 3693 has empty embedding vector, skipping
2025-09-05 20:09:11,189 - WARNING - api.rag - rag.py:285 - Document 3694 has empty embedding vector, skipping
2025-09-05 20:09:11,189 - WARNING - api.rag - rag.py:285 - Document 3695 has empty embedding vector, skipping
2025-09-05 20:09:11,189 - WARNING - api.rag - rag.py:285 - Document 3696 has empty embedding vector, skipping
2025-09-05 20:09:11,189 - WARNING - api.rag - rag.py:285 - Document 3697 has empty embedding vector, skipping
2025-09-05 20:09:11,189 - WARNING - api.rag - rag.py:285 - Document 3698 has empty embedding vector, skipping
2025-09-05 20:09:11,189 - WARNING - api.rag - rag.py:285 - Document 3699 has empty embedding vector, skipping
2025-09-05 20:09:11,190 - WARNING - api.rag - rag.py:285 - Document 3700 has empty embedding vector, skipping
2025-09-05 20:09:11,190 - WARNING - api.rag - rag.py:285 - Document 3701 has empty embedding vector, skipping
2025-09-05 20:09:11,190 - WARNING - api.rag - rag.py:285 - Document 3702 has empty embedding vector, skipping
2025-09-05 20:09:11,190 - WARNING - api.rag - rag.py:285 - Document 3703 has empty embedding vector, skipping
2025-09-05 20:09:11,190 - WARNING - api.rag - rag.py:285 - Document 3704 has empty embedding vector, skipping
2025-09-05 20:09:11,190 - WARNING - api.rag - rag.py:285 - Document 3705 has empty embedding vector, skipping
2025-09-05 20:09:11,190 - WARNING - api.rag - rag.py:285 - Document 3706 has empty embedding vector, skipping
2025-09-05 20:09:11,190 - WARNING - api.rag - rag.py:285 - Document 3707 has empty embedding vector, skipping
2025-09-05 20:09:11,191 - WARNING - api.rag - rag.py:285 - Document 3708 has empty embedding vector, skipping
2025-09-05 20:09:11,191 - WARNING - api.rag - rag.py:285 - Document 3709 has empty embedding vector, skipping
2025-09-05 20:09:11,191 - WARNING - api.rag - rag.py:285 - Document 3710 has empty embedding vector, skipping
2025-09-05 20:09:11,191 - WARNING - api.rag - rag.py:285 - Document 3711 has empty embedding vector, skipping
2025-09-05 20:09:11,191 - WARNING - api.rag - rag.py:285 - Document 3712 has empty embedding vector, skipping
2025-09-05 20:09:11,191 - WARNING - api.rag - rag.py:285 - Document 3713 has empty embedding vector, skipping
2025-09-05 20:09:11,191 - WARNING - api.rag - rag.py:285 - Document 3714 has empty embedding vector, skipping
2025-09-05 20:09:11,191 - WARNING - api.rag - rag.py:285 - Document 3715 has empty embedding vector, skipping
2025-09-05 20:09:11,191 - WARNING - api.rag - rag.py:285 - Document 3716 has empty embedding vector, skipping
2025-09-05 20:09:11,192 - WARNING - api.rag - rag.py:285 - Document 3717 has empty embedding vector, skipping
2025-09-05 20:09:11,192 - WARNING - api.rag - rag.py:285 - Document 3718 has empty embedding vector, skipping
2025-09-05 20:09:11,192 - WARNING - api.rag - rag.py:285 - Document 3719 has empty embedding vector, skipping
2025-09-05 20:09:11,192 - WARNING - api.rag - rag.py:285 - Document 3720 has empty embedding vector, skipping
2025-09-05 20:09:11,192 - WARNING - api.rag - rag.py:285 - Document 3721 has empty embedding vector, skipping
2025-09-05 20:09:11,192 - WARNING - api.rag - rag.py:285 - Document 3722 has empty embedding vector, skipping
2025-09-05 20:09:11,192 - WARNING - api.rag - rag.py:285 - Document 3723 has empty embedding vector, skipping
2025-09-05 20:09:11,192 - WARNING - api.rag - rag.py:285 - Document 3724 has empty embedding vector, skipping
2025-09-05 20:09:11,193 - WARNING - api.rag - rag.py:285 - Document 3725 has empty embedding vector, skipping
2025-09-05 20:09:11,193 - WARNING - api.rag - rag.py:285 - Document 3726 has empty embedding vector, skipping
2025-09-05 20:09:11,193 - WARNING - api.rag - rag.py:285 - Document 3727 has empty embedding vector, skipping
2025-09-05 20:09:11,193 - WARNING - api.rag - rag.py:285 - Document 3728 has empty embedding vector, skipping
2025-09-05 20:09:11,193 - WARNING - api.rag - rag.py:285 - Document 3729 has empty embedding vector, skipping
2025-09-05 20:09:11,193 - WARNING - api.rag - rag.py:285 - Document 3730 has empty embedding vector, skipping
2025-09-05 20:09:11,193 - WARNING - api.rag - rag.py:285 - Document 3731 has empty embedding vector, skipping
2025-09-05 20:09:11,193 - WARNING - api.rag - rag.py:285 - Document 3732 has empty embedding vector, skipping
2025-09-05 20:09:11,193 - WARNING - api.rag - rag.py:285 - Document 3733 has empty embedding vector, skipping
2025-09-05 20:09:11,194 - WARNING - api.rag - rag.py:285 - Document 3734 has empty embedding vector, skipping
2025-09-05 20:09:11,194 - WARNING - api.rag - rag.py:285 - Document 3735 has empty embedding vector, skipping
2025-09-05 20:09:11,194 - WARNING - api.rag - rag.py:285 - Document 3736 has empty embedding vector, skipping
2025-09-05 20:09:11,194 - WARNING - api.rag - rag.py:285 - Document 3737 has empty embedding vector, skipping
2025-09-05 20:09:11,194 - WARNING - api.rag - rag.py:285 - Document 3738 has empty embedding vector, skipping
2025-09-05 20:09:11,194 - WARNING - api.rag - rag.py:285 - Document 3739 has empty embedding vector, skipping
2025-09-05 20:09:11,195 - WARNING - api.rag - rag.py:285 - Document 3740 has empty embedding vector, skipping
2025-09-05 20:09:11,195 - WARNING - api.rag - rag.py:285 - Document 3741 has empty embedding vector, skipping
2025-09-05 20:09:11,195 - WARNING - api.rag - rag.py:285 - Document 3742 has empty embedding vector, skipping
2025-09-05 20:09:11,195 - WARNING - api.rag - rag.py:285 - Document 3743 has empty embedding vector, skipping
2025-09-05 20:09:11,195 - WARNING - api.rag - rag.py:285 - Document 3744 has empty embedding vector, skipping
2025-09-05 20:09:11,195 - WARNING - api.rag - rag.py:285 - Document 3745 has empty embedding vector, skipping
2025-09-05 20:09:11,195 - WARNING - api.rag - rag.py:285 - Document 3746 has empty embedding vector, skipping
2025-09-05 20:09:11,195 - WARNING - api.rag - rag.py:285 - Document 3747 has empty embedding vector, skipping
2025-09-05 20:09:11,196 - WARNING - api.rag - rag.py:285 - Document 3748 has empty embedding vector, skipping
2025-09-05 20:09:11,196 - WARNING - api.rag - rag.py:285 - Document 3749 has empty embedding vector, skipping
2025-09-05 20:09:11,196 - WARNING - api.rag - rag.py:285 - Document 3750 has empty embedding vector, skipping
2025-09-05 20:09:11,196 - WARNING - api.rag - rag.py:285 - Document 3751 has empty embedding vector, skipping
2025-09-05 20:09:11,196 - WARNING - api.rag - rag.py:285 - Document 3752 has empty embedding vector, skipping
2025-09-05 20:09:11,196 - WARNING - api.rag - rag.py:285 - Document 3753 has empty embedding vector, skipping
2025-09-05 20:09:11,196 - WARNING - api.rag - rag.py:285 - Document 3754 has empty embedding vector, skipping
2025-09-05 20:09:11,196 - WARNING - api.rag - rag.py:285 - Document 3755 has empty embedding vector, skipping
2025-09-05 20:09:11,196 - WARNING - api.rag - rag.py:285 - Document 3756 has empty embedding vector, skipping
2025-09-05 20:09:11,197 - WARNING - api.rag - rag.py:285 - Document 3757 has empty embedding vector, skipping
2025-09-05 20:09:11,197 - WARNING - api.rag - rag.py:285 - Document 3758 has empty embedding vector, skipping
2025-09-05 20:09:11,197 - WARNING - api.rag - rag.py:285 - Document 3759 has empty embedding vector, skipping
2025-09-05 20:09:11,197 - WARNING - api.rag - rag.py:285 - Document 3760 has empty embedding vector, skipping
2025-09-05 20:09:11,197 - WARNING - api.rag - rag.py:285 - Document 3761 has empty embedding vector, skipping
2025-09-05 20:09:11,197 - WARNING - api.rag - rag.py:285 - Document 3762 has empty embedding vector, skipping
2025-09-05 20:09:11,197 - WARNING - api.rag - rag.py:285 - Document 3763 has empty embedding vector, skipping
2025-09-05 20:09:11,197 - WARNING - api.rag - rag.py:285 - Document 3764 has empty embedding vector, skipping
2025-09-05 20:09:11,198 - WARNING - api.rag - rag.py:285 - Document 3765 has empty embedding vector, skipping
2025-09-05 20:09:11,198 - WARNING - api.rag - rag.py:285 - Document 3766 has empty embedding vector, skipping
2025-09-05 20:09:11,198 - WARNING - api.rag - rag.py:285 - Document 3767 has empty embedding vector, skipping
2025-09-05 20:09:11,198 - WARNING - api.rag - rag.py:285 - Document 3768 has empty embedding vector, skipping
2025-09-05 20:09:11,198 - WARNING - api.rag - rag.py:285 - Document 3769 has empty embedding vector, skipping
2025-09-05 20:09:11,198 - WARNING - api.rag - rag.py:285 - Document 3770 has empty embedding vector, skipping
2025-09-05 20:09:11,198 - WARNING - api.rag - rag.py:285 - Document 3771 has empty embedding vector, skipping
2025-09-05 20:09:11,198 - WARNING - api.rag - rag.py:285 - Document 3772 has empty embedding vector, skipping
2025-09-05 20:09:11,198 - WARNING - api.rag - rag.py:285 - Document 3773 has empty embedding vector, skipping
2025-09-05 20:09:11,199 - WARNING - api.rag - rag.py:285 - Document 3774 has empty embedding vector, skipping
2025-09-05 20:09:11,199 - WARNING - api.rag - rag.py:285 - Document 3775 has empty embedding vector, skipping
2025-09-05 20:09:11,199 - WARNING - api.rag - rag.py:285 - Document 3776 has empty embedding vector, skipping
2025-09-05 20:09:11,199 - WARNING - api.rag - rag.py:285 - Document 3777 has empty embedding vector, skipping
2025-09-05 20:09:11,199 - WARNING - api.rag - rag.py:285 - Document 3778 has empty embedding vector, skipping
2025-09-05 20:09:11,199 - WARNING - api.rag - rag.py:285 - Document 3779 has empty embedding vector, skipping
2025-09-05 20:09:11,199 - WARNING - api.rag - rag.py:285 - Document 3780 has empty embedding vector, skipping
2025-09-05 20:09:11,199 - WARNING - api.rag - rag.py:285 - Document 3781 has empty embedding vector, skipping
2025-09-05 20:09:11,200 - WARNING - api.rag - rag.py:285 - Document 3782 has empty embedding vector, skipping
2025-09-05 20:09:11,200 - WARNING - api.rag - rag.py:285 - Document 3783 has empty embedding vector, skipping
2025-09-05 20:09:11,200 - WARNING - api.rag - rag.py:285 - Document 3784 has empty embedding vector, skipping
2025-09-05 20:09:11,200 - WARNING - api.rag - rag.py:285 - Document 3785 has empty embedding vector, skipping
2025-09-05 20:09:11,200 - WARNING - api.rag - rag.py:285 - Document 3786 has empty embedding vector, skipping
2025-09-05 20:09:11,200 - WARNING - api.rag - rag.py:285 - Document 3787 has empty embedding vector, skipping
2025-09-05 20:09:11,200 - WARNING - api.rag - rag.py:285 - Document 3788 has empty embedding vector, skipping
2025-09-05 20:09:11,200 - WARNING - api.rag - rag.py:285 - Document 3789 has empty embedding vector, skipping
2025-09-05 20:09:11,200 - WARNING - api.rag - rag.py:285 - Document 3790 has empty embedding vector, skipping
2025-09-05 20:09:11,201 - WARNING - api.rag - rag.py:285 - Document 3791 has empty embedding vector, skipping
2025-09-05 20:09:11,201 - WARNING - api.rag - rag.py:285 - Document 3792 has empty embedding vector, skipping
2025-09-05 20:09:11,201 - WARNING - api.rag - rag.py:285 - Document 3793 has empty embedding vector, skipping
2025-09-05 20:09:11,201 - WARNING - api.rag - rag.py:285 - Document 3794 has empty embedding vector, skipping
2025-09-05 20:09:11,201 - WARNING - api.rag - rag.py:285 - Document 3795 has empty embedding vector, skipping
2025-09-05 20:09:11,201 - WARNING - api.rag - rag.py:285 - Document 3796 has empty embedding vector, skipping
2025-09-05 20:09:11,201 - WARNING - api.rag - rag.py:285 - Document 3797 has empty embedding vector, skipping
2025-09-05 20:09:11,201 - WARNING - api.rag - rag.py:285 - Document 3798 has empty embedding vector, skipping
2025-09-05 20:09:11,202 - WARNING - api.rag - rag.py:285 - Document 3799 has empty embedding vector, skipping
2025-09-05 20:09:11,202 - WARNING - api.rag - rag.py:285 - Document 3800 has empty embedding vector, skipping
2025-09-05 20:09:11,202 - WARNING - api.rag - rag.py:285 - Document 3801 has empty embedding vector, skipping
2025-09-05 20:09:11,202 - WARNING - api.rag - rag.py:285 - Document 3802 has empty embedding vector, skipping
2025-09-05 20:09:11,202 - WARNING - api.rag - rag.py:285 - Document 3803 has empty embedding vector, skipping
2025-09-05 20:09:11,202 - WARNING - api.rag - rag.py:285 - Document 3804 has empty embedding vector, skipping
2025-09-05 20:09:11,202 - WARNING - api.rag - rag.py:285 - Document 3805 has empty embedding vector, skipping
2025-09-05 20:09:11,202 - WARNING - api.rag - rag.py:285 - Document 3806 has empty embedding vector, skipping
2025-09-05 20:09:11,202 - WARNING - api.rag - rag.py:285 - Document 3807 has empty embedding vector, skipping
2025-09-05 20:09:11,203 - WARNING - api.rag - rag.py:285 - Document 3808 has empty embedding vector, skipping
2025-09-05 20:09:11,203 - WARNING - api.rag - rag.py:285 - Document 3809 has empty embedding vector, skipping
2025-09-05 20:09:11,203 - WARNING - api.rag - rag.py:285 - Document 3810 has empty embedding vector, skipping
2025-09-05 20:09:11,203 - WARNING - api.rag - rag.py:285 - Document 3811 has empty embedding vector, skipping
2025-09-05 20:09:11,203 - WARNING - api.rag - rag.py:285 - Document 3812 has empty embedding vector, skipping
2025-09-05 20:09:11,203 - WARNING - api.rag - rag.py:285 - Document 3813 has empty embedding vector, skipping
2025-09-05 20:09:11,203 - WARNING - api.rag - rag.py:285 - Document 3814 has empty embedding vector, skipping
2025-09-05 20:09:11,203 - WARNING - api.rag - rag.py:285 - Document 3815 has empty embedding vector, skipping
2025-09-05 20:09:11,204 - WARNING - api.rag - rag.py:285 - Document 3816 has empty embedding vector, skipping
2025-09-05 20:09:11,204 - WARNING - api.rag - rag.py:285 - Document 3817 has empty embedding vector, skipping
2025-09-05 20:09:11,204 - WARNING - api.rag - rag.py:285 - Document 3818 has empty embedding vector, skipping
2025-09-05 20:09:11,204 - WARNING - api.rag - rag.py:285 - Document 3819 has empty embedding vector, skipping
2025-09-05 20:09:11,204 - WARNING - api.rag - rag.py:285 - Document 3820 has empty embedding vector, skipping
2025-09-05 20:09:11,204 - WARNING - api.rag - rag.py:285 - Document 3821 has empty embedding vector, skipping
2025-09-05 20:09:11,204 - WARNING - api.rag - rag.py:285 - Document 3822 has empty embedding vector, skipping
2025-09-05 20:09:11,204 - WARNING - api.rag - rag.py:285 - Document 3823 has empty embedding vector, skipping
2025-09-05 20:09:11,204 - WARNING - api.rag - rag.py:285 - Document 3824 has empty embedding vector, skipping
2025-09-05 20:09:11,205 - WARNING - api.rag - rag.py:285 - Document 3825 has empty embedding vector, skipping
2025-09-05 20:09:11,205 - WARNING - api.rag - rag.py:285 - Document 3826 has empty embedding vector, skipping
2025-09-05 20:09:11,205 - WARNING - api.rag - rag.py:285 - Document 3827 has empty embedding vector, skipping
2025-09-05 20:09:11,205 - WARNING - api.rag - rag.py:285 - Document 3828 has empty embedding vector, skipping
2025-09-05 20:09:11,205 - WARNING - api.rag - rag.py:285 - Document 3829 has empty embedding vector, skipping
2025-09-05 20:09:11,205 - WARNING - api.rag - rag.py:285 - Document 3830 has empty embedding vector, skipping
2025-09-05 20:09:11,205 - WARNING - api.rag - rag.py:285 - Document 3831 has empty embedding vector, skipping
2025-09-05 20:09:11,205 - WARNING - api.rag - rag.py:285 - Document 3832 has empty embedding vector, skipping
2025-09-05 20:09:11,206 - WARNING - api.rag - rag.py:285 - Document 3833 has empty embedding vector, skipping
2025-09-05 20:09:11,206 - WARNING - api.rag - rag.py:285 - Document 3834 has empty embedding vector, skipping
2025-09-05 20:09:11,206 - WARNING - api.rag - rag.py:285 - Document 3835 has empty embedding vector, skipping
2025-09-05 20:09:11,206 - WARNING - api.rag - rag.py:285 - Document 3836 has empty embedding vector, skipping
2025-09-05 20:09:11,206 - WARNING - api.rag - rag.py:285 - Document 3837 has empty embedding vector, skipping
2025-09-05 20:09:11,206 - WARNING - api.rag - rag.py:285 - Document 3838 has empty embedding vector, skipping
2025-09-05 20:09:11,206 - WARNING - api.rag - rag.py:285 - Document 3839 has empty embedding vector, skipping
2025-09-05 20:09:11,206 - WARNING - api.rag - rag.py:285 - Document 3840 has empty embedding vector, skipping
2025-09-05 20:09:11,206 - WARNING - api.rag - rag.py:285 - Document 3841 has empty embedding vector, skipping
2025-09-05 20:09:11,207 - WARNING - api.rag - rag.py:285 - Document 3842 has empty embedding vector, skipping
2025-09-05 20:09:11,207 - WARNING - api.rag - rag.py:285 - Document 3843 has empty embedding vector, skipping
2025-09-05 20:09:11,207 - WARNING - api.rag - rag.py:285 - Document 3844 has empty embedding vector, skipping
2025-09-05 20:09:11,207 - WARNING - api.rag - rag.py:285 - Document 3845 has empty embedding vector, skipping
2025-09-05 20:09:11,207 - WARNING - api.rag - rag.py:285 - Document 3846 has empty embedding vector, skipping
2025-09-05 20:09:11,207 - WARNING - api.rag - rag.py:285 - Document 3847 has empty embedding vector, skipping
2025-09-05 20:09:11,207 - WARNING - api.rag - rag.py:285 - Document 3848 has empty embedding vector, skipping
2025-09-05 20:09:11,207 - WARNING - api.rag - rag.py:285 - Document 3849 has empty embedding vector, skipping
2025-09-05 20:09:11,208 - WARNING - api.rag - rag.py:285 - Document 3850 has empty embedding vector, skipping
2025-09-05 20:09:11,208 - WARNING - api.rag - rag.py:285 - Document 3851 has empty embedding vector, skipping
2025-09-05 20:09:11,208 - WARNING - api.rag - rag.py:285 - Document 3852 has empty embedding vector, skipping
2025-09-05 20:09:11,208 - WARNING - api.rag - rag.py:285 - Document 3853 has empty embedding vector, skipping
2025-09-05 20:09:11,208 - WARNING - api.rag - rag.py:285 - Document 3854 has empty embedding vector, skipping
2025-09-05 20:09:11,208 - WARNING - api.rag - rag.py:285 - Document 3855 has empty embedding vector, skipping
2025-09-05 20:09:11,208 - WARNING - api.rag - rag.py:285 - Document 3856 has empty embedding vector, skipping
2025-09-05 20:09:11,208 - WARNING - api.rag - rag.py:285 - Document 3857 has empty embedding vector, skipping
2025-09-05 20:09:11,208 - WARNING - api.rag - rag.py:285 - Document 3858 has empty embedding vector, skipping
2025-09-05 20:09:11,209 - WARNING - api.rag - rag.py:285 - Document 3859 has empty embedding vector, skipping
2025-09-05 20:09:11,209 - WARNING - api.rag - rag.py:285 - Document 3860 has empty embedding vector, skipping
2025-09-05 20:09:11,209 - WARNING - api.rag - rag.py:285 - Document 3861 has empty embedding vector, skipping
2025-09-05 20:09:11,209 - WARNING - api.rag - rag.py:285 - Document 3862 has empty embedding vector, skipping
2025-09-05 20:09:11,209 - WARNING - api.rag - rag.py:285 - Document 3863 has empty embedding vector, skipping
2025-09-05 20:09:11,209 - WARNING - api.rag - rag.py:285 - Document 3864 has empty embedding vector, skipping
2025-09-05 20:09:11,209 - WARNING - api.rag - rag.py:285 - Document 3865 has empty embedding vector, skipping
2025-09-05 20:09:11,209 - WARNING - api.rag - rag.py:285 - Document 3866 has empty embedding vector, skipping
2025-09-05 20:09:11,210 - WARNING - api.rag - rag.py:285 - Document 3867 has empty embedding vector, skipping
2025-09-05 20:09:11,210 - WARNING - api.rag - rag.py:285 - Document 3868 has empty embedding vector, skipping
2025-09-05 20:09:11,210 - WARNING - api.rag - rag.py:285 - Document 3869 has empty embedding vector, skipping
2025-09-05 20:09:11,210 - WARNING - api.rag - rag.py:285 - Document 3870 has empty embedding vector, skipping
2025-09-05 20:09:11,210 - WARNING - api.rag - rag.py:285 - Document 3871 has empty embedding vector, skipping
2025-09-05 20:09:11,210 - WARNING - api.rag - rag.py:285 - Document 3872 has empty embedding vector, skipping
2025-09-05 20:09:11,210 - WARNING - api.rag - rag.py:285 - Document 3873 has empty embedding vector, skipping
2025-09-05 20:09:11,210 - WARNING - api.rag - rag.py:285 - Document 3874 has empty embedding vector, skipping
2025-09-05 20:09:11,211 - WARNING - api.rag - rag.py:285 - Document 3875 has empty embedding vector, skipping
2025-09-05 20:09:11,211 - WARNING - api.rag - rag.py:285 - Document 3876 has empty embedding vector, skipping
2025-09-05 20:09:11,212 - WARNING - api.rag - rag.py:285 - Document 3877 has empty embedding vector, skipping
2025-09-05 20:09:11,212 - WARNING - api.rag - rag.py:285 - Document 3878 has empty embedding vector, skipping
2025-09-05 20:09:11,212 - WARNING - api.rag - rag.py:285 - Document 3879 has empty embedding vector, skipping
2025-09-05 20:09:11,212 - WARNING - api.rag - rag.py:285 - Document 3880 has empty embedding vector, skipping
2025-09-05 20:09:11,212 - WARNING - api.rag - rag.py:285 - Document 3881 has empty embedding vector, skipping
2025-09-05 20:09:11,212 - WARNING - api.rag - rag.py:285 - Document 3882 has empty embedding vector, skipping
2025-09-05 20:09:11,212 - WARNING - api.rag - rag.py:285 - Document 3883 has empty embedding vector, skipping
2025-09-05 20:09:11,212 - WARNING - api.rag - rag.py:285 - Document 3884 has empty embedding vector, skipping
2025-09-05 20:09:11,212 - WARNING - api.rag - rag.py:285 - Document 3885 has empty embedding vector, skipping
2025-09-05 20:09:11,212 - WARNING - api.rag - rag.py:285 - Document 3886 has empty embedding vector, skipping
2025-09-05 20:09:11,212 - WARNING - api.rag - rag.py:285 - Document 3887 has empty embedding vector, skipping
2025-09-05 20:09:11,212 - WARNING - api.rag - rag.py:285 - Document 3888 has empty embedding vector, skipping
2025-09-05 20:09:11,213 - WARNING - api.rag - rag.py:285 - Document 3889 has empty embedding vector, skipping
2025-09-05 20:09:11,213 - WARNING - api.rag - rag.py:285 - Document 3890 has empty embedding vector, skipping
2025-09-05 20:09:11,213 - WARNING - api.rag - rag.py:285 - Document 3891 has empty embedding vector, skipping
2025-09-05 20:09:11,213 - WARNING - api.rag - rag.py:285 - Document 3892 has empty embedding vector, skipping
2025-09-05 20:09:11,213 - WARNING - api.rag - rag.py:285 - Document 3893 has empty embedding vector, skipping
2025-09-05 20:09:11,213 - WARNING - api.rag - rag.py:285 - Document 3894 has empty embedding vector, skipping
2025-09-05 20:09:11,213 - WARNING - api.rag - rag.py:285 - Document 3895 has empty embedding vector, skipping
2025-09-05 20:09:11,213 - WARNING - api.rag - rag.py:285 - Document 3896 has empty embedding vector, skipping
2025-09-05 20:09:11,214 - WARNING - api.rag - rag.py:285 - Document 3897 has empty embedding vector, skipping
2025-09-05 20:09:11,214 - WARNING - api.rag - rag.py:285 - Document 3898 has empty embedding vector, skipping
2025-09-05 20:09:11,214 - WARNING - api.rag - rag.py:285 - Document 3899 has empty embedding vector, skipping
2025-09-05 20:09:11,214 - WARNING - api.rag - rag.py:285 - Document 3900 has empty embedding vector, skipping
2025-09-05 20:09:11,214 - WARNING - api.rag - rag.py:285 - Document 3901 has empty embedding vector, skipping
2025-09-05 20:09:11,214 - WARNING - api.rag - rag.py:285 - Document 3902 has empty embedding vector, skipping
2025-09-05 20:09:11,214 - WARNING - api.rag - rag.py:285 - Document 3903 has empty embedding vector, skipping
2025-09-05 20:09:11,214 - WARNING - api.rag - rag.py:285 - Document 3904 has empty embedding vector, skipping
2025-09-05 20:09:11,214 - WARNING - api.rag - rag.py:285 - Document 3905 has empty embedding vector, skipping
2025-09-05 20:09:11,215 - WARNING - api.rag - rag.py:285 - Document 3906 has empty embedding vector, skipping
2025-09-05 20:09:11,215 - WARNING - api.rag - rag.py:285 - Document 3907 has empty embedding vector, skipping
2025-09-05 20:09:11,215 - WARNING - api.rag - rag.py:285 - Document 3908 has empty embedding vector, skipping
2025-09-05 20:09:11,215 - WARNING - api.rag - rag.py:285 - Document 3909 has empty embedding vector, skipping
2025-09-05 20:09:11,215 - WARNING - api.rag - rag.py:285 - Document 3910 has empty embedding vector, skipping
2025-09-05 20:09:11,215 - WARNING - api.rag - rag.py:285 - Document 3911 has empty embedding vector, skipping
2025-09-05 20:09:11,215 - WARNING - api.rag - rag.py:285 - Document 3912 has empty embedding vector, skipping
2025-09-05 20:09:11,215 - WARNING - api.rag - rag.py:285 - Document 3913 has empty embedding vector, skipping
2025-09-05 20:09:11,216 - WARNING - api.rag - rag.py:285 - Document 3914 has empty embedding vector, skipping
2025-09-05 20:09:11,216 - WARNING - api.rag - rag.py:285 - Document 3915 has empty embedding vector, skipping
2025-09-05 20:09:11,216 - WARNING - api.rag - rag.py:285 - Document 3916 has empty embedding vector, skipping
2025-09-05 20:09:11,216 - WARNING - api.rag - rag.py:285 - Document 3917 has empty embedding vector, skipping
2025-09-05 20:09:11,216 - WARNING - api.rag - rag.py:285 - Document 3918 has empty embedding vector, skipping
2025-09-05 20:09:11,216 - WARNING - api.rag - rag.py:285 - Document 3919 has empty embedding vector, skipping
2025-09-05 20:09:11,216 - WARNING - api.rag - rag.py:285 - Document 3920 has empty embedding vector, skipping
2025-09-05 20:09:11,216 - WARNING - api.rag - rag.py:285 - Document 3921 has empty embedding vector, skipping
2025-09-05 20:09:11,216 - WARNING - api.rag - rag.py:285 - Document 3922 has empty embedding vector, skipping
2025-09-05 20:09:11,217 - WARNING - api.rag - rag.py:285 - Document 3923 has empty embedding vector, skipping
2025-09-05 20:09:11,217 - WARNING - api.rag - rag.py:285 - Document 3924 has empty embedding vector, skipping
2025-09-05 20:09:11,217 - WARNING - api.rag - rag.py:285 - Document 3925 has empty embedding vector, skipping
2025-09-05 20:09:11,217 - WARNING - api.rag - rag.py:285 - Document 3926 has empty embedding vector, skipping
2025-09-05 20:09:11,217 - WARNING - api.rag - rag.py:285 - Document 3927 has empty embedding vector, skipping
2025-09-05 20:09:11,217 - WARNING - api.rag - rag.py:285 - Document 3928 has empty embedding vector, skipping
2025-09-05 20:09:11,217 - WARNING - api.rag - rag.py:285 - Document 3929 has empty embedding vector, skipping
2025-09-05 20:09:11,217 - WARNING - api.rag - rag.py:285 - Document 3930 has empty embedding vector, skipping
2025-09-05 20:09:11,217 - WARNING - api.rag - rag.py:285 - Document 3931 has empty embedding vector, skipping
2025-09-05 20:09:11,218 - WARNING - api.rag - rag.py:285 - Document 3932 has empty embedding vector, skipping
2025-09-05 20:09:11,218 - WARNING - api.rag - rag.py:285 - Document 3933 has empty embedding vector, skipping
2025-09-05 20:09:11,218 - WARNING - api.rag - rag.py:285 - Document 3934 has empty embedding vector, skipping
2025-09-05 20:09:11,218 - WARNING - api.rag - rag.py:285 - Document 3935 has empty embedding vector, skipping
2025-09-05 20:09:11,218 - WARNING - api.rag - rag.py:285 - Document 3936 has empty embedding vector, skipping
2025-09-05 20:09:11,218 - WARNING - api.rag - rag.py:285 - Document 3937 has empty embedding vector, skipping
2025-09-05 20:09:11,218 - WARNING - api.rag - rag.py:285 - Document 3938 has empty embedding vector, skipping
2025-09-05 20:09:11,218 - WARNING - api.rag - rag.py:285 - Document 3939 has empty embedding vector, skipping
2025-09-05 20:09:11,219 - WARNING - api.rag - rag.py:285 - Document 3940 has empty embedding vector, skipping
2025-09-05 20:09:11,219 - WARNING - api.rag - rag.py:285 - Document 3941 has empty embedding vector, skipping
2025-09-05 20:09:11,219 - WARNING - api.rag - rag.py:285 - Document 3942 has empty embedding vector, skipping
2025-09-05 20:09:11,219 - WARNING - api.rag - rag.py:285 - Document 3943 has empty embedding vector, skipping
2025-09-05 20:09:11,219 - WARNING - api.rag - rag.py:285 - Document 3944 has empty embedding vector, skipping
2025-09-05 20:09:11,219 - WARNING - api.rag - rag.py:285 - Document 3945 has empty embedding vector, skipping
2025-09-05 20:09:11,219 - WARNING - api.rag - rag.py:285 - Document 3946 has empty embedding vector, skipping
2025-09-05 20:09:11,219 - WARNING - api.rag - rag.py:285 - Document 3947 has empty embedding vector, skipping
2025-09-05 20:09:11,219 - WARNING - api.rag - rag.py:285 - Document 3948 has empty embedding vector, skipping
2025-09-05 20:09:11,220 - WARNING - api.rag - rag.py:285 - Document 3949 has empty embedding vector, skipping
2025-09-05 20:09:11,220 - WARNING - api.rag - rag.py:285 - Document 3950 has empty embedding vector, skipping
2025-09-05 20:09:11,220 - WARNING - api.rag - rag.py:285 - Document 3951 has empty embedding vector, skipping
2025-09-05 20:09:11,220 - WARNING - api.rag - rag.py:285 - Document 3952 has empty embedding vector, skipping
2025-09-05 20:09:11,220 - WARNING - api.rag - rag.py:285 - Document 3953 has empty embedding vector, skipping
2025-09-05 20:09:11,220 - WARNING - api.rag - rag.py:285 - Document 3954 has empty embedding vector, skipping
2025-09-05 20:09:11,220 - WARNING - api.rag - rag.py:285 - Document 3955 has empty embedding vector, skipping
2025-09-05 20:09:11,220 - WARNING - api.rag - rag.py:285 - Document 3956 has empty embedding vector, skipping
2025-09-05 20:09:11,221 - WARNING - api.rag - rag.py:285 - Document 3957 has empty embedding vector, skipping
2025-09-05 20:09:11,221 - WARNING - api.rag - rag.py:285 - Document 3958 has empty embedding vector, skipping
2025-09-05 20:09:11,221 - WARNING - api.rag - rag.py:285 - Document 3959 has empty embedding vector, skipping
2025-09-05 20:09:11,221 - WARNING - api.rag - rag.py:285 - Document 3960 has empty embedding vector, skipping
2025-09-05 20:09:11,221 - WARNING - api.rag - rag.py:285 - Document 3961 has empty embedding vector, skipping
2025-09-05 20:09:11,221 - WARNING - api.rag - rag.py:285 - Document 3962 has empty embedding vector, skipping
2025-09-05 20:09:11,221 - WARNING - api.rag - rag.py:285 - Document 3963 has empty embedding vector, skipping
2025-09-05 20:09:11,221 - WARNING - api.rag - rag.py:285 - Document 3964 has empty embedding vector, skipping
2025-09-05 20:09:11,221 - WARNING - api.rag - rag.py:285 - Document 3965 has empty embedding vector, skipping
2025-09-05 20:09:11,222 - WARNING - api.rag - rag.py:285 - Document 3966 has empty embedding vector, skipping
2025-09-05 20:09:11,222 - WARNING - api.rag - rag.py:285 - Document 3967 has empty embedding vector, skipping
2025-09-05 20:09:11,222 - WARNING - api.rag - rag.py:285 - Document 3968 has empty embedding vector, skipping
2025-09-05 20:09:11,222 - WARNING - api.rag - rag.py:285 - Document 3969 has empty embedding vector, skipping
2025-09-05 20:09:11,222 - WARNING - api.rag - rag.py:285 - Document 3970 has empty embedding vector, skipping
2025-09-05 20:09:11,222 - WARNING - api.rag - rag.py:285 - Document 3971 has empty embedding vector, skipping
2025-09-05 20:09:11,222 - WARNING - api.rag - rag.py:285 - Document 3972 has empty embedding vector, skipping
2025-09-05 20:09:11,222 - WARNING - api.rag - rag.py:285 - Document 3973 has empty embedding vector, skipping
2025-09-05 20:09:11,223 - WARNING - api.rag - rag.py:285 - Document 3974 has empty embedding vector, skipping
2025-09-05 20:09:11,223 - WARNING - api.rag - rag.py:285 - Document 3975 has empty embedding vector, skipping
2025-09-05 20:09:11,223 - WARNING - api.rag - rag.py:285 - Document 3976 has empty embedding vector, skipping
2025-09-05 20:09:11,223 - WARNING - api.rag - rag.py:285 - Document 3977 has empty embedding vector, skipping
2025-09-05 20:09:11,223 - WARNING - api.rag - rag.py:285 - Document 3978 has empty embedding vector, skipping
2025-09-05 20:09:11,223 - WARNING - api.rag - rag.py:285 - Document 3979 has empty embedding vector, skipping
2025-09-05 20:09:11,223 - WARNING - api.rag - rag.py:285 - Document 3980 has empty embedding vector, skipping
2025-09-05 20:09:11,223 - WARNING - api.rag - rag.py:285 - Document 3981 has empty embedding vector, skipping
2025-09-05 20:09:11,223 - WARNING - api.rag - rag.py:285 - Document 3982 has empty embedding vector, skipping
2025-09-05 20:09:11,224 - WARNING - api.rag - rag.py:285 - Document 3983 has empty embedding vector, skipping
2025-09-05 20:09:11,224 - WARNING - api.rag - rag.py:285 - Document 3984 has empty embedding vector, skipping
2025-09-05 20:09:11,224 - WARNING - api.rag - rag.py:285 - Document 3985 has empty embedding vector, skipping
2025-09-05 20:09:11,224 - WARNING - api.rag - rag.py:285 - Document 3986 has empty embedding vector, skipping
2025-09-05 20:09:11,224 - WARNING - api.rag - rag.py:285 - Document 3987 has empty embedding vector, skipping
2025-09-05 20:09:11,224 - WARNING - api.rag - rag.py:285 - Document 3988 has empty embedding vector, skipping
2025-09-05 20:09:11,224 - WARNING - api.rag - rag.py:285 - Document 3989 has empty embedding vector, skipping
2025-09-05 20:09:11,224 - WARNING - api.rag - rag.py:285 - Document 3990 has empty embedding vector, skipping
2025-09-05 20:09:11,225 - WARNING - api.rag - rag.py:285 - Document 3991 has empty embedding vector, skipping
2025-09-05 20:09:11,225 - WARNING - api.rag - rag.py:285 - Document 3992 has empty embedding vector, skipping
2025-09-05 20:09:11,225 - WARNING - api.rag - rag.py:285 - Document 3993 has empty embedding vector, skipping
2025-09-05 20:09:11,225 - WARNING - api.rag - rag.py:285 - Document 3994 has empty embedding vector, skipping
2025-09-05 20:09:11,225 - WARNING - api.rag - rag.py:285 - Document 3995 has empty embedding vector, skipping
2025-09-05 20:09:11,225 - WARNING - api.rag - rag.py:285 - Document 3996 has empty embedding vector, skipping
2025-09-05 20:09:11,225 - WARNING - api.rag - rag.py:285 - Document 3997 has empty embedding vector, skipping
2025-09-05 20:09:11,225 - WARNING - api.rag - rag.py:285 - Document 3998 has empty embedding vector, skipping
2025-09-05 20:09:11,225 - WARNING - api.rag - rag.py:285 - Document 3999 has empty embedding vector, skipping
2025-09-05 20:09:11,236 - INFO - api.rag - rag.py:300 - Target embedding size: 256 (found in 21386 documents)
2025-09-05 20:09:11,238 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\utils.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,238 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\utils.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,238 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\utils.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,238 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\_project_spec.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,239 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\_project_spec.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,239 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\_project_spec.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,239 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\_project_spec.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,239 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\_project_spec.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,239 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\_project_spec.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,239 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\_project_spec.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,239 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\_project_spec.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,239 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\_project_spec.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,240 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\_project_spec.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,240 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\_project_spec.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,240 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\_project_spec.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,240 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\_project_spec.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,240 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\_project_spec.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,240 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\_project_spec.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,240 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\_project_spec.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,240 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,241 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,241 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,241 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,241 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,241 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,241 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,241 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,241 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,242 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,242 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,242 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,242 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,242 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,243 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,243 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,243 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\backend\abstract_backend.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,243 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\backend\abstract_backend.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,244 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\backend\loader.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,244 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\backend\local.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,244 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\backend\local.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,244 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\backend\local.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,244 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\backend\local.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,244 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\backend\local.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,244 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\backend\local.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,244 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\backend\local.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,245 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\backend\local.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,245 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\backend\local.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,245 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\backend\local.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,245 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\backend\local.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,245 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\backend\local.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,245 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\backend\local.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,246 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\backend\local.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,246 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\backend\local.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,246 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\backend\local.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,246 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\projects\backend\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,247 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\prompt\constants.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,247 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\prompt\promptlab_model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,248 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\prompt\promptlab_model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,248 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\prompt\promptlab_model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,249 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\prompt\promptlab_model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,249 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\prompt\promptlab_model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,250 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\prompt\promptlab_model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,250 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\prompt\registry_utils.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,251 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\prompt\registry_utils.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,251 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\prompt\registry_utils.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,252 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\prompt\registry_utils.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,252 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\prompt\registry_utils.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,253 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\prompt\registry_utils.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,253 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\prompt\registry_utils.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,253 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\prompt\registry_utils.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,253 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\prompt\registry_utils.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,253 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\promptflow\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,253 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\promptflow\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,253 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\promptflow\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,254 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\promptflow\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,254 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\promptflow\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,254 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\promptflow\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,254 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\promptflow\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,254 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\promptflow\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,255 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\promptflow\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,255 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\promptflow\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,256 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\promptflow\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,256 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\promptflow\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,256 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\promptflow\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,257 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\promptflow\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,258 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\promptflow\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,258 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\promptflow\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,259 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\promptflow\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,259 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\prophet\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,260 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\prophet\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,260 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\prophet\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,261 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\prophet\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,261 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\prophet\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,262 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\prophet\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,262 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\prophet\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,263 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\prophet\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,263 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\prophet\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,264 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\prophet\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,264 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\prophet\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,264 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\prophet\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,265 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\assessments_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,265 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\assessments_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,266 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\assessments_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,266 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_artifacts_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,267 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_artifacts_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,267 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_artifacts_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,268 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_artifacts_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,268 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_artifacts_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,268 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_artifacts_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,269 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_artifacts_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,269 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_artifacts_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,270 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_artifacts_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,270 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_artifacts_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,271 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_filesystem_service_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,271 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_filesystem_service_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,271 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_filesystem_service_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,272 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_filesystem_service_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,272 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_managed_catalog_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,273 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_managed_catalog_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,273 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_managed_catalog_service_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,274 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,274 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,274 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,276 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,276 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,276 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_trace_server_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,276 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_trace_server_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,276 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_trace_server_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,277 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_trace_server_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,277 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_trace_server_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,278 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_trace_server_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,278 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_trace_server_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,279 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_trace_server_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,280 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_uc_registry_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,280 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_uc_registry_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,281 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_uc_registry_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,281 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_uc_registry_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,281 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_uc_registry_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,282 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_uc_registry_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,282 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_uc_registry_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,282 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_uc_registry_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,282 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_uc_registry_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,282 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_uc_registry_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,282 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_uc_registry_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,283 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_uc_registry_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,283 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_uc_registry_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,283 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_uc_registry_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,283 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_uc_registry_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,283 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_uc_registry_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,283 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_uc_registry_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,284 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_uc_registry_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,284 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_uc_registry_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,284 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_uc_registry_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,284 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_uc_registry_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,284 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_uc_registry_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,285 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_uc_registry_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,285 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_uc_registry_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,285 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_uc_registry_service_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,285 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_uc_registry_service_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,285 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_uc_registry_service_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,285 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_uc_registry_service_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,285 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\databricks_uc_registry_service_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,286 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\datasets_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,286 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\datasets_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,286 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\facet_feature_statistics_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,286 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\facet_feature_statistics_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,286 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\facet_feature_statistics_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,286 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\facet_feature_statistics_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,286 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\facet_feature_statistics_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,286 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\facet_feature_statistics_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,287 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\internal_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,287 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\mlflow_artifacts_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,287 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\mlflow_artifacts_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,287 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\mlflow_artifacts_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,287 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\mlflow_artifacts_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,287 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\mlflow_artifacts_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,287 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\mlflow_artifacts_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,287 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\mlflow_artifacts_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,288 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\model_registry_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,288 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\model_registry_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,288 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\model_registry_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,288 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\model_registry_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,289 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\model_registry_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,289 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\model_registry_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,289 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\model_registry_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,289 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\model_registry_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,289 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\model_registry_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,289 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\model_registry_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,290 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\model_registry_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,290 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\model_registry_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,291 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\model_registry_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,291 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\model_registry_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,291 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\model_registry_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,291 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\model_registry_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,291 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\model_registry_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,291 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\model_registry_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,291 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\model_registry_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,291 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\model_registry_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,292 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\model_registry_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,292 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\model_registry_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,292 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\model_registry_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,292 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_oss_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,292 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_oss_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,292 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_oss_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,293 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_oss_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,293 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_oss_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,293 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_oss_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,293 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_oss_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,293 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_oss_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,293 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_oss_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,293 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_oss_service_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,293 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_oss_service_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,294 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_oss_service_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,294 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,294 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,294 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,294 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,294 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,294 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,294 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,294 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_messages_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,295 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_messages_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,295 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,295 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,295 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,295 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,295 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,295 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,295 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,296 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,296 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,296 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,296 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,296 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,296 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,296 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,297 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,297 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,297 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,297 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,297 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,297 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,297 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,297 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,298 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,298 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,298 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,298 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,298 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,298 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,298 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,299 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,299 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,299 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,299 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,299 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,299 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,299 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,299 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,300 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,300 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,300 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\unity_catalog_prompt_service_pb2_grpc.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,300 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\webhooks_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,300 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\webhooks_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,300 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\webhooks_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,300 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\webhooks_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,300 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\webhooks_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,300 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\webhooks_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,301 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\webhooks_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,301 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\webhooks_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,302 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\scalapb\scalapb_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,302 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\protos\scalapb\scalapb_pb2.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,302 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pydantic_ai\autolog.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,302 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pydantic_ai\autolog.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,302 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pydantic_ai\autolog.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,302 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pydantic_ai\autolog.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,302 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pydantic_ai\autolog.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,302 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pydantic_ai\autolog.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,302 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pydantic_ai\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,302 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pydantic_ai\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,302 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pydantic_ai\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,302 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\backend.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,302 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\backend.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,303 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\backend.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,303 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\backend.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,303 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\backend.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,303 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\backend.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,303 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\backend.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,303 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\backend.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,303 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\backend.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,303 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\backend.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,303 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\backend.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,304 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\backend.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,304 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\backend.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,304 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\backend.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,304 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\backend.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,304 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\backend.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,304 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\backend.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,304 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\backend.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,305 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\backend.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,305 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\backend.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,305 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\backend.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,305 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\backend.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,305 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\backend.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,305 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\backend.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,305 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\context.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,305 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\context.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,305 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\context.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,306 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\dbconnect_artifact_cache.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,306 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\dbconnect_artifact_cache.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,306 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\dbconnect_artifact_cache.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,306 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\dbconnect_artifact_cache.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,306 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\dbconnect_artifact_cache.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,306 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\mlserver.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,306 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,306 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,307 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,307 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,307 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,307 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,307 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,307 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,307 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,307 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,307 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,308 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,308 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,308 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,308 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,308 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,308 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,308 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,308 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,309 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,309 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,309 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,309 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,309 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,309 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,309 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,309 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,309 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,310 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,310 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,310 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,310 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,310 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,310 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,310 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,310 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,311 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,311 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,311 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,311 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,311 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,311 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,311 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,311 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,311 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,312 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,312 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,312 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,312 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,312 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,312 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,312 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,312 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,313 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,313 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,313 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,313 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,313 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,313 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,313 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,313 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,313 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,314 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,314 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,314 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,314 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,314 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,314 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,314 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\model.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,314 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\spark_model_cache.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,315 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\spark_model_cache.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,315 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\stdin_server.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,315 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\_mlflow_pyfunc_backend_predict.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,315 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,315 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,315 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,315 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,315 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,315 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,316 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,316 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,316 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,316 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,316 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,316 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,316 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,316 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,317 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,317 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,317 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,317 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,317 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,317 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,317 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,317 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,317 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,318 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,318 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,318 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,318 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,318 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,318 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,318 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,318 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,318 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,319 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,319 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,319 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,319 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,319 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,319 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,319 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,319 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,320 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,320 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,320 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,320 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,320 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,320 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,320 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,321 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,321 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,321 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,321 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,321 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,321 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,321 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,321 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,321 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,322 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,322 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,322 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,322 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,322 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,322 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,322 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,322 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,323 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,323 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,323 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,323 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,323 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,323 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,323 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,323 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,323 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,324 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,324 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,324 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,324 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,324 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,324 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,324 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,324 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,325 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,325 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,325 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,325 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,325 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,325 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,325 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,325 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,325 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,326 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,326 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,326 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,326 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,326 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,326 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,326 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,326 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,327 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,327 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,327 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,327 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,327 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,327 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,327 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,327 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,327 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,328 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,328 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,328 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,328 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,328 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,328 - WARNING - api.rag - rag.py:327 - Filtering out document 'mlflow\pyfunc\__init__.py' due to embedding size mismatch: 0 != 256
2025-09-05 20:09:11,337 - INFO - api.rag - rag.py:334 - Embedding validation complete: 21386/21886 documents have valid embeddings
2025-09-05 20:09:11,337 - WARNING - api.rag - rag.py:340 - Filtered out 500 documents due to embedding issues
2025-09-05 20:09:11,338 - INFO - api.rag - rag.py:379 - Using 21386 documents with valid embeddings for retrieval
2025-09-05 20:09:11,338 - INFO - adalflow.optim.grad_component - grad_component.py:79 - EvalFnToTextLoss: No backward engine provided. Creating one using model_client and model_kwargs.
2025-09-05 20:09:11,441 - INFO - adalflow.components.retriever.faiss_retriever - faiss_retriever.py:190 - Index built with 21386 chunks
2025-09-05 20:09:11,442 - INFO - api.rag - rag.py:390 - FAISS retriever created successfully
2025-09-05 20:09:11,442 - INFO - api.chat - chat.py:106 - Retriever prepared for ./projs/mlflow
2025-09-05 20:09:11,442 - INFO - api.openai_client - openai_client.py:415 - api_kwargs: {'model': 'text-embedding-3-small', 'dimensions': 256, 'encoding_format': 'float', 'input': ['Your task is to find all entries exposed to users in this web application.\n\nYour answer should only contain a list, the items of which is a dict, containing the following attributes: method, name, parameters, file_path.\n"parameters" is a str list, if there\'s no parameters, return an empty list: [].\n"name" is the url of this entry\n"file_path" is path of the file that contains definition of the entry\n\nIMPORTANT: Generate all the content in English.\n\nYou will be given content of related source files.\n\nRemember to ground every claim in the provided source files.']}
2025-09-05 20:09:12,563 - INFO - httpx - _client.py:1025 - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-09-05 20:09:12,575 - INFO - api.chat - chat.py:202 - Retrieved 20 documents
2025-09-05 20:09:12,575 - INFO - api.chat - chat.py:226 - LIU: Retrieved files: ['mlflow\\server\\js\\src\\experiment-tracking\\components\\evaluation-artifacts-compare\\utils\\PromptExamples.ts', 'mlflow\\server\\js\\src\\experiment-tracking\\components\\run-page\\artifact-fixtures\\json\\table_eval_data.json', 'tests\\entities\\test_trace_info.py', 'tests\\openai\\mock_openai.py', 'dev\\check_function_signatures.py', 'tests\\store\\artifact\\test_azure_data_lake_artifact_repo.py', 'tests\\gateway\\providers\\test_openai_uc_functions.py', 'tests\\crewai\\test_crewai_autolog.py', 'tests\\evaluate\\test_evaluation.py', 'tests\\tracing\\utils\\test_environment.py', 'tests\\models\\test_resources.py', 'tests\\gateway\\providers\\test_togetherai.py', 'dev\\clint\\src\\clint\\index.py', 'tests\\metrics\\genai\\test_genai_metrics.py', 'mlflow\\server\\js\\src\\experiment-tracking\\components\\run-page\\artifact-fixtures\\json\\table_eval_broken.json']
2025-09-05 20:09:12,576 - INFO - api.rag - rag.py:74 - Dialog turns list exists but is empty
2025-09-05 20:09:12,577 - INFO - api.rag - rag.py:88 - Returning 0 dialog turns from memory
2025-09-05 20:09:12,577 - INFO - api.chat - chat.py:480 - Using Openai protocol with model: gpt-4o
2025-09-05 20:09:13,592 - INFO - api.chat - chat.py:591 - Making Openai API call
2025-09-05 20:09:14,334 - ERROR - asyncio - base_events.py:1785 - Task exception was never retrieved
future: <Task finished name='Task-8' coro=<AsyncClient.aclose() done, defined at d:\Anaconda\anaconda\envs\pyvuldetect\Lib\site-packages\httpx\_client.py:1978> exception=RuntimeError('Event loop is closed')>
Traceback (most recent call last):
  File "d:\Anaconda\anaconda\envs\pyvuldetect\Lib\site-packages\httpx\_client.py", line 1988, in aclose
    await proxy.aclose()
  File "d:\Anaconda\anaconda\envs\pyvuldetect\Lib\site-packages\httpx\_transports\default.py", line 406, in aclose
    await self._pool.aclose()
  File "d:\Anaconda\anaconda\envs\pyvuldetect\Lib\site-packages\httpcore\_async\connection_pool.py", line 353, in aclose
    await self._close_connections(closing_connections)
  File "d:\Anaconda\anaconda\envs\pyvuldetect\Lib\site-packages\httpcore\_async\connection_pool.py", line 345, in _close_connections
    await connection.aclose()
  File "d:\Anaconda\anaconda\envs\pyvuldetect\Lib\site-packages\httpcore\_async\http_proxy.py", line 349, in aclose
    await self._connection.aclose()
  File "d:\Anaconda\anaconda\envs\pyvuldetect\Lib\site-packages\httpcore\_async\http11.py", line 258, in aclose
    await self._network_stream.aclose()
  File "d:\Anaconda\anaconda\envs\pyvuldetect\Lib\site-packages\httpcore\_backends\anyio.py", line 53, in aclose
    await self._stream.aclose()
  File "d:\Anaconda\anaconda\envs\pyvuldetect\Lib\site-packages\anyio\streams\tls.py", line 234, in aclose
    await self.transport_stream.aclose()
  File "d:\Anaconda\anaconda\envs\pyvuldetect\Lib\site-packages\anyio\_backends\_asyncio.py", line 1314, in aclose
    self._transport.close()
  File "d:\Anaconda\anaconda\envs\pyvuldetect\Lib\asyncio\proactor_events.py", line 109, in close
    self._loop.call_soon(self._call_connection_lost, None)
  File "d:\Anaconda\anaconda\envs\pyvuldetect\Lib\asyncio\base_events.py", line 762, in call_soon
    self._check_closed()
  File "d:\Anaconda\anaconda\envs\pyvuldetect\Lib\asyncio\base_events.py", line 520, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
2025-09-05 20:09:16,684 - INFO - httpx - _client.py:1740 - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
